Selected Files Directory Structure:

└── ./
    ├── demos
    │   ├── run_vizier_client.py
    │   └── run_vizier_server.py
    ├── docs
    │   └── conf.py
    ├── vizier
    │   ├── _src
    │   │   ├── algorithms
    │   │   │   ├── classification
    │   │   │   │   ├── classifiers_test.py
    │   │   │   │   └── classifiers.py
    │   │   │   ├── core
    │   │   │   │   ├── __init__.py
    │   │   │   │   └── abstractions.py
    │   │   │   ├── designers
    │   │   │   │   ├── eagle_strategy
    │   │   │   │   │   ├── eagle_designer_convergence_test.py
    │   │   │   │   │   ├── eagle_simplekd_test.py
    │   │   │   │   │   ├── eagle_strategy_test.py
    │   │   │   │   │   ├── eagle_strategy_utils_test.py
    │   │   │   │   │   ├── eagle_strategy_utils.py
    │   │   │   │   │   ├── eagle_strategy.py
    │   │   │   │   │   ├── serialization_test.py
    │   │   │   │   │   ├── serialization.py
    │   │   │   │   │   └── testing.py
    │   │   │   │   ├── gp
    │   │   │   │   │   ├── acquisitions_test.py
    │   │   │   │   │   ├── acquisitions.py
    │   │   │   │   │   ├── gp_models_test.py
    │   │   │   │   │   ├── gp_models.py
    │   │   │   │   │   ├── output_warpers_test.py
    │   │   │   │   │   ├── output_warpers.py
    │   │   │   │   │   ├── transfer_learning_test.py
    │   │   │   │   │   ├── transfer_learning.py
    │   │   │   │   │   └── yjt.py
    │   │   │   │   ├── meta_learning
    │   │   │   │   │   ├── eagle_meta_learning_convergence_test.py
    │   │   │   │   │   ├── eagle_meta_learning.py
    │   │   │   │   │   ├── meta_learning_test.py
    │   │   │   │   │   ├── meta_learning_utils_test.py
    │   │   │   │   │   ├── meta_learning_utils.py
    │   │   │   │   │   └── meta_learning.py
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── bocs_test.py
    │   │   │   │   ├── bocs.py
    │   │   │   │   ├── cmaes_test.py
    │   │   │   │   ├── cmaes.py
    │   │   │   │   ├── gp_bandit_test.py
    │   │   │   │   ├── gp_bandit.py
    │   │   │   │   ├── gp_ucb_pe_test.py
    │   │   │   │   ├── gp_ucb_pe.py
    │   │   │   │   ├── grid_test.py
    │   │   │   │   ├── grid.py
    │   │   │   │   ├── harmonica_test.py
    │   │   │   │   ├── harmonica.py
    │   │   │   │   ├── pycmaes_test.py
    │   │   │   │   ├── pycmaes.py
    │   │   │   │   ├── quasi_random_test.py
    │   │   │   │   ├── quasi_random.py
    │   │   │   │   ├── random_test.py
    │   │   │   │   ├── random.py
    │   │   │   │   ├── scalarization_test.py
    │   │   │   │   ├── scalarization.py
    │   │   │   │   ├── scalarizing_designer_test.py
    │   │   │   │   ├── scalarizing_designer.py
    │   │   │   │   ├── scheduled_designer_test.py
    │   │   │   │   ├── scheduled_designer.py
    │   │   │   │   ├── scheduled_gp_bandit.py
    │   │   │   │   ├── scheduled_gp_ucb_pe.py
    │   │   │   │   ├── unsafe_as_infeasible_designer_test.py
    │   │   │   │   └── unsafe_as_infeasible_designer.py
    │   │   │   ├── ensemble
    │   │   │   │   ├── ensemble_design_test.py
    │   │   │   │   ├── ensemble_design.py
    │   │   │   │   ├── ensemble_designer_test.py
    │   │   │   │   └── ensemble_designer.py
    │   │   │   ├── evolution
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── nsga2_test.py
    │   │   │   │   ├── nsga2.py
    │   │   │   │   ├── numpy_populations_test.py
    │   │   │   │   ├── numpy_populations.py
    │   │   │   │   └── templates.py
    │   │   │   ├── optimizers
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── base.py
    │   │   │   │   ├── designer_optimizer_test.py
    │   │   │   │   ├── designer_optimizer.py
    │   │   │   │   ├── eagle_optimizer_convergence_test.py
    │   │   │   │   ├── eagle_strategy_test.py
    │   │   │   │   ├── eagle_strategy.py
    │   │   │   │   ├── lbfgsb_optimizer_test.py
    │   │   │   │   ├── lbfgsb_optimizer.py
    │   │   │   │   ├── random_vectorized_optimizer_test.py
    │   │   │   │   ├── random_vectorized_optimizer.py
    │   │   │   │   ├── vectorized_base_test.py
    │   │   │   │   └── vectorized_base.py
    │   │   │   ├── policies
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── designer_policy_test.py
    │   │   │   │   ├── designer_policy.py
    │   │   │   │   ├── random_policy_test.py
    │   │   │   │   ├── random_policy.py
    │   │   │   │   ├── trial_caches_test.py
    │   │   │   │   └── trial_caches.py
    │   │   │   ├── random
    │   │   │   │   ├── random_sample_test.py
    │   │   │   │   └── random_sample.py
    │   │   │   ├── regression
    │   │   │   │   ├── trial_regression_utils_test.py
    │   │   │   │   └── trial_regression_utils.py
    │   │   │   ├── testing
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── comparator_runner_test.py
    │   │   │   │   ├── comparator_runner.py
    │   │   │   │   ├── failing_test.py
    │   │   │   │   ├── failing.py
    │   │   │   │   ├── optimizer_test_utils.py
    │   │   │   │   ├── simplekd_runner.py
    │   │   │   │   └── test_runners.py
    │   │   │   └── __init__.py
    │   │   ├── benchmarks
    │   │   │   ├── analyzers
    │   │   │   │   ├── convergence_curve_test.py
    │   │   │   │   ├── convergence_curve.py
    │   │   │   │   ├── exploration_score_utils_test.py
    │   │   │   │   ├── exploration_score_utils.py
    │   │   │   │   ├── plot_utils_test.py
    │   │   │   │   ├── plot_utils.py
    │   │   │   │   ├── simple_regret_score_test.py
    │   │   │   │   ├── simple_regret_score.py
    │   │   │   │   ├── state_analyzer_test.py
    │   │   │   │   └── state_analyzer.py
    │   │   │   ├── experimenters
    │   │   │   │   ├── combo
    │   │   │   │   │   └── common.py
    │   │   │   │   ├── hpob
    │   │   │   │   │   └── handler.py
    │   │   │   │   ├── synthetic
    │   │   │   │   │   ├── bbob_test.py
    │   │   │   │   │   ├── bbob.py
    │   │   │   │   │   ├── branin_test.py
    │   │   │   │   │   ├── branin.py
    │   │   │   │   │   ├── deb_test.py
    │   │   │   │   │   ├── deb.py
    │   │   │   │   │   ├── hartmann_test.py
    │   │   │   │   │   ├── hartmann.py
    │   │   │   │   │   ├── multiarm.py
    │   │   │   │   │   ├── multiobjective_optproblems_test.py
    │   │   │   │   │   ├── multiobjective_optproblems.py
    │   │   │   │   │   ├── simplekd_test.py
    │   │   │   │   │   └── simplekd.py
    │   │   │   │   ├── atari100k_experimenter_test.py
    │   │   │   │   ├── atari100k_experimenter.py
    │   │   │   │   ├── combo_experimenter_test.py
    │   │   │   │   ├── combo_experimenter.py
    │   │   │   │   ├── discretizing_experimenter_test.py
    │   │   │   │   ├── discretizing_experimenter.py
    │   │   │   │   ├── experimenter_factory_test.py
    │   │   │   │   ├── experimenter_factory.py
    │   │   │   │   ├── experimenter.py
    │   │   │   │   ├── infeasible_experimenter_test.py
    │   │   │   │   ├── infeasible_experimenter.py
    │   │   │   │   ├── l1_categorical_experimenter_test.py
    │   │   │   │   ├── l1_categorical_experimenter.py
    │   │   │   │   ├── multiobjective_experimenter_test.py
    │   │   │   │   ├── multiobjective_experimenter.py
    │   │   │   │   ├── nasbench101_experimenter_test.py
    │   │   │   │   ├── nasbench101_experimenter.py
    │   │   │   │   ├── nasbench201_experimenter_test.py
    │   │   │   │   ├── nasbench201_experimenter.py
    │   │   │   │   ├── noisy_experimenter_test.py
    │   │   │   │   ├── noisy_experimenter.py
    │   │   │   │   ├── normalizing_experimenter_test.py
    │   │   │   │   ├── normalizing_experimenter.py
    │   │   │   │   ├── numpy_experimenter_test.py
    │   │   │   │   ├── numpy_experimenter.py
    │   │   │   │   ├── permuting_experimenter_test.py
    │   │   │   │   ├── permuting_experimenter.py
    │   │   │   │   ├── shifting_experimenter_test.py
    │   │   │   │   ├── shifting_experimenter.py
    │   │   │   │   ├── sign_flip_experimenter_test.py
    │   │   │   │   ├── sign_flip_experimenter.py
    │   │   │   │   ├── sparse_experimenter_test.py
    │   │   │   │   ├── sparse_experimenter.py
    │   │   │   │   ├── surrogate_experimenter_test.py
    │   │   │   │   ├── surrogate_experimenter.py
    │   │   │   │   ├── switch_experimenter_test.py
    │   │   │   │   └── switch_experimenter.py
    │   │   │   ├── runners
    │   │   │   │   ├── benchmark_runner_test.py
    │   │   │   │   ├── benchmark_runner.py
    │   │   │   │   ├── benchmark_state_test.py
    │   │   │   │   └── benchmark_state.py
    │   │   │   └── testing
    │   │   │       └── experimenter_testing.py
    │   │   ├── jax
    │   │   │   ├── models
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── continuous_only_kernel.py
    │   │   │   │   ├── gaussian_process_model_test.py
    │   │   │   │   ├── gaussian_process_model.py
    │   │   │   │   ├── hebo_gp_model_test.py
    │   │   │   │   ├── hebo_gp_model.py
    │   │   │   │   ├── mask_features_test.py
    │   │   │   │   ├── mask_features.py
    │   │   │   │   ├── multitask_tuned_gp_models_test.py
    │   │   │   │   ├── multitask_tuned_gp_models.py
    │   │   │   │   ├── tuned_gp_models_test.py
    │   │   │   │   └── tuned_gp_models.py
    │   │   │   ├── optimizers
    │   │   │   │   ├── testing
    │   │   │   │   │   └── sinusoidal.py
    │   │   │   │   ├── core_test.py
    │   │   │   │   ├── core.py
    │   │   │   │   ├── jaxopt_wrappers_test.py
    │   │   │   │   ├── jaxopt_wrappers.py
    │   │   │   │   ├── optax_wrappers_test.py
    │   │   │   │   └── optax_wrappers.py
    │   │   │   ├── gp_bandit_utils.py
    │   │   │   ├── stochastic_process_model_test.py
    │   │   │   ├── stochastic_process_model.py
    │   │   │   ├── types_test.py
    │   │   │   ├── types.py
    │   │   │   ├── xla_pareto_test.py
    │   │   │   └── xla_pareto.py
    │   │   ├── pyglove
    │   │   │   ├── algorithms.py
    │   │   │   ├── backend.py
    │   │   │   ├── client.py
    │   │   │   ├── constants.py
    │   │   │   ├── converters_test.py
    │   │   │   ├── converters.py
    │   │   │   ├── core.py
    │   │   │   ├── e2e_test.py
    │   │   │   ├── oss_vizier_test.py
    │   │   │   ├── oss_vizier.py
    │   │   │   ├── performance_test.py
    │   │   │   ├── pythia_test.py
    │   │   │   ├── pythia.py
    │   │   │   └── vizier_test_lib.py
    │   │   ├── pythia
    │   │   │   ├── local_policy_supporters_test.py
    │   │   │   ├── local_policy_supporters.py
    │   │   │   ├── policy_factory.py
    │   │   │   ├── policy_supporter.py
    │   │   │   ├── policy.py
    │   │   │   ├── pythia_errors.py
    │   │   │   ├── singleton_params_test.py
    │   │   │   ├── singleton_params.py
    │   │   │   ├── suggest_default_test.py
    │   │   │   └── suggest_default.py
    │   │   ├── pyvizier
    │   │   │   ├── multimetric
    │   │   │   │   ├── hypervolume_test.py
    │   │   │   │   ├── hypervolume.py
    │   │   │   │   ├── pareto_optimal_test.py
    │   │   │   │   ├── pareto_optimal.py
    │   │   │   │   ├── safety_test.py
    │   │   │   │   └── safety.py
    │   │   │   ├── oss
    │   │   │   │   ├── automated_stopping_test.py
    │   │   │   │   ├── automated_stopping.py
    │   │   │   │   ├── compare.py
    │   │   │   │   ├── metadata_util_test.py
    │   │   │   │   ├── metadata_util.py
    │   │   │   │   ├── proto_converters_test.py
    │   │   │   │   ├── proto_converters.py
    │   │   │   │   ├── study_config_test.py
    │   │   │   │   └── study_config.py
    │   │   │   ├── pythia
    │   │   │   │   └── study.py
    │   │   │   └── shared
    │   │   │       ├── base_study_config_test.py
    │   │   │       ├── base_study_config.py
    │   │   │       ├── common_test.py
    │   │   │       ├── common.py
    │   │   │       ├── context_test.py
    │   │   │       ├── context.py
    │   │   │       ├── parameter_config_test.py
    │   │   │       ├── parameter_config.py
    │   │   │       ├── parameter_iterators_test.py
    │   │   │       ├── parameter_iterators.py
    │   │   │       ├── study.py
    │   │   │       ├── trial_test.py
    │   │   │       └── trial.py
    │   │   ├── raytune
    │   │   │   ├── converters_test.py
    │   │   │   ├── converters.py
    │   │   │   ├── run_tune_test.py
    │   │   │   ├── run_tune.py
    │   │   │   ├── vizier_search_test.py
    │   │   │   └── vizier_search.py
    │   │   ├── service
    │   │   │   ├── testing
    │   │   │   │   └── util.py
    │   │   │   ├── clients_test.py
    │   │   │   ├── clients.py
    │   │   │   ├── constants.py
    │   │   │   ├── custom_errors.py
    │   │   │   ├── datastore_test_lib.py
    │   │   │   ├── datastore.py
    │   │   │   ├── grpc_util.py
    │   │   │   ├── performance_test.py
    │   │   │   ├── policy_factory.py
    │   │   │   ├── pythia_service.py
    │   │   │   ├── pythia_util.py
    │   │   │   ├── ram_datastore_test.py
    │   │   │   ├── ram_datastore.py
    │   │   │   ├── resources_test.py
    │   │   │   ├── resources.py
    │   │   │   ├── service_policy_supporter_test.py
    │   │   │   ├── service_policy_supporter.py
    │   │   │   ├── sql_datastore_test.py
    │   │   │   ├── sql_datastore.py
    │   │   │   ├── stubs_util_test.py
    │   │   │   ├── stubs_util.py
    │   │   │   ├── types.py
    │   │   │   ├── vizier_client_test.py
    │   │   │   ├── vizier_client.py
    │   │   │   ├── vizier_server.py
    │   │   │   ├── vizier_service_test.py
    │   │   │   └── vizier_service.py
    │   │   └── __init__.py
    │   ├── algorithms
    │   │   ├── designers
    │   │   │   └── __init__.py
    │   │   ├── policies
    │   │   │   └── __init__.py
    │   │   ├── __init__.py
    │   │   └── evolution.py
    │   ├── benchmarks
    │   │   ├── experimenters
    │   │   │   ├── hpo
    │   │   │   │   └── __init__.py
    │   │   │   ├── multiobjective_optproblems
    │   │   │   │   └── __init__.py
    │   │   │   ├── nas
    │   │   │   │   └── __init__.py
    │   │   │   ├── rl
    │   │   │   │   └── __init__.py
    │   │   │   └── __init__.py
    │   │   ├── __init__.py
    │   │   └── analyzers.py
    │   ├── client
    │   │   ├── client_abc_testing.py
    │   │   └── client_abc.py
    │   ├── interfaces
    │   │   └── serializable.py
    │   ├── jax
    │   │   ├── __init__.py
    │   │   ├── models.py
    │   │   └── optimizers.py
    │   ├── pyglove
    │   │   └── __init__.py
    │   ├── pyvizier
    │   │   ├── converters
    │   │   │   ├── __init__.py
    │   │   │   ├── core_test.py
    │   │   │   ├── core.py
    │   │   │   ├── embedder_test.py
    │   │   │   ├── embedder.py
    │   │   │   ├── feature_mapper_test.py
    │   │   │   ├── feature_mapper.py
    │   │   │   ├── input_warping_test.py
    │   │   │   ├── input_warping.py
    │   │   │   ├── jnp_converters_test.py
    │   │   │   ├── jnp_converters.py
    │   │   │   ├── padding_test.py
    │   │   │   ├── padding.py
    │   │   │   ├── spatio_temporal_test.py
    │   │   │   └── spatio_temporal.py
    │   │   ├── multimetric
    │   │   │   ├── __init__.py
    │   │   │   └── xla_pareto.py
    │   │   └── __init__.py
    │   ├── raytune
    │   │   └── __init__.py
    │   ├── service
    │   │   ├── clients
    │   │   │   └── __init__.py
    │   │   ├── protos
    │   │   │   └── __init__.py
    │   │   ├── pyvizier
    │   │   │   └── __init__.py
    │   │   ├── servers
    │   │   │   └── __init__.py
    │   │   └── __init__.py
    │   ├── testing
    │   │   ├── __init__.py
    │   │   ├── numpy_assertions.py
    │   │   └── test_studies.py
    │   ├── utils
    │   │   ├── attrs_utils_test.py
    │   │   ├── attrs_utils.py
    │   │   ├── json_utils_test.py
    │   │   ├── json_utils.py
    │   │   ├── profiler_test.py
    │   │   └── profiler.py
    │   ├── __init__.py
    │   └── pythia.py
    └── setup.py



--- demos/run_vizier_client.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Example of a Vizier Client, which can be run on multiple machines.

For distributed cases, this is meant to be used after the Vizier Server (see
run_vizier_server.py`) has been launched and provided an address to connect to.
Example of a launch command:

```
python run_vizier_client.py --address="localhost:[PORT]"
```

where `address` was provided by the server.

If not provided, the Vizier Server will be created locally, which still allows
parallelization via multithreading, but will not be able to coordinate jobs
across different machines.
"""

from typing import Sequence

from absl import app
from absl import flags
from absl import logging
from vizier import service
from vizier.service import clients
from vizier.service import pyvizier as vz

flags.DEFINE_string(
    'address',
    service.NO_ENDPOINT,
    (
        'Address of the Vizier Server which will be used by this demo. Should'
        " be of the form e.g. 'localhost:6006' if running on the same machine,"
        ' or `[IP]:[PORT]` if running on a remote machine. If unset, a local'
        ' Vizier server will be created inside this process.'
    ),
)
flags.DEFINE_integer(
    'max_num_iterations',
    10,
    'Maximum number of possible iterations / calls to get suggestions.',
)
flags.DEFINE_integer(
    'suggestion_count',
    1,
    (
        'Number of suggestions to evaluate per iteration. Useful for batched'
        ' evaluations.'
    ),
)
flags.DEFINE_boolean(
    'multiobjective',
    True,
    (
        'Whether to demonstrate multiobjective or single-objective capabilities'
        ' and API.'
    ),
)
flags.DEFINE_string(
    'client_id',
    clients.UNUSED_CLIENT_ID,
    'The client id to use for the study. NOTE: For distributed cases, this'
    ' needs to be unique for every client.',
)

FLAGS = flags.FLAGS


def evaluate_trial(trial: vz.Trial) -> vz.Measurement:
  """Dummy evaluator used as an example."""
  learning_rate = trial.parameters.get_value('learning_rate')
  num_layers = trial.parameters.get_value('num_layers')
  m = vz.Measurement()
  m.metrics = {'accuracy': learning_rate * num_layers}  # dummy accuracy
  if FLAGS.multiobjective:
    m.metrics['latency'] = 0.5 * num_layers
  return m


def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')

  if FLAGS.address == service.NO_ENDPOINT:
    logging.info(
        'You did not specify the server address. The Vizier Service will be'
        ' created locally.'
    )
  else:
    # Set address.
    clients.environment_variables.server_endpoint = FLAGS.address

  study_config = vz.StudyConfig()  # Search space, metrics, and algorithm.
  root = study_config.search_space.root
  root.add_float_param(
      'learning_rate',
      min_value=1e-4,
      max_value=1e-2,
      scale_type=vz.ScaleType.LOG,
  )
  root.add_int_param('num_layers', min_value=1, max_value=5)
  study_config.metric_information.append(
      vz.MetricInformation(
          name='accuracy',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          min_value=0.0,
          max_value=1.0,
      )
  )

  if FLAGS.multiobjective:
    # No need to specify min/max values.
    study_config.metric_information.append(
        vz.MetricInformation(
            name='latency', goal=vz.ObjectiveMetricGoal.MINIMIZE
        )
    )

  if FLAGS.multiobjective:
    study_config.algorithm = vz.Algorithm.NSGA2
  else:
    study_config.algorithm = vz.Algorithm.GAUSSIAN_PROCESS_BANDIT

  study = clients.Study.from_study_config(
      study_config, owner='my_name', study_id='cifar10'
  )
  logging.info('Client created with study name: %s', study.resource_name)

  # Evaluate the suggestion(s) and report the results to Vizier.
  for _ in range(FLAGS.max_num_iterations):
    trials = study.suggest(
        count=FLAGS.suggestion_count, client_id=FLAGS.client_id
    )
    for trial in trials:
      materialized_trial = trial.materialize()
      measurement = evaluate_trial(materialized_trial)
      trial.complete(measurement)
      logging.info(
          'Trial %d completed with metrics: %s', trial.id, measurement.metrics
      )

  # Mark study as completed after finishing tuning.
  study.set_state(vz.StudyState.COMPLETED)

  # Obtain optimal trials found.
  optimal_trials = study.optimal_trials()
  for optimal_trial in optimal_trials:
    optimal_trial = optimal_trial.materialize(include_all_measurements=True)
    assert optimal_trial.final_measurement is not None
    logging.info(
        'Pareto-optimal trial found so far has parameters %s and metrics %s',
        optimal_trial.parameters,
        optimal_trial.final_measurement.metrics,
    )


if __name__ == '__main__':
  app.run(main)


--- demos/run_vizier_server.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Sets up the Vizier Service gRPC server.

This should be done on a server machine:

```
python run_vizier_server.py
```

After running the command, the address of the server, formatted as:
"localhost:[PORT]" will be logged to stdout.
This address should be used as a command line argument to run_vizier_client.py
"""

import time
from typing import Sequence

from absl import app
from absl import flags
from absl import logging

from vizier import service
from vizier.service import servers

flags.DEFINE_string(
    'host',
    'localhost',
    'Host location for the server. For distributed cases, use the IP address.',
)

flags.DEFINE_string(
    'database_url',
    service.SQL_LOCAL_URL,
    'Location of the database for saving studies.',
)

FLAGS = flags.FLAGS

_ONE_DAY_IN_SECONDS = 60 * 60 * 24


def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')

  server = servers.DefaultVizierServer(
      host=FLAGS.host, database_url=FLAGS.database_url
  )
  logging.info('Address to Vizier Server is: %s', server.endpoint)

  # prevent the main thread from exiting
  try:
    while True:
      time.sleep(_ONE_DAY_IN_SECONDS)
  except KeyboardInterrupt:
    del server


if __name__ == '__main__':
  app.run(main)


--- docs/conf.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Sphinx configuration."""
import os
import sys

# Tells Sphinx where the code is (for automatic Pydoc scraping)
sys.path.insert(0, os.path.abspath('..'))

# -- Project information -----------------------------------------------------

project = 'Open Source Vizier'
copyright = 'Copyright 2022, The OSS Vizier Authors'  # pylint: disable=redefined-builtin
author = 'The OSS Vizier Authors'

# The short X.Y version
version = ''

# The full version, including alpha/beta/rc tags
release = ''

# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones. Custom ones are installed via `requirements.txt`.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.autosectionlabel',
    'sphinx.ext.autosummary',
    'sphinx.ext.doctest',
    'sphinx.ext.intersphinx',
    'sphinx.ext.mathjax',
    'sphinx.ext.napoleon',
    'sphinx.ext.viewcode',
    'sphinx.ext.graphviz',
    'sphinx_autodoc_typehints',
    'autodocsumm',
    # 'myst_parser', TODO: 'myst_commonmark_only' flag conflict
    'sphinx_rtd_theme',
    'myst_nb',
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

intersphinx_mapping = {
    'python': ('https://docs.python.org/3/', None),
}

suppress_warnings = [
    # 'ref.citation',  # Many duplicated citations in numpy/scipy docstrings.
    # 'ref.footnote',  # Many unreferenced footnotes in numpy/scipy docstrings
]

# Add any paths that contain templates here, relative to this directory.
# templates_path = ['_templates']

# The suffix(es) of source filenames.
# NOTE: Ordering matters, as myst will choose which to convert based on
# the order in the source_suffix list.
source_suffix = ['.rst', '.ipynb', '.md']

# The main toctree document.
main_doc = 'index'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = []

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = None

html_show_sourcelink = True

autosummary_generate = True

napolean_use_rtype = False

# -- Options for HTML output -------------------------------------------------

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = './assets/vizier_logo2.png'
html_favicon = './assets/vizier_logo2.png'

# -- Options for myst --------------------------------------------------------

# Enable LaTeX for HTML-generated docs.
myst_enable_extensions = ['dollarmath']

# -- Extension configuration -------------------------------------------------

# Tell sphinx-autodoc-typehints to generate stub parameter annotations including
# types, even if the parameters aren't explicitly documented.
# always_document_param_types = True

add_module_names = False

jupyter_execute_notebooks = 'off'

html_theme = 'sphinx_rtd_theme'


--- setup.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Setup for pip package."""

import datetime
import itertools
import os
import sys
from setuptools import find_namespace_packages
from setuptools import setup
from setuptools.command.build import build


def _get_version():
  with open('vizier/__init__.py') as fp:
    for line in fp:
      if line.startswith('__version__'):
        g = {}
        exec(line, g)  # pylint: disable=exec-used
        return g['__version__']
    raise ValueError('`__version__` not defined in `vizier/__init__.py`')


def _strip_comments_from_line(s: str) -> str:
  """Parses a line of a requirements.txt file."""
  requirement, *_ = s.split('#')
  return requirement.strip()


def _parse_requirements(requirements_txt_path: str) -> list[str]:
  """Returns a list of dependencies for setup() from requirements.txt."""

  # Currently a requirements.txt is being used to specify dependencies. In order
  # to avoid specifying it in two places, we're going to use that file as the
  # source of truth.
  with open(requirements_txt_path) as fp:
    # Parse comments.
    lines = [_strip_comments_from_line(line) for line in fp.read().splitlines()]
    # Remove empty lines and direct github repos (not allowed in PyPI setups)
    return [l for l in lines if (l and 'github.com' not in l)]


class BuildCmd(build):
  """Custom installation script to build the protos."""

  def run(self):
    current_path = os.path.dirname(os.path.realpath(__file__))
    sys.stdout.write('current_path: {}'.format(current_path))
    with os.scandir('.') as it:
      for entry in it:
        if entry.name.startswith('build_protos.sh'):
          sys.stdout.write('{}'.format(entry))
    if os.system('bash build_protos.sh'):
      raise OSError('Failed to run build_protos.sh')
    build.run(self)


_VERSION = _get_version()
_NAME = 'google-vizier'

if '--dev' in sys.argv:
  sys.argv.remove('--dev')
  _VERSION += '.dev' + datetime.datetime.now().strftime('%Y%m%d%H%M%S')
  _NAME += '-dev'

extras_require = {
    'jax': _parse_requirements('requirements-jax.txt'),
    'tf': _parse_requirements('requirements-tf.txt'),
    'algorithms': _parse_requirements('requirements-algorithms.txt'),
    'benchmarks': _parse_requirements('requirements-benchmarks.txt'),
    'test': _parse_requirements('requirements-test.txt'),
}

extras_require['all'] = list(
    itertools.chain.from_iterable(extras_require.values())
)

setup(
    name=_NAME,
    version=_VERSION,
    url='https://github.com/google/vizier',
    license='Apache License 2.0',
    author='Vizier Team',
    description=(
        'Open Source Vizier: Distributed service framework for blackbox'
        ' optimization and research.'
    ),
    long_description=open('README.md').read(),
    long_description_content_type='text/markdown',
    author_email='oss-vizier-dev@google.com',
    # Contained modules and scripts.
    packages=find_namespace_packages(
        include=['vizier*'], exclude=['*_test.py', 'examples']
    ),
    install_requires=_parse_requirements('requirements.txt'),
    extras_require=extras_require,
    python_requires='>=3.8',
    include_package_data=True,
    zip_safe=False,
    cmdclass={'build': BuildCmd},
    # PyPI package information.
    classifiers=[
        'Development Status :: 3 - Alpha',
        'Intended Audience :: Developers',
        'Intended Audience :: Education',
        'Intended Audience :: Science/Research',
        'License :: OSI Approved :: Apache Software License',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.9',
        'Topic :: Scientific/Engineering :: Mathematics',
        'Topic :: Software Development :: Libraries :: Python Modules',
        'Topic :: Software Development :: Libraries',
    ],
    keywords=(
        'ai machine learning hyperparameter blackbox optimization framework'
    ),
)


--- vizier/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Init file."""
import os
import sys

THIS_DIR = os.path.dirname(os.path.realpath(__file__))
PROTO_ROOT = os.path.realpath(os.path.join(THIS_DIR, "_src", "service"))

sys.path.append(PROTO_ROOT)

__version__ = "0.1.24"


--- vizier/_src/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/classification/classifiers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Binary classifiers for Bayesian Optimization."""

from typing import Optional

import attr
import numpy as np
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel
from sklearn.gaussian_process.kernels import RBF

# TODO: Replace the sklearn GP classifier with TFP GP classifier
# once implemented.


@attr.define
class SklearnClassifier:
  """Class for Sklearn classifiers.

  Attributes:
    classifier: a sklearn classifier such as SVM and GaussianProcessClassifier.
    features: (n, d) shaped array of n samples in dimension d.
    labels: (n, 1) shaped array of binary labels in {0, 1}.
    features_test: (m, d) shaped array of m samples in dimension d.
    eval_metric: a string denoting the evaluation metric. Accepted options are
      `probability` which estimates the probability of belonging to each class
      and `decision` which estimates a non-probability based metric such as the
      margin from the classification boundary.
  """
  classifier: Optional[GaussianProcessClassifier] = attr.field(
      kw_only=True,
      default=GaussianProcessClassifier(
          kernel=ConstantKernel(1.) * RBF(length_scale=1.)))
  features: np.ndarray = attr.field(kw_only=True)
  labels: np.ndarray = attr.field(kw_only=True)
  features_test: np.ndarray = attr.field(kw_only=True)
  eval_metric: str = attr.field(kw_only=True, default='probability')

  def _check_features_and_labels_shapes(self) -> None:
    """Checks the compatibility between features and labels shapes."""
    if np.ndim(self.features) != 2:
      raise ValueError(f'{self} expects 2d features.')
    if self.labels.shape[0] != self.features.shape[0]:
      raise ValueError(f'There are `{self.features.shape[0]}` features and '
                       f'`{self.labels.shape[0]}` labels which is incompatible')
    if np.ndim(self.labels) != 1 and self.labels.shape[1] != 1:
      raise ValueError(
          f'{self} expects 1d labels or labels of shape (num_samples, 1), but'
          f'was given labels of shape `{self.labels.shape}` .')
    if self.features_test.shape[1] != self.features.shape[1]:
      raise ValueError(
          f'{self} features_test to have `{self.features.shape[1]}`,'
          f'but it has `{self.features_test.shape[1]}` features.')

  def _check_labels_values(self) -> None:
    if not set(self.labels).issubset({0, 1}):
      raise ValueError('Labels should be either zero or one.')
    if set(self.labels).issubset({0}) or set(self.labels).issubset({1}):
      raise ValueError(f'{self} expects at least one sample per class, but all'
                       'training labels contain the same class.')

  def _check_eval_metric(self) -> None:
    if self.eval_metric not in ['probability', 'decision']:
      raise ValueError(f'{self} expects the evaluation metric to be'
                       f'`probability` or `decision ` but `{self.eval_metric}`'
                       'was given.')

  # TODO: separate the training and evaluation for extra speed up.
  # Currently, the classifiers we use are reasonably fast.
  def __call__(self) -> np.ndarray:
    self._check_features_and_labels_shapes()
    self._check_labels_values()
    self._check_eval_metric()
    if self.classifier is None:
      raise RuntimeError('Classifier is None.')
    self.classifier.fit(np.asarray(self.features), np.asarray(self.labels))
    if self.eval_metric == 'probability':
      return self.classifier.predict_proba(np.asarray(self.features_test))[:, 1]
    else:
      return self.classifier.decision_function(np.asarray(self.features_test))  # pytype:disable=attribute-error


--- vizier/_src/algorithms/classification/classifiers_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for Sklearn classifier."""

import numpy as np
from sklearn import svm
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel
from sklearn.gaussian_process.kernels import RBF
from sklearn.metrics.pairwise import euclidean_distances
from vizier._src.algorithms.classification import classifiers

from absl.testing import absltest
from absl.testing import parameterized

svm_classifier = svm.SVC(kernel='rbf', C=1.)
gpc_classifier = GaussianProcessClassifier(
    kernel=ConstantKernel(1.) * RBF(length_scale=1.))


# TODO: convert it into a generic ClassifierTest for all subclasses.
class SklearnClassifierTest(parameterized.TestCase):
  """Class for testing the classifier."""

  def setUp(self):
    super().setUp()
    self.features_train = np.array([[1.], [2.], [3.], [4.]])
    self.labels_train = np.array([0, 0, 1, 1])
    self.labels_train_invalid = np.array([10., 10., 11., 11.])
    self.features_test = np.array([[-1.], [6.], [0.], [6.]])

  def test_raise_error_invalid_labels(self):
    classifier_instance = classifiers.SklearnClassifier(
        classifier=gpc_classifier,
        features=self.features_train,
        labels=self.labels_train_invalid,
        features_test=self.features_test,
        eval_metric='probability')
    with self.assertRaises(ValueError):
      classifier_instance._check_labels_values()

  @parameterized.parameters([
      dict(classifier=svm_classifier, eval_metric='decision'),
      dict(classifier=gpc_classifier, eval_metric='probability')
  ])
  def test_scores_shape(self, classifier, eval_metric):
    classifier_instance = classifiers.SklearnClassifier(
        classifier=classifier,
        features=self.features_train,
        labels=self.labels_train,
        features_test=self.features_test,
        eval_metric=eval_metric)
    scores = classifier_instance()
    self.assertEqual(scores.shape[0], self.features_test.shape[0])

  @parameterized.parameters([
      dict(classifier=svm_classifier, eval_metric='decision', threshold=0.),
      dict(classifier=gpc_classifier, eval_metric='probability', threshold=0.5)
  ])
  def test_labels_shape(self, classifier, eval_metric, threshold):
    classifier_instance = classifiers.SklearnClassifier(
        classifier=classifier,
        features=self.features_train,
        labels=self.labels_train,
        features_test=self.features_test,
        eval_metric=eval_metric)
    scores = classifier_instance()
    labels_test_pred = (scores >= threshold).astype(float)
    labels_test_real = np.array([0, 1, 0, 1])
    self.assertTrue((labels_test_pred == labels_test_real).all())

  @parameterized.parameters([dict(label_val=0.), dict(label_val=1.)])
  def test_raise_error_identical_labels(self, label_val):
    labels_train_identical = np.ones(
        (self.features_train.shape[0],)) * label_val
    classifier_instance = classifiers.SklearnClassifier(
        classifier=gpc_classifier,
        features=self.features_train,
        labels=labels_train_identical,
        features_test=self.features_test,
        eval_metric='probability')
    with self.assertRaises(ValueError):
      classifier_instance._check_labels_values()

  @parameterized.parameters([
      dict(classifier=svm_classifier, eval_metric='decision'),
      dict(classifier=gpc_classifier, eval_metric='probability')
  ])
  def test_scores_range(self, classifier, eval_metric):
    classifier_instance = classifiers.SklearnClassifier(
        classifier=classifier,
        features=self.features_train,
        labels=self.labels_train,
        features_test=self.features_test,
        eval_metric=eval_metric)
    scores = classifier_instance()
    if eval_metric == 'probability':
      self.assertGreaterEqual(scores.min(), 0.)
      self.assertLessEqual(scores.max(), 1.)
    else:
      max_distance = max(
          euclidean_distances(self.features_test).max(),
          euclidean_distances(self.features_train).max(),
          euclidean_distances(self.features_train, self.features_test).max())
      self.assertLessEqual(scores.max(), max_distance)

  @parameterized.parameters([
      dict(classifier=svm_classifier, eval_metric='decision', threshold=0.),
      dict(classifier=gpc_classifier, eval_metric='probability', threshold=0.5)
  ])
  def test_prediction_on_train_data(self, classifier, eval_metric, threshold):
    classifier_instance = classifiers.SklearnClassifier(
        classifier=classifier,
        features=self.features_train,
        labels=self.labels_train,
        features_test=self.features_train,
        eval_metric=eval_metric)
    scores = classifier_instance()
    labels_test_pred = (scores >= threshold).astype(float)
    self.assertTrue((labels_test_pred == self.labels_train).all())


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/core/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/core/abstractions.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Abstractions."""

import abc
from typing import Optional, Protocol, Sequence, TypeVar

import attr
import chex
import jax
from vizier import pyvizier as vz
from vizier.interfaces import serializable

_T = TypeVar('_T')


@attr.define(frozen=True)
class CompletedTrials:
  """A group of completed trials.

  Attributes:
    trials: Completed Trials.
  """

  def __attrs_post_init__(self):
    for trial in self.trials:
      if trial.status != vz.TrialStatus.COMPLETED:
        raise ValueError(f'All trials must be completed. Bad trial:\n{trial}')

  trials: Sequence[vz.Trial] = attr.field(
      converter=tuple,
      validator=attr.validators.deep_iterable(
          attr.validators.instance_of(vz.Trial)
      ),
  )


@attr.define(frozen=True)
class ActiveTrials:
  """A group of active (a.k.a pending) trials.

  Attributes:
    trials: Active Trials.
  """

  def __attrs_post_init__(self):
    for trial in self.trials:
      if trial.status != vz.TrialStatus.ACTIVE:
        raise ValueError(f'All trials must be active. Bad trial:\n{trial}')

  trials: Sequence[vz.Trial] = attr.field(
      converter=tuple,
      default=attr.Factory(list),
      validator=attr.validators.deep_iterable(
          attr.validators.instance_of(vz.Trial)
      ),
  )


class _SuggestionAlgorithm(abc.ABC):
  """Suggestion algorithm."""

  @abc.abstractmethod
  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """


class Designer(_SuggestionAlgorithm):
  """Interface for sequential suggestion algorithms.

  A Designer can be wrapped into a pythia `Policy` via `DesignerPolicy`.
  Prefer implementing a `Designer` interface over `Policy` interface, for
  shared error handling, performance monitoring, logging, etc.


  A big limitation of vanilla `DesignerPolicy` is that it does not retain states
  between consecutive suggestion requests. It creates a new `Designer` from
  scratch and calls `Designer.update` with all trials of the study.
  Many algorithms with a compact state, such as evolutionary search algorithms,
  can gain a huge performance boost from incremental updates. (Side note:
  GP-UCB does not fit into this category because its state includes
  all previous observations, which is not compact).

  If your Designer can take advantage of a persistent state, implement
  `(Partially)SerializableDesigner` interface, which can be wrapped into
  `(Partially)SerializableDesignerPolicy`.
  These Policies serialize the Designer's state, store it in Vizier DB, and
  load it for the next suggest operation. `Designer.update()` is called only
  with the newly completed trials (delta).

  IMPORTANT: `Designer` should not change its state inside `suggest()` (e.g. to
  incorporate its own suggestions before completion). If it does, use
  (Partially)SerializableDesigner interface.

  NOTE: When run inside a service binary, a `Designer` instance does not
  persist during the lifetime of a `Study`. This goes true even for the
  serializable variants; the states are recovered into a new `Designer`
  instance.

  NOTE: `Designer`s are designed to be used directly in benchmarks without
  a `Policy` wrapper. Create a single `Designer` instance for the entire study,
  and incrementally update its state with delta only.
  """

  @abc.abstractmethod
  def update(
      self, completed: CompletedTrials, all_active: ActiveTrials
  ) -> None:
    """Incorporates trials into the designer's state.

    Example:
      [t1, t2] # CompletedTrials
      [t3, t4] # Active Trials
      designer.update([t1], [t3])  # state includes: t1 and t3.
      designer.update([t2])        # state includes: t1 and t2 (not t3).
      designer.update([], [t3,t4]) # state includes: t1, t2, t3, and t4.
      designer.update([], [t3])    # state includes: t1, t2, and t3.

    Arguments:
      completed: COMPLETED trials that this Designer should additionaly
        incorporate.
      all_active: All ACTIVE (aka PENDING) trials in the study from its
        beginning.
    """


@attr.define(frozen=True)
class Prediction:
  """Container to hold predictions.

  The shape of the 'mean' and 'stddev' depends on the number of predictions
  and the number of objectives/metrics.

  In the single-objective case the shape is (num_predictions,).
  In the mulit-objective case the shape is (num_predictions, num_metrics).

  The metadata could be used to supply additional information about the
  prediction.
  """

  mean: chex.Array
  stddev: chex.Array
  metadata: Optional[vz.Metadata] = None

  def __attrs_post_init__(self):
    if self.mean.shape != self.stddev.shape:
      raise ValueError('The shape of mean and stddev needs to be the same.')


class Predictor(abc.ABC):
  """Predicts objective values, given suggestions.

  For algorithms which involve the use of function regressors, this class also
  acts as a mixin to expose their prediction API.
  """

  @abc.abstractmethod
  def predict(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: Optional[int] = None,
  ) -> Prediction:
    """Returns the mean and stddev for any given suggestions.

    Arguments:
      trials: The suggestions where the predictions will be made. Can also be
        completed trials for retrospective predictions.
      rng: The sampling random key used for approximation (if applicable).
      num_samples: The number of samples used for the approximation (if
        applicable).

    Returns:
      The predictions for the given suggestions.
    """


class DesignerFactory(Protocol[_T]):
  """Protocol (PEP-544) for a designer factory."""

  def __call__(self, problem: vz.ProblemStatement, **kwargs) -> _T:
    pass


class PartiallySerializableDesigner(
    Designer, serializable.PartiallySerializable
):
  pass


class SerializableDesigner(Designer, serializable.Serializable):
  pass


--- vizier/_src/algorithms/designers/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/designers/bocs.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Bayesian Optimization of Combinatorial Structures (BOCS) from https://arxiv.org/abs/1806.08838.

Code is a cleaned-up exact version from /BOCSpy/ in
https://github.com/baptistar/BOCS.
"""
# pylint:disable=invalid-name
import abc
import itertools
from typing import Callable, Optional, Sequence, Tuple, Union

from absl import logging
import cvxpy as cvx
import numpy as np

from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random

FloatType = Union[float, np.float32, np.float64]


class _BayesianHorseshoeLinearRegression:
  """Computes conditional poster parameter distributions from a sparsity-inducing prior."""

  def _fastmvg(self, Phi: np.ndarray, alpha: np.ndarray,
               D: np.ndarray) -> np.ndarray:
    """Fast sampler for multivariate Gaussian distributions (large p, p > n) of the form N(mu, S).

    We have:
      mu = S Phi' y
      S  = inv(Phi'Phi + inv(D))
    Reference: https://arxiv.org/abs/1506.04778

    Args:
      Phi:
      alpha:
      D:

    Returns:
    """
    n, p = Phi.shape
    d = np.diag(D)
    u = np.random.randn(p) * np.sqrt(d)
    delta = np.random.randn(n)
    v = np.dot(Phi, u) + delta
    mult_vector = np.vectorize(np.multiply)
    Dpt = mult_vector(Phi.T, d[:, np.newaxis])
    w = np.linalg.solve(np.matmul(Phi, Dpt) + np.eye(n), alpha - v)
    return u + np.dot(Dpt, w)

  def _fastmvg_rue(self, Phi: np.ndarray, PtP: np.ndarray, alpha: np.ndarray,
                   D: np.ndarray) -> np.ndarray:
    """Another sampler for multivariate Gaussians (small p) of the form N(mu, S).

    We have:
      mu = S Phi' y
      S  = inv(Phi'Phi + inv(D))
    Here, PtP = Phi'*Phi (X'X is precomputed).

    Reference: https://www.jstor.org/stable/2680602

    Args:
      Phi:
      PtP:
      alpha:
      D:

    Returns:
    """
    p = Phi.shape[1]
    Dinv = np.diag(1. / np.diag(D))

    # Regularize PtP + Dinv matrix for small negative eigenvalues.
    try:
      L = np.linalg.cholesky(PtP + Dinv)
    except np.linalg.LinAlgError:
      mat = PtP + Dinv
      Smat = (mat + mat.T) / 2.
      maxEig_Smat = np.max(np.linalg.eigvals(Smat))
      L = np.linalg.cholesky(Smat + maxEig_Smat * 1e-15 * np.eye(Smat.shape[0]))

    v = np.linalg.solve(L, np.dot(Phi.T, alpha))
    m = np.linalg.solve(L.T, v)
    w = np.linalg.solve(L.T, np.random.randn(p))
    return m + w

  def regress(
      self,
      X: np.ndarray,
      Y: np.ndarray,
      nsamples: int = 1000,
      burnin: int = 0,
      thin: int = 1
  ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Implementation of the Bayesian horseshoe linear regression hierarchy.

    References:
      https://arxiv.org/abs/1508.03884
      https://www.jstor.org/stable/25734098
      (c) Copyright Enes Makalic and Daniel F. Schmidt, 2015
      Adapted to python by Ricardo Baptista, 2018

    Args:
      X: regressor matrix [n x p]
      Y: response vector  [n x 1]
      nsamples: number of samples for the Gibbs sampler (nsamples > 0)
      burnin: number of burnin (burnin >= 0)
      thin: thinning (thin >= 1)

    Returns:
      beta: regression parameters  [p x nsamples]
      b0: regression param. for constant [1 x nsamples]
      s2: noise variance sigma^2 [1 x nsamples]
      t2: hypervariance tau^2    [1 x nsamples]
      l2: hypervariance lambda^2 [p x nsamples]
    """

    n, p = X.shape

    # Standardize y's
    muY = np.mean(Y)
    Y = Y - muY

    # Return values
    beta = np.zeros((p, nsamples))
    s2 = np.zeros((1, nsamples))
    t2 = np.zeros((1, nsamples))
    l2 = np.zeros((p, nsamples))

    # Initial values
    sigma2 = 1.
    lambda2 = np.random.uniform(size=p)
    tau2 = 1.
    nu = np.ones(p)
    xi = 1.

    # pre-compute X'*X (used with fastmvg_rue)
    XtX = np.matmul(X.T, X)

    # Gibbs sampler
    k = 0
    iter_count = 0
    while k < nsamples:

      # Sample from the conditional posterior distribution
      sigma = np.sqrt(sigma2)
      lambda_star = tau2 * np.diag(lambda2)
      # Determine best sampler for conditional posterior of beta's
      if (p > n) and (p > 200):
        b = self._fastmvg(X / sigma, Y / sigma, sigma2 * lambda_star)
      else:
        b = self._fastmvg_rue(X / sigma, XtX / sigma2, Y / sigma,
                              sigma2 * lambda_star)

      # Sample sigma2
      e = Y - np.dot(X, b)
      shape = (n + p) / 2.
      scale = np.dot(e.T, e) / 2. + np.sum(b**2 / lambda2) / tau2 / 2.
      sigma2 = 1. / np.random.gamma(shape, 1. / scale)

      # Sample lambda2
      scale = 1. / nu + b**2. / 2. / tau2 / sigma2
      lambda2 = 1. / np.random.exponential(1. / scale)

      # Sample tau2
      shape = (p + 1.) / 2.
      scale = 1. / xi + np.sum(b**2. / lambda2) / 2. / sigma2
      tau2 = 1. / np.random.gamma(shape, 1. / scale)

      # Sample nu
      scale = 1. + 1. / lambda2
      nu = 1. / np.random.exponential(1. / scale)

      # Sample xi
      scale = 1. + 1. / tau2
      xi = 1. / np.random.exponential(1. / scale)

      # Store samples
      iter_count += 1
      if iter_count > burnin:
        # thinning
        if (iter_count % thin) == 0:
          beta[:, k] = b
          s2[:, k] = sigma2
          t2[:, k] = tau2
          l2[:, k] = lambda2
          k = k + 1

    b0 = muY
    return beta, b0, s2, t2, l2


class _GibbsLinearRegressor:
  """Stateful Gibbs sampler."""

  def __init__(self, order: int, num_gibbs_retries: int = 10):
    self._order = order
    self._num_gibbs_retries = num_gibbs_retries

    # Below are stateful attributes calulated after `regress()`.
    self._alpha: Optional[np.ndarray] = None
    self._num_vars: Optional[int] = None
    self._X_inf: Optional[np.ndarray] = None
    self._y_inf: Optional[np.ndarray] = None

  def _preprocess(
      self,
      X: np.ndarray,
      Y: np.ndarray,
      inf_threshold: float = 1e6
  ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Preprocess data to ensure unique points and remove outliers."""
    # Limit data to unique points.
    unique_X, X_idx = np.unique(X, axis=0, return_index=True)
    unique_Y = Y[X_idx]

    # separate samples based on Inf output
    Y_Infidx = np.where(np.abs(unique_Y) > inf_threshold)[0]
    Y_nInfidx = np.setdiff1d(np.arange(len(unique_Y)), Y_Infidx)

    X_train = unique_X[Y_nInfidx, :]
    Y_train = unique_Y[Y_nInfidx]

    # Large-value outliers.
    X_inf = X[Y_Infidx, :]
    Y_inf = Y[Y_Infidx]

    return X_train, Y_train, X_inf, Y_inf

  def regress(self, X: np.ndarray, Y: np.ndarray) -> None:
    """Computes and saves alpha (regressor coefficients from the data."""
    # Preprocess data for training and store relevant data.
    X_train, Y_train, self._X_inf, self._Y_inf = self._preprocess(X, Y)
    self._num_vars = X_train.shape[1]

    # Create matrix with all covariates based on order.
    X_train = self._order_effects(X_train)
    (nSamps, nCoeffs) = X_train.shape

    # Check if X_train contains columns w/ zeros and find corresponding indices.
    check_zero = np.all(X_train == np.zeros((nSamps, 1)), axis=0)
    idx_nnzero = np.where(check_zero == False)[0]  # pylint:disable=singleton-comparison,g-explicit-bool-comparison

    # Remove columns of zeros in X_train.
    if np.any(check_zero):
      X_train = X_train[:, idx_nnzero]

    # Run Gibbs sampler.
    bhs = _BayesianHorseshoeLinearRegression()

    counter = 0
    while counter < self._num_gibbs_retries:
      # re-run if there is an error during sampling
      counter += 1
      try:
        alphaGibbs, a0, _, _, _ = bhs.regress(X_train, Y_train)
        # run until alpha matrix does not contain any NaNs
        if not np.isnan(alphaGibbs).any():
          break
      except np.linalg.LinAlgError:
        logging.error('Numerical error during Gibbs sampling. Trying again.')
        continue

    if counter >= self._num_gibbs_retries:
      raise ValueError('Gibbs sampling failed for %d tries.' %
                       self._num_gibbs_retries)

    # append zeros back - note alpha(1,:) is linear intercept
    alpha_pad = np.zeros(nCoeffs)
    alpha_pad[idx_nnzero] = alphaGibbs[:, -1]
    self._alpha = np.append(a0, alpha_pad)

  @property
  def alpha(self) -> np.ndarray:
    if self._alpha is None:
      raise ValueError('You first need to call `regress()` on the data.')
    return self._alpha

  @property
  def num_vars(self) -> int:
    if self._num_vars is None:
      raise ValueError('You first need to call `regress()` on the data.')
    return self._num_vars

  def surrogate_model(self, x: np.ndarray) -> FloatType:
    """Surrogate model.

    Args:
      x: Should only contain one row.

    Returns:
      Surrogate objective.
    """

    # Generate x_all (all basis vectors) based on model order.
    x_all = np.append(1, self._order_effects(x))

    # check if previous x led to Inf output (if so, barrier=Inf)
    barrier = 0.
    if self._X_inf is None:
      raise ValueError('You first need to call `regress()` on the data.')
    if self._X_inf.shape[0] != 0 and np.equal(x, self._X_inf).all(axis=1).any():
      barrier = np.inf

    return np.dot(x_all, self._alpha) + barrier

  def _order_effects(self, X: np.ndarray) -> np.ndarray:
    """Function computes data matrix for all coupling."""

    # Find number of variables
    n_samp, n_vars = X.shape

    # Generate matrix to store results
    x_allpairs = X

    for ord_i in range(2, self._order + 1):

      # generate all combinations of indices (without diagonals)
      offdProd = np.array(
          list(itertools.combinations(np.arange(n_vars), ord_i)))

      # generate products of input variables
      x_comb = np.zeros((n_samp, offdProd.shape[0], ord_i))
      for j in range(ord_i):
        x_comb[:, :, j] = X[:, offdProd[:, j]]
      x_allpairs = np.append(x_allpairs, np.prod(x_comb, axis=2), axis=1)

    return x_allpairs


class AcquisitionOptimizer(abc.ABC):
  """Base class for BOCS acquisition optimizers."""

  def __init__(self, lin_reg: _GibbsLinearRegressor, lamda: float = 1e-4):
    self._lin_reg = lin_reg
    self._num_vars = self._lin_reg.num_vars
    self._lamda = lamda

  @abc.abstractmethod
  def argmin(self) -> np.ndarray:
    """Computes argmin using the regressor."""
    pass


class SimulatedAnnealing(AcquisitionOptimizer):
  """Simulated Annealing solver."""

  def __init__(self,
               lin_reg: _GibbsLinearRegressor,
               lamda: float = 1e-4,
               num_iters: int = 10,
               num_reruns: int = 5,
               initial_temp: float = 1.0,
               annealing_factor: float = 0.8):
    super().__init__(lin_reg=lin_reg, lamda=lamda)
    self._num_iters = num_iters
    self._num_reruns = num_reruns
    self._initial_temp = initial_temp
    self._annealing_factor = annealing_factor

  def argmin(self) -> np.ndarray:
    """Computes argmin via multiple rounds of Simulated Annealing."""
    SA_model = np.zeros((self._num_reruns, self._num_vars))
    SA_obj = np.zeros(self._num_reruns)

    penalty = lambda x: self._lamda * np.sum(x, axis=1)
    acquisition_fn = lambda x: self._lin_reg.surrogate_model(x) + penalty(x)

    for j in range(self._num_reruns):
      optModel, objVals = self._optimization_loop(acquisition_fn)
      SA_model[j, :] = optModel[-1, :]
      SA_obj[j] = objVals[-1]

    # Find optimal solution
    min_idx = np.argmin(SA_obj)
    x_new = SA_model[min_idx, :]
    return x_new

  def _optimization_loop(
      self, objective: Callable[[np.ndarray], FloatType]
  ) -> Tuple[np.ndarray, np.ndarray]:
    """Single optimization round of Simulated Annealing."""

    # Declare vectors to save solutions
    model_iter = np.zeros((self._num_iters, self._num_vars))
    obj_iter = np.zeros(self._num_iters)

    # Set initial temperature and cooling schedule
    T = self._initial_temp
    cool = lambda t: self._annealing_factor * t

    # Set initial condition and evaluate objective
    old_x = np.zeros((1, self._num_vars))
    old_obj = objective(old_x)

    # Set best_x and best_obj
    best_x = old_x
    best_obj = old_obj

    # Run simulated annealing
    for t in range(self._num_iters):

      # Decrease T according to cooling schedule.
      T = cool(T)

      # Find new sample
      flip_bit = np.random.randint(self._num_vars)
      new_x = old_x.copy()
      new_x[0, flip_bit] = 1. - new_x[0, flip_bit]

      # Evaluate objective function.
      new_obj = objective(new_x)

      # Update current solution iterate.
      if (new_obj < old_obj) or (np.random.rand() < np.exp(
          (old_obj - new_obj) / T)):
        old_x = new_x
        old_obj = new_obj

      # Update best solution
      if new_obj < best_obj:
        best_x = new_x
        best_obj = new_obj

      # Save solution
      model_iter[t, :] = best_x
      obj_iter[t] = float(np.squeeze(best_obj))

    return model_iter, obj_iter


class SemiDefiniteProgramming(AcquisitionOptimizer):
  """SDP solver for quadratic acquisition functions."""

  def __init__(self,
               lin_reg: _GibbsLinearRegressor,
               lamda: float = 1e-4,
               num_repeats: int = 100):
    super().__init__(lin_reg=lin_reg, lamda=lamda)
    self._num_repeats = num_repeats

  def argmin(self) -> np.ndarray:
    """Perform SDP over the quadratic xt*A*x + bt*x.

    (A,b) is recovered from alpha.

    Returns:
      Argmin of the SDP problem.
    """
    alpha = self._lin_reg.alpha

    # Extract vector of coefficients
    b = alpha[1:self._num_vars + 1] + self._lamda
    a = alpha[self._num_vars + 1:]

    # Get indices for quadratic terms.
    idx_prod = np.array(
        list(itertools.combinations(np.arange(self._num_vars), 2)))
    n_idx = idx_prod.shape[0]

    # Check number of coefficients
    if a.size != n_idx:
      raise ValueError('Number of Coefficients does not match indices!')

    # Convert a to matrix form
    A = np.zeros((self._num_vars, self._num_vars))
    for i in range(n_idx):
      A[idx_prod[i, 0], idx_prod[i, 1]] = a[i] / 2.
      A[idx_prod[i, 1], idx_prod[i, 0]] = a[i] / 2.

    # Convert to standard form.
    bt = b / 2. + np.dot(A, np.ones(self._num_vars)) / 2.
    bt = bt.reshape((self._num_vars, 1))
    At = np.vstack((np.append(A / 4., bt / 2., axis=1), np.append(bt.T, 2.)))

    # Run SDP relaxation.
    X = cvx.Variable((self._num_vars + 1, self._num_vars + 1), PSD=True)
    obj = cvx.Minimize(cvx.trace(cvx.matmul(At, X)))
    constraints = [cvx.diag(X) == np.ones(self._num_vars + 1)]
    prob = cvx.Problem(obj, constraints)
    prob.solve(solver=cvx.CVXOPT)

    # Extract vectors and compute Cholesky.
    try:
      L = np.linalg.cholesky(X.value)
    except np.linalg.LinAlgError:
      XpI = X.value + 1e-15 * np.eye(self._num_vars + 1)
      L = np.linalg.cholesky(XpI)

    suggest_vect = np.zeros((self._num_vars, self._num_repeats))
    obj_vect = np.zeros(self._num_repeats)

    for kk in range(self._num_repeats):
      # Generate a random cutting plane vector (uniformly distributed on the
      # unit sphere - normalized vector)
      r = np.random.randn(self._num_vars + 1)
      r = r / np.linalg.norm(r)
      y_soln = np.sign(np.dot(L.T, r))

      # Convert solution to original domain and assign to output vector.
      suggest_vect[:, kk] = (y_soln[:self._num_vars] + 1.) / 2.
      obj_vect[kk] = np.dot(
          np.dot(suggest_vect[:, kk].T, A), suggest_vect[:, kk]) + np.dot(
              b, suggest_vect[:, kk])

    # Find optimal rounded solution.
    opt_idx = np.argmin(obj_vect)
    return suggest_vect[:, opt_idx]


AcqusitionOptimizerFactory = Callable[[_GibbsLinearRegressor, float],
                                      AcquisitionOptimizer]


class BOCSDesigner(vza.Designer):
  """BOCS Designer."""

  def __init__(self,
               problem_statement: vz.ProblemStatement,
               order: int = 2,
               acquisition_optimizer_factory:
               AcqusitionOptimizerFactory = SemiDefiniteProgramming,
               lamda: float = 1e-4,
               num_initial_randoms: int = 10):
    """Init.

    Args:
      problem_statement: Must use a boolean search space.
      order: Statistical model order.
      acquisition_optimizer_factory: Which acquisition optimizer to use.
      lamda: Sparsity regularization coefficient.
      num_initial_randoms: Number of initial random suggestions for seeding the
        model.
    """

    if problem_statement.search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.')
    for p_config in problem_statement.search_space.parameters:
      if p_config.external_type != vz.ExternalType.BOOLEAN:
        raise ValueError('Only boolean search spaces are supported.')

    self._problem_statement = problem_statement
    self._metric_name = self._problem_statement.metric_information.item().name
    self._search_space = problem_statement.search_space
    self._current_index = 0

    self._order = order
    self._acquisition_optimizer_factory = acquisition_optimizer_factory
    self._lamda = lamda
    self._num_initial_randoms = num_initial_randoms

    self._trials = []

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    self._trials += tuple(completed.trials)

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    """Core BOCS method.

    Initially will use random search for the first `num_initial_randoms`
    suggestions, and then will use SDP or Simulated Annealing for acqusition
    minimization.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1
    if count > 1:
      raise ValueError('This designer does not support batched suggestions.')

    if len(self._trials) < self._num_initial_randoms:
      random_designer = random.RandomDesigner(self._search_space)
      return random_designer.suggest(count)

    X = []
    Y = []
    for t in self._trials:
      single_x = [
          float(t.parameters.get_value(p.name) == 'True')
          for p in self._search_space.parameters
      ]
      single_y = t.final_measurement.metrics[self._metric_name].value
      X.append(single_x)
      Y.append(single_y)
    X = np.array(X)
    Y = np.array(Y)

    if self._problem_statement.metric_information.item(
    ).goal == vz.ObjectiveMetricGoal.MAXIMIZE:
      Y = -Y

    # Train initial statistical model
    lin_reg = _GibbsLinearRegressor(self._order)
    lin_reg.regress(X, Y)

    # Run acquisition optimization.
    optimizer = self._acquisition_optimizer_factory(lin_reg, self._lamda)
    x_new = optimizer.argmin()

    parameters = vz.ParameterDict()
    for i, p in enumerate(self._search_space.parameters):
      parameters[p.name] = 'True' if x_new[i] == 1.0 else 'False'
    return [vz.TrialSuggestion(parameters=parameters)]


--- vizier/_src/algorithms/designers/bocs_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for bocs."""
from vizier._src.algorithms.designers import bocs
from vizier._src.algorithms.testing import test_runners
from vizier._src.benchmarks.experimenters import combo_experimenter

from absl.testing import absltest
from absl.testing import parameterized


class BocsTest(parameterized.TestCase):

  @parameterized.parameters((bocs.SemiDefiniteProgramming,),
                            (bocs.SimulatedAnnealing,))
  def test_make_suggestions(self, acquisition_optimizer_factory):
    experimenter = combo_experimenter.IsingExperimenter(lamda=0.01)
    designer = bocs.BOCSDesigner(
        experimenter.problem_statement(),
        acquisition_optimizer_factory=acquisition_optimizer_factory,
        num_initial_randoms=1)

    trials = test_runners.run_with_random_metrics(
        designer,
        experimenter.problem_statement(),
        iters=5,
        batch_size=1,
        verbose=1,
        validate_parameters=True)
    self.assertLen(trials, 5)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/cmaes.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""CMA-ES designer."""
import json
import queue
from typing import Optional, Sequence

from evojax.algo import cma_jax
import jax.numpy as jnp
import numpy as np

from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.pyvizier import converters
from vizier.utils import json_utils


class CMAESDesigner(vza.PartiallySerializableDesigner):
  """CMA-ES designer wrapping evo-jax.

  NOTE: Since the base version of CMA-ES expects the entire population size to
  be evaluated before an update, we must use temporary queues to hold partially
  finished populations.
  """

  def __init__(self, problem_statement: vz.ProblemStatement, **cma_kwargs):
    """Init.

    Args:
      problem_statement: Must use a flat DOUBLE-only search space.
      **cma_kwargs: Keyword arguments for the CMA_ES_JAX class.
    """
    self._problem_statement = problem_statement
    self._metric_name = self._problem_statement.metric_information.item().name

    self._search_space = self._problem_statement.search_space
    if self._search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.')
    for parameter_config in self._search_space.parameters:
      if not parameter_config.type.is_continuous():
        raise ValueError(
            f'This designer {self} only supports continuous parameters.')
    self._num_params = len(self._search_space.parameters)
    if self._num_params < 2:
      raise ValueError(
          'CMA-ES only supports search spaces with >=2 parameters. Current'
          f' number of parameters: {self._num_params}'
      )

    # CMA-ES expects a maximization problem by default, so we flip signs for
    # minimization metrics.
    self._converter = converters.TrialToArrayConverter.from_study_config(
        self._problem_statement,
        scale=True,
        flip_sign_for_minimization_metrics=True,
    )
    self._cma_es_jax = cma_jax.CMA_ES_JAX(
        param_size=self._num_params, **cma_kwargs)
    self._trial_population = queue.Queue(
        maxsize=self._cma_es_jax.hyper_parameters.pop_size)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    completed_trials = list(completed.trials)

    # Keep inserting completed trials into population. If population is full,
    # a CMA-ES update and queue clear are triggered.
    while completed_trials:
      self._trial_population.put(completed_trials.pop())

      if self._trial_population.full():
        # Once full, make a full CMA-ES update.
        features, labels = self._converter.to_xy(
            list(self._trial_population.queue))
        # CMA-ES expects fitness to be shape (pop_size,) and solutions of shape
        # (pop_size, num_params).
        self._cma_es_jax.tell(
            fitness=jnp.array(labels[:, 0]), solutions=jnp.array(features))
        self._trial_population.queue.clear()

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1
    cma_suggestions = np.array(self._cma_es_jax.ask(count))

    # Convert CMA suggestions to suggestions.
    return [
        vz.TrialSuggestion(params)
        for params in self._converter.to_parameters(cma_suggestions)
    ]

  def load(self, metadata: vz.Metadata) -> None:
    cma_state = json.loads(
        metadata.ns('cma')['state'], object_hook=json_utils.numpy_hook)
    self._cma_es_jax.load_state(cma_state)

  def dump(self) -> vz.Metadata:
    cma_state = self._cma_es_jax.save_state()
    metadata = vz.Metadata()
    metadata.ns('cma')['state'] = json.dumps(
        cma_state, cls=json_utils.NumpyEncoder)
    return metadata


--- vizier/_src/algorithms/designers/cmaes_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for cmaes."""
from vizier._src.algorithms.designers import cmaes
from vizier._src.algorithms.testing import test_runners
from vizier.benchmarks import experimenters

from absl.testing import absltest


class CmaesTest(absltest.TestCase):

  def setUp(self):
    self.experimenter = experimenters.BBOBExperimenterFactory('Sphere', 2)()
    super().setUp()

  def test_e2e_and_serialization(self):
    designer = cmaes.CMAESDesigner(self.experimenter.problem_statement())

    trials = test_runners.run_with_random_metrics(
        designer,
        self.experimenter.problem_statement(),
        iters=10,
        batch_size=3,
        verbose=1,
        validate_parameters=True,
    )
    self.assertLen(trials, 30)

    new_designer = cmaes.CMAESDesigner(self.experimenter.problem_statement())
    new_designer.load(designer.dump())

    suggestions = designer.suggest(10)
    same_suggestions = new_designer.suggest(10)

    self.assertEqual(suggestions, same_suggestions)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_designer_convergence_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Convergence test for Eagle Strategy."""

from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.designers.eagle_strategy import testing
from vizier._src.algorithms.testing import comparator_runner
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class EagleStrategyConvergenceTest(parameterized.TestCase):
  """Convergence test for Eagle Strategy designer.

  Note that all optimization problems are MINIMIZATION.
  """

  @parameterized.parameters(
      testing.create_continuous_exptr(bbob.Gallagher101Me),
      testing.create_continuous_log_scale_exptr(bbob.Rastrigin),
      testing.create_categorical_exptr(),
  )
  def test_convergence(self, exptr):

    def _random_designer_factory(problem, seed):
      return random.RandomDesigner(problem.search_space, seed=seed)

    def _eagle_designer_factory(problem, seed):
      return eagle_strategy.EagleStrategyDesigner(problem, seed=seed)

    random_benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        designer_factory=_random_designer_factory, experimenter=exptr)

    eagle_benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        designer_factory=_eagle_designer_factory,
        experimenter=exptr,
    )
    evaluations = 1000
    # Random designer batch size is large to expedite run time.
    comparator_runner.SimpleRegretComparisonTester(
        baseline_num_trials=2 * evaluations,
        candidate_num_trials=evaluations,
        baseline_suggestion_batch_size=2 * evaluations,
        candidate_suggestion_batch_size=5,
        baseline_num_repeats=5,
        candidate_num_repeats=1,
        alpha=0.05,
        goal=vz.ObjectiveMetricGoal.MINIMIZE,
    ).assert_benchmark_state_better_simple_regret(
        baseline_benchmark_state_factory=random_benchmark_state_factory,
        candidate_benchmark_state_factory=eagle_benchmark_state_factory,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_simplekd_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Simple4d convergence tests for Eagle designer."""

from absl.testing import parameterized
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.testing import simplekd_runner
from vizier._src.benchmarks.experimenters.synthetic import simplekd
from absl.testing import absltest


class SimpleKDEagleDesignerTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(best_category='corner'),
      dict(best_category='center'),
      dict(best_category='mixed'),
  )
  def test_simple4d(self, best_category: simplekd.SimpleKDCategory) -> None:
    def _eagle_designer_factory(problem, seed):
      return eagle_strategy.EagleStrategyDesigner(problem, seed=seed)

    simplekd_runner.SimpleKDConvergenceTester(
        best_category=best_category,
        designer_factory=_eagle_designer_factory,
        num_trials=5000,
        max_relative_error=0.05,
        num_repeats=20,
        target_num_convergence=10,
        is_deterministic=True,
    ).assert_convergence()


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_strategy.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Pythia Eagle Strategy Designer.

The designer implements a variation of Eagle Strategy without the Levy random
walk, aka Firefly Algorithm [1].

[1] Yang XS. (2009) Firefly Algorithms for Multimodal Optimization.
In: Stochastic Algorithms: Foundations and Applications (SAGA) 2009.
DOI: https://doi.org/10.1007/978-3-642-04944-6_14


Firefly Algorithm Summary
=========================
The Firefly algorithm is a genetic algorithm that operates by maintaining a
population of fireflies. Each firefly emits a light whose intensity corresponds
to the objective value being optimized. During each iteration, a firefly chases
after a brighter firefly in its vicinity, with the perceived brightness
diminishing as the distance increases. This feature fosters the formation of
multiple "clusters" of fireflies, as opposed to all fireflies converging to a
single point.

In addition to the inherent "attraction" forces guiding fireflies towards
brighter areas, we have incorporated a "repulsion" mechanism into the algorithm.
This means that fireflies not only move towards the brighter spots but also
actively move away from darker regions, enhancing the algorithm's exploration
capabilities.

We also incorporated support for various parameter types, including categorical,
discrete, and integer variables. We treat these parameter types uniquely when
computing distances, applying perturbations, and performing mutations on
fireflies, ensuring the algorithm's versatility in optimizing a wide range of
problem domains.

For more details about the Firefly algorithm, please refer to the linked paper.

OSS Vizier Implementation Summary
=================================
We maintain a dynamic pool of fireflies, with each firefly holding information
about the best trial it has produced. During the 'Suggest' phase, each firefly
takes its turn to propose a new trial. The best trial parameters are used to
calculate distances and generate fresh parameter suggestions by invoking the
'_mutate' and '_perturb' methods.

When the results of the suggested trials come in during the 'Update' phase,
we update/remove the corresponding firefly from the pool or create a new firefly
if necessary. To facilitate this association, during the 'Suggest' phase, each
newly sugggested trial stores in its metadata the identifier of its parent
firefly, denoted as 'parent_fly_id.' Over the course of the designer run, the
same firefly can be linked to multiple (child) trials that originated from it.

In our stateful implementation, across successive 'Suggest' calls, we
persistently maintain the firefly pool and update it using any new COMPLETED
trials that have not been seen before. We also ensure to not consecutively use
the same firefly for generating suggestions. Lastly we persist the maximum value
of firefly id created, to ensure that each firefly has its own unique id.
"""

import json
import time
from typing import Optional, Sequence
from absl import logging
import attr
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.core import abstractions
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy_utils
from vizier._src.algorithms.designers.eagle_strategy import serialization
from vizier.interfaces import serializable
from vizier.pyvizier import converters


EagleStrategyUtils = eagle_strategy_utils.EagleStrategyUtils
FireflyAlgorithmConfig = eagle_strategy_utils.FireflyAlgorithmConfig
Firefly = eagle_strategy_utils.Firefly
FireflyPool = eagle_strategy_utils.FireflyPool


class EagleStrategyDesigner(vza.PartiallySerializableDesigner):
  """The eagle strategy partially serializable designer."""

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      *,
      config: Optional[FireflyAlgorithmConfig] = None,
      seed: Optional[int] = None,
      initial_designer_factory: Optional[abstractions.DesignerFactory] = None,
  ):
    """Initializes the Eagle Strategy designer.

    Args:
      problem_statement: A problem description including the search space.
      config: The Firefly algorithm hyperparameters.
      seed: A seed to deterministically generate samples from random variables.
      initial_designer_factory: A partially serializable designer factory to
        generate the initial suggestions.

    Raises:
      Exception: if the problem statement includes condional search space,
        mutli-objectives or safety metrics.
    """
    # Problem statement validation.
    if problem_statement.search_space.is_conditional:
      raise ValueError(
          "Eagle Strategy designer doesn't support conditional parameters."
      )
    if not problem_statement.is_single_objective:
      raise ValueError(
          "Eagle Strategy designer doesn't support multi-objectives."
      )
    if problem_statement.is_safety_metric:
      raise ValueError(
          "Eagle Strategy designer doesn't support safety metrics."
      )

    if seed is None:
      # When a key is not provided it will be set based on the current time to
      # ensure non-repeated behavior.
      seed = int(time.time())
      logging.info(
          (
              'A seed was not provided to Eagle Strategy designer constructor. '
              'Setting the seed to %s'
          ),
          str(seed),
      )
    self._scaler = converters.ProblemAndTrialsScaler(problem_statement)
    self._problem = self._scaler.problem_statement
    self._rng = np.random.default_rng(seed=seed)
    self._config = config or FireflyAlgorithmConfig()
    self._utils = EagleStrategyUtils(self._problem, self._config, self._rng)
    self._firefly_pool = FireflyPool(
        utils=self._utils, capacity=self._utils.compute_pool_capacity()
    )

    if initial_designer_factory is None:
      initial_designer_factory = quasi_random.QuasiRandomDesigner.from_problem
    self._initial_designer = initial_designer_factory(self._problem, seed=seed)

    logging.info(
        (
            'Eagle Strategy designer initialized. Pool capacity: %s. '
            'Eagle config:\n%s\nProblem statement:\n%s'
        ),
        self._utils.compute_pool_capacity(),
        json.dumps(attr.asdict(self._config), indent=2),
        self._problem,
    )

  def dump(self) -> vz.Metadata:
    """Dumps the current state of the algorithm.

    Returns:
      Metadata with the current pool serialized.
    """
    metadata = vz.Metadata()
    metadata.ns('eagle')['rng'] = serialization.serialize_rng(self._rng)
    metadata.ns('eagle')['firefly_pool'] = (
        serialization.partially_serialize_firefly_pool(self._firefly_pool)
    )
    metadata.ns('eagle')['serialization_version'] = 'v1'
    metadata.ns('eagle')['dump_timestamp'] = str(time.time())
    metadata.ns('eagle').ns('random_designer').attach(
        self._initial_designer.dump()
    )
    return metadata

  def load(self, metadata: vz.Metadata) -> None:
    """Loads the current state of the algorithm run.

    The function is used as part of the `_restore_designer` method in
    PartiallySerializableDesignerPolicy to restore the state of the designer
    after it was already initialized with the problem statement.

    1. Set EagleStrategy, EagleStrategyUtils with the recovered random generator
    2. Recover the FireflyPool and populate it with the Fireflies.
    3. Recover the random designer state.

    Args:
      metadata: Metadata
    """
    if metadata.ns('eagle').get('serialization_version', default=None) is None:
      # First time the designer is called, so the namespace doesn't exist yet.
      logging.info(
          'Eagle designer was called for the first time. No state was'
          ' recovered.'
      )
    else:
      try:
        self._rng = serialization.restore_rng(metadata.ns('eagle')['rng'])
      except Exception as e:
        raise serializable.FatalDecodeError(
            "Couldn't load random generator from metadata."
        ) from e
      self._utils.rng = self._rng

      try:
        firefly_pool = metadata.ns('eagle')['firefly_pool']
        self._firefly_pool = serialization.restore_firefly_pool(
            self._utils, firefly_pool
        )
      except Exception as e:
        raise serializable.HarmlessDecodeError(
            "Couldn't load firefly pool from metadata."
        ) from e

      try:
        self._initial_designer = quasi_random.QuasiRandomDesigner(
            self._problem.search_space
        )
        self._initial_designer.load(metadata.ns('eagle').ns('random_designer'))
      except Exception as e:
        raise serializable.HarmlessDecodeError(
            "Couldn't load random designer from metadata."
        ) from e

      logging.info(
          (
              'Eagle designer restored state from timestamp %s. Firefly pool'
              ' now contains %s fireflies.'
          ),
          metadata.ns('eagle')['dump_timestamp'],
          self._firefly_pool.size,
      )

  def suggest(self, count: int = 1) -> Sequence[vz.TrialSuggestion]:
    """Suggests trials."""
    scaled_suggestions = [self._suggest_one() for _ in range(count)]
    #  Unscale suggestion parameters to the original search space.
    return self._scaler.unmap(scaled_suggestions)

  def _suggest_one(self) -> vz.TrialSuggestion:
    """Generates a single suggestion based on the current pool of flies.

    In order to generate a trial suggestion, we find the next fly that should be
    moved and create a copy of it. Then we mutate and perturb its trial
    parameters inplace and assign them to the suggested trial.

    Returns:
      The suggested trial with the parent fly Id in the metadata.
    """
    suggested_trial = vz.TrialSuggestion()
    if self._firefly_pool.size < self._firefly_pool.capacity:
      # Pool is underpopulated. Generate a random trial parameters.
      suggested_parameters = self._initial_designer.suggest()[0].parameters
      # Create a new parent fly id and assign it to the trial, this will be
      # used during Update to match the trial to its parent fly in the pool.
      parent_fly_id = self._firefly_pool.generate_new_fly_id()
    else:
      moving_fly = self._firefly_pool.get_next_moving_fly_copy()
      self._mutate_fly(moving_fly)
      self._perturb_fly(moving_fly)
      suggested_parameters = moving_fly.trial.parameters
      parent_fly_id = moving_fly.id_

    suggested_trial.parameters = suggested_parameters
    suggested_trial.metadata.ns('eagle')['parent_fly_id'] = str(parent_fly_id)
    return suggested_trial

  def _mutate_fly(self, moving_fly: Firefly) -> None:
    """Mutates fly's trial parameters inplace.

    Apply pulls from the rest of the pool's flies on `moving_fly` to mutate
    its trial's parameters.

    Args:
      moving_fly: the fire from the pool to mutate its trial parameters.
    """
    mutated_parameters = moving_fly.trial.parameters
    # Shuffle the ordering, so to apply the attracting and repelling forces
    # in random order every time. Creates a deep copy of the pool members.
    shuffled_flies = self._firefly_pool.get_shuffled_flies(self._rng)
    for other_fly in shuffled_flies:
      is_other_fly_better = self._utils.is_better_than(
          other_fly.trial, moving_fly.trial
      )
      # Compute 'other_fly' pull weights by parameter type. In the paper
      # this is called "attractivness" and is denoted by beta(r).
      pull_weights = self._utils.compute_pull_weight_by_type(
          other_fly.trial.parameters, mutated_parameters, is_other_fly_better
      )
      # Apply the pulls from 'other_fly' on the moving fly's parameters.
      for param_config in self._problem.search_space.parameters:
        pull_weight = pull_weights[param_config.type]
        if other_fly.trial.infeasible:
          pull_weight *= self._config.infeasible_force_factor
        # Accentuate 'other_fly' pull using 'exploration_rate'.
        if pull_weight > 0.5:
          explore_pull_weight = (
              self._config.explore_rate * pull_weight
              + (1 - self._config.explore_rate) * 1.0
          )
        else:
          explore_pull_weight = self._config.explore_rate * pull_weight
        # Update the parameters using 'other_fly' and 'explore_pull_rate'.
        mutated_parameters[param_config.name] = (
            self._utils.combine_two_parameters(
                param_config=param_config,
                param1=other_fly.trial.parameters,
                param2=mutated_parameters,
                param1_weight=explore_pull_weight,
            )
        )

  def _perturb_fly(self, moving_fly: Firefly) -> None:
    """Perturbs the fly's trial parameters inplace.

    Apply random perturbation to the fly's parameter values based on the
    moving_fly's 'perturbation' value.

    Args:
      moving_fly: the fire from the pool to mutate its trial parameters.
    """
    suggested_parameters = moving_fly.trial.parameters
    perturbations = self._utils.create_perturbations(moving_fly.perturbation)
    for i, param_config in enumerate(self._problem.search_space.parameters):
      perturbed_value = self._utils.perturb_parameter(
          param_config,
          suggested_parameters[param_config.name].value,
          perturbations[i],
      )
      suggested_parameters[param_config.name] = perturbed_value

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Update the pool.

    Iterate over new completed trials and update the firefly pool. Every trial
    that was suggested from Eagle Strategy has a metadata data containing the
    parent fly id. For trials that were added to the study externally we assign
    a new parent fly id.

    Trials passed to 'update' are in the unscaled/original search space, and
    will be converted to the scaled search space, so that all other methods
    in the designer deal with scaled trial values.

    Arguments:
      completed: Trials in the original search space.
      all_active:
    """
    del all_active
    trials = self._scaler.map(completed.trials)
    for trial in trials:
      # Replaces trial metric name with a canonical metric name, which makes the
      # serialization and deserialization simpler.
      trial = self._utils.standardize_trial_metric_name(trial)
      if not trial.metadata.ns('eagle').get('parent_fly_id'):
        # Trial was not generated from Eagle Strategy. Set a new parent fly id.
        trial.metadata.ns('eagle')['parent_fly_id'] = str(
            self._firefly_pool.generate_new_fly_id()
        )
      self._update_one(trial)

  def _update_one(self, trial: vz.Trial) -> None:
    """Update the pool using a single trial."""
    if trial.infeasible and self._config.infeasible_force_factor > 0:
      # Add the infeasible firefly to the pool.
      infeasible_firefly_id = self._firefly_pool.generate_new_fly_id()
      self._firefly_pool.create_or_update_fly(trial, infeasible_firefly_id)

    parent_fly_id = int(trial.metadata.ns('eagle').get('parent_fly_id'))
    parent_fly = self._firefly_pool.find_parent_fly(parent_fly_id)
    if parent_fly is None:
      if trial.infeasible:
        # Ignore infeasible trials without parent fly.
        pass
      elif self._firefly_pool.size < self._firefly_pool.capacity:
        # Pool is below capacity. Create a new firefly or update existing one.
        self._firefly_pool.create_or_update_fly(trial, parent_fly_id)
        return
      else:
        # Pool is at capacity. Try assigning a parent to utilize the trial info.
        parent_fly = self._assign_closest_parent(trial)

    if parent_fly is None:
      # Parent fly wasn't established. No need to continue.
      return

    elif not trial.infeasible and self._utils.is_better_than(
        trial, parent_fly.trial
    ):
      # There's improvement. Update the parent with the new trial.
      parent_fly.trial = trial
      parent_fly.generation += 1
    else:
      # There's no improvement. Penalize the parent by decreasing its
      # exploration capability and potenitally remove it from the pool.
      self._penalize_parent_fly(parent_fly, trial)

  def _assign_closest_parent(self, trial: vz.Trial) -> Optional[Firefly]:
    """Finds the closest parent fly and checks that the trial improves on it.

    Note that the trial's `parent_fly_id` won't exist in the pool when:

    1. The trial was randomly generated during 'Suggest', and therefore is not
    associated with any existing fly in the pool.

    2. The fly was removed from the pool but not all associated trials were
    processed yet.

    3. The trial was added to the study independently of Eagle Strategy
    suggestions.

    Args:
      trial:

    Returns:
      None or a fly from the pool that is closest to the trial.
    """
    closest_parent_fly = self._firefly_pool.find_closest_parent(trial)
    if self._utils.is_better_than(trial, closest_parent_fly.trial):
      # Only returns the closest fly if there's improvement. Otherwise, we don't
      # return it to not count it as a failure, as the closest parent is not
      # reponsible for it.
      return closest_parent_fly

  def _penalize_parent_fly(self, parent_fly: Firefly, trial: vz.Trial) -> None:
    """Penalizes a parent fly.

    The method is called on a fly after its generated trial didn't improve the
    objective function. The fly is penalized by decreasing its exploration
    capability and potentially removing it from the pool.

    Args:
      parent_fly: The parent fly with to be penalized.
      trial: The generated trial from the parent fly that didn't imporove.
    """
    if trial.parameters == parent_fly.trial.parameters:
      # If the new trial is identical to the parent trial, it means that the
      # fly is stuck, and so we increase its perturbation.
      parent_fly.perturbation = min(
          parent_fly.perturbation * 10, self._config.max_perturbation
      )
    else:
      # Otherwise, penalize the parent by decreasing its perturbation factor.
      parent_fly.perturbation *= self._config.penalize_factor

    if parent_fly.perturbation < self._config.perturbation_lower_bound:
      # If the perturbation factor is too low we attempt to eliminate the
      # unsuccessful parent fly from the pool.
      if self._firefly_pool.size == self._firefly_pool.capacity:
        # Only remove if the pool is at capacity. This is critical in studies
        # with few feasible/safe trials to retain the feasible trials.
        if not self._firefly_pool.is_best_fly(parent_fly):
          # Check that the fly is not the best one we have thus far.
          self._firefly_pool.remove_fly(parent_fly)


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_strategy_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for eagle_strategy module."""

import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy_utils
from vizier._src.algorithms.designers.eagle_strategy import testing
from vizier.testing import test_studies

from absl.testing import absltest
from absl.testing import parameterized


EagleStrategyDesigner = eagle_strategy.EagleStrategyDesigner
FireflyAlgorithmConfig = eagle_strategy_utils.FireflyAlgorithmConfig


class EagleStrategyTest(parameterized.TestCase):

  @parameterized.parameters(
      testing.create_fake_populated_eagle_designer,  # suggest by mutation.
      testing.create_fake_empty_eagle_designer,  # suggest by initial designer.
  )
  def test_dump_and_load(self, designer_factory):
    eagle_designer = designer_factory()
    metadata = eagle_designer.dump()
    # Create a new eagle designer and load state
    eagle_designer_restored = testing.create_fake_empty_eagle_designer()
    eagle_designer_restored.load(metadata)
    # Generate suggestions from the two designers and test if they're equal.
    for _ in range(10):
      self.assertEqual(
          eagle_designer.suggest()[0].parameters,
          eagle_designer_restored.suggest()[0].parameters,
      )

  def test_load_with_no_state(self):
    problem = testing.create_fake_problem_statement()
    eagle_designer = EagleStrategyDesigner(problem)
    metadata = vz.Metadata()
    # Check that the designer can accept empty metadata in load.
    eagle_designer.load(metadata)

  def test_suggest_one(self):
    eagle_designer = testing.create_fake_populated_eagle_designer()
    trial_suggestion = eagle_designer._suggest_one()
    self.assertIsInstance(trial_suggestion, vz.TrialSuggestion)
    self.assertIsNotNone(
        trial_suggestion.metadata.ns('eagle').get('parent_fly_id')
    )

  def test_embedding(self):
    eagle_designer = testing.create_fake_populated_eagle_designer()
    # Check that the problem was converted.
    self.assertEqual(
        eagle_designer._problem.search_space.parameters[0].bounds, (0.0, 1.0)
    )
    # Check that internal suggestions are in normalized range.
    for _ in range(10):
      trial_suggestion = eagle_designer._suggest_one()
      self.assertBetween(trial_suggestion.parameters['x'].value, 0.0, 1.0)
    # Check that update maps the trials correctly to the normalized space.
    eagle_designer = testing.create_fake_empty_eagle_designer()
    trial = vz.Trial({'x': 10.0})
    trial = trial.complete(
        vz.Measurement(metrics={'objective': np.random.uniform()})
    )
    complete_trials = vza.CompletedTrials([trial])
    eagle_designer.update(complete_trials, vza.ActiveTrials())
    self.assertEqual(
        eagle_designer._firefly_pool._pool[0].trial.parameters['x'].value, 1.0
    )

  @parameterized.parameters(1e-4, 1.0)
  def test_penalize_parent_fly_no_trial_change(self, perturbation):
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.0, 2.0, 3.0], obj_values=[1, 2, 3], parent_fly_ids=[1, 2, 3]
    )
    trial = testing.create_fake_trial(parent_fly_id=2, x_value=2.0, obj_value=2)
    parent_fly = eagle_designer._firefly_pool._pool[2]
    # Set the perturbation.
    parent_fly.perturbation = perturbation
    before_perturbation = parent_fly.perturbation
    eagle_designer._penalize_parent_fly(parent_fly, trial)
    after_perturbation = parent_fly.perturbation
    if perturbation == 1.0:
      # Perturbation is already high so capped by the maximimum.
      self.assertEqual(
          after_perturbation,
          before_perturbation * eagle_designer._config.max_perturbation,
      )
    elif perturbation == 1e-4:
      # Not reaching the maximum yet, multiply by 10.
      self.assertEqual(after_perturbation, 1e-3)

  def test_penalize_parent_fly(self):
    # Capacitated pool size has 11 fireflies.
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.0, 2.0, 3.0], obj_values=[1, 2, 3], parent_fly_ids=[1, 2, 3]
    )
    trial = testing.create_fake_trial(
        parent_fly_id=2, x_value=1.42, obj_value=0.5
    )
    parent_fly = eagle_designer._firefly_pool._pool[2]
    before_perturbation = parent_fly.perturbation
    eagle_designer._penalize_parent_fly(parent_fly, trial)
    after_perturbation = parent_fly.perturbation
    self.assertEqual(after_perturbation, before_perturbation * 0.9)

  def test_suggest(self):
    eagle_designer = testing.create_fake_populated_eagle_designer()
    trial_suggestions = eagle_designer.suggest(count=10)
    self.assertLen(trial_suggestions, 10)
    self.assertIsInstance(trial_suggestions[0], vz.TrialSuggestion)

  def test_suggest_flat(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.root
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 2.0)
    metric = vz.MetricInformation(
        name='obj',
        goal=vz.ObjectiveMetricGoal.MAXIMIZE,
    )
    problem.metric_information.append(metric)

    designer = EagleStrategyDesigner(problem)
    suggestions = designer.suggest(count=1)
    self.assertLen(suggestions, 1)

  def test_update_capacitated_pool_no_parent_fly_trial_is_better(self):
    # Capacitated pool size has 11 fireflies.
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1],
        obj_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    )
    trial = testing.create_fake_trial(
        parent_fly_id=98, x_value=1.42, obj_value=100.0
    )
    eagle_designer._update_one(trial)
    self.assertIs(eagle_designer._firefly_pool._pool[3].trial, trial)

  def test_update_capacitated_pool_no_parent_fly_trial_is_not_better(self):
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1],
        obj_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    )
    trial = testing.create_fake_trial(
        parent_fly_id=98, x_value=1.42, obj_value=-80.0
    )
    prev_trial = eagle_designer._firefly_pool._pool[3].trial
    eagle_designer._update_one(trial)
    self.assertIs(eagle_designer._firefly_pool._pool[3].trial, prev_trial)

  def test_update_capacitated_pool_with_parent_fly_trial_is_better(self):
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1],
        obj_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    )
    trial = testing.create_fake_trial(
        parent_fly_id=2, x_value=3.3, obj_value=80.0
    )
    eagle_designer._update_one(trial)
    self.assertIs(eagle_designer._firefly_pool._pool[2].trial, trial)

  def test_update_capacitated_pool_with_parent_fly_trial_is_not_better(self):
    eagle_designer = testing.create_fake_populated_eagle_designer(
        x_values=[1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1],
        obj_values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
    )
    trial = testing.create_fake_trial(
        parent_fly_id=2, x_value=3.3, obj_value=-80.0
    )
    prev_trial = eagle_designer._firefly_pool._pool[2].trial
    eagle_designer._update_one(trial)
    self.assertIs(eagle_designer._firefly_pool._pool[2].trial, prev_trial)

  def test_update_empty_pool(self):
    eagle_designer = testing.create_fake_empty_eagle_designer()
    trial = testing.create_fake_trial(
        parent_fly_id=0, x_value=3.3, obj_value=0.0
    )
    eagle_designer._update_one(trial)
    self.assertIs(eagle_designer._firefly_pool._pool[0].trial, trial)

  def test_seeded_random_samples(self):
    problem = testing.create_fake_problem_statement()
    suggestions1 = EagleStrategyDesigner(problem, seed=1).suggest(count=3)
    suggestions2 = EagleStrategyDesigner(problem, seed=1).suggest(count=3)
    for suggestions1, suggest2 in zip(suggestions1, suggestions2):
      self.assertEqual(suggestions1.parameters, suggest2.parameters)

  @parameterized.parameters(1, 3, 5)
  def test_suggest_update(self, batch_size):
    problem = vz.ProblemStatement()
    problem.search_space.select_root().add_float_param(
        'float1', 1e-2, 1e3, scale_type=vz.ScaleType.LOG
    )
    problem.search_space.select_root().add_float_param(
        'float2', -2.0, 5.0, scale_type=vz.ScaleType.LINEAR
    )
    problem.search_space.select_root().add_int_param(
        'int', min_value=0, max_value=10
    )
    problem.search_space.select_root().add_discrete_param(
        'discrete', feasible_values=[0.0, 0.6]
    )
    problem.search_space.select_root().add_categorical_param(
        'categorical', feasible_values=['a', 'b', 'c']
    )
    problem.metric_information.append(
        vz.MetricInformation(goal=vz.ObjectiveMetricGoal.MINIMIZE, name='')
    )
    eagle_designer = EagleStrategyDesigner(problem)

    tid = 1
    # Simulate running the designer for 3 suggestions each with a batch.
    for _ in range(3):
      suggestions = eagle_designer.suggest(batch_size)
      completed = []
      # Completing the suggestions while assigning unique trial id.
      for suggestion in suggestions:
        completed.append(
            suggestion.to_trial(tid).complete(
                vz.Measurement(metrics={'': np.random.uniform()})
            )
        )
        tid += 1
      eagle_designer.update(vza.CompletedTrials(completed), vza.ActiveTrials())

  @parameterized.named_parameters(
      dict(
          testcase_name='Less suggestions than pool capacity',
          num_feasible_suggestions=3,
          num_infeasible_suggestions=2,
      ),
      dict(
          testcase_name='More suggestions than pool capacity',
          num_feasible_suggestions=50,
          num_infeasible_suggestions=5,
      ),
  )
  def test_infeasible_trials(
      self, num_feasible_suggestions, num_infeasible_suggestions
  ):
    """Tests that Eagle works with infeasible trials."""
    problem = vz.ProblemStatement()
    problem.search_space.select_root().add_float_param(
        'float1', 1e-2, 1e3, scale_type=vz.ScaleType.LOG
    )
    problem.search_space.select_root().add_float_param(
        'float2', -2.0, 5.0, scale_type=vz.ScaleType.LINEAR
    )
    problem.search_space.select_root().add_int_param(
        'int', min_value=0, max_value=10
    )
    problem.search_space.select_root().add_discrete_param(
        'discrete', feasible_values=[0.0, 0.6]
    )
    problem.search_space.select_root().add_categorical_param(
        'categorical', feasible_values=['a', 'b', 'c']
    )
    problem.metric_information.append(
        vz.MetricInformation(goal=vz.ObjectiveMetricGoal.MINIMIZE, name='')
    )
    config = FireflyAlgorithmConfig(infeasible_force_factor=0.1)
    eagle_designer = EagleStrategyDesigner(problem, config=config)

    def _suggest_and_update(
        eagle_designer: EagleStrategyDesigner, tid: int, infeasible: bool
    ):
      suggestion = eagle_designer.suggest(count=1)[0]
      completed = suggestion.to_trial(tid).complete(
          vz.Measurement(metrics={'': np.random.uniform()}),
          infeasibility_reason='infeasible' if infeasible else None,
      )
      eagle_designer.update(
          vza.CompletedTrials([completed]), vza.ActiveTrials()
      )

    # Suggest trials and update designer for less than pool capacity.
    tid = 1
    for _ in range(num_feasible_suggestions):
      _suggest_and_update(eagle_designer, tid, infeasible=False)
      tid += 1

    # Suggest another trial and return it as infeasible.
    for _ in range(num_infeasible_suggestions):
      _suggest_and_update(eagle_designer, tid, infeasible=True)
      tid += 1

    # Test that the pool size is not affected by infeasible trials..
    self.assertEqual(
        eagle_designer._firefly_pool.size,
        min(eagle_designer._firefly_pool.capacity, num_feasible_suggestions),
    )
    # Test that the pool contains infeasible trials.
    self.assertEqual(
        eagle_designer._firefly_pool._infeasible_count,
        num_infeasible_suggestions,
    )
    self.assertEqual(
        sum([
            1
            for firefly in eagle_designer._firefly_pool._pool.values()
            if firefly.trial.infeasible
        ]),
        num_infeasible_suggestions,
    )
    # Suggest more trials while having infeasible trials in the pool.
    for _ in range(3):
      _suggest_and_update(eagle_designer, tid, infeasible=False)
      tid += 1

  def test_on_singleton_search_space(self):
    problem = vz.ProblemStatement(
        test_studies.flat_space_with_all_types_with_singletons()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    designer = eagle_strategy.EagleStrategyDesigner(problem)
    initial_suggestions = designer.suggest(25)
    trials = []
    for idx, suggestion in enumerate(initial_suggestions):
      trial = suggestion.to_trial(idx)
      trial.complete(vz.Measurement({'metric': 1.0}))
      trials.append(trial)
    designer.update(vza.CompletedTrials(trials), vza.ActiveTrials([]))
    self.assertLen(designer.suggest(25), 25)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_strategy_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utils functions to support Eagle Strategy designer."""

import collections
import copy
import math
from typing import DefaultDict, Dict, Optional
from absl import logging
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.random import random_sample

# Standardize name used to assign for trials' metric name, which simplifies
# serializing trials as part of the FireflyPool.
OBJECTIVE_NAME = 'objective'


@attr.define
class FireflyAlgorithmConfig:
  """Configuration hyperparameters for Eagle Strategy / Firefly Algorithm."""
  # ---------------------------- Gravity ---------------------------------------
  # The amount of pulling force a brighter firefly applies to its neighbors.
  gravity: float = 1.0
  # The amount of repel force a darker firefly applies to its neighbors.
  negative_gravity: float = 0.02

  # --------------------------- Visibility -------------------------------------
  # The visibility associated with continuous parameters.
  visibility: float = 3.0
  # The visibility associated with categorical parameters.
  categorical_visibility: float = 0.2
  # The visibility associated with discrete parameters.
  discrete_visibility: float = 1.0

  # ---------------------------- Perturbation ----------------------------------
  # The default perturbation that fireflies start with.
  perturbation: float = 1e-1
  # The lower bound of the perturbation factor below which firefly are removed.
  perturbation_lower_bound: float = 1e-3
  # The perturbation factor for categorical parameters.
  categorical_perturbation_factor: float = 25.0
  # The perturbation factor for discrete parameters.
  discrete_perturbation_factor: float = 10.0
  # The perturbation of purely categorical search space.
  pure_categorical_perturbation: float = 0.1
  # The maximum perturbation factor.
  max_perturbation: float = 0.5
  # The penalize factor used to penalize a parent firefly's lack of improvement.
  penalize_factor: float = 0.9

  # ---------------------------- Exploration -----------------------------------
  # Exploration rate (value > 1.0 encourages more exploration)
  explore_rate: float = 1.0
  # The factor to apply on infeasible trial repel force.
  infeasible_force_factor: float = 0.0

  # ----------------------------- Pool Size ------------------------------------
  # The factor used to compute the pool size.
  pool_size_factor: float = 1.2
  # The maximum pool size.
  max_pool_size: int = 1000


@attr.define
class Firefly:
  """The Firefly class represents a single firefly in the pool.

  Attributes:
    id_: A unique firefly identifier. This is used to associate trials with
      their parent fireflies.
    perturbation: Controls the amount of exploration. Signifies the amount of
      perturbation to add to the generated trial parameters. The value of
      'perturbation' keeps decreasing if the suggested trial doesn't improve the
      objective function until it reaches 'perturbation_lower_bound' in which
      case we remove the firefly from the pool.
    generation: The number of "successful" (better than the last trial) trials
      suggested from the firefly.
    trial: The best trial associated with the firefly.
  """
  id_: int
  perturbation: float
  generation: int
  trial: vz.Trial


@attr.define
class EagleStrategyUtils:
  """Eagle Strategy utils.

  Attributes:
    search_space: The search space that Eagle attempts to optimize on.
    config: The Eagle Strategy configuration.
    rng: The random generator to sample random distributions.
  """
  problem_statement: vz.ProblemStatement
  config: FireflyAlgorithmConfig
  rng: np.random.Generator
  _search_space: vz.SearchSpace = attr.field(init=False)
  _n_parameters: int = attr.field(init=False)
  _degrees_of_freedom: DefaultDict[vz.ParameterType, int] = attr.field(
      init=False, factory=lambda: collections.defaultdict(int))
  _original_metric_name: Optional[str] = attr.field(init=False)
  _goal: vz.ObjectiveMetricGoal = attr.field(init=False)

  def __attrs_post_init__(self):
    """Initialize and cache common values and objects."""
    self._search_space = self.problem_statement.search_space
    self._n_parameters = len(self._search_space.parameters)
    self._cache_degrees_of_freedom()
    self._original_metric_name = (
        self.problem_statement.single_objective_metric_name
    )
    self._goal = self.problem_statement.metric_information.item().goal
    logging.info('EagleStrategyUtils instance was created.\n%s', str(self))

  def compute_pull_weight_by_type(
      self,
      other_parameters: vz.ParameterDict,
      suggested_parameters: vz.ParameterDict,
      is_other_fly_better: bool,
  ) -> Dict[vz.ParameterType, float]:
    """Computes the pull weights by type."""
    # Compute squared distances between the vector of parameters of each type.
    squared_distances = self._compute_canonical_distance_squared_by_type(
        other_parameters, suggested_parameters)
    # Determine the direction (attraction vs. repulsion).
    if is_other_fly_better > 0:
      pull_direction = self.config.gravity
    else:
      pull_direction = -self.config.negative_gravity

    pull_weights = {}
    # Iterate over the squared distance by type and compute the pull force.
    for param_type, squared_distance in squared_distances.items():
      degree_of_freedom = self._degrees_of_freedom[param_type]
      if degree_of_freedom == 0:
        pull_weights[param_type] = 0
      else:
        scaled_squared_distance = squared_distance / degree_of_freedom * 10
        # Determine the visibilty based on the parameter type.
        if param_type == vz.ParameterType.CATEGORICAL:
          visiblity = self.config.categorical_visibility
        elif param_type in [
            vz.ParameterType.DISCRETE, vz.ParameterType.INTEGER
        ]:
          visiblity = self.config.discrete_visibility
        elif param_type == vz.ParameterType.DOUBLE:
          visiblity = self.config.visibility
        else:
          raise ValueError('Unsupported parameter type: %s' % param_type)
        # Compute the pull weight and insert to dictionary.
        pull_weights[param_type] = math.exp(
            -visiblity * scaled_squared_distance) * pull_direction

    return pull_weights

  @property
  def _param_perturb_scales(self):
    """Set the parameter perturbation scaling."""
    scales = [1.0 for _ in range(self._n_parameters)]
    for i, param_config in enumerate(self._search_space.parameters):
      if param_config.type == vz.ParameterType.CATEGORICAL:
        # For CATEGORICAL parameters, the perturbation is interpreted as a
        # probability of replacing the value with a uniformly random category.
        scales[i] = self.config.categorical_perturbation_factor
      if param_config.type == vz.ParameterType.DISCRETE:
        scales[i] = self.config.discrete_perturbation_factor / (
            param_config.num_feasible_values * self.config.perturbation)
    return np.asarray(scales)

  def _cache_degrees_of_freedom(self) -> None:
    """Computes the degrees of freedom for each parameter type and cache it.

    The function counts the number of parameter configs in the search space that
    has more than only a single feasible point.
    """
    for param_config in self._search_space.parameters:
      if param_config.num_feasible_values > 1:
        self._degrees_of_freedom[param_config.type] += 1

  def _compute_canonical_distance_squared_by_type(
      self,
      p1: vz.ParameterDict,
      p2: vz.ParameterDict,
  ) -> Dict[vz.ParameterType, float]:
    """Computes the canonical distance squared by parameter type."""
    dist_squared_by_type = {
        vz.ParameterType.DOUBLE: 0.0,
        vz.ParameterType.DISCRETE: 0.0,
        vz.ParameterType.INTEGER: 0.0,
        vz.ParameterType.CATEGORICAL: 0.0
    }

    for param_config in self._search_space.parameters:
      p1_value = p1[param_config.name].value
      p2_value = p2[param_config.name].value

      if param_config.type == vz.ParameterType.CATEGORICAL:
        dist_squared_by_type[param_config.type] += int(p1_value == p2_value)
      else:
        min_value, max_value = param_config.bounds
        if max_value == min_value:
          dist = 0
        else:
          dist = (p1_value - p2_value) / (max_value - min_value)
        dist_squared_by_type[param_config.type] += dist * dist

    return dist_squared_by_type

  def compute_cononical_distance(
      self,
      p1: vz.ParameterDict,
      p2: vz.ParameterDict,
  ) -> float:
    """Computes the canonical squared distance between two parameters."""
    dist_by_type = self._compute_canonical_distance_squared_by_type(p1, p2)
    return sum(dist_by_type.values())

  def compute_pool_capacity(self) -> int:
    """Computes the pool capacity."""
    df = self._n_parameters
    return min(
        10 + round((df**self.config.pool_size_factor + df) * 0.5),
        self.config.max_pool_size,
    )

  def combine_two_parameters(
      self,
      param_config: vz.ParameterConfig,
      param1: vz.ParameterDict,
      param2: vz.ParameterDict,
      param1_weight: float,
  ) -> vz.ParameterValueTypes:
    """Combnies the values of two parameters based on their weights and type.

    For decimal parameters, performs a linear combination of two parameters,
    by computing f1 * f1_weight + f2 * (1 - f1_weight). Note that this is not a
    convex combination, because 'f1_weight' can be outside [0, 1].

    For categorical parameters, uses Bernuolli to choose between the value of
    the first parameter and the second with probability that equals the weight.

    Args:
      param_config:
      param1:
      param2:
      param1_weight:

    Returns:
      The combined weighted value of the two parameters.
    """
    value1 = param1[param_config.name].value
    value2 = param2[param_config.name].value
    if param_config.type == vz.ParameterType.CATEGORICAL:
      if 0.0 < param1_weight < 1.0:
        prob1 = param1_weight
        new_value = random_sample.sample_bernoulli(
            self.rng, prob1, value1, value2
        )
      elif param1_weight <= 0.0:
        new_value = value2
      else:  # param1_weight >= 1.0
        new_value = value1
    else:
      weighted_param_value = value1 * param1_weight + value2 * (
          1 - param1_weight
      )
      if param_config.type == vz.ParameterType.DOUBLE:
        new_value = weighted_param_value
      elif param_config.type == vz.ParameterType.INTEGER:
        new_value = round(weighted_param_value)
      elif param_config.type == vz.ParameterType.DISCRETE:
        new_value = random_sample.get_closest_element(
            param_config.feasible_values, weighted_param_value)
      else:
        raise ValueError('Invalid parameter type: %s' % param_config.type)
      new_value = min(new_value, param_config.bounds[1])
      new_value = max(new_value, param_config.bounds[0])
    return new_value

  def create_perturbations(self, perturbation: float) -> list[float]:
    """Creates perturbations array."""
    if self.is_pure_categorical():
      return [
          self.config.pure_categorical_perturbation
          for _ in range(self._n_parameters)
      ]
    perturbations = self.rng.laplace(size=(self._n_parameters,))
    perturbation_direction = perturbations / max(abs(perturbations))
    perturbations = perturbation_direction * perturbation
    return [float(x) for x in perturbations * self._param_perturb_scales]

  def perturb_parameter(
      self,
      param_config: vz.ParameterConfig,
      value: vz.ParameterValueTypes,
      perturbation: float,
  ) -> vz.ParameterValueTypes:
    """Perturbs the parameter based on its type and the amount of perturbation.

    For categorical parameters, with probability 'perturbation', replace with a
    uniformly random category.

    For numeric parameters, 'perturbation' determines the fraction of the
    parameter range to add, where perturbation=-1 will always return the minimum
    possible value, and perturbation=+1 will return the maximum possible value.

    Args:
      param_config: The configuration of the parameter
      value: The parameter value before perturbation
      perturbation: The amount of pertrubation to apply

    Returns:
      The parameter value after perturbation.

    Raises:
      Exception: if the parameter has invalid type.
    """
    if param_config.type == vz.ParameterType.CATEGORICAL:
      if random_sample.sample_uniform(self.rng) < abs(perturbation):
        return random_sample.sample_categorical(self.rng,
                                                param_config.feasible_values)
      else:
        return value

    min_value, max_value = param_config.bounds
    perturb_val = value + perturbation * (max_value - min_value)

    if param_config.type == vz.ParameterType.DISCRETE:
      return random_sample.get_closest_element(param_config.feasible_values,
                                               perturb_val)

    perturb_val = min(perturb_val, param_config.bounds[1])
    perturb_val = max(perturb_val, param_config.bounds[0])
    if param_config.type == vz.ParameterType.DOUBLE:
      return perturb_val
    elif param_config.type == vz.ParameterType.INTEGER:
      return round(perturb_val)
    else:
      raise ValueError('Invalid parameter type: %s' % param_config.type)

  def get_metric(self, trial: vz.Trial) -> float:
    """Returns the trial metric."""
    if trial.infeasible:
      return np.nan
    if trial.final_measurement is None:
      raise ValueError('Trial is not completed.')
    return trial.final_measurement.metrics[OBJECTIVE_NAME]  # pytype: disable=bad-return-type

  def is_better_than(
      self,
      trial1: vz.Trial,
      trial2: vz.Trial,
  ) -> bool:
    """Checks whether the 'trial1' is better than 'trial2'.

    The comparison is based on the value of final measurement and whether it
    goal is MAXIMIZATION or MINIMIZATON.

    If either trials is not completed, infeasible or missing the final
    measurement returns False.

    Args:
      trial1:
      trial2:

    Returns:
      Whether trial1 is greater than trial2.
    """
    if not trial1.is_completed or not trial2.is_completed:
      return False
    if trial1.infeasible and not trial2.infeasible:
      return False
    if not trial1.infeasible and trial2.infeasible:
      return True
    if trial1.final_measurement is None or trial2.final_measurement is None:
      return False

    if self._goal == vz.ObjectiveMetricGoal.MAXIMIZE:
      return trial1.final_measurement.metrics[
          OBJECTIVE_NAME] > trial2.final_measurement.metrics[OBJECTIVE_NAME]
    else:
      return trial1.final_measurement.metrics[
          OBJECTIVE_NAME] < trial2.final_measurement.metrics[OBJECTIVE_NAME]

  def is_pure_categorical(self) -> bool:
    """Returns True if all parameters in search_space are categorical."""
    return all([
        p.type == vz.ParameterType.CATEGORICAL
        for p in self._search_space.parameters
    ])

  def standardize_trial_metric_name(self, trial: vz.Trial) -> vz.Trial:
    """Creates a new trial with canonical metric name."""
    if trial.infeasible:
      return trial
    if trial.final_measurement is None:
      raise ValueError('Trial is not completed.')
    value = trial.final_measurement.metrics[self._original_metric_name].value
    new_trial = vz.Trial(parameters=trial.parameters, metadata=trial.metadata)
    new_trial.complete(
        measurement=vz.Measurement(metrics={OBJECTIVE_NAME: value}))
    return new_trial

  def display_trial(self, trial: vz.Trial) -> str:
    """Construct a string to represent a completed trial."""
    parameters = {
        k: v if isinstance(v, str) else round(v, 3)
        for k, v in trial.parameters.as_dict().items()
    }
    if trial.final_measurement:
      obj_value = (
          f'{list(trial.final_measurement.metrics.values())[0].value:.5f}'
      )
      return f'Value: {obj_value}, Parameters: {parameters}'
    else:
      return f'Parameters: {parameters}'


@attr.define
class FireflyPool:
  """The class maintains the Firefly pool and relevent operations.

  Attributes:
    utils: Eagle Strategy utils class.
    capacity: The maximum number of non-feasible fireflies in the pool.
    size: The current number of non-feasible fireflies in the pool.
    _pool: A dictionary of Firefly objects organized by firefly id.
    _last_id: The last firefly id used to generate a suggestion. It's persistent
      across calls to ensure we don't use the same fly repeatedly.
    _max_fly_id: The maximum value of any fly id ever created. It's persistent
      persistent accross calls to ensure unique ids even if trails were deleted.
    _infeasible_count: The number of infeasible fireflies in the pool.
  """
  _utils: EagleStrategyUtils
  _capacity: int
  _pool: Dict[int, Firefly] = attr.field(init=False, default=attr.Factory(dict))
  _last_id: int = attr.field(init=False, default=0)
  _max_fly_id: int = attr.field(init=False, default=0)
  _infeasible_count: int = attr.field(init=False, default=0)

  @property
  def capacity(self) -> int:
    return self._capacity

  @property
  def size(self) -> int:
    """Returns the number of feasible fireflies in the pool."""
    return len(self._pool) - self._infeasible_count

  def remove_fly(self, fly: Firefly):
    """Removes a fly from the pool."""
    if fly.trial.infeasible:
      raise ValueError('Infeasible firefly should not be removed from pool.')
    del self._pool[fly.id_]

  def get_shuffled_flies(self, rng: np.random.Generator) -> list[Firefly]:
    """Shuffles the fireflies and returns them as a list."""
    return random_sample.shuffle_list(rng, list(self._pool.values()))

  def generate_new_fly_id(self) -> int:
    """Generates a unique fly id (starts from 0) to identify a fly in the pool."""
    self._max_fly_id += 1
    return self._max_fly_id - 1

  def get_next_moving_fly_copy(self) -> Firefly:
    """Finds the next fly, returns a copy of it and updates '_last_id'.

    To find the next moving fly, we start from index of '_last_id'+1 and
    incremently check whether the index exists in the pool. When the loop
    reaches 'max_fly_id' it goes back to index 0. We return a copy of the
    first fly we find an index for. Before returning we also set '_last_id'
    to the next moving fly id.

    Note that we don't assume the existance of '_last_id' in the pool, as the
    fly with `_last_id` might be removed from the pool.

    Returns:
      A copy of the next moving fly.
    """
    curr_id = self._last_id + 1
    while curr_id != self._last_id:
      if curr_id > self._max_fly_id:
        # Passed the maximum id. Start from the first one as ids are monotonic.
        curr_id = next(iter(self._pool))
      if curr_id in self._pool and not self._pool[curr_id].trial.infeasible:
        self._last_id = curr_id
        return copy.deepcopy(self._pool[curr_id])
      curr_id += 1

    return copy.deepcopy(self._pool[self._last_id])

  def is_best_fly(self, fly: Firefly) -> bool:
    """Checks if the 'fly' has the best final measurement in the pool."""
    for other_fly_id, other_fly in self._pool.items():
      if other_fly_id != fly.id_ and self._utils.is_better_than(
          other_fly.trial, fly.trial
      ):
        return False
    return True

  def find_parent_fly(self, parent_fly_id: Optional[int]) -> Optional[Firefly]:
    """Obtains the parent firefly associated with the trial.

    Extract the associated parent id from the trial's metadata and attempt
    to find the parent firefly in the pool. If it doesn't exist return None.

    Args:
      parent_fly_id:

    Returns:
      Firefly or None.
    """
    parent_fly = self._pool.get(parent_fly_id, None)
    return parent_fly

  def find_closest_parent(self, trial: vz.Trial) -> Firefly:
    """Finds the closest fly in the pool to a given trial."""
    if not self._pool:
      raise ValueError('Pool was empty when searching for closest parent.')

    min_dist, closest_parent = float('inf'), next(iter(self._pool.values()))
    for other_fly in self._pool.values():
      if other_fly.trial.infeasible:
        continue
      curr_dist = self._utils.compute_cononical_distance(
          other_fly.trial.parameters, trial.parameters
      )
      if curr_dist < min_dist:
        min_dist = curr_dist
        closest_parent = other_fly

    return closest_parent

  def create_or_update_fly(self, trial: vz.Trial, parent_fly_id: int) -> None:
    """Creates a new fly in the pool or update an existing one.

    This method is called when the pool is below capacity.

    The newly created fly is assigned with 'id_' equals 'parent_fly_id' which
    is taken from the trial's metadata. The fly's id is determined during the
    'Suggest' method and stored in the metadata.

    Edge case: if the 'parent_fly_id' already exists in the pool (and the pool
    is below capacity) we update the matching fly with the better trial. This
    scenario could happen if for example a batch larger than the pool capacity
    was suggested, and then trials associated with the same fly were reported
    as COMPLETED sooner than the other trials, so when the pool is updated
    during 'Update' we encounter trials from the same fly before the pool is at
    capactiy.

    Args:
      trial:
      parent_fly_id:
    """
    if parent_fly_id not in self._pool:
      # Create a new Firefly in pool.
      new_fly = Firefly(
          id_=parent_fly_id,
          perturbation=self._utils.config.perturbation,
          generation=1,
          trial=trial,
      )
      self._pool[parent_fly_id] = new_fly
      if trial.infeasible:
        self._infeasible_count += 1
    else:
      # Parent fly id already in pool. Update trial if there was improvement.
      if self._utils.is_better_than(trial, self._pool[parent_fly_id].trial):
        self._pool[parent_fly_id].trial = trial


--- vizier/_src/algorithms/designers/eagle_strategy/eagle_strategy_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for Eagle Strategy utils."""

import copy
from typing import Optional

import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy_utils
from vizier._src.algorithms.designers.eagle_strategy import testing
from absl.testing import absltest
from absl.testing import parameterized

EagleStrategyUtils = eagle_strategy_utils.EagleStrategyUtils
FireflyAlgorithmConfig = eagle_strategy_utils.FireflyAlgorithmConfig
EagleStrategyUtils = eagle_strategy_utils.EagleStrategyUtils
FireflyPool = eagle_strategy_utils.FireflyPool
Firefly = eagle_strategy_utils.Firefly


def _get_parameter_config(search_space: vz.SearchSpace,
                          param_name: str) -> Optional[vz.ParameterConfig]:
  """Iterates over the search space parameters to find parameter by name."""
  for param_config in search_space.parameters:
    if param_config.name == param_name:
      return param_config


class UtilsTest(parameterized.TestCase):
  """Tests for the EagleStrategyUtuls class."""

  def setUp(self):
    super(UtilsTest, self).setUp()
    self.rng = np.random.default_rng(seed=0)
    self.search_space = vz.SearchSpace()
    root = self.search_space.root
    root.add_bool_param('b1')
    root.add_discrete_param('d1', [1.0, 2.0, 9.0, 10.0])
    root.add_float_param('f1', 0.0, 15.0, scale_type=vz.ScaleType.LINEAR)
    root.add_float_param('f2', 0.0, 10.0, scale_type=vz.ScaleType.LINEAR)
    root.add_int_param('i1', 0, 10, scale_type=vz.ScaleType.LINEAR)
    root.add_categorical_param('c1', ['a', 'b', 'c'])

    self.param_dict1 = vz.ParameterDict()
    self.param_dict1['f1'] = 5.0
    self.param_dict1['f2'] = 1.0
    self.param_dict1['c1'] = 'a'
    self.param_dict1['d1'] = 10.0
    self.param_dict1['i1'] = 3
    self.param_dict1['b1'] = 'True'

    self.param_dict2 = vz.ParameterDict()
    self.param_dict2['f1'] = 2.0
    self.param_dict2['f2'] = 8.0
    self.param_dict2['c1'] = 'b'
    self.param_dict2['d1'] = 1.0
    self.param_dict2['i1'] = 1
    self.param_dict2['b1'] = 'True'

    metric_information_max = vz.MetricInformation(
        name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
    metric_information_min = vz.MetricInformation(
        name='obj', goal=vz.ObjectiveMetricGoal.MINIMIZE)

    self.problem_max = vz.ProblemStatement(
        search_space=self.search_space,
        metric_information=[metric_information_max])
    self.problem_min = vz.ProblemStatement(
        search_space=self.search_space,
        metric_information=[metric_information_min])

    self.utils = EagleStrategyUtils(self.problem_max, FireflyAlgorithmConfig(),
                                    self.rng)

  def test_compute_pool_capacity(self):
    factor = FireflyAlgorithmConfig().pool_size_factor
    expected_capacity = 10 + round((6**factor + 6) * 0.5)
    self.assertEqual(self.utils.compute_pool_capacity(), expected_capacity)

  def test_compute_cononical_distance(self):
    dist = self.utils.compute_cononical_distance(
        self.param_dict1, self.param_dict2
    )
    self.assertIsInstance(dist, float)
    self.assertAlmostEqual(dist, 2.57)

  def test_compute_canonical_distance_squared_by_type(self):
    dists = self.utils._compute_canonical_distance_squared_by_type(
        self.param_dict1, self.param_dict2
    )
    self.assertEqual(dists[vz.ParameterType.CATEGORICAL], 1.0 + 0.0)
    self.assertEqual(dists[vz.ParameterType.INTEGER], ((3 - 1) / 10) ** 2)
    self.assertEqual(
        dists[vz.ParameterType.DOUBLE], (3.0 / 15) ** 2 + (7.0 / 10) ** 2
    )
    self.assertEqual(dists[vz.ParameterType.DISCRETE], (9.0 / 9.0) ** 2)

  def test_is_better_than(self):
    trial1 = vz.Trial(parameters=self.param_dict1)
    trial1.complete(
        vz.Measurement(
            metrics={eagle_strategy_utils.OBJECTIVE_NAME: vz.Metric(value=2.0)}
        ),
        inplace=True,
    )
    trial2 = vz.Trial(parameters=self.param_dict2)
    trial2.complete(
        vz.Measurement(
            metrics={eagle_strategy_utils.OBJECTIVE_NAME: vz.Metric(value=1.5)}
        ),
        inplace=True,
    )
    # Test for maximization problem
    self.assertTrue(self.utils.is_better_than(trial1, trial2))
    self.assertFalse(self.utils.is_better_than(trial2, trial1))
    # Test for minimization problem
    min_problem_utils = copy.deepcopy(self.utils)
    min_problem_utils.problem_statement = self.problem_min
    min_problem_utils.__attrs_post_init__()
    self.assertFalse(min_problem_utils.is_better_than(trial1, trial2))
    self.assertTrue(min_problem_utils.is_better_than(trial2, trial1))

  def test_is_better_than_infeasible(self):
    trial1 = vz.Trial(parameters=self.param_dict1)
    trial1.complete(
        vz.Measurement(metrics={'obj': vz.Metric(value=2.0)}),
        inplace=True,
        infeasibility_reason='infeasible reason',
    )
    trial2 = vz.Trial(parameters=self.param_dict2)
    trial2.complete(
        vz.Measurement(metrics={'obj': vz.Metric(value=1.5)}), inplace=True
    )
    self.assertFalse(self.utils.is_better_than(trial1, trial2))
    self.assertTrue(self.utils.is_better_than(trial2, trial1))

  def test_is_pure_categorical(self):
    pure_categorical_space = vz.SearchSpace()
    pure_categorical_space.root.add_bool_param('b1')
    pure_categorical_space.root.add_bool_param('b2')
    pure_categorical_space.root.add_categorical_param('c1', ['a', 'b'])
    pure_categorical_space.root.add_categorical_param('c2', ['a', 'b'])
    pure_categorical_space_utils = copy.deepcopy(self.utils)
    pure_categorical_space_utils._search_space = pure_categorical_space
    self.assertTrue(pure_categorical_space_utils.is_pure_categorical())
    self.assertFalse(self.utils.is_pure_categorical())

  def test_combine_two_parameters_integer(self):
    int_param_config = _get_parameter_config(self.search_space, 'i1')
    new_value = self.utils.combine_two_parameters(
        int_param_config, self.param_dict1, self.param_dict2, 0.1
    )
    self.assertEqual(new_value, round(3 * 0.1 + 1 * 0.9))

  def test_combine_two_parameters_float(self):
    float_param_config = _get_parameter_config(self.search_space, 'f1')
    new_value = self.utils.combine_two_parameters(
        float_param_config, self.param_dict1, self.param_dict2, 0.1
    )
    self.assertEqual(new_value, 5.0 * 0.1 + 2.0 * 0.9)

  def test_combine_two_parameters_discrete(self):
    float_param_config = _get_parameter_config(self.search_space, 'd1')
    new_value = self.utils.combine_two_parameters(
        float_param_config, self.param_dict1, self.param_dict2, 0.1
    )
    self.assertEqual(new_value, 2.0)

  @parameterized.named_parameters(
      dict(testcase_name='prob=0', prob=0.0, target='b'),
      dict(testcase_name='prob=1.0', prob=1.0, target='a'),
      dict(testcase_name='prob=1.5', prob=1.5, target='a'),
      dict(testcase_name='prob=-0.1', prob=-0.1, target='b'),
  )
  def test_combine_two_parameters_categorical1(self, prob, target):
    categorical_param_config = _get_parameter_config(self.search_space, 'c1')
    new_value = self.utils.combine_two_parameters(
        categorical_param_config, self.param_dict1, self.param_dict2, prob
    )
    self.assertEqual(new_value, target)

  def test_combine_two_parameters_categorical2(self):
    categorical_param_config = _get_parameter_config(self.search_space, 'c1')
    new_value = self.utils.combine_two_parameters(
        categorical_param_config, self.param_dict1, self.param_dict2, 0.5
    )
    self.assertIn(new_value, ['a', 'b'])

  @parameterized.named_parameters(
      dict(testcase_name='Above1', value=9.0, prob=0.999, target=10.0),
      dict(testcase_name='Above2', value=10.0, prob=0.999, target=10.0),
      dict(testcase_name='Below1', value=2.0, prob=-0.999, target=0.0),
      dict(testcase_name='Below2', value=0.0, prob=-0.999, target=0.0),
      dict(testcase_name='Change1', value=5.0, prob=0.212, target=7.12),
      dict(testcase_name='Change2', value=5.0, prob=-0.1, target=4.0),
      dict(testcase_name='Change3', value=2.0, prob=0.111, target=3.11),
  )
  def test_perturb_parameter_float(self, value, prob, target):
    decimal = vz.ParameterConfig.factory('f1', bounds=(0.0, 10.0))
    new_value = self.utils.perturb_parameter(decimal, value, prob)
    self.assertIsInstance(new_value, float)
    self.assertAlmostEqual(new_value, target)

  def test_perturb_parameter_categorical(self):
    categorical = vz.ParameterConfig.factory(
        'c1', feasible_values=['a', 'b', 'c']
    )
    new_value1 = self.utils.perturb_parameter(categorical, 'b', 0.2)
    self.assertIn(new_value1, ['a', 'b', 'c'])
    new_value2 = self.utils.perturb_parameter(categorical, 'b', 0.0)
    self.assertEqual(new_value2, 'b')

  @parameterized.named_parameters(
      dict(testcase_name='Above1', value=9, prob=0.999, target=10),
      dict(testcase_name='Above2', value=10, prob=0.999, target=10),
      dict(testcase_name='Below1', value=2, prob=-0.999, target=0),
      dict(testcase_name='Below2', value=0, prob=-0.999, target=0),
      dict(testcase_name='NoChange', value=2, prob=0.000001, target=2),
      dict(testcase_name='Change1', value=1, prob=0.09, target=2),
      dict(testcase_name='Change2', value=1, prob=0.51, target=6),
  )
  def test_perturb_parameter_integer(self, value, prob, target):
    integer = vz.ParameterConfig.factory('i1', bounds=(0, 10))
    new_value = self.utils.perturb_parameter(integer, value, prob)
    self.assertIsInstance(new_value, int)
    self.assertEqual(new_value, target)

  @parameterized.named_parameters(
      dict(testcase_name='Above1', value=9.0, prob=0.999, target=10.0),
      dict(testcase_name='Above2', value=10.0, prob=0.999, target=10.0),
      dict(testcase_name='Below1', value=2.0, prob=-0.999, target=1.0),
      dict(testcase_name='Below2', value=1.0, prob=-0.999, target=1.0),
      dict(testcase_name='NoChange', value=2.0, prob=0.000001, target=2.0),
      dict(testcase_name='Change', value=1.0, prob=0.09, target=2.0),
  )
  def test_perturb_parameter_discrete(self, value, prob, target):
    discrete = vz.ParameterConfig.factory(
        'd1', feasible_values=[1.0, 2.0, 9.0, 10.0]
    )
    new_value = self.utils.perturb_parameter(discrete, value, prob)
    self.assertIsInstance(new_value, float)
    self.assertEqual(new_value, target)

  def test_degrees_of_freedom(self):
    dof = self.utils._degrees_of_freedom
    self.assertEqual(dof[vz.ParameterType.DOUBLE], 2)
    self.assertEqual(dof[vz.ParameterType.CATEGORICAL], 2)
    self.assertEqual(dof[vz.ParameterType.INTEGER], 1)
    self.assertEqual(dof[vz.ParameterType.DISCRETE], 1)

  def test_replace_trial_metric_name(self):
    search_space = vz.SearchSpace()
    root = search_space.root
    root.add_float_param('f1', 0.0, 15.0, scale_type=vz.ScaleType.LINEAR)
    metric_information = vz.MetricInformation(
        name='obj123', goal=vz.ObjectiveMetricGoal.MAXIMIZE
    )
    problem = vz.ProblemStatement(
        search_space=search_space, metric_information=[metric_information]
    )

    utils = EagleStrategyUtils(problem, FireflyAlgorithmConfig(), self.rng)
    metadata = vz.Metadata()
    metadata.ns('eagle')['parent_fly_id'] = '123'
    trial = vz.Trial(parameters={'f1': 0.0}, metadata=metadata)
    trial.complete(measurement=vz.Measurement(metrics={'obj123': 1123.3}))
    new_trial = utils.standardize_trial_metric_name(trial)
    self.assertEqual(
        new_trial.final_measurement_or_die.metrics['objective'].value, 1123.3
    )
    self.assertEqual(new_trial.parameters['f1'].value, 0.0)
    self.assertEqual(new_trial.metadata.ns('eagle')['parent_fly_id'], '123')


class FireflyPoolTest(parameterized.TestCase):
  """Tests for the FireflyPool class."""

  def test_generate_new_fly_id(self):
    firefly_pool = testing.create_fake_empty_firefly_pool(capacity=2)
    self.assertEqual(firefly_pool.generate_new_fly_id(), 0)
    self.assertEqual(firefly_pool.generate_new_fly_id(), 1)
    self.assertEqual(firefly_pool.generate_new_fly_id(), 2)
    self.assertEqual(firefly_pool.generate_new_fly_id(), 3)

  def test_create_or_update_fly(self):
    # Test creating a new fly in the pool.
    firefly_pool = testing.create_fake_empty_firefly_pool()
    trial = testing.create_fake_trial(
        parent_fly_id=112, x_value=0, obj_value=0.8
    )
    firefly_pool.create_or_update_fly(trial, 112)
    self.assertEqual(firefly_pool.size, 1)
    self.assertLen(firefly_pool._pool, 1)
    self.assertIs(firefly_pool._pool[112].trial, trial)
    # Test that another trial with the same parent id updates the fly.
    trial2 = testing.create_fake_trial(
        parent_fly_id=112, x_value=1, obj_value=1.5
    )
    firefly_pool.create_or_update_fly(trial2, 112)
    self.assertEqual(firefly_pool.size, 1)
    self.assertLen(firefly_pool._pool, 1)
    self.assertIs(firefly_pool._pool[112].trial, trial2)

  @parameterized.parameters(
      {'x_values': [1, 2, 5], 'obj_values': [2, 10, -2]},
      {'x_values': [1, 2, 5], 'obj_values': [None, 10, -2]},
  )
  def test_find_closest_parent(self, x_values, obj_values):
    """Tests that the find_closest_parent method returns the closest fly (with infeasible trials)."""
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=x_values, obj_values=obj_values, capacity=4
    )
    trial = testing.create_fake_trial(
        parent_fly_id=123, x_value=4.2, obj_value=8
    )
    parent_fly = firefly_pool.find_closest_parent(trial)
    self.assertEqual(parent_fly.id_, 2)

  @parameterized.parameters(
      {'x_values': [1, 2, 5], 'obj_values': [2, 10, -2]},
      {'x_values': [1, 2, 5], 'obj_values': [None, 10, -2]},
  )
  def test_is_best_fly(self, x_values, obj_values):
    """Tests that the is_best_fly method returns true if the fly is the best fly (with infeasible trials)."""
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=x_values, obj_values=obj_values, capacity=4
    )
    self.assertTrue(firefly_pool.is_best_fly(firefly_pool._pool[1]))
    self.assertFalse(firefly_pool.is_best_fly(firefly_pool._pool[0]))
    self.assertFalse(firefly_pool.is_best_fly(firefly_pool._pool[2]))

  def test_get_next_moving_fly_copy(self):
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=[1, 2, 5], obj_values=[2, 10, -2], capacity=5
    )
    firefly_pool._last_id = 1
    moving_fly1 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly1.id_, 2)
    moving_fly2 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly2.id_, 0)
    moving_fly3 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly3.id_, 1)

  @parameterized.parameters(
      {
          'x_values': [1, 2, 5, 3],
          'obj_values': [2, 10, None, None],
          'next_fly_indices': [0, 1, 0],
      },
      {
          'x_values': [1, 2, 5, 3, 0, 4],
          'obj_values': [2, 10, None, None, 3, None],
          'next_fly_indices': [4, 0, 1, 4],
      },
  )
  def test_get_next_moving_fly_copy_with_infeasible(
      self, x_values, obj_values, next_fly_indices
  ):
    """Tests that the get_next_moving_fly_copy method doesn't return infeasible fly."""
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=x_values, obj_values=obj_values, capacity=5
    )
    firefly_pool._last_id = 1
    for index in next_fly_indices:
      moving_fly = firefly_pool.get_next_moving_fly_copy()
      self.assertEqual(moving_fly.id_, index)

  def test_get_next_moving_fly_copy_after_removing_last_id_fly(self):
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=[1, 2, 5], obj_values=[2, 10, -2], capacity=5
    )
    firefly_pool._last_id = 1
    # Remove the fly associated with `_last_id` from the pool.
    del firefly_pool._pool[1]
    moving_fly1 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly1.id_, 2)
    moving_fly2 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly2.id_, 0)
    moving_fly3 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly3.id_, 2)

  def test_get_next_moving_fly_copy_after_removing_multiple_flies(self):
    firefly_pool = testing.create_fake_populated_firefly_pool(
        x_values=[1, 2, 5, -1], obj_values=[2, 10, -2, 8], capacity=5
    )
    firefly_pool._last_id = 3
    # Remove the several flies
    del firefly_pool._pool[0]
    del firefly_pool._pool[2]
    moving_fly1 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly1.id_, 1)
    moving_fly2 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly2.id_, 3)
    moving_fly3 = firefly_pool.get_next_moving_fly_copy()
    self.assertEqual(moving_fly3.id_, 1)


def test_pool_size_with_infeasible(self):
  """Tests that the pool size doesn't change when adding an infeasible fly."""
  firefly_pool = testing.create_fake_populated_firefly_pool(
      x_values=[1, 2, 5, -1], obj_values=[2, 10, -2, 8], capacity=5
  )
  infeasible_firefly_id = firefly_pool.generate_new_fly_id()
  infeasible_trial = testing.create_fake_trial(
      parent_fly_id=infeasible_firefly_id, x_value=-1, obj_value=None
  )
  self.assertEqual(firefly_pool.size, 4)
  firefly_pool.create_or_update_fly(
      infeasible_trial, parent_fly_id=infeasible_firefly_id
  )
  self.assertEqual(firefly_pool.capacity, 5)
  # Test that adding the infeasible trial doesn't change the pool size.
  self.assertEqual(firefly_pool.size, 4)
  self.assertEqual(firefly_pool._infeasible_count, 1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/serialization.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Eagle strategy designer serialization."""

import json
from typing import Any
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy_utils
from vizier.interfaces import serializable

FireflyPool = eagle_strategy_utils.FireflyPool
Firefly = eagle_strategy_utils.Firefly
EagleStrategyUtils = eagle_strategy_utils.EagleStrategyUtils
OBJECTIVE_NAME = eagle_strategy_utils.OBJECTIVE_NAME


class PartialFireflyPoolEncoder(json.JSONEncoder):
  """Eagle strategy pool partial encoder.

  The encoder encodes the '_pool' dictionary, 'capacity', '_last_id' and
  '_max_fly_id' into a string format.

  The encoder does not store the EagleStrategyUtils as its state only depends
  on the random generator which is handled separately.
  """

  def default(self, o: Any) -> Any:
    if isinstance(o, FireflyPool):
      return {
          'capacity': o._capacity,  # pylint: disable=protected-access
          '_last_id': o._last_id,  # pylint: disable=protected-access
          '_max_fly_id': o._max_fly_id,  # pylint: disable=protected-access
          '_pool': o._pool,  # pylint: disable=protected-access
      }
    elif isinstance(o, Firefly):
      return {
          'id_': o.id_,
          'perturbation': o.perturbation,
          'generation': o.generation,
          'trial': o.trial,
      }
    elif isinstance(o, vz.Trial):
      return {
          'parameters': o.parameters.as_dict(),
          'objective': o.final_measurement.metrics[
              eagle_strategy_utils.OBJECTIVE_NAME
          ].value,
          'infeasibility_reason': o.infeasibility_reason,
      }
    else:
      return json.JSONEncoder.default(self, o)


@attr.define
class FireflyPoolDecoder:
  """Eagle strategy pool decoder.

  Fully restores the state of the FireflyPool.

  Attributes:
    utils: EagleStrategyUtils initialized with the appropriate random generator.
  """

  _utils: EagleStrategyUtils

  def decode(self, obj: Any) -> FireflyPool:
    """Decodes a string object to partial FireflyPool."""
    obj_dict = json.loads(obj)
    restored_pool = {}

    # Check that all keys appear in restored dictionary.
    missing_keys = set(['_pool', 'capacity', '_last_id', '_max_fly_id']) - set(
        obj_dict.keys())
    if missing_keys:
      raise serializable.HarmlessDecodeError(
          "Couldn't load FireflyPool from metadata. The following keys are "
          'missing: %s' % str(missing_keys))

    # Restore FireFly objects in the pool.
    for id_, fly in obj_dict['_pool'].items():
      trial = vz.Trial(parameters=fly['trial']['parameters'])
      trial.complete(
          measurement=vz.Measurement(
              metrics={'objective': fly['trial'][OBJECTIVE_NAME]}
          ),
          infeasibility_reason=fly['trial']['infeasibility_reason'],
      )
      restored_pool[int(id_)] = Firefly(
          id_=fly['id_'],
          perturbation=fly['perturbation'],
          generation=fly['generation'],
          trial=trial,
      )

    restored_capacity = int(obj_dict['capacity'])
    restored_firefly_pool = FireflyPool(
        capacity=restored_capacity, utils=self._utils
    )
    # pylint: disable=protected-access
    restored_firefly_pool._pool = restored_pool
    restored_firefly_pool._last_id = int(obj_dict['_last_id'])
    restored_firefly_pool._max_fly_id = int(obj_dict['_max_fly_id'])
    return restored_firefly_pool


def partially_serialize_firefly_pool(firefly_pool: FireflyPool) -> str:
  """Serialize parts of the FireflyPool."""
  return json.dumps(firefly_pool, cls=PartialFireflyPoolEncoder)


def restore_firefly_pool(utils: EagleStrategyUtils, obj: str) -> FireflyPool:
  """Fully restore the FireflyPool."""
  return FireflyPoolDecoder(utils).decode(obj)


def serialize_rng(rng: np.random.Generator) -> str:
  """Serialize Numpy Random Genertor."""
  return json.dumps(rng.bit_generator.state)


def restore_rng(obj: str) -> np.random.Generator:
  """Restore Numpy Random Genertor."""
  rng = np.random.default_rng()
  rng.bit_generator.state = json.loads(obj)
  return rng


--- vizier/_src/algorithms/designers/eagle_strategy/serialization_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for serialization."""

import numpy as np
from vizier._src.algorithms.designers.eagle_strategy import serialization
from vizier._src.algorithms.designers.eagle_strategy import testing

from absl.testing import absltest
from absl.testing import parameterized


class SerializationTest(parameterized.TestCase):

  @parameterized.parameters(
      {'x_values': [1, 2, 5, -2], 'obj_values': [2, 10, -2, 5]},
      {'x_values': [1, 2, 5, -2], 'obj_values': [None, 10, -2, None]},
  )
  def test_restore_pool(self, x_values, obj_values):
    firefly_pool = testing.create_fake_populated_firefly_pool(
        capacity=20, x_values=x_values, obj_values=obj_values
    )
    encoded = serialization.partially_serialize_firefly_pool(firefly_pool)
    # Restore the firefly pool.
    utils = firefly_pool._utils
    restored_firefly_pool = serialization.restore_firefly_pool(utils, encoded)
    # Check that the restored and original firefly_pool are the same.
    self.assertEqual(restored_firefly_pool._capacity, firefly_pool._capacity)
    self.assertEqual(restored_firefly_pool._last_id, firefly_pool._last_id)
    self.assertEqual(restored_firefly_pool._max_fly_id,
                     firefly_pool._max_fly_id)
    self.assertEqual(
        set(firefly_pool._pool.keys()), set(restored_firefly_pool._pool.keys()))
    for fly_id, firefly in firefly_pool._pool.items():
      restored_firefly = restored_firefly_pool._pool[fly_id]
      self.assertEqual(restored_firefly.id_, firefly.id_)
      self.assertEqual(restored_firefly.perturbation, firefly.perturbation)
      self.assertEqual(restored_firefly.generation, firefly.generation)
      self.assertEqual(restored_firefly.trial.parameters,
                       firefly.trial.parameters)
      self.assertEqual(
          restored_firefly.trial.infeasible, firefly.trial.infeasible
      )
      self.assertEqual(
          restored_firefly.trial.infeasibility_reason,
          firefly.trial._infeasibility_reason,
      )
      self.assertEqual(
          restored_firefly.trial.final_measurement_or_die.metrics[
              'objective'
          ].value,
          firefly.trial.final_measurement_or_die.metrics['objective'].value,
      )

  def test_restore_rng(self):
    rng = np.random.default_rng(0)
    serialized_rng = serialization.serialize_rng(rng)
    rand_value = rng.normal()
    restored_rng = serialization.restore_rng(serialized_rng)
    assert restored_rng.normal() == rand_value


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/eagle_strategy/testing.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Testing utils for Eagle designer."""
from typing import Optional

import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy_utils
from vizier.benchmarks import experimenters

EagleStrategyDesiger = eagle_strategy.EagleStrategyDesigner
FireflyAlgorithmConfig = eagle_strategy.FireflyAlgorithmConfig
EagleStrategyUtils = eagle_strategy_utils.EagleStrategyUtils
FireflyPool = eagle_strategy_utils.FireflyPool
Firefly = eagle_strategy_utils.Firefly


def create_fake_trial(
    parent_fly_id: int,
    x_value: float,
    obj_value: Optional[float],
) -> vz.Trial:
  """Create a fake completed trial ('obj_value' = None means infeasible trial)."""
  trial = vz.Trial()
  measurement = vz.Measurement(
      metrics={
          eagle_strategy_utils.OBJECTIVE_NAME: vz.Metric(
              value=obj_value or float('inf')
          )
      }
  )
  trial.parameters['x'] = x_value
  trial.complete(
      measurement,
      inplace=True,
      infeasibility_reason='infeasible' if obj_value is None else None,
  )
  trial.metadata.ns('eagle')['parent_fly_id'] = str(parent_fly_id)
  return trial


def create_fake_problem_statement() -> vz.ProblemStatement:
  """Create a fake problem statement."""
  problem = vz.ProblemStatement()
  problem.search_space.root.add_float_param('x', 0.0, 10.0)
  problem.metric_information.append(
      vz.MetricInformation(
          name=eagle_strategy_utils.OBJECTIVE_NAME,
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
      )
  )
  return problem


def create_fake_fly(
    parent_fly_id: int,
    x_value: float,
    obj_value: Optional[float],
) -> Firefly:
  """Create a fake firefly with a fake completed trial."""
  trial = create_fake_trial(parent_fly_id, x_value, obj_value)
  return Firefly(id_=parent_fly_id, perturbation=1.0, generation=1, trial=trial)


def create_fake_empty_firefly_pool(capacity: int = 10) -> FireflyPool:
  """Create a fake empty Firefly pool."""
  problem = create_fake_problem_statement()
  # By default incorporating infeasible trials is disabled; setting it manually.
  config = FireflyAlgorithmConfig(infeasible_force_factor=0.1)
  rng = np.random.default_rng(0)
  utils = EagleStrategyUtils(problem_statement=problem, config=config, rng=rng)
  return FireflyPool(utils, capacity)


def create_fake_populated_firefly_pool(
    *,
    capacity: int,
    x_values: Optional[list[float]] = None,
    obj_values: Optional[list[Optional[float]]] = None,
    parent_fly_ids: Optional[list[int]] = None,
) -> FireflyPool:
  """Create a fake populated Firefly pool with a given capacity."""
  firefly_pool = create_fake_empty_firefly_pool(capacity=capacity)
  rng = np.random.default_rng(0)
  if not x_values:
    x_values = [float(x) for x in rng.uniform(low=0, high=10, size=(5,))]
  if not obj_values:
    obj_values = [
        float(o) for o in rng.uniform(low=-1.5, high=1.5, size=(len(x_values),))
    ]
  if not parent_fly_ids:
    parent_fly_ids = list(range(len(obj_values)))

  if not len(obj_values) == len(x_values) == len(parent_fly_ids):
    raise ValueError('Length of obj_values, ')

  for parent_fly_id, x_value, obj_value in zip(
      parent_fly_ids, x_values, obj_values
  ):
    firefly = create_fake_fly(
        parent_fly_id=parent_fly_id, x_value=x_value, obj_value=obj_value
    )
    # pylint: disable=protected-access
    firefly_pool._pool[parent_fly_id] = firefly

  # pylint: disable=protected-access
  firefly_pool._max_fly_id = capacity
  return firefly_pool


def create_fake_empty_eagle_designer() -> EagleStrategyDesiger:
  """Create a fake empty eagle designer."""
  problem = create_fake_problem_statement()
  return EagleStrategyDesiger(problem_statement=problem)


def create_fake_populated_eagle_designer(
    *,
    x_values: Optional[list[float]] = None,
    obj_values: Optional[list[Optional[float]]] = None,
    parent_fly_ids: Optional[list[int]] = None,
) -> EagleStrategyDesiger:
  """Create a fake populated eagle designer."""
  problem = create_fake_problem_statement()
  eagle_designer = EagleStrategyDesiger(problem_statement=problem)
  # pylint: disable=protected-access
  pool_capacity = eagle_designer._firefly_pool._capacity
  # Override the eagle designer's firefly pool with a populated firefly pool.
  eagle_designer._firefly_pool = create_fake_populated_firefly_pool(
      capacity=pool_capacity,
      x_values=x_values,
      obj_values=obj_values,
      parent_fly_ids=parent_fly_ids,
  )
  return eagle_designer


def create_continuous_exptr(func, dim=6):
  problem = experimenters.bbob.DefaultBBOBProblemStatement(dim)
  rng = np.random.default_rng(0)
  shift = rng.uniform(low=-2.0, high=2.0, size=(dim,))
  return experimenters.ShiftingExperimenter(
      exptr=experimenters.NumpyExperimenter(func, problem), shift=shift
  )


def create_continuous_log_scale_exptr(func, dim=6):
  problem = experimenters.bbob.DefaultBBOBProblemStatement(
      dim,
      scale_type=vz.ScaleType.LOG,
      min_value=1e1,
      max_value=1e4,
  )
  rng = np.random.default_rng(0)
  shift = rng.uniform(low=-1e2, high=0.0, size=(dim,))
  return experimenters.ShiftingExperimenter(
      exptr=experimenters.NumpyExperimenter(func, problem), shift=shift
  )


def create_categorical_exptr(num_params: int = 9, num_feasible_values: int = 5):
  return experimenters.L1CategorialExperimenter(
      num_categories=[num_feasible_values] * num_params, verbose=True
  )


--- vizier/_src/algorithms/designers/gp/acquisitions.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Acquisition functions and builders implementations."""

from typing import Callable, Iterable, Mapping, Optional, Protocol

import chex
import equinox as eqx
from flax import struct
import jax
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.designers import scalarization
from vizier._src.jax import types
from vizier._src.jax.models import continuous_only_kernel

tfd = tfp.distributions
tfp_bo = tfp.experimental.bayesopt
tfpke = tfp.experimental.psd_kernels


class AcquisitionFunction(Protocol):
  """Acquisition function protocol."""

  # TODO: Acquisition functions should take xs as additional input.
  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    pass


class ScoreFunction(Protocol):
  """Protocol for scoring candidate points."""

  def score(self, xs: types.ModelInput, seed: jax.Array) -> jax.Array:
    pass

  def score_with_aux(
      self, xs: types.ModelInput, seed: jax.Array
  ) -> tuple[jax.Array, chex.ArrayTree]:
    pass


class Predictive(Protocol):
  """Protocol for predicting distributions given candidate points."""

  def predict_with_aux(
      self, features: types.ModelInput
  ) -> tuple[tfd.Distribution, chex.ArrayTree]:
    pass


class ScoringFunctionFactory(Protocol):

  def __call__(
      self,
      data: types.ModelData,
      predictive: Predictive,
      continuous_feasible_values: Iterable[jax.Array],
      use_trust_region: bool = False,
  ) -> ScoreFunction:
    pass


def sample_from_predictive(
    predictive: Predictive,
    xs: types.ModelInput,
    num_samples: int,
    *,
    key: jax.Array
) -> jax.Array:
  return predictive.predict_with_aux(xs)[0].sample([num_samples], seed=key)


def get_best_labels(labels: types.PaddedArray) -> jax.Array:
  """Returns the maximum values of labels.

  A note on "labels" in TFP acquisition functions: TFP acquisition functions
  (EI, PI, qEI, qUCB) take the maximum of `"observations"` (labels) over the
  rightmost axis, which is assumed to correspond to the number of observations.
  `best_labels` has a (singleton) rightmost dimension corresponding to the
  number of metrics. The shapes therefore work out correctly, although the
  semantics are different.

  Args:
    labels: Observed labels with padded shape `(num_observations, num_metrics)`.

  Returns: Maximum label values for each metric.
  """
  if jnp.size(labels.padded_array) == 0:
    return -np.inf
  return jnp.max(labels.replace_fill_value(-np.inf).padded_array, axis=-2)


def get_worst_labels(labels: types.PaddedArray) -> jax.Array:
  """Returns the minimum values of labels.

  A note on "labels" in TFP acquisition functions: TFP acquisition functions
  (EI, PI, qEI, qUCB) take the minimum of `"observations"` (labels) over the
  rightmost axis, which is assumed to correspond to the number of observations.
  `best_labels` has a (singleton) rightmost dimension corresponding to the
  number of metrics. The shapes therefore work out correctly, although the
  semantics are different.

  Args:
    labels: Observed labels with padded shape `(num_observations, num_metrics)`.

  Returns: Minimum label values for each metric.
  """
  if jnp.size(labels.padded_array) == 0:
    return np.inf
  return jnp.min(labels.replace_fill_value(np.inf).padded_array, axis=-2)


def get_reference_point(
    labels: types.PaddedArray, scale: float = 0.1
) -> jax.Array:
  """Returns the reference point for hypervolume computations.

  Reference point is generally set to nadir - 0.1 * range. See [Ishibuchi2011]
  find 0.1 to be a robust multiplier for scaling the nadir point.

  Args:
    labels: Observed labels with padded shape `(num_observations, num_metrics)`.
    scale: The scaling factor for the range.

  Returns: Reference point.
  """
  best_labels = get_best_labels(labels)
  worst_labels = get_worst_labels(labels)
  labels_range = best_labels - worst_labels
  return worst_labels - scale * labels_range


def _apply_trust_region(
    region: 'TrustRegion',
    xs: types.ModelInput,
    acquisition: jax.Array,
    pred: tfd.Distribution,
    aux: chex.ArrayTree,
) -> tuple[jax.Array, chex.ArrayTree]:
  """Applies the trust region to acquisition values."""
  distance = region.min_linf_distance(xs)
  raw_acquisition = acquisition
  acquisition = jnp.where(
      ((distance <= region.trust_radius) | (region.trust_radius > 0.5)),
      acquisition,
      -1e4 - distance,
  )
  aux = aux | {
      'mean': pred.mean(),
      'stddev': pred.stddev(),
      'raw_acquisition': raw_acquisition,
      'linf_distance': distance,
      'radius': jnp.ones_like(distance) * region.trust_radius,
  }
  return acquisition, aux


class BayesianScoringFunction(eqx.Module):
  """Combines `Predictive` with acquisition function."""

  predictor: Predictive
  acquisition_fn: AcquisitionFunction

  # TODO: This should be moved out of here.
  # If set, uses trust region.
  trust_region: Optional['TrustRegion']

  def score(self, xs, seed: jax.Array) -> jax.Array:
    return self.score_with_aux(xs, seed)[0]

  def score_with_aux(
      self, xs, seed: jax.Array
  ) -> tuple[jax.Array, chex.ArrayTree]:
    jax.monitoring.record_event(
        '/vizier/jax/acquisitions/bayesian_scoring_function/score_with_aux/traced'
    )
    pred, aux = self.predictor.predict_with_aux(xs)

    acquisition = self.acquisition_fn(pred, seed=seed)
    if self.trust_region is not None:
      acquisition, aux = _apply_trust_region(
          self.trust_region,
          xs,
          acquisition,
          pred,
          aux,
      )
    return acquisition, aux


# Vizier library acquisition functions use `flax.struct`, instead of `attrs` and
# a hash function, so that acquisition functions can be passed as args to JIT-ed
# functions without triggering retracing when attribute values change.
@struct.dataclass
class UCB(AcquisitionFunction):
  """UCB AcquisitionFunction."""

  coefficient: float = 1.8

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    del seed
    return dist.mean() + self.coefficient * dist.stddev()


@struct.dataclass
class LCB(AcquisitionFunction):
  """LCB AcquisitionFunction."""

  coefficient: float = 1.8

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    del seed
    return dist.mean() - self.coefficient * dist.stddev()


@struct.dataclass
class EI(AcquisitionFunction):
  """Expected Improvement acquisition function."""

  best_labels: jax.Array

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    del seed
    return tfp_bo.acquisition.GaussianProcessExpectedImprovement(
        dist, observations=self.best_labels
    )()


@struct.dataclass
class PI(AcquisitionFunction):
  """Probability of Improvement acquisition function."""

  best_labels: jax.Array

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    del seed
    return tfp_bo.acquisition.GaussianProcessProbabilityOfImprovement(
        dist, observations=self.best_labels
    )()


@struct.dataclass
class Sample(AcquisitionFunction):
  """Sample AcquisitionFunction."""

  num_samples: int = 1000

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    if seed is None:
      seed = jax.random.PRNGKey(0)
    return dist.sample([self.num_samples], seed=seed)


class MaxValueEntropySearch(eqx.Module):
  """MES score function. Implements the `ScoreFunction` protocol."""

  tfp_mes: tfp_bo.acquisition.GaussianProcessMaxValueEntropySearch
  trust_region: Optional['TrustRegion']

  @classmethod
  def scoring_fn_factory(
      cls,
      data: types.ModelData,
      predictive: Predictive,
      continuous_feasible_values: Iterable[jax.Array],
      use_trust_region: bool = False,
  ) -> 'MaxValueEntropySearch':
    """Builds a MaxValueEntropySearch scoring function."""
    if data.features.categorical.shape[-1] > 0:
      raise ValueError('Categorical data is not supported.')
    pred_gp = predictive.predict_with_aux(data.features)[0]
    continuous_gp = pred_gp.copy(
        kernel=continuous_only_kernel.EmptyCategoricalKernel(pred_gp.kernel),
        observation_index_points=(pred_gp.observation_index_points.continuous),
        index_points=jnp.zeros_like(
            pred_gp.index_points.continuous,
            shape=(0,) + pred_gp.index_points.continuous.shape[1:],
        ),
        # pylint: disable=protected-access
        _conditional_kernel=continuous_only_kernel.EmptyCategoricalKernel(
            pred_gp._conditional_kernel
        ),
        _conditional_mean_fn=lambda x: pred_gp._conditional_mean_fn(  # pylint: disable=g-long-lambda
            continuous_only_kernel._continuous_to_cacv(x)
        ),
        # pylint: enable=protected-access
    )

    observations = pred_gp.observations
    # Allow the score function to build with no observations.
    if not observations.size:
      observations = jnp.asarray([-jnp.inf])
    tfp_mes = tfp_bo.acquisition.GaussianProcessMaxValueEntropySearch(
        predictive_distribution=continuous_gp,
        observations=observations,
        # TODO: Finalize API and plumb seed through.
        seed=jax.random.PRNGKey(123),
        num_max_value_samples=100,
    )
    trust_region = (
        TrustRegion(data.features, continuous_feasible_values)
        if use_trust_region
        else None
    )
    return cls(tfp_mes, trust_region=trust_region)

  def score(self, xs: types.ModelInput, seed: jax.Array) -> jax.Array:
    return self.score_with_aux(xs, seed)[0]

  def score_with_aux(
      self, xs: types.ModelInput, seed: jax.Array
  ) -> tuple[jax.Array, chex.ArrayTree]:
    del seed
    if xs.categorical.shape[-1] > 0:
      raise ValueError('Categorical data is not supported.')
    acquisition = self.tfp_mes(index_points=xs.continuous.padded_array)
    aux = {}
    if self.trust_region is not None:
      acquisition, aux = _apply_trust_region(
          self.trust_region,
          xs,
          acquisition,
          self.tfp_mes.predictive_distribution,
          aux,
      )
    return acquisition, aux


def bayesian_scoring_function_factory(
    acquisition_fn_factory: Callable[[types.ModelData], AcquisitionFunction],
) -> ScoringFunctionFactory:
  """Builds a ScoringFunctionFactory."""

  def f(
      data: types.ModelData,
      predictive: Predictive,
      continuous_feasible_values: Iterable[jax.Array],
      use_trust_region: bool = False,
  ) -> ScoreFunction:
    acquisition_fn = acquisition_fn_factory(data)
    trust_region = (
        TrustRegion(data.features, continuous_feasible_values)
        if use_trust_region
        else None
    )
    return BayesianScoringFunction(predictive, acquisition_fn, trust_region)

  return f


@struct.dataclass
class AcquisitionTrustRegion(AcquisitionFunction):
  """Acquisition masked by a thresholding acquisition values.

  When optimizing the main acquisition, the goal is to consider idea is to
  disregard the region in the search space with
  thresholding_acquisition values lower than a threshold.
  Attributes:
    main_acquisition: The main acquisition function.
    thresholding_acquisition: The acquisition function used to detect promising
      (trust) regions.
    bad_acq_value: The lower bound for the main acquisition value.
    labels:
    threshold: The threshold for the thresholding_acquisition values to
      distinguish between the promising and unpromising regions.
    apply_tr_after: The minimum number of labels required to apply the trust
      region.
  """

  main_acquisition: AcquisitionFunction
  thresholding_acquisition: AcquisitionFunction
  bad_acq_value: float = struct.field(kw_only=True)
  labels: Optional[types.PaddedArray] = struct.field(kw_only=True)
  threshold: Optional[float] = struct.field(kw_only=True, default=None)
  apply_tr_after: Optional[int] = struct.field(kw_only=True, default=0)

  # TODO: Move factories to `vza_designer_factory.py`.
  @classmethod
  def default_ucb_pi(cls, data: types.ModelData) -> 'AcquisitionTrustRegion':
    best_labels = get_best_labels(data.labels)
    return cls(
        UCB(1.8),
        PI(best_labels),
        bad_acq_value=-1e4,
        labels=data.labels,
        threshold=0.3,
        apply_tr_after=0,
    )

  @classmethod
  def default_ucb_lcb(cls, data: types.ModelData) -> 'AcquisitionTrustRegion':
    return cls(
        UCB(1.8),
        LCB(1.8),
        labels=data.labels,
        bad_acq_value=-1e4,
        threshold=None,
        apply_tr_after=0,
    )

  @classmethod
  def default_ucb_lcb_wide(
      cls, data: types.ModelData
  ) -> 'AcquisitionTrustRegion':
    return cls(
        UCB(1.8),
        LCB(2.5),
        labels=data.labels,
        bad_acq_value=-1e4,
        threshold=None,
        apply_tr_after=0,
    )

  @classmethod
  def default_ucb_lcb_delay_tr(
      cls, data: types.ModelData
  ) -> 'AcquisitionTrustRegion':
    return cls(
        UCB(1.8),
        LCB(1.8),
        labels=data.labels,
        bad_acq_value=-1e4,
        threshold=None,
        apply_tr_after=5,
    )

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    del seed
    threshold_values = self.thresholding_acquisition(dist)
    acq_values = self.main_acquisition(dist)

    # TODO: Refactor so that the following is handled in the factory
    # instead.
    threshold = -jnp.inf
    apply_tr = False
    if self.labels is not None:
      labels_padded = self.labels.replace_fill_value(np.nan).padded_array
      threshold = jnp.minimum(
          jnp.nanmean(labels_padded), jnp.nanmedian(labels_padded)
      )
      apply_tr = self.labels._original_shape[0] <= self.apply_tr_after
    if self.threshold is not None:
      threshold = self.threshold
    cond = jnp.isnan(threshold) | (threshold_values >= threshold) | apply_tr
    return jnp.where(
        cond,
        acq_values,
        self.bad_acq_value - threshold_values,
    )


@struct.dataclass
class QEI(AcquisitionFunction):
  """Sampling-based batch expected improvement."""

  best_labels: jax.Array
  num_samples: int = 100

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    if seed is None:
      raise ValueError('QEI requires a value for `seed`.')
    return tfp_bo.acquisition.ParallelExpectedImprovement(
        dist,
        observations=self.best_labels,
        seed=seed,
        num_samples=self.num_samples,
    )()


@struct.dataclass
class QPI(AcquisitionFunction):
  """Sampling-based batch probability of improvement."""

  best_labels: jax.Array
  num_samples: int = 100

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    if seed is None:
      raise ValueError('QPI requires a value for `seed`.')
    return tfp_bo.acquisition.ParallelProbabilityOfImprovement(
        dist,
        observations=self.best_labels,
        seed=seed,
        num_samples=self.num_samples,
    )()


@struct.dataclass
class QUCB(AcquisitionFunction):
  """Sampling-based batch upper confidence bound.

  Attributes:
    coefficient: UCB coefficient. For a Gaussian distribution, note that
      `UCB(coefficient=c)` is equivalent to `QUCB(coefficient=c * sqrt(pi / 2))`
      if QUCB batch size is 1. See the TensorFlow Probability docs for more
      details:
      https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/bayesopt/acquisition/ParallelUpperConfidenceBound
    num_samples: Number of distribution samples used to compute qUCB.
  """

  coefficient: float = 1.8
  num_samples: int = 100

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    if seed is None:
      raise ValueError('QEI requires a value for `seed`.')
    return tfp_bo.acquisition.ParallelUpperConfidenceBound(
        dist,
        seed=seed,
        exploration=self.coefficient,
        num_samples=self.num_samples,
        observations=None,
    )()


def create_hv_scalarization(
    num_scalarizations: int, labels: types.PaddedArray, rng: jax.Array
):
  """Creates a HyperVolumeScalarization with random weights.

  Args:
    num_scalarizations: The number of scalarizations to create.
    labels: The labels used to create the reference point.
    rng: The random key to use for sampling the weights.

  Returns:
    A HyperVolumeScalarization with random weights.
  """
  weights = jax.random.normal(
      rng,
      shape=(num_scalarizations, labels.shape[1]),
  )
  weights = jnp.abs(weights)
  weights = weights / jnp.linalg.norm(weights, axis=-1, keepdims=True)
  ref_point = (
      get_reference_point(labels, scale=0.01) if labels.shape[0] > 0 else None
  )
  return scalarization.HyperVolumeScalarization(weights, ref_point)


# TODO: What do we end up jitting? If we end up directly jitting this call
# then we should make it `eqx.Module` and set
# `reduction_fn=eqx.field(static=True)` instead.
@struct.dataclass
class ScalarizeOverAcquisitions(AcquisitionFunction):
  """Wrapper that scalarizes a vectorized acquisition function."""

  acquisition_fn: AcquisitionFunction
  scalarizer: scalarization.Scalarization
  reduction_fn: Callable[[jax.Array], jax.Array] = struct.field(
      pytree_node=False, default=lambda x: x
  )
  max_scalarized: Optional[jax.Array] = struct.field(
      pytree_node=False, default=None
  )

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    scalarized = self.scalarizer(self.acquisition_fn(dist, seed).squeeze())
    # Broadcast max_scalarized to the same shape as scalarized and take max.
    if self.max_scalarized is not None:
      shape_mismatch = len(scalarized.shape) - len(self.max_scalarized.shape)
      expand_max = jnp.expand_dims(
          self.max_scalarized, axis=range(-shape_mismatch, 0)
      )
      scalarized = jnp.maximum(scalarized, expand_max)
    return self.reduction_fn(scalarized)


@struct.dataclass
class AcquisitionOverScalarized(AcquisitionFunction):
  """Wrapper that applies acquisition over a scalarized distribution."""

  acquisition_fn: AcquisitionFunction
  scalarizer: scalarization.Scalarization
  max_scalarized: Optional[jax.Array] = struct.field(
      pytree_node=False, default=None
  )
  num_samples: int = 1000

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    # S = num_samples, M = num_objs, W = num_scalarized
    samples = dist.sample(self.num_samples, seed=seed).squeeze()  # [S, ..., M]
    scalarized_samples = self.scalarizer(samples)  # [W, S, ...]
    # Transpose to move sample dim out. Treat all inner dims as events.
    dist_samples = jnp.swapaxes(scalarized_samples, 0, 1)  # [S, W, ...]
    # Event shape is [W, ...].
    scalar_dist = tfd.Empirical(
        dist_samples, event_ndims=len(dist_samples.shape) - 1
    )

    acq_score = self.acquisition_fn(scalar_dist, seed)  # [W, ...]
    # Broadcast max_scalarized to the same shape as scalarized and take max.
    if self.max_scalarized is not None:
      shape_mismatch = len(acq_score.shape) - len(self.max_scalarized.shape)
      expand_max = jnp.expand_dims(
          self.max_scalarized, axis=range(-shape_mismatch, 0)
      )
      acq_score = jnp.maximum(acq_score, expand_max)
    return jnp.mean(acq_score, axis=0)  # [...], removed W-axis.


@struct.dataclass
class MultiAcquisitionFunction(AcquisitionFunction):
  """Wrapper that calls multiple acquisition functions."""

  acquisition_fns: Mapping[str, AcquisitionFunction]

  def __call__(
      self,
      dist: tfd.Distribution,
      seed: Optional[jax.Array] = None,
  ) -> jax.Array:
    acquisitions = []
    if seed is None:
      seeds = [None] * len(self.acquisition_fns)
    else:
      seeds = jax.random.split(seed, num=len(self.acquisition_fns))
    for i, acquisition_fn in enumerate(self.acquisition_fns.values()):
      acquisitions.append(acquisition_fn(dist, seeds[i]))
    # TODO: Change the return type to a dict with the same
    # structure as `acquisition_fns` to clarify the meaning of the return
    # values.
    return jnp.stack(acquisitions, axis=0)


# TODO: Support categoricals.
# TODO: Support custom distances.
class TrustRegion(eqx.Module):
  """L-inf norm based TrustRegion.

  Limits the suggestion within the union of small L-inf norm balls around each
  of the trusted points, which are in most cases observed points. The radius
  of the L-inf norm ball grows in the number of observed points. Discrete
  parameters that have large gaps in their feasible values are ignored from the
  L-inf norm computation and the trust region computation to ensure all feasible
  values can be explored.

  Some hard-coded constants, e.g. min_radius, are determined based on the
  assumption that all points are in the unit hypercube after embedding into
  scaled space.

  The trust region can be used e.g. during acquisition optimization:
    converter = converters.TrialToModelInputConverter.from_problem(
        problem,
        scale=True,
        max_discrete_indices=0,
        flip_sign_for_minimization_metrics=True,
    )
    data = converter.to_xy(trials)
    tr = TrustRegion(data.features, converter.continuous_feasible_values)
    # xs is a point in the search space.
    distance = tr.min_linf_distance(xs)
    if distance <= tr.trust_radius:
      print('xs in trust region')
  """

  trusted: types.ModelInput
  # A list of feasible values for each continuified parameter. An empty array
  # means that the parameter is continuous and all values within the its range
  # are feasible.
  continuous_feasible_values: Iterable[jax.Array]
  # A list of booleans indicating whether each continuous dimension should be
  # used for trust region computation. True means that the dimension should be
  # used. False means that the dimension should be ignored. This attribute is
  # initialized in `__post_init__` using `continuous_feasible_values`, and not
  # to be set by the user.
  _continuous_dimensions_mask: list[bool] = eqx.field(
      static=True, default_factory=list
  )

  def __post_init__(self):
    for feasible_values in self.continuous_feasible_values:
      if feasible_values.size == 0:
        # Continuous dimension, always included in the trust region computation.
        self._continuous_dimensions_mask.append(True)
      elif feasible_values.size == 1:
        # Discrete dimension with a single feasible value, always excluded from
        # the trust region computation to avoid undesirable behavior when the
        # trusted trials were generated with a larger set of feasible values
        # for this dimension from an old study config.
        self._continuous_dimensions_mask.append(False)
      else:
        sorted_feasible_values = jnp.sort(feasible_values)
        self._continuous_dimensions_mask.append(
            bool(jnp.max(jnp.diff(sorted_feasible_values)) <= self.min_radius)
        )

  @property
  def min_radius(self) -> float:
    """Minimum radius of the trust region. Hyperparameter."""
    return 0.2

  @property
  def trust_radius(self) -> jax.Array:
    # TODO: Make hyperparameters configurable.
    dimension_factor = 5.0  # Hyperparameter

    continuous_dof = jnp.sum(jnp.asarray(self._continuous_dimensions_mask))

    # pylint: disable=protected-access
    categorical_dof = self.trusted.categorical._original_shape[-1]
    dof = continuous_dof + categorical_dof
    # pylint: enable=protected-access
    num_obs = jnp.sum(~self.trusted.continuous.is_missing[0])
    # TODO: Discount the infeasible points. The 0.1 and 0.9 split
    # is associated with weights to feasible and infeasbile trials.
    original_num_obs = 0.1 * num_obs + 0.9 * num_obs
    # `trust_level` can be an arbitrarily large positive float.
    trust_level = original_num_obs / (dimension_factor * (dof + 1))
    return jnp.where(
        num_obs == 0,
        1.0,
        self.min_radius + (0.5 - self.min_radius) * trust_level,
    )

  def min_linf_distance(self, xs: types.ModelInput) -> jax.Array:
    """l-inf norm distance to the closest trusted point.

    Caps distances between one-hot encoded features to the trust-region radius,
    so that the trust region cutoff does not discourage exploration of these
    features.

    Args:
      xs: (M_0, M_1, ..., D) array where each element is in [0, 1]. The leading
        axes are the batch dimensions, and the last axis is the feature
        dimension.

    Returns:
      (M_0, M_1, ...) array of floating numbers that has one fewer axis than
      `xs`: L-infinity distances to the nearest trusted point.
    """
    trusted = self.trusted.continuous.replace_fill_value(
        0.0
    ).padded_array  # (N, D)
    xs = xs.continuous.replace_fill_value(0.0).padded_array
    distances = jnp.abs(
        trusted - xs[..., jnp.newaxis, :]
    )  # (M_0, M_1, ..,, N, D)
    padded_dimension_mask = types.PaddedArray.from_array(
        jnp.asarray(self._continuous_dimensions_mask),
        target_shape=[trusted.shape[-1]],
        fill_value=False,
    ).padded_array
    # Drop the dimensions that are not to be used for trust region by setting
    # their distances to 0.
    distances = jnp.where(padded_dimension_mask, distances, 0.0)
    # Mask out padded features. We set these distances to infinite since
    # they should never be considered.
    distances = jnp.where(
        self.trusted.continuous.is_missing[0][..., jnp.newaxis],
        np.inf,
        distances,
    )
    if distances.size == 0:
      return -np.inf * jnp.ones_like(xs, shape=xs.shape[:-1])
    linf_distance = jnp.max(distances, axis=-1)  # (M_0, M_1, ..., N)
    return jnp.min(linf_distance, axis=-1)  # (M_0, M_1, ...)


--- vizier/_src/algorithms/designers/gp/acquisitions_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for acquisitions."""

import jax
from jax import config
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.designers.gp import acquisitions
from vizier._src.jax import types
from absl.testing import absltest
from absl.testing import parameterized


tfd = tfp.distributions
tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


def _make_test_model_data(labels_array, num_categorical=1):
  labels = types.PaddedArray.as_padded(labels_array)
  features = types.ModelInput(
      continuous=types.PaddedArray.as_padded(
          jnp.zeros((labels_array.shape[0], 3), dtype=jnp.float64)
      ),
      categorical=types.PaddedArray.as_padded(
          jnp.zeros(
              (labels_array.shape[0], num_categorical), dtype=types.INT_DTYPE
          ),
      ),
  )
  return types.ModelData(features=features, labels=labels)


class AcquisitionsTest(absltest.TestCase):

  def test_ucb(self):
    acq = acquisitions.UCB(coefficient=2.0)
    self.assertAlmostEqual(acq(tfd.Normal(0.1, 1)), 2.1)

  def test_lcb(self):
    acq = acquisitions.LCB(coefficient=2.0)
    self.assertAlmostEqual(acq(tfd.Normal(0.1, 1)), -1.9)

  def test_ei(self):
    labels = types.PaddedArray.as_padded(jnp.array([[0.2]]))
    best_labels = acquisitions.get_best_labels(labels)
    acq = acquisitions.EI(best_labels)
    self.assertAlmostEqual(
        acq(
            tfd.Normal(jnp.float64(0.1), 1),
        ),
        0.34635347,
    )

  def test_scalarized_ucb(self):
    labels = types.PaddedArray.as_padded(
        jnp.array([[0.2, 0.3], [0.01, 0.5], [0.5, 0.01]])
    )
    reference_point = acquisitions.get_worst_labels(labels)
    ucb = acquisitions.UCB(coefficient=0.1)
    scalarizer = scalarization.HyperVolumeScalarization(
        weights=jnp.array([0.1, 0.2]), reference_point=reference_point
    )

    acq = acquisitions.ScalarizeOverAcquisitions(ucb, scalarizer)
    self.assertAlmostEqual(
        acq(tfd.Normal([0.1, 0.2], [0.1, 0.1])), jnp.array([1.0]), delta=1e-2
    )

    # Tests that the scalarized acquisition is larger with max_scalarized.
    scalarized_labels = scalarizer(labels.unpad())
    max_scalarized = jnp.max(scalarized_labels, axis=-1)
    acq = acquisitions.ScalarizeOverAcquisitions(
        ucb, scalarizer, max_scalarized=max_scalarized
    )
    self.assertAlmostEqual(
        acq(tfd.Normal([0.1, 0.2], [0.1, 0.1])), jnp.array([2.10]), delta=1e-2
    )

  def test_ehvi_approximation(self):
    num_obj = 2
    num_scalarizations = 1000
    weights = jax.random.normal(
        jax.random.PRNGKey(0), shape=(num_scalarizations, num_obj)
    )
    weights = jnp.abs(weights)
    weights = weights / jnp.linalg.norm(weights, axis=1, keepdims=True)
    scalarizer = scalarization.HyperVolumeScalarization(weights)

    # Tests that the scalarizer gives the approximate hypervolume with mean
    # and uses constant rescaling of pi/4 for num_objs=2.
    hypervolume = acquisitions.ScalarizeOverAcquisitions(
        acquisitions.UCB(coefficient=0.0),
        scalarizer,
        reduction_fn=lambda x: jnp.mean(x, axis=0),
        max_scalarized=jnp.zeros(shape=(num_scalarizations,)),
    )
    # Expected hypervolume should be 2 * 1.5 = 3.0.
    np.testing.assert_allclose(
        hypervolume(tfd.Normal(jnp.array([2, 1.5]), jnp.ones(num_obj)))
        * (3.1415)
        / 4.0,
        jnp.array([3.0]),
        rtol=1e-1,
    )

  def test_ehvi_approximation_aq_over_scalar(self):
    num_obj = 2
    num_scalarizations = 1000
    weights = jax.random.normal(
        jax.random.PRNGKey(0), shape=(num_scalarizations, num_obj)
    )
    weights = jnp.abs(weights)
    weights = weights / jnp.linalg.norm(weights, axis=1, keepdims=True)
    scalarizer = scalarization.HyperVolumeScalarization(weights)

    # Tests that the scalarizer gives the approximate hypervolume with mean
    # and uses constant rescaling of pi/4 for num_objs=2.
    hypervolume = acquisitions.AcquisitionOverScalarized(
        acquisitions.UCB(coefficient=0.0), scalarizer
    )
    # Expected hypervolume should be 2 * 1.5 = 3.0.
    dist = tfd.Normal(jnp.array([2, 1.5]), jnp.ones(num_obj))
    np.testing.assert_allclose(
        hypervolume(dist, jax.random.PRNGKey(0)) * 3.1415 / 4.0,
        jnp.array([3.0]),
        rtol=1e-1,
    )

  def test_ehvi_mcmc(self):
    num_obj = 2
    num_scalarizations = 1000
    weights = jax.random.normal(
        jax.random.PRNGKey(0), shape=(num_scalarizations, num_obj)
    )
    weights = jnp.abs(weights)
    weights = weights / jnp.linalg.norm(weights, axis=1, keepdims=True)
    scalarizer = scalarization.HyperVolumeScalarization(weights)

    # Tests that the scalarizer gives the approximate hypervolume with mean
    # and uses constant rescaling of pi/4 for num_objs=2.
    hypervolume = acquisitions.ScalarizeOverAcquisitions(
        acquisitions.Sample(num_samples=100),
        scalarizer,
        reduction_fn=lambda x: jnp.mean(jax.nn.relu(x)),
    )
    # Expected hypervolume should be close to 2 * 1.5 = 3.0.
    stddev = 0.01 * jnp.ones(num_obj)
    np.testing.assert_allclose(
        hypervolume(tfd.Normal(jnp.array([2, 1.5]), stddev)) * (3.1415) / 4.0,
        jnp.array([3.0]),
        rtol=1e-1,
    )

  def test_pi(self):
    labels = types.PaddedArray.as_padded(jnp.array([[0.2]]))
    best_labels = acquisitions.get_best_labels(labels)
    acq = acquisitions.PI(best_labels)
    self.assertAlmostEqual(
        acq(
            tfd.Normal(jnp.float64(0.1), 1),
        ),
        0.46017216,
    )

  def test_max_value_entropy_search(self):
    num_obs = 10
    num_pred = 6
    labels = np.random.normal(size=([num_obs, 1]))
    data = _make_test_model_data(labels, num_categorical=0)
    init_features = types.ModelInput(
        continuous=types.PaddedArray.as_padded(
            jnp.ones((num_pred, 3), dtype=jnp.float64)
        ),
        categorical=types.PaddedArray.as_padded(
            jnp.zeros((num_pred, 0), dtype=types.INT_DTYPE)
        ),
    )

    class _TestPredictive:

      def predict_with_aux(self, x):
        gp = tfd.GaussianProcess(
            kernel=tfpke.FeatureScaledWithCategorical(
                kernel=tfpk.ExponentiatedQuadratic(),
                scale_diag=tfpke.ContinuousAndCategoricalValues(
                    continuous=jnp.ones([3]), categorical=jnp.ones([0])
                ),
            ),
            index_points=tfpke.ContinuousAndCategoricalValues(
                continuous=data.features.continuous.padded_array,
                categorical=data.features.categorical.padded_array,
            ),
            observation_noise_variance=np.float64(1.0),
        )
        return (
            gp.posterior_predictive(
                observations=data.labels.padded_array[:, 0],
                predictive_index_points=tfpke.ContinuousAndCategoricalValues(
                    x.continuous.padded_array, x.categorical.padded_array
                ),
            ),
            {},
        )

    score_fn = acquisitions.MaxValueEntropySearch.scoring_fn_factory(
        data=data,
        predictive=_TestPredictive(),
        continuous_feasible_values=[
            jnp.array([]),
            jnp.array([]),
            jnp.array([]),
        ],
        use_trust_region=True,
    )
    score = score_fn.score(init_features, seed=jax.random.PRNGKey(0))
    self.assertEqual(score.shape, (num_pred,))

  def test_acq_pi_tr_good_point(self):
    data = _make_test_model_data((jnp.array([[100.0]])))
    acq = acquisitions.AcquisitionTrustRegion.default_ucb_pi(data=data)
    self.assertAlmostEqual(
        acq(tfd.Normal(jnp.float64(0.1), 1)),
        -1e4,
    )

  def test_acq_pi_tr_bad_point(self):
    data = _make_test_model_data((jnp.array([[-100.0]])))
    acq = acquisitions.AcquisitionTrustRegion.default_ucb_pi(data=data)
    self.assertAlmostEqual(
        acq(tfd.Normal(jnp.float64(0.1), 1)),
        1.9,
    )

  def test_acq_lcb_tr_bad_point(self):
    data = _make_test_model_data((jnp.array([[100.0]])))
    acq = acquisitions.AcquisitionTrustRegion.default_ucb_lcb(data=data)
    self.assertAlmostEqual(
        acq(tfd.Normal(jnp.float64(0.1), 1)),
        jnp.array([-1e4]),
        delta=2.0,
    )

  def test_acq_lcb_tr_good_point(self):
    data = _make_test_model_data((jnp.array([[-100.0]])))
    acq = acquisitions.AcquisitionTrustRegion.default_ucb_lcb(data=data)
    self.assertAlmostEqual(
        acq(tfd.Normal(jnp.float64(0.1), 1)),
        1.9,
    )

  def test_acq_lcb_delay_tr(self):
    data = _make_test_model_data((jnp.array([[100.0], [-100.0]])))
    acq = acquisitions.AcquisitionTrustRegion.default_ucb_lcb_delay_tr(
        data=data
    )
    self.assertAlmostEqual(
        acq(tfd.Normal(jnp.float64(0.1), 1)),
        jnp.array([1.9]),
    )

  def test_qei(self):
    best_labels = jnp.array([0.2])
    acq = acquisitions.QEI(num_samples=2000, best_labels=best_labels)
    batch_shape = [6]
    dist = tfd.Normal(loc=0.1 * jnp.ones(batch_shape), scale=1.0)
    qei = acq(dist, seed=jax.random.PRNGKey(0))
    # QEI reduces over the batch shape.
    self.assertEmpty(qei.shape)

    dist_single_point = tfd.Normal(jnp.array([0.1], dtype=jnp.float64), 1)
    qei_single_point = acq(dist_single_point, seed=jax.random.PRNGKey(0))
    # Parallel matches non-parallel for a single point.
    np.testing.assert_allclose(qei_single_point, 0.346, atol=1e-2)
    self.assertEmpty(qei_single_point.shape)

  def test_qpi(self):
    best_labels = jnp.array([0.2])
    acq = acquisitions.QPI(num_samples=5000, best_labels=best_labels)
    batch_shape = [6]
    dist = tfd.Normal(loc=0.1 * jnp.ones(batch_shape), scale=1.0)
    qpi = acq(dist, seed=jax.random.PRNGKey(0))
    # QPI reduces over the batch shape.
    self.assertEmpty(qpi.shape)

    dist_single_point = tfd.Normal(jnp.array([0.1], dtype=jnp.float64), 1)
    qpi_single_point = acq(dist_single_point, seed=jax.random.PRNGKey(0))
    # Parallel matches non-parallel for a single point.
    pi_single_point = acquisitions.PI(best_labels=best_labels)(
        dist_single_point
    )
    np.testing.assert_allclose(qpi_single_point, pi_single_point[0], atol=1e-2)
    self.assertEmpty(qpi_single_point.shape)

  def test_qucb_shape(self):
    acq = acquisitions.QUCB()
    batch_shape = [6]
    dist = tfd.Normal(loc=0.1 * jnp.ones(batch_shape), scale=1.0)
    qucb = acq(dist, seed=jax.random.PRNGKey(0))
    # QUCB reduces over the batch shape.
    self.assertEmpty(qucb.shape)

  def test_qucb_equals_ucb(self):
    # The QUCB coefficient should be multiplied by sqrt(pi/2) for equivalency
    # with the UCB coefficient (assuming a Gaussian distribution).
    acq_ucb = acquisitions.UCB(coefficient=0.5)
    acq_qucb = acquisitions.QUCB(
        num_samples=5000,
        coefficient=0.5 * np.sqrt(np.pi / 2.0),
    )
    dist_single_point = tfd.Normal(jnp.array([0.1], dtype=jnp.float64), 1)
    qucb_single_point = acq_qucb(dist_single_point, seed=jax.random.PRNGKey(1))
    ucb_single_point = acq_ucb(dist_single_point)
    # Parallel matches non-parallel for a single point.
    np.testing.assert_allclose(
        qucb_single_point, ucb_single_point[0], atol=2e-2
    )
    self.assertEmpty(qucb_single_point.shape)

  def test_multi_acquisition(self):
    best_labels = jnp.array([0.2])
    ucb = acquisitions.UCB()
    ei = acquisitions.EI(best_labels=best_labels)
    acq = acquisitions.MultiAcquisitionFunction({'ucb': ucb, 'ei': ei})
    dist = tfd.Normal(jnp.float64(0.1), 1)
    acq_val = acq(dist)
    ucb_val = ucb(dist)
    ei_val = ei(dist)
    np.testing.assert_allclose(acq_val, jnp.stack([ucb_val, ei_val]))


class TrustRegionTest(parameterized.TestCase):

  def test_trust_region_small(self):
    trusted = types.ModelInput(
        continuous=types.PaddedArray.as_padded(
            np.array([[0.0, 0.0], [1.0, 1.0]]),
        ),
        categorical=types.PaddedArray.as_padded(np.array([[0, 0], [1, 1]])),
    )
    tr = acquisitions.TrustRegion(
        trusted=trusted,
        continuous_feasible_values=[jnp.array([]), jnp.array([])],
    )

    xs = types.ModelInput(
        continuous=types.PaddedArray.as_padded(
            np.array([
                [0.0, 0.3],
                [0.9, 0.8],
                [1.0, 1.0],
            ]),
        ),
        categorical=types.PaddedArray.as_padded(
            np.array([[0, 0], [1, 1], [0, 1]]),
        ),
    )
    np.testing.assert_allclose(
        tr.min_linf_distance(xs),
        np.array([0.3, 0.2, 0.0]),
    )
    self.assertAlmostEqual(tr.trust_radius, 0.224, places=3)

  @parameterized.named_parameters(
      (
          'no_padding',
          (
              2,
              3,
          ),
      ),
      (
          'with_padding',
          (
              5,
              10,
          ),
      ),
  )
  def test_trust_region_ignores_dimensions_with_sparse_feasible_values(
      self, target_shape: tuple[int, ...]
  ):
    trusted = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            # The third dimension intentionally has infeasible values to test
            # that the trust region computation ignores it.
            np.array([[0.0, 0.0, 100.0], [0.6, 0.0, -120.0]]),
            target_shape=target_shape,
            fill_value=np.nan,
        ),
        categorical=types.PaddedArray.as_padded(jnp.array([])),
    )
    tr = acquisitions.TrustRegion(
        trusted=trusted,
        continuous_feasible_values=[
            # The first dimension has dense feasible values.
            jnp.array(np.linspace(0.0, 1.0, 11)),
            # The second dimension has "sparse" feasible values, i.e., there
            # are large gaps in the feasible values, and this dimension should
            # be ignored in the trust region computation.
            jnp.array([0.0, 1.0]),
            # The third dimension has a single feasible value, and should
            # still be ignored in the trust region computation.
            jnp.array([5.0]),
        ],
    )

    xs = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            np.array([
                [0.5, 1.0, 5.0],
                [0.2, 1.0, 5.0],
            ]),
            target_shape=target_shape,
            fill_value=np.nan,
        ),
        categorical=types.PaddedArray.as_padded(jnp.array([])),
    )
    # The l-infinity distance should only depend on the first dimension.
    num_padded_trials = target_shape[0] - xs.continuous.unpad().shape[0]
    np.testing.assert_allclose(
        tr.min_linf_distance(xs),
        np.array(
            [0.1, 0.2] + [0.0] * num_padded_trials,
        ),
    )
    self.assertAlmostEqual(tr.trust_radius, 0.26, places=3)

  def test_trust_region_bigger(self):
    xs_cont = np.vstack(
        [
            [0.0, 0.0],
            [1.0, 1.0],
        ]
        * 10
    )
    xs = types.ModelInput(
        continuous=types.PaddedArray.as_padded(xs_cont),
        categorical=types.PaddedArray.as_padded(
            np.ones(xs_cont.shape, dtype=types.INT_DTYPE),
        ),
    )
    tr = acquisitions.TrustRegion(
        trusted=xs, continuous_feasible_values=[jnp.array([]), jnp.array([])]
    )

    xs_cont_test = np.array([[0.0, 0.3], [0.9, 0.8], [1.0, 1.0]])
    xs_test = types.ModelInput(
        continuous=types.PaddedArray.as_padded(xs_cont_test),
        categorical=types.PaddedArray.as_padded(
            np.ones(xs_cont_test.shape, dtype=types.INT_DTYPE),
        ),
    )
    np.testing.assert_allclose(
        tr.min_linf_distance(xs_test),
        np.array([0.3, 0.2, 0.0]),
    )
    self.assertAlmostEqual(tr.trust_radius, 0.44, places=3)

  def test_trust_region_padded_small(self):
    # Test that padding still retrieves the same distance computations as
    # `test_trust_region_small`.
    xs_cont = np.array([
        [0.0, 0.0],
        [1.0, 1.0],
    ])
    xs = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            xs_cont, target_shape=(4, 6), fill_value=0.0
        ),
        categorical=types.PaddedArray.from_array(
            np.ones(xs_cont.shape, dtype=types.INT_DTYPE),
            target_shape=(4, 5),
            fill_value=0,
        ),
    )
    tr = acquisitions.TrustRegion(
        trusted=xs,
        continuous_feasible_values=[jnp.array([]), jnp.array([])],
    )

    xs_cont_test = np.array([
        [[0.0, 0.3], [0.9, 0.8], [1.0, 1.0]],
        [[1.0, 1.0], [0.0, 0.3], [0.9, 0.8]],
    ])
    xs_test = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            xs_cont_test, target_shape=(3, 3, 6), fill_value=-100.0
        ),
        categorical=types.PaddedArray.from_array(
            np.ones(xs_cont_test.shape, dtype=types.INT_DTYPE),
            target_shape=(3, 3, 5),
            fill_value=-100,
        ),
    )
    np.testing.assert_allclose(
        tr.min_linf_distance(xs_test),
        np.array([[0.3, 0.2, 0.0], [0.0, 0.3, 0.2], [0.0, 0.0, 0.0]]),
    )
    self.assertAlmostEqual(tr.trust_radius, 0.224, places=3)

  def test_trust_region_padded_all_categorical_multi_batch_dims(self):
    xs = types.ModelInput(
        continuous=types.PaddedArray.as_padded(jnp.array([])),
        categorical=types.PaddedArray.from_array(
            np.ones((2, 2), dtype=types.INT_DTYPE),
            target_shape=(4, 5),
            fill_value=0,
        ),
    )
    tr = acquisitions.TrustRegion(
        trusted=xs,
        continuous_feasible_values=[],
    )
    xs_test = types.ModelInput(
        continuous=types.PaddedArray.as_padded(jnp.array([])),
        categorical=types.PaddedArray.from_array(
            np.ones((2, 3, 2), dtype=types.INT_DTYPE),
            target_shape=(3, 3, 5),
            fill_value=-100,
        ),
    )
    np.testing.assert_allclose(
        tr.min_linf_distance(xs_test),
        np.ones((3, 3)) * -np.inf,
    )


if __name__ == '__main__':
  config.update('jax_enable_x64', True)
  config.update('jax_threefry_partitionable', False)
  absltest.main()


--- vizier/_src/algorithms/designers/gp/gp_models.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Gaussian Process models."""

import logging
from typing import Iterable, Optional, Union

import chex
import equinox as eqx
import jax
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.designers.gp import acquisitions
from vizier._src.algorithms.designers.gp import transfer_learning as vtl
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier._src.jax.models import tuned_gp_models
from vizier.jax import optimizers
from vizier.utils import profiler

tfd = tfp.distributions


class GPTrainingSpec(eqx.Module):
  """Encapsulates all the information needed to train a singular GP model.

  Attributes:
    ard_optimizer: An `Optimizer` which should return a batch of hyperparameters
      to be ensembled.
    ard_rng: PRNG key for the ARD optimization.
    coroutine: The model coroutine.
    ensemble_size: If set, ensembles `ensemble_size` GP models together.
    ard_random_restarts: The number of random restarts.
  """

  ard_optimizer: optimizers.Optimizer[types.ParameterDict]
  ard_rng: jax.Array
  coroutine: sp.ModelCoroutine
  ensemble_size: int = eqx.field(static=True, default=1)
  ard_random_restarts: int = eqx.field(
      static=True, default=optimizers.DEFAULT_RANDOM_RESTARTS
  )


class GPState(eqx.Module):
  """A GP model and its training data. Implements `Predictive`.

  The data is kept around to deduce degrees of freedom and other related
  metrics. This implements `Predictive`, so that it and any of its dervied
  classes like `StackedResidualGP` can be used as a `Predictive` in
  `acquisitions.py`.
  """

  predictive: sp.UniformEnsemblePredictive
  data: types.ModelData

  def predict_with_aux(
      self, features: types.ModelInput
  ) -> tuple[tfd.Distribution, chex.ArrayTree]:
    return self.predictive.predict_with_aux(features)

  def num_hyperparameters(self) -> int:
    """Returns the number of hyperparameters optimized on `data`."""

    # For a GP model, this is feature dimensionality + 2
    # (length scales, amplitude, observation noise)
    # TODO: Compute this from the params returned by the ard
    # optimizer
    return (
        self.data.features.continuous.shape[1]  # (num_samples, num_features)
        + self.data.features.categorical.shape[1]  # (num_samples, num_features)
        + 2
    )


class StackedResidualGP(GPState):
  """GP that implements the `predictive` interface and contains stacked GPs.

  This GP handles sequential transfer learning. This holds one or no base
  (prior) GPs, along with a current top-level GP. The training process is such
  that the 'top' GP is trained on the residuals of the predictions from the
  base GP. The inference process is such that the predictions of the base
  GP and the 'top' GP are combined together.

  The base GP may also have its own base GPs and be a `StackedResidualGP`.
  """

  # `base_gp` refers to a GP trained and conditioned on previous data for
  # transfer learning. The top level GP is trained on the residuals from
  # `base_gp` on `data`.
  # If `None`, no transfer learning is used and all predictions happen through
  # `predictive`.
  base_gp: Optional[GPState] = None

  def predict_with_aux(
      self, features: types.ModelInput
  ) -> tuple[tfd.Distribution, chex.ArrayTree]:
    # Override the existing implementation of `predict_with_aux` to handle
    # combining `predictive` with `base_gp`.
    if not self.base_gp:
      return self.predictive.predict_with_aux(features)

    base_pred_dist, base_aux = self.base_gp.predict_with_aux(features)
    top_pred_dist, top_aux = self.predictive.predict_with_aux(features)

    base_pred = vtl.TransferPredictionState(
        pred=base_pred_dist,
        aux=base_aux,
        training_data_count=self.base_gp.data.labels.shape[0],
        num_hyperparameters=self.num_hyperparameters(),
    )
    top_pred = vtl.TransferPredictionState(
        pred=top_pred_dist,
        aux=top_aux,
        training_data_count=self.data.labels.shape[0],
        num_hyperparameters=self.num_hyperparameters(),
    )

    # TODO: Decide what to do with
    # `expected_base_stddev_mismatch` - currently set to default.
    comb_dist, aux = vtl.combine_predictions_with_aux(
        top_pred=top_pred, base_pred=base_pred
    )

    return comb_dist, aux


def get_vizier_gp_coroutine(
    data: types.ModelData,
    *,
    linear_coef: Optional[float] = None,
    multitask_type: multitask_tuned_gp_models.MultiTaskType = (
        multitask_tuned_gp_models.MultiTaskType.INDEPENDENT
    ),
) -> sp.ModelCoroutine:
  """Gets a GP model coroutine.

  Args:
    data: The data used to the train the GP model
    linear_coef: If non-zero, uses a linear kernel with `linear_coef`
      hyperparameter.
    multitask_type: The type of multitask kernel to use for multimetric GP.

  Returns:
    The model coroutine.
  """
  return tuned_gp_models.VizierGaussianProcess.build_model(
      data,
      linear_coef=linear_coef,
      multitask_type=multitask_type,
  ).coroutine


def _train_gp(spec: GPTrainingSpec, data: types.ModelData) -> GPState:
  """Trains a Gaussian Process model.

  1. Performs ARD to find the best model parameters.
  2. Pre-computes the Cholesky decomposition for the model.

  Args:
    spec: Spec required to train the GP. See `GPTrainingSpec` for more info.
    data: Data on which to train the GP.

  Returns:
    The trained GP model.
  """
  jax.monitoring.record_event(
      '/vizier/jax/designers/gp_bandit/train_gp', scope=profiler.current_scope()
  )

  jax.monitoring.record_event(
      '/vizier/jax/train_gp_with_data_shapes',
      **{
          'num_rows': data.features.categorical.shape[0],
          'num_categoricals': data.features.categorical.shape[1],
          'num_continuous': data.features.continuous.shape[1],
          'num_labels': (
              data.labels.shape[1] if data.labels.padded_array.ndim == 2 else 1
          ),
      },
  )
  model = sp.CoroutineWithData(spec.coroutine, data)

  # Optimize the parameters
  ard_rngs = jax.random.split(spec.ard_rng, spec.ard_random_restarts + 1)
  best_n = spec.ensemble_size or 1
  best_params, _ = spec.ard_optimizer(
      eqx.filter_jit(eqx.filter_vmap(model.setup))(ard_rngs[1:]),
      model.loss_with_aux,
      ard_rngs[0],
      constraints=model.constraints(),
      best_n=best_n,
  )
  if best_n == 1 and all(x.shape[0] == 1 for x in best_params.values()):
    best_params = jax.tree_util.tree_map(
        lambda x: jnp.squeeze(x, axis=0), best_params
    )
  best_models = sp.StochasticProcessWithCoroutine(
      coroutine=spec.coroutine, params=best_params
  )
  # Logging for debugging purposes.
  logging.info(
      'Best models: %s', eqx.tree_pformat(best_models, short_arrays=False)
  )
  predictive = sp.UniformEnsemblePredictive(
      eqx.filter_jit(best_models.precompute_predictive)(data)
  )
  return GPState(predictive=predictive, data=data)


@jax.jit
def _pred_mean(
    pred: acquisitions.Predictive, features: types.ModelInput
) -> types.Array:
  """Returns the mean of the predictions from `pred` on `features`.

  Workaround while `eqx.filter_jit(pred.pred_with_aux)(features)` is broken
  due to a bug in tensorflow probability.

  Args:
    pred: `Predictive` to predict with.
    features: Xs to predict on.

  Returns:
    Means of the predictions from `pred` on `features`.
  """
  return pred.predict_with_aux(features)[0].mean()


def train_stacked_residual_gp(
    base_gp: GPState,
    spec: GPTrainingSpec,
    data: types.ModelData,
) -> StackedResidualGP:
  """Trains a `StackedResidualGP`.

  Completes the following steps in order:
    1. Uses `base_gp` to predict on the `data`
    2. Computes the residuals from the above predictions
    3. Trains a top-level GP on the above residuals
    4. Returns a `StackedResidualGP` combining the base GP and newly-trained
    GP.

  Args:
    base_gp: The predictive to use as the base GP for the `StackedResidualGP`
      training.
    spec: Training spec for the top level GP.
    data: Training data for the top level GP.

  Returns:
    The trained `StackedResidualGP`.
  """
  # Compute the residuals of `data` as predicted by `base_gp`
  pred_means = _pred_mean(base_gp, data.features)

  has_no_padding = ~(
      data.features.continuous.is_missing[0]
      | data.features.categorical.is_missing[0]
      | data.labels.is_missing[0]
  )

  # Scope this to non-padded predictions only.
  pred_means_no_padding = pred_means[has_no_padding]
  residuals = (
      data.labels.unpad().reshape(pred_means_no_padding.shape)
      - pred_means_no_padding
  )

  # Train on the re-padded residuals
  residual_labels = types.PaddedArray.from_array(
      array=residuals,
      target_shape=data.labels.shape,
      fill_value=data.labels.fill_value,
  )
  data_with_residuals = types.ModelData(
      features=data.features, labels=residual_labels
  )

  top_gp = _train_gp(spec=spec, data=data_with_residuals)
  return StackedResidualGP(
      predictive=top_gp.predictive,
      data=top_gp.data,
      base_gp=base_gp,
  )


def train_gp(
    spec: Union[GPTrainingSpec, Iterable[GPTrainingSpec]],
    data: Union[types.ModelData, Iterable[types.ModelData]],
) -> GPState:
  """Trains a Gaussian Process model.

  If `spec` contains multiple elements, each will be used to train a
  `StackedResidualGP`, sequentially. The first entry will be used to train the
  first GP, and then subsequent GPs will be trained on the residuals from the
  previous GP. This process completes in the order that `spec` and `data are
  provided, such that `spec[0]` is the first GP trained and `spec[-1]` is the
  last GP trained.

  spec[-1] and data[-1] make up the top-level GP, and spec[:-1] and data[:-1]
  define the priors in context of transfer learning.

  Args:
    spec: Specification for how to train a GP model. If multiple specs are
      provided, transfer learning will train multiple models and combine into a
      single GP.
    data: Data on which to train GPs. NOTE: `spec` and `data` must be of the
      same shape. Trains a GP on `data[i]` with `spec[i]`.

  Returns:
    The trained GP model.
  """
  is_singleton_spec = isinstance(spec, GPTrainingSpec)
  is_singleton_data = isinstance(data, types.ModelData)
  if is_singleton_spec != is_singleton_data:
    raise ValueError(
        '`train_gp` expected the shapes of `spec` and `data`  to be identical.'
        f' Instead got `data` {data} but `spec` {spec}.'
    )

  if is_singleton_spec and is_singleton_data:
    return _train_gp(spec=spec, data=data)

  if len(spec) != len(data):
    raise ValueError(
        '`train_gp` expected the shapes of `spec` and `data` to be identical.'
        f' Instead got `spec` of length {len(spec)} but `data` of length'
        f' {len(data)}. `spec` was {spec} and `data` was {data}.'
    )

  curr_gp: Optional[GPState] = None
  for curr_spec, curr_data in zip(spec, data):
    if curr_gp is None:
      # We are on the first iteration.
      curr_gp = _train_gp(spec=curr_spec, data=curr_data)
    else:
      # Otherwise, we have a base GP to use - the GP trained on the last
      # iteration.
      curr_gp = train_stacked_residual_gp(
          base_gp=curr_gp,
          spec=curr_spec,
          data=curr_data,
      )

  if curr_gp is None:
    raise ValueError(
        f'Failed to train a GP with provided training spec: {spec} and'
        f' data: {data}. `curr_gp` was never updated. This should never happen.'
    )
  return curr_gp


--- vizier/_src/algorithms/designers/gp/gp_models_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for `train_gp.py`."""

from typing import Callable

import jax
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers.gp import acquisitions
from vizier._src.algorithms.designers.gp import gp_models
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier.jax import optimizers
from vizier.pyvizier import converters

from absl.testing import absltest
from absl.testing import parameterized

mt_type = multitask_tuned_gp_models.MultiTaskType


def _setup_lambda_search(
    f: Callable[[float], float],
    num_train: int = 100,
    num_test: int = 100,
    linear_coef: float = 0.0,
    ensemble_size: int = 1,
) -> tuple[gp_models.GPTrainingSpec, types.ModelData, types.ModelData]:
  """Sets up training state for a GP and outputs an test set for `f`.

  Args:
    f: 1D objective to be optimized, i.e. f(x), where x is a scalar in [-5., 5.)
    num_train: Number of training samples to generate.
    num_test: Number of testing samples to generate.
    linear_coef: If set, uses a linear kernel with coef `linear_coef` for the GP
    ensemble_size: Ensembles together `ensemble_size` GPs.

  Returns:
  A GP training spec.
  A generated train set.
  A generated test set.
  """
  assert num_train > 0 and num_test > 0, (
      f'Must provide a positive number of trials. Got {num_train} training and'
      f' {num_test} testing.'
  )

  search_space = vz.SearchSpace()
  search_space.root.add_float_param('x0', -5.0, 5.0)
  problem = vz.ProblemStatement(
      search_space=search_space,
      metric_information=vz.MetricsConfig(
          metrics=[
              vz.MetricInformation('obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
          ]
      ),
  )

  converter = converters.TrialToModelInputConverter.from_problem(problem)
  quasi_random_designer = quasi_random.QuasiRandomDesigner(
      problem.search_space, seed=1
  )

  def create_model_data(
      num_entries: int,
  ) -> tuple[types.ModelData, list[vz.Trial]]:
    suggestions = quasi_random_designer.suggest(num_entries)
    obs_trials: list[vz.Trial] = []
    for idx, suggestion in enumerate(suggestions):
      trial = suggestion.to_trial(idx)
      x = suggestions[idx].parameters['x0'].value
      trial.complete(vz.Measurement(metrics={'obj': f(x)}))
      obs_trials.append(trial)

    model_data = converter.to_xy(obs_trials)
    return model_data, obs_trials

  train_data, _ = create_model_data(num_entries=num_train)
  train_spec = gp_models.GPTrainingSpec(
      ard_optimizer=optimizers.default_optimizer(),
      ard_rng=jax.random.PRNGKey(0),
      coroutine=gp_models.get_vizier_gp_coroutine(
          data=train_data, linear_coef=linear_coef
      ),
      ensemble_size=ensemble_size,
      ard_random_restarts=optimizers.DEFAULT_RANDOM_RESTARTS,
  )
  test_data, _ = create_model_data(num_entries=num_test)
  return train_spec, train_data, test_data


def _compute_mse(
    predictive: acquisitions.Predictive, test_data: types.ModelData
) -> float:
  """Computes the mean-squared error of `predictive` on `test_data."""

  pred_dist, _ = predictive.predict_with_aux(test_data.features)

  # We need this reshape to prevent a broadcast from (num_samples, ) -
  # (num_samples, 1) yielding (num_samples, num_samples) and breaking this
  # calculation.
  test_labels_reshaped = np.asarray(test_data.labels.unpad()).reshape(-1)

  mse = np.sum(np.square(pred_dist.mean() - test_labels_reshaped))
  return mse


class TrainedGPTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(linear_coef=0.0, ensemble_size=1),
      dict(linear_coef=0.4, ensemble_size=1),
      dict(linear_coef=0.0, ensemble_size=5),
      # dict(linear_coef=0.4, ensemble_size=5),  # This is flaky.
  )
  def test_mse_no_base(
      self, *, linear_coef: float = 0.0, ensemble_size: int = 1
  ):
    f = lambda x: -((x - 0.5) ** 2)
    spec, train_data, test_data = _setup_lambda_search(
        f,
        num_train=100,
        num_test=100,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )
    gp = gp_models.train_gp(spec, train_data)
    mse = _compute_mse(gp, test_data)
    self.assertLess(mse, 2e-2)


class StackedResidualGPTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(linear_coef=0.0, ensemble_size=1),
      dict(linear_coef=0.4, ensemble_size=1),
      dict(linear_coef=0.0, ensemble_size=5),
      dict(linear_coef=0.4, ensemble_size=5),
  )
  def test_sequential_base_accuracy(
      self, *, linear_coef: float = 0.0, ensemble_size: int = 1
  ):
    """Tests that a good base with a bad top beats a bad independent model.

    Train a base on n = 100 samples, and a top GP with n = 5 samples.
    Combine these together into one predictor.

    Compare the MSE of this predictor with a test predictor trained on n = 5
    samples. The transfer learning enabled predictor should beat the test
    predictor.

    Args:
      linear_coef: The linear coefficient for the GP. Used for all trained GPs.
      ensemble_size: The number of GPs to ensemble together. Used for all
        trained GPs.
    """
    base_samples = 100
    bad_num_samples = 5
    num_test = 100
    f = lambda x: -((x - 0.5) ** 2)

    base_spec, base_train_data, _ = _setup_lambda_search(
        f,
        num_train=base_samples,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )

    # This is purposefully bad with `bad_num_samples`
    top_spec, top_train_data, test_data = _setup_lambda_search(
        f,
        num_train=bad_num_samples,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )

    # Combine the good base and the bad top into transfer learning GP.
    seq_base_gp = gp_models.train_gp(
        [base_spec, top_spec], [base_train_data, top_train_data]
    )

    # Create a purposefully-bad GP with `bad_num_samples` for comparison.
    test_gp_spec, test_gp_train_data, _ = _setup_lambda_search(
        f,
        num_train=bad_num_samples,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )
    test_gp = gp_models.train_gp(test_gp_spec, test_gp_train_data)

    seq_base_mse = _compute_mse(seq_base_gp, test_data)
    test_mse = _compute_mse(test_gp, test_data)

    self.assertLess(seq_base_mse, test_mse)

  @parameterized.parameters(
      dict(linear_coef=0.0, ensemble_size=1),
      dict(linear_coef=0.4, ensemble_size=1),
      dict(linear_coef=0.0, ensemble_size=5),
      # dict(linear_coef=0.4, ensemble_size=5),  # This is flaky.
  )
  def test_multi_base(
      self, *, linear_coef: float = 0.0, ensemble_size: int = 1
  ):
    """Tests that multiple bases predict well.

    Train two good bases on n = 100 samples, and a top on n = 5 samples

    The MSE of this predictor should be similar to a GP trained on n = 100
    samples.

    Args:
      linear_coef: The linear coefficient for the GP. Used for all trained GPs.
      ensemble_size: The number of GPs to ensemble together. Used for all
        trained GPs.
    """
    large_n = 100
    small_n = 5
    num_test = 100
    f = lambda x: -((x - 0.5) ** 2)

    # Create a top GP with low training data
    top_spec, top_train_data, _ = _setup_lambda_search(
        f,
        num_train=small_n,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )

    train_specs = []
    train_data = []

    for _ in range(2):
      base_spec, base_train_data, _ = _setup_lambda_search(
          f,
          num_train=large_n,
          num_test=num_test,
          linear_coef=linear_coef,
          ensemble_size=ensemble_size,
      )
      train_specs.append(base_spec)
      train_data.append(base_train_data)
    train_specs.append(top_spec)
    train_data.append(top_train_data)

    seq_base_gp = gp_models.train_gp(train_specs, train_data)

    # Create a good GP with sufficient training data
    test_gp_spec, test_gp_train_data, test_data = _setup_lambda_search(
        f,
        num_train=large_n,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )
    test_gp = gp_models.train_gp(test_gp_spec, test_gp_train_data)

    seq_base_mse = _compute_mse(seq_base_gp, test_data)
    test_mse = _compute_mse(test_gp, test_data)

    self.assertAlmostEqual(seq_base_mse, test_mse, places=4)
    self.assertLess(test_mse, 2e-2)

  @parameterized.parameters(
      dict(linear_coef=0.0, ensemble_size=1),
      dict(linear_coef=0.4, ensemble_size=1),
      dict(linear_coef=0.0, ensemble_size=5),
      dict(linear_coef=0.4, ensemble_size=5),
  )
  def test_bad_base_resilience(
      self, *, linear_coef: float = 0.0, ensemble_size: int = 1
  ):
    """Tests that predictions are resilient to a bad base.

    Train a bad base on a fake objective with n = 100 samples.
    Trains a top predictor on the actual objective with n = 100 samples.
    Combines them into one predictor.

    The MSE of this predictor better than a GP trained purely on the bad
    objective.

    Args:
      linear_coef: The linear coefficient for the GP. Used for all trained GPs.
      ensemble_size: The number of GPs to ensemble together. Used for all
        trained GPs.
    """
    large_n = 100
    num_test = 100
    f = lambda x: -((x - 0.5) ** 2)
    fake_f = lambda x: x**5

    bad_base_spec, bad_base_train_data, _ = _setup_lambda_search(
        fake_f,
        num_train=large_n,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )

    top_spec, top_train_data, test_data = _setup_lambda_search(
        f,
        num_train=large_n,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )

    # Combine the good base and the bad top into transfer learning GP.
    seq_base_gp = gp_models.train_gp(
        [
            bad_base_spec,
            top_spec,
        ],
        [bad_base_train_data, top_train_data],
    )

    # Create a GP on the fake objective with sufficient training data
    test_gp_spec, test_gp_train_data, _ = _setup_lambda_search(
        fake_f,
        num_train=large_n,
        num_test=num_test,
        linear_coef=linear_coef,
        ensemble_size=ensemble_size,
    )
    test_gp = gp_models.train_gp(test_gp_spec, test_gp_train_data)

    seq_base_mse = _compute_mse(seq_base_gp, test_data)
    test_mse = _compute_mse(test_gp, test_data)

    self.assertLess(seq_base_mse, test_mse)

  def test_single_list_same_as_singleton(self):
    """Tests that `[state]` and `state` are treated the same."""
    large_n = 100
    num_test = 100
    f = lambda x: -((x - 0.5) ** 2)
    spec, train_data, test_data = _setup_lambda_search(
        f, num_train=large_n, num_test=num_test
    )

    list_gp = gp_models.train_gp(spec, train_data)
    singleton_gp = gp_models.train_gp([spec], [train_data])

    list_gp_mse = _compute_mse(list_gp, test_data)
    singleton_gp_mse = _compute_mse(singleton_gp, test_data)

    self.assertAlmostEqual(list_gp_mse, singleton_gp_mse)

  @parameterized.parameters(
      dict(multitask_type=mt_type.INDEPENDENT),
      dict(multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR),
      dict(multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR),
      dict(multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR),
  )
  def test_multi_task(self, multitask_type: mt_type):
    search_space = vz.SearchSpace()
    search_space.root.add_float_param('x0', -5.0, 5.0)
    problem = vz.ProblemStatement(
        search_space=search_space,
        metric_information=vz.MetricsConfig(
            metrics=[
                vz.MetricInformation(
                    'obj1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
                vz.MetricInformation(
                    'obj2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
            ]
        ),
    )

    converter = converters.TrialToModelInputConverter.from_problem(problem)
    quasi_random_designer = quasi_random.QuasiRandomDesigner(
        problem.search_space, seed=1
    )
    num_entries = 100
    suggestions = quasi_random_designer.suggest(num_entries)
    obs_trials: list[vz.Trial] = []
    for idx, suggestion in enumerate(suggestions):
      trial = suggestion.to_trial(idx)
      x = suggestions[idx].parameters['x0'].value
      trial.complete(vz.Measurement(metrics={'obj1': x + 1, 'obj2': 2 * x - 1}))
      obs_trials.append(trial)

    train_entries = 60
    train_trials = obs_trials[:train_entries]
    test_trials = obs_trials[train_entries:]
    model_data = converter.to_xy(train_trials)
    train_spec = gp_models.GPTrainingSpec(
        ard_optimizer=optimizers.default_optimizer(),
        ard_rng=jax.random.PRNGKey(0),
        coroutine=gp_models.get_vizier_gp_coroutine(
            data=model_data, multitask_type=multitask_type
        ),
    )
    gp = gp_models.train_gp(train_spec, model_data)

    test_data = converter.to_xy(test_trials)
    pred_dist, _ = gp.predict_with_aux(test_data.features)
    mse = np.mean(np.square(pred_dist.mean() - test_data.labels.unpad()))
    self.assertLess(mse, 1e-2)


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  jax.config.update('jax_enable_x64', True)
  jax.config.update('jax_threefry_partitionable', False)
  absltest.main()


--- vizier/_src/algorithms/designers/gp/output_warpers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Output warper."""

import abc
import copy
from typing import Callable, Optional, Sequence, Tuple

import attr
import attrs
import equinox
import jax
from jax import numpy as jnp
import numpy as np
from scipy import stats
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import types


tfb = tfp.bijectors


def _validate_labels(labels_arr: types.Array) -> types.Array:
  """Checks and modifies the shape and values of the labels."""
  labels_arr = labels_arr.astype(float)
  if not (labels_arr.ndim == 2 and labels_arr.shape[-1] == 1):
    raise ValueError(
        'Labels need to be an array of shape (num_points, 1).'
        f'Got shape: {labels_arr.shape}'
    )
  if np.isposinf(labels_arr).any():
    raise ValueError('Infinity metric value is not valid.')
  if np.isneginf(labels_arr).any():
    labels_arr[np.isneginf(labels_arr)] = np.nan
  return labels_arr


class OutputWarper(abc.ABC):
  """Interface for different output warper methods."""

  # TODO: implement the unwarp for different warper classes. Currently,
  # they return an error.

  @abc.abstractmethod
  def warp(self, labels_arr: types.Array) -> types.Array:
    """Runs the output warper of choice on an array of labels.

    Args:
      labels_arr: (num_points, 1) shaped array of unwarped labels. Note that
        each call accomodates one metric, which can be an objective or a safety
        metric. NaN and infinity values are allowed and can be warped based on
        the choice of warper. Labels are assumed to be maximizing. labels_arr
        will not be mutated.

    Returns:
      (num_points, 1) shaped array of finite warped mutated labels.
    """
    pass

  @abc.abstractmethod
  def unwarp(self, labels_arr: types.Array) -> types.Array:
    """Runs the inverse of output warper of choice on an array of labels.

    Args:
      labels_arr: (num_points, 1) shaped array of warped labels. Note tha each
        call accomodates one metric, which can be an objective or a safety
        metric. NaN and infinity values are allowed and will untouched. Labels
        are assumed to be maximizing. labels_arr will not be mutated.
        Technically any array can be passed to the unwarp method, but we are
        interested in unwarping warped labels for purposes of GP training and
        prediction.

    Returns:
      (num_points, 1) shaped array of finite unwarped mutated labels.
    """
    pass

  def warp_padded(self, labels: types.PaddedArray) -> types.PaddedArray:
    warped_unpadded_list = []
    unpadded = np.array(labels.unpad())
    for metric_id in range(labels.shape[-1]):
      warped_unpadded_list.append(self.warp(unpadded[:, metric_id]))

    warped_unpadded = np.concatenate(warped_unpadded_list, axis=-1)
    return labels.replace_array(warped_unpadded)


@attr.define
class BijectorWarper(OutputWarper):
  _bijector_factory: Callable[[types.Array], tfb.Bijector] = attr.ib
  _bijector: Optional[tfb.Bijector] = attr.ib(init=False, default=None)

  def warp(self, labels_arr: types.Array) -> types.Array:
    self._bijector = self._bijector_factory(labels_arr)
    return self._bijector(labels_arr)

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    if self._bijector is None:
      raise ValueError('warp must be called first.')
    return self._bijector.inverse(labels_arr)


@attr.define
class OutputWarperPipeline(OutputWarper):
  """Performs a sequence of warpings on an input labels array."""

  warpers: Sequence[OutputWarper] = attr.ib(factory=list)

  def warp(self, labels_arr: types.Array) -> types.Array:
    """Sequntial warping of the labels.

    Note that if labels include one unique finite value, pipeline returns
    an array of zeros. Alternatively, if the labels include only infeasible
    values (eg. NaNs and -inf), pipeline returns an array of minus ones.
    Otherwise, it *sequntially* performs the warpings on labels. Warping is not
    done in place.

    Args:
      labels_arr: (num_points, 1) shaped array of labels.

    Returns:
      (num_points, 1) shaped array of warped labels.
    """
    labels_arr = copy.deepcopy(labels_arr)
    labels_arr = _validate_labels(labels_arr)
    if (
        np.isfinite(labels_arr).all()
        and len(np.unique(labels_arr).flatten()) == 1
    ):
      return np.zeros(labels_arr.shape)
    if np.isnan(labels_arr).all():
      return -1 * np.ones(shape=labels_arr.shape)
    for warper in self.warpers:
      labels_arr = warper.warp(labels_arr)
    return labels_arr

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    """Sequential unwarping of the warped labels.

    Note that if labels include one unique finite value zero, the pipeline
    returns the array itself. Alternatively, if the labels include only minus
    ones, the pipeline returns an array of NaNs. Otherwise, it *sequentially*
    performs the unwarpings on labels. Unwarping is not done in place. This
    method accomodates inputs with duplicate entries and outputs equal output
    values for such entries.

    Args:
      labels_arr: (num_points, 1) shaped array of warped labels.

    Returns:
      (num_points, 1) shaped array of unwarped labels.
    """
    labels_arr = copy.deepcopy(labels_arr)
    labels_arr = _validate_labels(labels_arr)
    if (
        np.isfinite(labels_arr).all()
        and len(np.unique(labels_arr).flatten()) == 1
    ):
      if np.unique(labels_arr).item() == 0.0:
        return labels_arr
      if np.unique(labels_arr).item() == -1.0:
        return np.nan * np.ones(shape=labels_arr.shape)

    # reverse the order of warpers for unwarping to maintain bijective property.
    warpers = self.warpers[::-1]
    for warper in warpers:
      labels_arr = warper.unwarp(labels_arr)
    return labels_arr


def create_default_warper(
    *,
    half_rank_warp: bool = True,
    log_warp: bool = True,
    infeasible_warp: bool = True,
) -> OutputWarperPipeline:
  """Creates an output warper pipeline.

  Args:
    half_rank_warp: boolean indicating if half-rank warping to be performed.
    log_warp: boolean indicating if log warping to be performed.
    infeasible_warp: boolean indicating if infeasible warping to be performed.

  Returns:
    an instance of OutputWarperPipeline.
  """
  if not half_rank_warp and not log_warp and not infeasible_warp:
    raise ValueError(
        'At least one of "half_rank_warp", "log_warp" or "infeasible_warp" '
        'must be True.'
    )
  warpers = []
  if half_rank_warp:
    warpers.append(HalfRankComponent())
  if log_warp:
    warpers.append(LogWarperComponent())
  if infeasible_warp:
    warpers.append(InfeasibleWarperComponent())
  return OutputWarperPipeline(warpers)


def create_warp_outliers_warper(
    *,
    warp_outliers: bool = True,
    infeasible_warp: bool = True,
    transform_gaussian: bool = True,
) -> OutputWarperPipeline:
  """Creates an output warper outline which detects outliers and warps them."""
  warpers = []
  if warp_outliers:
    warpers.append(DetectOutliers())
  if infeasible_warp:
    warpers.append(InfeasibleWarperComponent())
  if transform_gaussian:
    warpers.append(TransformToGaussian())
  return OutputWarperPipeline(warpers)


@attr.define
class _HalfRankUnwarper:
  """Inverse mapping for HalfRankComponent.

  Attributes:
    _original_labels: Array of shape [N,]. Must be finite, unique, and sorted.
    _warped_labels: Array of shape [N,]. _warped_labels[i] is the result of
      applying the halfrank warper on `_original_labels[i]`.
  """

  # Labels must be unique and sorted.
  _original_labels: np.ndarray
  _warped_labels: np.ndarray
  _original_label_median: float

  def unwarp(self, label: float) -> float:
    """Unwarps the label."""
    # Anything above median is a no-op.
    if label >= self._original_label_median:
      return label

    # Try looking up the value
    # Use searchsorted and pull out three numbers.
    idx = np.searchsorted(self._warped_labels, label)
    candidates = self._warped_labels[
        max(0, idx - 1) : min(len(self._warped_labels), idx + 1)
    ]
    best_idx = np.argmin(np.abs(candidates - label))
    if np.isclose(self._warped_labels[best_idx], label):
      return self._original_labels[best_idx]

    # Label is smaller than the image of the warper.
    # Extrapolate.
    if label < np.min(self._warped_labels):
      return self._original_labels[0] - (
          np.abs(label - self._warped_labels[0])
          / (self._warped_labels[-1] - self._warped_labels[0])
      ) * (self._original_labels[-1] - self._original_labels[0])

    # All other cases: reverse the forward mapping.
    # Largest number less than `label`.
    # (which must exist because label >= min(warped_labels).)
    lower = np.searchsorted(self._warped_labels, label) - 1
    # Smallest number greater than `label`.
    # (which must exist because label < median.)
    upper = np.searchsorted(self._warped_labels, label)

    original_gap = self._original_labels[upper] - self._original_labels[lower]
    warped_gap = self._warped_labels[upper] - self._warped_labels[lower]
    return (
        self._original_labels[lower]
        + (label - self._warped_labels[lower]) * original_gap / warped_gap
    )


@attr.define
class HalfRankComponent(OutputWarper):
  """Warps half of an array of labels to fit into a Gaussian distribution.

  Note that this warping is performed on finite values of the array and NaNs are
  untouched.
  """

  _unwarper: Optional[_HalfRankUnwarper] = attr.field(default=None)

  def _estimate_std_of_good_half(
      self, unique_labels: np.ndarray, threshold: float
  ) -> float:
    """Estimates the standard devation of the good half of the array.

    Args:
      unique_labels: (num_points, 1) shaped array of unique labels.
      threshold: minimum label value to be considered in "good half".

    Returns:
      float estimated standard devation of the good half of the array.
    """
    good_half = unique_labels[unique_labels >= threshold]
    std = np.sqrt(
        ((good_half - threshold) ** 2).sum() * (1 / good_half.shape[0])
    )
    if std > 0:
      return std
    std = np.sqrt(
        ((unique_labels - threshold) ** 2).sum() * (1 / unique_labels.shape[0])
    )
    if np.isfinite(std):
      return std
    std = (np.abs(unique_labels - threshold)).sum() * (
        1 / unique_labels.shape[0]
    )
    return std

  def warp(self, labels_arr: types.Array) -> types.Array:
    """See base class."""
    labels_arr = _validate_labels(labels_arr)
    if labels_arr.size == 1:
      return labels_arr
    labels_arr = labels_arr.flatten()
    # Compute median, unique labels, and ranks.
    median = np.nanmedian(labels_arr)
    is_finite = np.isfinite(labels_arr)
    # np.unique also sorts the array.
    unique_labels, unique_idx = np.unique(
        labels_arr[is_finite], return_index=True
    )

    # Rank sort.
    ranks = stats.rankdata(labels_arr, method='dense', nan_policy='omit')
    dedup_median_index = unique_labels.searchsorted(median, 'left')
    denominator = (
        dedup_median_index + (unique_labels[dedup_median_index] == median) * 0.5
    )
    estimated_std = self._estimate_std_of_good_half(unique_labels, median)
    # Apply transformation to points below median.
    for i, (yy, rank) in enumerate(zip(labels_arr, ranks)):
      if np.isfinite(yy) and yy < median:
        # FYI: a lot of effort went into choosing this arbitrary
        # denominator. rankdata(method='max') and simply using
        # rank_quantile = rank / np.isfinite(labels_arr).sum() should be
        # just as fine.
        rank_quantile = 0.5 * (rank - 0.5) / denominator
        # rank_ppf is always less than 0
        rank_ppf = stats.norm.ppf(rank_quantile)
        labels_arr[i] = rank_ppf * estimated_std + median

    # Save information needed for unwarping.
    self._unwarper = _HalfRankUnwarper(
        original_labels=unique_labels,
        warped_labels=labels_arr[is_finite][unique_idx],
        original_label_median=unique_labels[len(unique_labels) // 2],
    )
    return labels_arr[:, np.newaxis]

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    if self._unwarper is None:
      raise ValueError(' warp() needs to be called before unwarp() is called.')
    labels_arr = _validate_labels(labels_arr)
    labels_arr = labels_arr.flatten()
    if np.isnan(labels_arr).any():
      raise ValueError('unwarp does not support nan values.')

    for i in range(len(labels_arr)):
      labels_arr[i] = self._unwarper.unwarp(labels_arr[i])
    return labels_arr[:, np.newaxis]


@attr.define
class LogWarperComponent(OutputWarper):
  """Warps an array of labels to highlght the difference between good values.

  Note that this warping is performed on finite values of the array and NaNs are
  untouched.
  """

  _labels_min: Optional[float] = attr.field(default=None)
  _labels_max: Optional[float] = attr.field(default=None)
  offset: float = attr.field(default=1.5, validator=attrs.validators.gt(0.0))

  def warp(self, labels_arr: types.Array) -> types.Array:
    """See base class."""
    labels_arr = _validate_labels(labels_arr)
    self._labels_min = np.nanmin(labels_arr)
    self._labels_max = np.nanmax(labels_arr)
    labels_arr = labels_arr.flatten()
    finite_mask = np.isfinite(labels_arr)

    norm_diff = (self._labels_max - labels_arr[finite_mask]) / (
        self._labels_max - self._labels_min
    )
    labels_arr[finite_mask] = 0.5 - (
        np.log1p(norm_diff * (self.offset - 1)) / np.log(self.offset)
    )
    return labels_arr[:, np.newaxis]

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    if self._labels_max is None or self._labels_min is None:
      raise ValueError(' warp() needs to be called before unwarp() is called.')
    labels_arr = labels_arr.flatten()
    labels_arr = self._labels_max - (
        np.exp(np.log(self.offset) * (0.5 - labels_arr)) - 1
    ) * (self._labels_max - self._labels_min) / (self.offset - 1)
    return labels_arr[:, np.newaxis]


@attr.define
class InfeasibleWarperComponent(OutputWarper):
  """Warps the infeasible/nan value to feasible/finite values.

  This OutputWarper handles a mix of feasible and infeasible (NaN) labels, and
  returns feasible labels.

  Typically, it's the last element in an `OutputWarperPipeline`.
  What it does:
    * warp returns a feasible value.
    * If we have a mix of feasible values and infeasible values, warp() applied
      to any of the feasible values will be larger than warp() applied to an
      infeasible value.
    * The expected value of warp() over the set of feasible values will be zero.

  Recall that we typically use this class with a Gaussian Process regressor
  which has a prior mean of zero, and that the GP regressor's prediction
  approaches zero when it's far away from any support points. So, a zero output
  should correspond to the overall average of all label values.

  In other words, the expected value of warp() should be zero, when feasible and
  infeasible values are chosen with frequencies estimated from the set of all
  available values. I.e.
  0 = p_feasible * Expected(v_feasible) + (1 - p_feasible) * v_unfeasible,
  where p_feasible is the probability of getting a feasible value,
  Expected(v_feasible) is the expected value of warp() over feasible values,
  and v_unfeasible is the value we assign to infeasible values.

  This means that when we have mostly feasible values, we want
  Expected(v_feasible) near zero; and when we have mostly infeasible values,
  we want Expected(v_feasible) to be positive.

  This is entirely reasonable: when feasible values are rare, they are
  individually more surprising and should affect the regressor's model
  more, and vice versa.  Or, equivalently, when feasible values are very
  rare, and when the GP regressor is evaluated far away from any support
  points, the GP regressor will predict a value (i.e. zero) that should be
  nearly equal to v_infeasible.
  """

  _shift: Optional[float] = attr.field(default=None)

  def warp(self, labels_arr: types.Array) -> types.Array:
    labels_arr = _validate_labels(labels_arr)
    labels_arr = labels_arr.flatten()

    if np.isnan(labels_arr).all():
      # Edge case when all values are NaN.
      self._shift = np.nan
      labels_arr[:] = 0
      return labels_arr[:, np.newaxis]

    labels_range = np.nanmax(labels_arr) - np.nanmin(labels_arr)
    warped_bad_value = np.nanmin(labels_arr) - (0.5 * labels_range + 1)
    num_feasible = labels_arr.size - np.isnan(labels_arr).sum()
    # With the data we have available, we can estimate the relative frequency
    # of feasible (p_feasible) and unfeasible points (p_unfeasible).
    # We use the Jeffrey's version of Laplace Smoothing: see
    # https://en.wikipedia.org/wiki/Jeffreys_prior#N-sided_die_with_biased_probabilities
    # and
    # https://en.wikipedia.org/wiki/Additive_smoothing. The basic logic being
    # that even though we may have no observations of (e.g.) a feasible point,
    # we believe that feasible points are possible, and so we are reluctant to
    # assign them a zero probability.
    p_feasible = (0.5 + num_feasible) / (1 + labels_arr.size)
    self._shift = -np.nanmean(labels_arr) * p_feasible - warped_bad_value * (
        1 - p_feasible
    )
    labels_arr[np.isnan(labels_arr)] = warped_bad_value
    labels_arr[~np.isnan(labels_arr)] += self._shift
    return labels_arr[:, np.newaxis]

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    if self._shift is None:
      raise ValueError('warp() needs to be called before unwarp() is called.')
    return labels_arr - self._shift


class ZScoreLabels(OutputWarper):
  """Sandardizes finite label values, leaving the NaNs & infinities out."""

  def warp(self, labels_arr: types.Array) -> types.Array:
    """Sandardizes finite label values to scale the mean to 0 and std to 1.

    Args:
      labels_arr: (num_points, 1) shaped array of labels.

    Returns:
      (num_points, 1) shaped array of standardize labels.
    """
    labels_arr = _validate_labels(labels_arr)
    if np.isnan(labels_arr).all():
      raise ValueError('Labels need to have at least one non-NaN entry.')
    labels_finite_ind = np.isfinite(labels_arr)
    labels_arr_finite = labels_arr[labels_finite_ind]
    if np.nanstd(labels_arr_finite) == 0 or not np.isfinite(
        np.nanstd(labels_arr_finite)
    ):
      return labels_arr
    labels_arr_finite_normalized = (
        labels_arr_finite - np.nanmean(labels_arr_finite)
    ) / np.nanstd(labels_arr_finite)
    labels_arr[labels_finite_ind] = labels_arr_finite_normalized
    return labels_arr

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    raise NotImplementedError(
        'unwarp  method for ZScoreLabels is not implemented yet.'
    )


@attr.define
class NormalizeLabels(OutputWarper):
  """Normalizes the finite label values, leaving the NaNs & infinities out."""

  target_interval: Tuple[float, float] = attr.ib(default=(0.0, 1.0))

  def __attrs_post_init__(self):
    if self.target_interval[0] > self.target_interval[1]:
      raise ValueError(f'Bounds {self.target_interval} is invalid.')

  def warp(self, labels_arr: types.Array) -> types.Array:
    """Normalizes the finite label values to bring them within target_interval.

    If all finite labels are equal, they get mapped to target_interval's
    midpoint.

    Args:
      labels_arr: (num_points, 1) shaped array of labels.

    Returns:
      (num_points, 1) shaped array of normalized labels.
    """
    labels_arr = _validate_labels(labels_arr)
    if np.isnan(labels_arr).all():
      raise ValueError('Labels need to have at least one non-NaN entry.')

    labels_finite_ind = np.isfinite(labels_arr)
    labels_arr_finite = labels_arr[labels_finite_ind]

    source_interval = (np.min(labels_arr_finite), np.max(labels_arr_finite))

    if source_interval[0] == source_interval[1]:
      midpoint = (self.target_interval[0] + self.target_interval[1]) / 2
      labels_arr[labels_finite_ind] = midpoint
      return labels_arr

    labels_arr_finite_normalized = np.interp(
        labels_arr_finite, source_interval, self.target_interval
    )
    labels_arr[labels_finite_ind] = labels_arr_finite_normalized
    return labels_arr

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    raise NotImplementedError(
        'unwarp  method for NormalizeLabels is not implemented yet.'
    )


@attr.define
class DetectOutliers(OutputWarper):
  """Detects outliers from an array of labels.

  The goal of this warper is to detect the unreasonably bad labels, aka
  outliers. Eg, assuming a maximization problem where the `normal` range of
  labels is [1, 10], the occasionally observed value of -10**76 counts as an
  outlier. For this warper, we only consider the *finite* values in the labels
  and leave the NaN and infinity values untouched.

  The proposed warping finds the difference between the maximum label and the
  median of the labels.

  Attributes:
    min_zscore: number of stds below the median for variance estimation.
    max_zscore: number of stds above the median for outlier detection.
  """

  min_zscore: float = attr.field(kw_only=True, default=6.0)
  max_zscore: float = attr.field(kw_only=True, default=None)

  def _estimate_variance(self, labels_arr: types.Array) -> float:
    """Estimates the variance of labels array using the top half values.

    The estimation is a function of the size of the labels array. For details
    and derivations, see the following paper:
    `Estimating the mean and variance from the median, range, and the size of
    a sample` paper found below
    https://link.springer.com/content/pdf/10.1186/1471-2288-5-13.pdf.

    Args:
      labels_arr: (num_points, 1) shaped array of labels.

    Returns:
      The estiamated variance of the labels.
    """
    num_points = labels_arr.shape[0]
    labels_median = np.nanmedian(labels_arr)
    labels_max = np.nanmax(labels_arr)

    if not np.isfinite(labels_max) or np.isnan(labels_max):
      raise ValueError('The max label value should be finite.')

    if not np.isfinite(labels_median) or np.isnan(labels_median):
      raise ValueError('The median label value should be finite.')

    if self.max_zscore:
      return (labels_max - labels_median) / self.min_zscore

    if num_points >= 70:
      return (labels_max - labels_median) / 3
    elif num_points >= 15:
      return (labels_max - labels_median) / 2
    else:
      # We hallucinate the min labels value
      labels_min_hallucinated = labels_median - np.max(labels_arr)
      # Following the paper assumptions, we need to make sure the hallucinated
      # min is non-negative and shift the rest of the values accordingly.
      if labels_min_hallucinated < 0:
        labels_min_hallucinated = 0
        labels_max -= labels_min_hallucinated
        labels_median -= labels_min_hallucinated

      # Eq. 12 in the paper mentioned above.
      a, m, b = labels_min_hallucinated, labels_median, labels_max
      n = num_points

      out = a**2 + m**2 + b**2
      out += ((n - 3) / 2) * ((a + m) ** 2 + (b + m) ** 2) / 4
      out -= n * ((a + 2 * m + b) / 4 + (a - 2 * m + b) / (4 * n)) ** 2
      return out / (n - 1)

  def warp(self, labels_arr: types.Array) -> types.Array:
    labels_arr = _validate_labels(labels_arr)
    labels_finite_ind = np.isfinite(labels_arr)
    labels_arr_finite = labels_arr[labels_finite_ind]
    labels_median = np.median(labels_arr_finite)
    labels_std = np.sqrt(self._estimate_variance(labels_arr_finite))
    threshold = labels_median - self.min_zscore * labels_std
    labels_arr_finite[labels_arr_finite < threshold] = np.nan
    labels_arr[labels_finite_ind] = labels_arr_finite
    return labels_arr

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    raise NotImplementedError(
        'unwarp  method for DetectOutliers is not implemented yet.'
    )


class TransformToGaussian(OutputWarper):
  """Transforms the labels into a Gaussian distribution.

  The goal of this warper is to transform the label into a Gaussian sample to
  better suit it for a Gaussian process. Here, we use a non-parametric warper
  called quantile transformer. Traditonally, this transformer relies on the
  relational rank of the labels, however this approach does not take into
  account the values of the labels. To address this issue, we also provide
  another option to use the normalized distances instead.

  Alternatives include a box-cox transformation (for non-negative labels) or a
  yeo-johnson transformation. Both these methods are parameteric and their
  parameters can be learned using a maximum likelihood approach.
  """

  def __init__(
      self,
      *,
      softclip_low: float = 1e-10,
      softclip_high: float = 1 - 1e-10,
      softclip_hinge_softness: float = 0.01,
      use_rank: bool = False,
  ):
    self.softclip_low = softclip_low
    self.softclip_high = softclip_high
    self.softclip_hinge_softness = softclip_hinge_softness
    self.use_rank = use_rank

  def warp(self, labels_arr: types.Array) -> types.Array:
    labels_arr = _validate_labels(labels_arr)
    labels_arr = np.asarray(labels_arr, dtype=np.float64)
    labels_arr_flattened = labels_arr.flatten()
    if self.use_rank:
      base_for_transform = np.argsort(labels_arr_flattened)
    else:
      base_for_transform = labels_arr_flattened
    base_for_transform_normalized = (
        base_for_transform - np.min(base_for_transform)
    ) / (np.max(base_for_transform) - np.min(base_for_transform))
    clip = tfp.bijectors.SoftClip(
        low=np.array(self.softclip_low, dtype=labels_arr.dtype),
        high=np.array(self.softclip_high, dtype=labels_arr.dtype),
        hinge_softness=self.softclip_hinge_softness,
    )
    base_for_transform_normalized_clipped = np.array(
        clip.forward(base_for_transform_normalized)
    )
    normal_dist = tfp.distributions.Normal(0.0, 1)
    labels_arr_transformed = normal_dist.quantile(
        base_for_transform_normalized_clipped
    )
    labels_arr_transformed = np.reshape(
        labels_arr_transformed, labels_arr.shape
    )
    return labels_arr_transformed

  def unwarp(self, labels_arr: types.Array) -> types.Array:
    raise NotImplementedError(
        'unwarp  method for TransformToGaussian is not implemented yet.'
    )


class LinearOutputWarper(equinox.Module):
  """Linear output warper.

  The LinearOutputWarper applies affine transformation to transform the labels
  to [low_bound, high_bound].

  Note: low_bound, high_bound are scalars.
  """

  low_bound: types.Array  # shape: ()
  high_bound: types.Array  # shape: ()
  min_value: types.Array  # shape: (num_metrics,)
  max_value: types.Array  # shape: (num_metrics,)
  min_range: types.Array  # shape: ()

  @classmethod
  def from_obs(
      cls,
      y_obs: types.Array,
      low_bound: float = -2.0,
      high_bound: float = 2.0,
      min_range: float = 1e-20,
  ) -> 'LinearOutputWarper':
    # y shape: (num_samples, num_metrics)
    min_value = jnp.min(y_obs, axis=0)
    max_value = jnp.max(y_obs, axis=0)

    return cls(
        low_bound=jnp.asarray(low_bound, dtype=jnp.float64),
        high_bound=jnp.asarray(high_bound, dtype=jnp.float64),
        min_value=min_value,
        max_value=max_value,
        min_range=jnp.asarray(min_range, dtype=jnp.float64),
    )

  @property
  def _bijector(self) -> tfb.Bijector:
    # The linear transformation is:
    # norm_y = (y - min_value) / (max_value - min_value)
    # y --> norm_y * (self.high_bound - self.low_bound) + self.low_bound
    return tfb.Chain([
        tfb.Shift(self.low_bound),
        self._slope_bijector,
        tfb.Shift(-self.min_value),
    ])

  @property
  def _slope_bijector(self) -> tfb.Bijector:
    return tfb.Scale(
        (self.high_bound - self.low_bound)
        / jnp.maximum(self.max_value - self.min_value, self.min_range),
    )

  def warp(self, y: types.Array) -> jax.Array:
    """Warp the y values into [low_bound, high_bound]."""
    # y shape: (num_samples, num_metrics)
    return self._bijector.forward(y)

  def unwarp(self, y: types.Array) -> jax.Array:
    """Un-warp the y values into [min_value, max_value]."""
    # y shape: (num_samples, num_metrics)
    return self._bijector.inverse(y)

  def unwarp_stddev(self, warped_stddev: types.Array) -> jax.Array:
    """Un-warp the standard deviation."""
    return self._slope_bijector.inverse(warped_stddev)


--- vizier/_src/algorithms/designers/gp/output_warpers_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for outputwarpers."""

from jax import numpy as jnp
import numpy as np
import scipy
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.designers.gp import output_warpers
from absl.testing import absltest
from absl.testing import parameterized


OutputWarper = output_warpers.OutputWarper


@absltest.skipThisClass('Base class')
class _OutputWarperTestCase(absltest.TestCase):

  @property
  def warper(self) -> OutputWarper:
    raise RuntimeError('Subclasses should override this method!')

  @property
  def always_maps_to_finite(self) -> bool:
    # Override it to True if the warper should map every value to a
    # finite value.
    return False

  def labels_with_outliers(self):
    return np.array([[1.0], [1.0], [5.0], [-1e80], [np.nan], [-np.inf]])

  def test_always_maps_to_finite(self):
    if not self.always_maps_to_finite:
      self.skipTest('This class does not map every value to a finite value.')

    labels = np.array([[1.0], [1.0], [5.0], [-1e80], [np.nan], [-np.inf]])
    labels_warped = self.warper.warp(labels)
    np.testing.assert_allclose(
        np.isfinite(labels_warped), True, err_msg=f'warped: {labels_warped}'
    )

  def test_input_is_not_mutated(self):
    labels_input = np.array([[1.0], [1.0], [5.0], [10.0]])
    _ = self.warper.warp(labels_input)
    self.assertTrue(
        (
            labels_input.flatten()
            == np.array([[1.0], [1.0], [5.0], [10.0]]).flatten()
        ).all()
    )

  def test_shape_is_preserved(self):
    labels = self.labels_with_outliers()
    labels_warped = self.warper.warp(labels)
    self.assertEqual(labels_warped.shape, labels.shape)

  def test_preserve_rank_despite_outliers(self):
    labels = self.labels_with_outliers()
    finite_indices = np.isfinite(labels)
    labels_warped = self.warper.warp(labels)
    np.testing.assert_array_equal(
        scipy.stats.rankdata(labels[finite_indices]),
        scipy.stats.rankdata(labels_warped[finite_indices]),
        err_msg=f'Unwarped: {labels}\nWarped: {labels_warped}',
    )

  def test_preserve_rank_if_no_outliers(self):
    labels = np.array([[1.0], [1.0], [5.0], [-1], [-4], [np.nan], [np.nan]])
    finite_indices = np.isfinite(labels)
    labels_warped = self.warper.warp(labels)
    np.testing.assert_array_equal(
        scipy.stats.rankdata(labels[finite_indices]),
        scipy.stats.rankdata(labels_warped[finite_indices]),
        err_msg=f'Unwarped: {labels}\nWarped: {labels_warped}',
    )

  def test_finite_maps_to_finite(self):
    labels = self.labels_with_outliers()
    finite_indices = np.isfinite(labels)
    labels_warped = self.warper.warp(labels)
    np.testing.assert_allclose(
        np.isfinite(labels_warped[finite_indices]),
        True,
        err_msg=f'warped: {labels_warped}',
    )


class DefaultOutputWarperTest(_OutputWarperTestCase, parameterized.TestCase):

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.create_default_warper()

  @property
  def always_maps_to_finite(self) -> bool:
    return True

  def test_unwarp_duplicate_labels(self):
    warper = self.warper
    _ = warper.warp(np.array([[1.0], [1.0], [5.0], [-1e80]]))
    labels = np.array([[1.0], [15.0], [10.0], [1.0]])
    np.testing.assert_array_equal(
        scipy.stats.rankdata(warper.unwarp(labels).flatten(), method='dense'),
        scipy.stats.rankdata(labels.flatten(), method='dense'),
    )

  @parameterized.parameters([
      dict(labels=np.zeros(shape=(5, 1))),
      dict(labels=np.ones(shape=(5, 1))),
      dict(labels=100 * np.ones(shape=(5, 1))),
      dict(labels=-100 * np.ones(shape=(5, 1))),
  ])
  def test_all_identical_values_map_to_zero(self, labels):
    np.testing.assert_array_equal(self.warper.warp(labels), 0.0)

  @parameterized.named_parameters([
      dict(
          testcase_name='case1',
          unwarped=np.array(
              [[1.0], [1.0], [5.0], [-1e80], [np.nan], [-np.inf]]
          ),
          expected=np.array([
              [0.61848423],
              [0.61848423],
              [1.25966537],
              [0.25966537],
              [-1.24033463],
              [-1.24033463],
          ]),
      ),
      dict(
          testcase_name='case_all_NaNs',
          unwarped=np.array([[np.nan], [np.nan]]),
          expected=np.array([
              [-1.0],
              [-1.0],
          ]),
      ),
  ])
  def test_known_arrays(self, unwarped: np.ndarray, expected: np.ndarray):
    actual = self.warper.warp(unwarped)
    np.testing.assert_allclose(actual, expected, err_msg=f'actual: {actual}')

  def test_default_warper_empty_warpers(self):
    with self.assertRaises(ValueError):
      output_warpers.create_default_warper(
          half_rank_warp=False, log_warp=False, infeasible_warp=False
      )

  def test_unwarp(self):
    warper = self.warper
    labels_arr = np.array(
        [[-100.0], [-200.0], [1.0], [2.0], [3.0], [10.0], [15.0]]
    )
    np.testing.assert_array_almost_equal(
        warper.unwarp(warper.warp(labels_arr)), labels_arr
    )


class ZScoreLabelsTest(_OutputWarperTestCase):

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.ZScoreLabels()

  def test_preserve_rank_despite_outliers(self):
    # TODO: Fix this test, or add an explanation why this can be skipped.
    pass

  def test_known_arrays(self):
    # TODO: Add a couple of parameterized test cases.
    self.skipTest('No test cases provided')


class NormalizeLabelsTest(_OutputWarperTestCase):

  def setUp(self):
    super().setUp()
    self.labels_arr = np.asarray([10.0, 15.0, 20.0])[:, np.newaxis]

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.NormalizeLabels()

  def labels_with_outliers(self):
    # Uses a less extreme outlier (-1e10 instead of -1e80) because linear
    # warping from [-1e80, 5.0] to [0.0, 1.0] maps 1.0, 1.0, 5.0, -1e80 to
    # 1.0, 1.0, 1.0, 0.0 due to numerical precision issues and fails to preserve
    # rank.
    return np.array([[1.0], [1.0], [5.0], [-1e10], [np.nan], [-np.inf]])

  def test_known_arrays(self):
    actual = self.warper.warp(self.labels_arr)
    expected = np.asarray([0.0, 0.5, 1.0])[:, np.newaxis]
    np.testing.assert_allclose(
        actual, expected, err_msg=f'actual: {actual.tolist()}'
    )


class DetectOutliersTest(_OutputWarperTestCase):

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.create_warp_outliers_warper()

  # TODO: Add extra test coverage for the warp_outliers_warper.

  @property
  def always_maps_to_finite(self) -> bool:
    return True


class TransformToGaussianTest(_OutputWarperTestCase):

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.TransformToGaussian()

  def test_finite_maps_to_finite(self):
    # TODO: Fix this test, or add an explanation why this can be skipped.
    pass

  def test_preserve_rank_if_no_outliers(self):
    # TODO: Fix this test, or add an explanation why this can be skipped.
    pass

  def test_preserve_rank_despite_outliers(self):
    # TODO: Fix this test, or add an explanation why this can be skipped.
    pass


class HalfRankComponentTest(_OutputWarperTestCase, parameterized.TestCase):

  def setUp(self):
    super().setUp()
    self.labels_arr = np.asarray(
        [10.0, -100.0, -200.0, 4.0, 0.0, 1.0, 2.0, -200.0, 3.0, 10.0, 15.0]
    )[:, np.newaxis]

  @property
  def warper(self) -> output_warpers.HalfRankComponent:
    return output_warpers.HalfRankComponent()

  @parameterized.named_parameters([
      dict(
          testcase_name='case1',
          unwarped=np.array([[
              np.nan,
              1,
              4,
              2,
              10,
              12,
              -np.inf,
              2,
              3,
              5,
              6,
          ]]).T,
          expected=np.array([
              [np.nan],
              [-2.7145447657886415],
              [4.0],
              [0.3722561569665319],
              [10.0],
              [12.0],
              [np.nan],
              [0.3722561569665319],
              [2.322289907556879],
              [5.0],
              [6.0],
          ]),
      ),
      dict(
          testcase_name='case2',
          unwarped=np.array([[np.nan, -4, -3, -2, 1.1, 1.2, 1.3, 1.4, 1.5]]).T,
          expected=np.array([
              [np.nan],
              [0.7984888240158797],
              [0.9467291870388195],
              [1.0380072549079085],
              [1.1139555940074284],
              [1.2],
              [1.3],
              [1.4],
              [1.5],
          ]),
      ),
      dict(
          testcase_name='case3',
          unwarped=np.array(
              [[np.nan, 1, 2, 3, 4, 4, 6, 7, 10, 11, 12]], dtype=np.float64
          ).T,
          expected=np.array([
              [np.nan],
              [-2.3573836671676096],
              [0.7453945664588675],
              [2.655910679724611],
              [4.2455644597926385],
              [4.2455644597926385],
              [6.0],
              [7.0],
              [10.0],
              [11.0],
              [12.0],
          ]),
      ),
  ])
  def test_known_arrays(self, unwarped: np.ndarray, expected: np.ndarray):
    actual = self.warper.warp(unwarped)
    np.testing.assert_allclose(
        actual, expected, err_msg=f'actual: {actual.tolist()}'
    )

  def test_unwarp_shape(self):
    warper = self.warper
    _ = warper.warp(self.labels_arr)
    np.testing.assert_equal(
        warper.unwarp(self.labels_arr).shape, self.labels_arr.shape
    )

  def test_bijective_at_exact_points(self):
    warper = self.warper
    labels_arr_warped = warper.warp(self.labels_arr)
    np.testing.assert_array_almost_equal(
        self.labels_arr, warper.unwarp(labels_arr_warped)
    )

  def test_unwarp_preserve_rank_interpolate(self):
    """Tests rank preservation among points interpolated between the training labels."""
    warper = self.warper
    _ = warper.warp(self.labels_arr)
    labels_test_warped = np.array([
        [-4.05487106],
        [-9.20688355],
        [-0.80017519],
        [2.0],
        [3.0],
        [10.0],
        [15.0],
        [1.0],
        [11.0],
        [-2.0],
    ])
    labels_test = warper.unwarp(labels_test_warped)
    np.testing.assert_array_almost_equal(
        np.argsort(labels_test, axis=0), np.argsort(labels_test_warped, axis=0)
    )

  def test_unwarp_preserve_rank_extrapolate(self):
    """Tests rank preservation among points extrapolated beyond the training labels."""
    warper = self.warper
    _ = warper.warp(self.labels_arr)
    labels_test_warped = np.array([
        [-4.05487106],
        [-9.20688355],
        [-0.80017519],
        [2.0],
        [3.0],
        [10.0],
        [15.0],
        [-10.0],
        [-20.0],
        [30.0],
        [50.0],
    ])
    labels_test = warper.unwarp(labels_test_warped)
    np.testing.assert_array_almost_equal(
        np.argsort(labels_test, axis=0), np.argsort(labels_test_warped, axis=0)
    )


class LogWarperComponentTest(_OutputWarperTestCase):

  def setUp(self):
    super().setUp()
    self.labels_arr = np.array(
        [[-100.0], [-200.0], [1.0], [2.0], [3.0], [10.0], [15.0]]
    )

  @property
  def warper(self) -> OutputWarper:
    return output_warpers.LogWarperComponent()

  def test_preserve_rank_despite_outliers(self):
    # TODO: Fix this test, or add an explanation why this can be skipped.
    pass

  def test_known_arrays(self):
    # TODO: Add a couple of parameterized test cases.
    self.skipTest('No test cases provided')

  def test_unwarp_shape(self):
    warper = self.warper
    _ = warper.warp(self.labels_arr)
    np.testing.assert_equal(
        warper.unwarp(self.labels_arr).shape, self.labels_arr.shape
    )

  def test_warp_shape(self):
    warper = self.warper
    _ = warper.warp(self.labels_arr)
    np.testing.assert_equal(
        warper.unwarp(self.labels_arr).shape, self.labels_arr.shape
    )

  def test_unwarp_values(self):
    warper = self.warper
    labels_arr_warped = warper.warp(self.labels_arr)
    expected = np.array([
        [-90.7415054],
        [-137.82329233],
        [-38.55794983],
        [-38.01309563],
        [-37.46762612],
        [-33.63193395],
        [-30.87322547],
    ])
    np.testing.assert_array_almost_equal(
        warper.unwarp(labels_arr_warped / 2), expected
    )

  def test_bijective_at_exact_points(self):
    warper = self.warper
    labels_arr_warped = warper.warp(self.labels_arr)
    np.testing.assert_array_almost_equal(
        self.labels_arr, warper.unwarp(labels_arr_warped)
    )

  def test_unwarp_preserve_rank_interpolate(self):
    """Tests rank preservation among points interpolated between the training labels.

    In details, the interplated array is labels_test_warped which includes
    mid-points between every two consective elements in the warped sorted array.
    We test wether the rank of warped labels augmented with the interpolated
    array in the warped domain is equal to the rank of unwarped labels and the
    unwarped interpolated array.
    """
    warper = self.warper
    labels_arr_warped = warper.warp(self.labels_arr)
    labels_test_warped = (
        np.sort(labels_arr_warped, axis=0)[0:-1]
        + np.sort(labels_arr_warped, axis=0)[1:]
    ) / 2
    labels_test = warper.unwarp(labels_test_warped)
    labels_all_warped = np.vstack((labels_arr_warped, labels_test_warped))
    labels_all = np.vstack((self.labels_arr, labels_test))
    np.testing.assert_array_almost_equal(
        np.argsort(labels_all_warped, axis=0),
        np.argsort(labels_all, axis=0),
    )

  def test_unwarp_preserve_rank_extrapolate(self):
    """Tests rank preservation among points extrapolated out of the range of the training labels."""

    warper = self.warper
    labels_arr_warped = warper.warp(self.labels_arr)
    labels_test_warped = np.array(
        [[np.min(labels_arr_warped) - 10.0], [np.max(labels_arr_warped) + 10.0]]
    )
    labels_test = warper.unwarp(labels_test_warped)
    labels_all_warped = np.vstack((labels_arr_warped, labels_test_warped))
    labels_all = np.vstack((self.labels_arr, labels_test))
    np.testing.assert_array_almost_equal(
        np.argsort(labels_all_warped, axis=0),
        np.argsort(labels_all, axis=0),
    )


class InfeasibleWarperTest(parameterized.TestCase):

  @property
  def always_maps_to_finite(self) -> bool:
    return True

  def test_warper_removes_nans(self):
    warper_infeasible = output_warpers.InfeasibleWarperComponent()
    labels = np.array(
        [[-200.0], [np.nan], [-1000.0], [np.nan], [1.0], [2.0], [3.0]]
    )

    labels_warped_infeasible = warper_infeasible.warp(labels)
    self.assertEqual(np.isnan(labels_warped_infeasible).sum(), 0)

  def test_all_nans(self):
    warper_infeasible = output_warpers.InfeasibleWarperComponent()
    labels = np.array([[np.nan], [np.nan], [np.nan], [np.nan]])
    labels_warped_infeasible = warper_infeasible.warp(labels)
    expected = np.array([[0.0], [0.0], [0.0], [0.0]])
    np.testing.assert_equal(labels_warped_infeasible, expected)

    unwarped = warper_infeasible.unwarp(labels_warped_infeasible)
    np.testing.assert_equal(unwarped, labels)

  def test_known_arrays(self):
    # TODO: Add a couple of parameterized test cases.
    self.skipTest('No test cases provided')


class BijectorWarperTest(absltest.TestCase):

  def test_trivial(self):
    warper = output_warpers.BijectorWarper(
        lambda arr: tfp.bijectors.Shift(arr.mean())
    )

    original = np.array([
        0.0,
        1.0,
        2.0,
    ])
    warped = original + 1
    np.testing.assert_allclose(warped, warper.warp(original))
    np.testing.assert_allclose(original, warper.unwarp(warped))


class OutputWarperPipelineTest(absltest.TestCase):
  """Tests the default outpur warper edge cases."""

  def test_all_nonfinite_labels(self):
    warper = output_warpers.OutputWarperPipeline()
    labels_infeaible = np.array([[-np.inf], [np.nan], [np.nan], [-np.inf]])
    self.assertTrue(
        (
            warper.warp(labels_infeaible)
            == -1 * np.ones(shape=labels_infeaible.shape).flatten()
        ).all()
    )


class LinearOutputWarperTest(parameterized.TestCase):
  """Tests for LinearOutputWarperTest."""

  @parameterized.parameters(
      {'low': -2.0, 'high': 2.0, 'dtype': 'numpy'},
      {'low': -5.0, 'high': 7.0, 'dtype': 'numpy'},
      {'low': 0.0, 'high': 1.0, 'dtype': 'jax'},
      {'low': 8.0, 'high': 10.0, 'dtype': 'jax'},
      {'low': -10.0, 'high': -2.0, 'dtype': 'jax'},
  )
  def test_warp_unwarp_and_range(self, low, high, dtype):
    num_samples = 50
    num_metrics = 3
    y = np.random.randn(num_samples, num_metrics)
    if dtype == 'jax':
      y = jnp.asarray(y, dtype=np.float64)
    output_warper = output_warpers.LinearOutputWarper.from_obs(
        y_obs=y, low_bound=low, high_bound=high
    )
    np.testing.assert_allclose(
        output_warper.unwarp(output_warper.warp(y)),
        y,
        atol=1e-05,
    )
    np.testing.assert_allclose(
        np.min(output_warper.warp(y), axis=0),
        np.array([low] * num_metrics),
        atol=1e-05,
    )
    np.testing.assert_allclose(
        np.max(output_warper.warp(y), axis=0),
        np.array([high] * num_metrics),
        atol=1e-05,
    )

  @parameterized.parameters(
      {'dtype': 'numpy', 'label_value': 0.0},
      {'dtype': 'jax', 'label_value': 123.0},
      {'dtype': 'jax', 'label_value': -54321.0},
  )
  def test_warp_unwarp_constant_labels(self, dtype: str, label_value: float):
    y = np.ones(shape=(10, 3)) * label_value
    if dtype == 'jax':
      y = jnp.asarray(y, dtype=np.float64)
    output_warper = output_warpers.LinearOutputWarper.from_obs(
        y_obs=y, low_bound=-2.0, high_bound=2.0
    )
    warped_y = output_warper.warp(y)
    np.testing.assert_allclose(
        warped_y,
        -2.0,
        atol=1e-5,
    )
    np.testing.assert_allclose(
        output_warper.unwarp(warped_y),
        y,
        atol=1e-5,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/gp/transfer_learning.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Algorithms for transfer learning."""

import chex
import equinox as eqx
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp


tfd = tfp.distributions


class TransferPredictionState(eqx.Module):
  """State and metadata for combining this prediction in transfer learning.

  Attributes:
    pred: The prediction to combine
    aux: Auxiliary information about the prediction
    training_data_count: Number of samples to train the `Predictive` that made
      the prediction.
    num_hyperparameters: Hyperparameters of the `Predictive` that made the
      prediction.
  """

  pred: tfd.Distribution
  aux: chex.ArrayTree
  training_data_count: int
  num_hyperparameters: int


def _compute_dof(training_data_count: int, num_hyperparameters: int) -> float:
  """Computes the DOF of a `Predictive`.

  This is a maximum of two measures of DOF.

  The first represents the DOF associated with a log likelihood computation,
  after optimizing the hyperparameters of the kernel, i.e. the
  degrees-of-freedom (dof) of a finite linear regression problem.

  The second represents the fact we know something about the standard
  deviation even when there are more hyperpameters than training data.

  Args:
    training_data_count: Number of samples used to train the `Predictive`.
    num_hyperparameters: Number of hyperparameters in the `Predictive`.

  Returns:
    Returns DOF of the predictive
  """
  return max(
      training_data_count - num_hyperparameters,
      training_data_count / (1 + num_hyperparameters),
  )


def combine_predictions_with_aux(
    top_pred: TransferPredictionState,
    base_pred: TransferPredictionState,
    *,
    expected_base_stddev_mismatch: float = 1.0
) -> tuple[tfd.Distribution, chex.ArrayTree]:
  """Combines two predictions from transfer learning.

  The means are combined as a simple sum.

  The standard deviations are combined using a geometric mean, with a
  weighting coefficient `alpha` that sets their relative importance.

  See the below code for the exact computation of `alpha`, which is a function
  of the uncertainty of the base to the uncertainty of the top-level
  prediction.
  Args:
    top_pred: Prediction from the top model (trained on the base's residuals)
    base_pred: Prediction from the base model (trained on the full data)
    expected_base_stddev_mismatch: Used for combining a base standard deviation
      with the top-level model's standard deviation estimate. Formally, it is
      the expected RMS fractional mismatch in standard deviation between a
      typical base and a typical top-level model (averaged over the feasible
      region). Allowable values are [0, 1] with (0.1, 0.8) being more likely.
      Assumes that the value is allowable, due to compatibility with `jax` and
      avoiding `jax.checkify`. Unexpected results may occur if value is set
      out-of-bounds.

  Returns:
    The combined distribution, assumed to be Normal, and auxiliary information.
  """

  dof_base = _compute_dof(
      training_data_count=base_pred.training_data_count,
      num_hyperparameters=base_pred.num_hyperparameters,
  )
  dof_top = _compute_dof(
      training_data_count=top_pred.training_data_count,
      num_hyperparameters=top_pred.num_hyperparameters,
  )

  # `beta_squared` is the ratio of uncertainty of the base to the uncertainty
  # in the top-level model.  More precisely, it is the
  # variance{ log { stddev returned by the base}} /
  # variance{ log { stddev returned by the top model}}.
  # This is a large number when the top-level stddev is more trustworthy, and
  # small when the base stddev is relatively trustworthy.
  beta_squared = (dof_top / dof_base) * (
      1 + dof_base + (expected_base_stddev_mismatch**2)
  )

  # Finally, compute the geometric mean weight, `alpha`.
  alpha = beta_squared / (1 + beta_squared)

  # Combine the means.
  comb_mean = top_pred.pred.mean() + base_pred.pred.mean()

  # Use `alpha` to combine the stddevs in a weighted geometric mean.
  comb_stddev = jnp.power(top_pred.pred.stddev(), alpha) * jnp.power(
      base_pred.pred.stddev(), (1 - alpha)
  )

  prev_aux = {
      'base_aux': base_pred.aux,
      'top_aux': top_pred.aux,
  }

  # Entries in `aux` must have the same batch shape as the predictions.
  batch_shape = comb_mean.shape[0]
  aux = {
      'prev_aux': prev_aux,
      'mean': comb_mean,
      'stddev': comb_stddev,
      'alpha': jnp.ones(batch_shape) * alpha,
      'expected_base_stddev_mismatch': (
          jnp.ones(batch_shape) * expected_base_stddev_mismatch
      ),
      'beta_squared': jnp.ones(batch_shape) * beta_squared,
  }

  # Assume a multivariate normal distribution with diagonal covariance as output
  return tfd.MultivariateNormalDiag(loc=comb_mean, scale_diag=comb_stddev), aux


--- vizier/_src/algorithms/designers/gp/transfer_learning_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for `transfer_learning.py`."""

import chex
import jax
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.designers.gp import transfer_learning as vtl

from absl.testing import absltest


tfd = tfp.distributions


class GoogleGpBanditTest(absltest.TestCase):

  def test_sequential_combine_predictions(self):
    prior_pred = vtl.TransferPredictionState(
        pred=tfd.Normal(loc=[0.1, 0.2], scale=[10.0, 5.0]),
        aux={},
        training_data_count=10,
        num_hyperparameters=5,
    )  # Try one case with `training_data_count` > `num_hyperparameters`.

    top_pred = vtl.TransferPredictionState(
        pred=tfd.Normal(loc=[0.1, 0.2], scale=[10.0, 5.0]),
        aux={},
        training_data_count=10,
        num_hyperparameters=15,
    )  # Try one case with `training_data_count` < `num_hyperparameters`.

    comb_pred, comb_aux = vtl.combine_predictions_with_aux(
        top_pred=top_pred, base_pred=prior_pred
    )

    # The sum of means should be precisely `0.1 + 0.1` and `0.2 + 0.2`.
    self.assertEqual(list(comb_pred.mean()), [0.2, 0.4])

    # The combination of stddevs should be approximately the same as each
    # individual standard deviation, because we are combining the same
    # standard deviations together.
    self.assertSequenceAlmostEqual(comb_pred.stddev(), [10.0, 5.0], places=4)

    # Batch shapes in aux must be the same as the batch shape of predictions
    chex.assert_tree_shape_prefix(comb_aux, [2])


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  jax.config.update('jax_enable_x64', True)
  jax.config.update('jax_log_compiles', True)
  absltest.main()


--- vizier/_src/algorithms/designers/gp/yjt.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Temporary file for finding the optimal Yeo-Johnson transformation."""

from typing import Literal

from absl import logging
import numpy as np
from sklearn import preprocessing
from tensorflow_probability.substrates import jax as tfp

tfsb = tfp.staging.bijectors
tfb = tfp.bijectors
tfd = tfp.distributions


# TODO: Use Jax or numpy to find the optimum, and remove
# dependency on sklearn.
def optimal_transformation(
    data: np.ndarray,
    method: Literal['yeo-johnson', 'box-cox'] = 'yeo-johnson',
    *,
    standardize: bool = True) -> tfb.AutoCompositeTensorBijector:
  """Returns the power transformation with optimal parameterization.

  The optimal parameterization makes the transformed data as "normal"-esque
  as possible.

  Args:
    data: 1-D or 2-D array. If 1-D, then the bijector has batch_shape =[]. If
      2-D, then the bijector has batch shape equal to the last dimension
    method: 'yeo-johnson' or 'box-cox'. Boxcox can only be used for
      positive-only data.
    standardize: If True, returns a bijector that applies a power transform and
      then normalize so that the data maps to zero mean unit stddev normal. 1e-6
      is added to the stddev so that division by zero never happens.

  Returns:
    Bijector that maps data such that it follows a normal distribution.
    (standard normal if standardize=True).
  """
  dtype = data.dtype
  dimension = len(data.shape)
  if dimension not in {1, 2}:
    raise ValueError('Data must be 1-D or 2-D array')

  if dimension == 1:
    # PowerTransformer.fit() expects 2D array.
    data = data[:, np.newaxis]
    reduce_axis = None
  else:
    reduce_axis = 0

  if method == 'yeo-johnson':
    # For yeo-johnson, center the median to zero.
    # In the long run, we should consider identifying outliers that are very
    # far away from the optimum, and softclipping them to reasonable numbers.
    # This will help prevent them from having too much influence in deciding
    # the warping parameters.
    medians = np.median(data, axis=reduce_axis)
    shift1 = tfb.Shift(-medians)
    data = shift1(data)
  else:
    shift1 = tfb.Identity()
  lambdas = preprocessing.PowerTransformer(
      method, standardize=False).fit(data).lambdas_.astype(dtype)

  logging.info('Optimal lambda was: %s, %s', lambdas, lambdas.dtype)

  if dimension == 1:
    # Make it a scalar, so we don't end up with batch_shape = [1] in the
    # bijector.
    lambdas = lambdas.item()
  if method == 'yeo-johnson':
    # Cast the default values of `rho` and `shift` to the same dtype as `data`
    # to avoid dtype mismatch errors.
    warp = tfsb.YeoJohnson(
        lambdas, rho=np.asarray(2.0, dtype=dtype), shift=np.asarray(1.0, dtype)
    )
  elif method == 'box-cox':
    # Cast the default values of `rho` and `shift` to the same dtype as `data`
    # to avoid dtype mismatch errors.
    warp = tfsb.YeoJohnson(
        lambdas, rho=np.asarray(2.0, dtype), shift=np.asarray(0.0, dtype)
    )
  else:
    raise ValueError(f'Unknown method: {method}')

  if standardize:
    transformed = warp(data)  # 2-D array.
    shift2 = tfb.Shift(-np.mean(transformed, axis=reduce_axis))
    scale = tfb.Scale(1.0 / (np.std(transformed, axis=reduce_axis) + 1e-6))
    return tfb.Chain([scale, shift2, warp, shift1])
  else:
    return tfb.Chain([warp, shift1])


--- vizier/_src/algorithms/designers/gp_bandit.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""GP-Bandit using a Flax model and a TFP Gaussian Process.

A Python implementation of Google Vizier's GP-Bandit algorithm.
"""

# pylint: disable=logging-fstring-interpolation, g-long-lambda

import copy
import dataclasses
import datetime
import random
from typing import Optional, Sequence

from absl import logging
import attr
import equinox as eqx
import jax
import jax.numpy as jnp
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers.gp import acquisitions as acq_lib
from vizier._src.algorithms.designers.gp import gp_models
from vizier._src.algorithms.designers.gp import output_warpers
from vizier._src.algorithms.optimizers import eagle_strategy as es
from vizier._src.algorithms.optimizers import lbfgsb_optimizer as lo
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier.jax import optimizers
from vizier.pyvizier import converters
from vizier.pyvizier.converters import padding
from vizier.utils import profiler


# The maximum number of feasible values to use for the trust region. If a
# discrete or integer parameter has more than this many feasible values, it is
# considered continuous and always included in the trust region computation.
_MAX_NUM_FEASIBLE_VALUES_FOR_TRUST_REGION = 1000


default_acquisition_optimizer_factory = vb.VectorizedOptimizerFactory(
    strategy_factory=es.VectorizedEagleStrategyFactory(
        eagle_config=es.EagleStrategyConfig()
    ),
    max_evaluations=75_000,
    suggestion_batch_size=25,
)

default_scoring_function_factory = acq_lib.bayesian_scoring_function_factory(
    lambda _: acq_lib.UCB()
)


def _experimental_override_allowed(fun):
  """No-op.

  Marks functions that can be easily overridden for experimentation.

  Args:
    fun:

  Returns:
    fun:
  """
  return fun


@attr.define(auto_attribs=False)
class VizierGPBandit(vza.Designer, vza.Predictor):
  """GP-Bandit using a Flax model.

  A minimal example of creating this designer:
  problem = vz.ProblemStatement(...)  # Configure a minimal problem statement.
  designer = VizierGPBandit(problem)

  Optionally set other attributes to change the defaults, e.g.:
  problem = vz.ProblemStatement(...)  # Configure a minimal problem statement.
  designer = VizierGPBandit(problem, use_trust_region=False)

  Attributes:
    problem: Must be a flat study with a single metric.
    acquisition_optimizer: Typically either a designer wrapped as an optimizer
      or a batched optimizer (like Eagle).
    ard_optimizer: An optimizer which should return a batch of hyperparameters
      to be ensembled.
    ard_random_restarts: The number of random initializations to run GP
      hyper-parameter optimization with.
    num_seed_trials: If greater than zero, first trial is the center of the
      search space. Afterwards, uses quasirandom until this number of trials are
      observed.
    scoring_function_factory: Callable that returns the scoring function to use.
    use_trust_region: Uses trust region to constrain initial exploration.
    rng: If not set, uses random numbers.
    metadata_ns: Metadata namespace that this designer writes to.
  """

  _problem: vz.ProblemStatement = attr.field(kw_only=False)
  _acquisition_optimizer_factory: vb.VectorizedOptimizerFactory = attr.field(
      default=default_acquisition_optimizer_factory,
      kw_only=True,
  )
  _ard_optimizer: optimizers.Optimizer[types.ParameterDict] = attr.field(
      factory=optimizers.default_optimizer,
      kw_only=True,
  )
  _ard_random_restarts: int = attr.field(
      default=optimizers.DEFAULT_RANDOM_RESTARTS, kw_only=True
  )
  _num_seed_trials: int = attr.field(default=1, kw_only=True)
  # If used, should set to 1.0 as prior uses a sum of Matern and linear but ARD
  # still tunes its amplitude. Only used for single-objective.
  _linear_coef: Optional[float] = attr.field(default=None, kw_only=True)
  _scoring_function_factory: acq_lib.ScoringFunctionFactory = attr.field(
      default=default_scoring_function_factory,
      kw_only=True,
  )
  _scoring_function_is_parallel: bool = attr.field(default=False, kw_only=True)
  # Whether to pad all inputs, and what type of schedule to use. This is to
  # ensure fewer JIT compilation passes. (Default implies no padding.)
  _padding_schedule: padding.PaddingSchedule = attr.field(
      factory=padding.PaddingSchedule, kw_only=True
  )
  _use_trust_region: bool = attr.field(default=True, kw_only=True)
  _rng: jax.Array = attr.field(
      factory=lambda: jax.random.PRNGKey(random.getrandbits(32)), kw_only=True
  )
  _metadata_ns: str = attr.field(
      default='oss_gp_bandit', kw_only=True, init=False
  )
  _ensemble_size: Optional[int] = attr.field(default=1, kw_only=True)
  _output_warper: output_warpers.OutputWarper = attr.field(
      factory=output_warpers.create_default_warper, kw_only=True
  )

  # Multi-objective parameters.
  _num_scalarizations: int = attr.field(default=1000, kw_only=True)
  _ref_scaling: float = attr.field(default=0.01, kw_only=True)
  _multitask_type: multitask_tuned_gp_models.MultiTaskType = attr.field(
      default=multitask_tuned_gp_models.MultiTaskType.INDEPENDENT,
      kw_only=True,
  )

  # ------------------------------------------------------------------
  # Internal attributes which should not be set by callers.
  # ------------------------------------------------------------------
  _trials: list[vz.Trial] = attr.field(factory=list, init=False)
  # The number of trials that have been incorporated
  # into the designer state (Cholesky decomposition, ARD).
  _incorporated_trials_count: int = attr.field(
      default=0, kw_only=True, init=False
  )
  _acquisition_optimizer: vb.VectorizedOptimizer = attr.field(init=False)

  _last_computed_gp: gp_models.GPState = attr.field(init=False)

  # The prior GP used in transfer learning. `last_computed_gp` is trained
  # on the residuals of `_prior_gp`, if one is trained.
  _prior_gp: Optional[gp_models.GPState] = attr.field(init=False, default=None)

  def __attrs_post_init__(self):
    # Extra validations
    if self._problem.search_space.is_conditional:
      raise ValueError(f'{type(self)} does not support conditional search.')
    if self._problem.search_space.num_parameters() == 0:
      raise ValueError(
          'SearchSpace should contain at least one parameter config.'
      )

    # Extra initializations.
    # Discrete parameters are continuified to account for their actual values.
    self._converter = converters.TrialToModelInputConverter.from_problem(
        self._problem,
        scale=True,
        max_discrete_indices=0,
        flip_sign_for_minimization_metrics=True,
        padding_schedule=self._padding_schedule,
    )
    self._quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        self._problem.search_space,
        seed=int(jax.random.randint(self._rng, [], 0, 2**16)),
    )

    self._acquisition_optimizer = self._acquisition_optimizer_factory(
        self._converter
    )
    self._acquisition_problem = copy.deepcopy(self._problem)
    empty_data = types.ModelData(
        features=self._converter.to_features([]),
        labels=types.PaddedArray.as_padded(
            np.zeros((0, len(self._problem.metric_information)))
        ),
    )

    # Multi-objective overrides.
    m_info = self._problem.metric_information
    if not m_info.is_single_objective:

      # Create scalarization weights.
      self._rng, weights_rng = jax.random.split(self._rng)

      def acq_fn_factory(data: types.ModelData) -> acq_lib.AcquisitionFunction:
        # Scalarized UCB.
        scalarizer = acq_lib.create_hv_scalarization(
            self._num_scalarizations, data.labels, weights_rng
        )

        labels_array = data.labels.padded_array
        has_labels = labels_array.shape[0] > 0
        max_scalarized = None
        if has_labels:
          max_scalarized = jnp.max(scalarizer(labels_array), axis=-1)

        return acq_lib.ScalarizeOverAcquisitions(
            acquisition_fn=acq_lib.UCB(),
            scalarizer=scalarizer,
            reduction_fn=lambda x: jnp.mean(x, axis=0),
            max_scalarized=max_scalarized,
        )

      self._scoring_function_factory = (
          acq_lib.bayesian_scoring_function_factory(acq_fn_factory)
      )
      self._use_trust_region = False

    # Additional validations
    coroutine = gp_models.get_vizier_gp_coroutine(
        empty_data, multitask_type=self._multitask_type
    )
    params = sp.CoroutineWithData(coroutine, empty_data).setup(self._rng)
    model = sp.StochasticProcessWithCoroutine(coroutine, params)
    predictive = sp.UniformEnsemblePredictive(
        eqx.filter_jit(model.precompute_predictive)(empty_data)
    )
    scoring_fn = self._scoring_function_factory(
        empty_data,
        predictive,
        self._converter.continuous_feasible_values(
            max_num_feasible_values=_MAX_NUM_FEASIBLE_VALUES_FOR_TRUST_REGION
        ),
        self._use_trust_region,
    )
    if (
        isinstance(scoring_fn, acq_lib.MaxValueEntropySearch)
        and self._ensemble_size > 1
    ):
      raise ValueError(
          'MaxValueEntropySearch is not supported with ensemble '
          'size greater than one.'
      )

    acquisition_function = getattr(scoring_fn, 'acquisition_fn', None)
    self._acquisition_problem.metric_information = vz.MetricsConfig()
    if isinstance(acquisition_function, acq_lib.MultiAcquisitionFunction):
      for k in acquisition_function.acquisition_fns.keys():
        metric = vz.MetricInformation(k, goal=vz.ObjectiveMetricGoal.MAXIMIZE)
        self._acquisition_problem.metric_information.append(metric)
    else:
      metric = vz.MetricInformation(
          'acquisition', goal=vz.ObjectiveMetricGoal.MAXIMIZE
      )
      self._acquisition_problem.metric_information.append(metric)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Update the list of completed trials."""
    del all_active
    self._trials.extend(copy.deepcopy(completed.trials))

  def set_priors(self, prior_studies: Sequence[vza.CompletedTrials]) -> None:
    """Updates the list of prior studies for transfer learning.

    Each element is treated as a new prior study, and will be stacked in order
    received - i.e. the first entry is for the first GP, the second entry is for
    the GP trained on the residuals of the first GP, etc.

    See section 3.3 of https://dl.acm.org/doi/10.1145/3097983.3098043 for more
    information, or see `gp/gp_models.py` and `gp/transfer_learning.py`

    Transfer learning is resilient to bad priors.

    Multiple calls are permitted, but unadvised. Each call will trigger
    retraining of the prior GPs - on only the state provided to `set_priors`.
    State is not incrementally updated.

    TODO: Decide on whether this method should become part of an
    interface.

    Args:
      prior_studies: A list of lists of completed trials, with one list per
        prior study. The designer will train a prior GP for each list of prior
        trials (for each `CompletedStudy` entry), in the order received.
    """
    self._rng, ard_rng = jax.random.split(self._rng)
    prior_data = [
        self._trials_to_data(prior_study.trials)
        for prior_study in prior_studies
    ]
    self._prior_gp = self._train_prior_gp(priors=prior_data, ard_rng=ard_rng)

  @property
  def _metric_info(self) -> vz.MetricInformation:
    return self._problem.metric_information.item()

  # TODO: Check the latency of `_generate_seed_trials` and look
  # into reducing it.
  @profiler.record_runtime
  def _generate_seed_trials(self, count: int) -> Sequence[vz.TrialSuggestion]:
    """Generate seed trials.

    The first seed trial is chosen as the search space center, the rest of the
    seed trials are chosen quasi-randomly.

    Arguments:
      count: The number of seed trials.

    Returns:
      The seed trials.
    """
    seed_suggestions = []
    if not self._trials:
      # TODO: Should track number of pending suggestions
      # so we don't suggest the center more than once.
      features = self._converter.to_features([])  # to extract shape.
      # NOTE: The code below assumes that a scaled value of 0.5 corresponds
      #   to the center of the feasible range.  This is true, but only by
      #   accident; ideally, we should get the center from the converters.
      continuous = self._padding_schedule.pad_features(
          0.5 * np.ones([1, features.continuous.shape[1]])
      )
      categorical = self._padding_schedule.pad_features(
          np.zeros([1, features.categorical.shape[1]], dtype=types.INT_DTYPE)
      )
      model_input = types.ModelInput(continuous, categorical)
      parameters = self._converter.to_parameters(model_input)[0]
      suggestion = vz.TrialSuggestion(
          parameters, metadata=vz.Metadata({'seeded': 'center'})
      )
      seed_suggestions.append(suggestion)
    with profiler.timeit('quasi_random_sampler_seed_trials'):
      if (remaining_counts := count - len(seed_suggestions)) > 0:
        seed_suggestions.extend(
            self._quasi_random_sampler.suggest(remaining_counts)
        )
    return seed_suggestions

  @_experimental_override_allowed
  def _warp_labels(self, labels: types.Array) -> types.Array:
    """Subclasses can override this method for experiments."""
    return np.concatenate(
        [
            self._output_warper.warp(labels[:, i : i + 1])
            for i in range(labels.shape[1])
        ],
        axis=-1,
    )

  @profiler.record_runtime
  def _trials_to_data(self, trials: Sequence[vz.Trial]) -> types.ModelData:
    """Convert trials to scaled features and warped labels."""
    model_data = self._converter.to_xy(trials)
    logging.info(
        'Transforming the labels of shape %s. Features has shape: %s',
        model_data.labels.padded_array.shape,
        types.ContinuousAndCategorical(
            model_data.features.continuous.padded_array.shape,
            model_data.features.categorical.padded_array.shape,
        ),
    )

    # Warp the output.
    unpad_labels = np.asarray(model_data.labels.unpad())
    warped_labels = self._warp_labels(unpad_labels)

    labels = types.PaddedArray.from_array(
        warped_labels,
        model_data.labels.padded_array.shape,
        fill_value=model_data.labels.fill_value,
    )
    logging.info('Transformed the labels. Now has shape: %s', labels.shape)
    return types.ModelData(model_data.features, labels)

  @_experimental_override_allowed
  def _create_gp_spec(
      self, data: types.ModelData, ard_rng: jax.Array
  ) -> gp_models.GPTrainingSpec:
    """Overrideable creation of a training spec for a GP model."""
    return gp_models.GPTrainingSpec(
        ard_optimizer=self._ard_optimizer,
        ard_rng=ard_rng,
        coroutine=gp_models.get_vizier_gp_coroutine(
            data=data,
            linear_coef=self._linear_coef,
            multitask_type=self._multitask_type,
        ),
        ensemble_size=self._ensemble_size,
        ard_random_restarts=self._ard_random_restarts,
    )

  @_experimental_override_allowed
  def _train_prior_gp(
      self,
      priors: Sequence[types.ModelData],
      ard_rng: jax.Array,
  ):
    """Trains a transfer-learning-enabled GP with prior studies.

    Args:
      priors: Data for each sequential prior to train for transfer learning.
        Assumed to be in order of training, i.e. element 0 is priors[0] is the
        first GP trained, and priors[1] trains a GP on the residuals of the GP
        trained on priors[0], and so on.
      ard_rng: RNG to do ARD to optimize GP parameters.

    Returns:
      A trained pre-computed ensemble GP.
    """
    ard_rngs = jax.random.split(ard_rng, len(priors))

    # Order `specs` in training order, i.e. `specs[0]` is trained first.
    specs = [
        self._create_gp_spec(prior_data, ard_rngs[i])
        for i, prior_data in enumerate(priors)
    ]

    # `train_gp` expects `specs` and `data` in training order, which is how
    # they were prepared above.
    return gp_models.train_gp(spec=specs, data=priors)

  @profiler.record_runtime
  def _update_gp(self, data: types.ModelData) -> gp_models.GPState:
    """Compute the designer's GP and caches the result. No-op without new data.

    Args:
      data: Data to go into GP.

    Returns:
      `GPState` object containing the trained GP.

    1. Convert trials to features and labels.
    2. Trains a pre-computed ensemble GP.

    If no new trials were added since last call, no update will occur.
    """
    if len(self._trials) == self._incorporated_trials_count:
      # If there's no change in the number of completed trials, don't update
      # state. The assumption is that trials can't be removed.
      return self._last_computed_gp
    self._incorporated_trials_count = len(self._trials)

    self._rng, ard_rng = jax.random.split(self._rng, 2)
    spec = self._create_gp_spec(data, ard_rng)
    if self._prior_gp:
      self._last_computed_gp = gp_models.train_stacked_residual_gp(
          base_gp=self._prior_gp, spec=spec, data=data
      )
    else:
      self._last_computed_gp = gp_models.train_gp(spec=spec, data=data)

    return self._last_computed_gp

  @_experimental_override_allowed
  @profiler.record_runtime
  def _optimize_acquisition(
      self, scoring_fn: acq_lib.BayesianScoringFunction, count: int
  ) -> list[vz.Trial]:
    jax.monitoring.record_event(
        '/vizier/jax/gp_bandit/optimize_acquisition/called'
    )
    # Set up optimizer and run
    seed_features = vb.trials_to_sorted_array(self._trials, self._converter)
    acq_rng, self._rng = jax.random.split(self._rng)

    score = scoring_fn.score
    score_with_aux = scoring_fn.score_with_aux

    n_parallel = None
    if self._scoring_function_is_parallel:
      n_parallel = count
      count = 1

    acquisition_optimizer = self._acquisition_optimizer
    if not isinstance(acquisition_optimizer, lo.LBFGSBOptimizer):
      acquisition_optimizer = eqx.filter_jit(acquisition_optimizer)
    best_candidates: vb.VectorizedStrategyResults = acquisition_optimizer(
        eqx.filter_jit(score),
        prior_features=seed_features,
        count=count,
        seed=acq_rng,
        score_with_aux_fn=eqx.filter_jit(score_with_aux),
        n_parallel=n_parallel,
    )

    best_candidates = dataclasses.replace(
        best_candidates, features=best_candidates.features
    )

    # Convert best_candidates (in scaled space) into suggestions (in unscaled
    # space); also append debug information like model predictions. Output shape
    # [N, D].
    logging.info('Converting the optimization result into suggestions...')
    return vb.best_candidates_to_trials(best_candidates, self._converter)

  @profiler.record_runtime
  def suggest(self, count: int = 1) -> Sequence[vz.TrialSuggestion]:
    logging.info('Suggest called with count=%d', count)
    if count > 1 and not self._scoring_function_is_parallel:
      logging.warning(
          'GAUSSIAN_PROCESS_BANDIT currently is not optimized for batched'
          ' suggestions. Suggestions in the batch are likely to be very'
          ' similar.'
      )
    if len(self._trials) < self._num_seed_trials:
      return self._generate_seed_trials(count)

    suggest_start_time = datetime.datetime.now()
    logging.info('Updating the designer state based on trials...')
    data = self._trials_to_data(self._trials)
    gp = self._update_gp(data)

    # Define acquisition function.
    scoring_fn = self._scoring_function_factory(
        data,
        gp,
        self._converter.continuous_feasible_values(
            max_num_feasible_values=_MAX_NUM_FEASIBLE_VALUES_FOR_TRUST_REGION
        ),
        self._use_trust_region,
    )
    logging.info('Optimizing acquisition: %s', scoring_fn)
    best_trials = self._optimize_acquisition(scoring_fn, count)

    suggestions = []
    for t in best_trials:
      metadata = t.metadata.ns(self._metadata_ns).ns('devinfo')
      metadata['time_spent'] = f'{datetime.datetime.now() - suggest_start_time}'
      suggestions.append(
          vz.TrialSuggestion(parameters=t.parameters, metadata=t.metadata)
      )
    return suggestions

  @profiler.record_runtime
  def sample(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: int = 1000,
  ) -> types.Array:
    """Returns unwarped samples from the model for any given trials.

    Arguments:
      trials: The trials where the predictions will be made.
      rng: The sampling random key.
      num_samples: The number of samples per trial.

    Returns:
      The samples in the specified trials. shape: (num_samples, num_trials)
    """
    if rng is None:
      rng = jax.random.PRNGKey(0)

    if not trials:
      return np.zeros((num_samples, 0))

    data = self._trials_to_data(self._trials)
    gp = self._update_gp(data)
    xs = self._converter.to_features(trials)
    xs = types.ModelInput(
        continuous=xs.continuous.replace_fill_value(0.0),
        categorical=xs.categorical.replace_fill_value(0),
    )
    samples = eqx.filter_jit(acq_lib.sample_from_predictive)(
        gp, xs, num_samples, key=rng
    )  # (num_samples, num_trials)
    # Scope the samples to non-padded only (there's a single padded dimension).
    samples = samples[
        :, ~(xs.continuous.is_missing[0] | xs.categorical.is_missing[0])
    ]
    # TODO: vectorize output warping.
    return np.vstack([
        self._output_warper.unwarp(samples[i][..., np.newaxis]).reshape(-1)
        for i in range(samples.shape[0])
    ])

  @profiler.record_runtime
  def predict(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: Optional[int] = 1000,
  ) -> vza.Prediction:
    """Returns the mean and stddev for any given trials.

    The method performs sampling of the warped GP model, unwarp the samples and
    compute the empirical mean and standard deviation as an apprixmation.

    Arguments:
      trials: The trials where the predictions will be made.
      rng: The sampling random key used for approximation.
      num_samples: The number of samples used for the approximation.

    Returns:
      The predictions in the specified trials.
    """
    unwarped_samples = self.sample(trials, rng, num_samples)
    mean = np.mean(unwarped_samples, axis=0)
    stddev = np.std(unwarped_samples, axis=0)
    return vza.Prediction(mean=mean, stddev=stddev)

  @classmethod
  def from_problem(
      cls,
      problem: vz.ProblemStatement,
      seed: Optional[int] = None,
      **kwargs,
  ) -> 'VizierGPBandit':
    rng = (
        jax.random.PRNGKey(random.getrandbits(32))
        if seed is None
        else jax.random.PRNGKey(seed)
    )
    return cls(problem, rng=rng, **kwargs)


--- vizier/_src/algorithms/designers/gp_bandit_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for gp_bandit."""

from typing import Callable, Union
import unittest
from unittest import mock

import jax
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import gp_bandit
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers.gp import acquisitions
from vizier._src.algorithms.optimizers import eagle_strategy as es
from vizier._src.algorithms.optimizers import lbfgsb_optimizer as lo
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.algorithms.testing import simplekd_runner
from vizier._src.algorithms.testing import test_runners
from vizier._src.benchmarks.experimenters.synthetic import simplekd
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier.jax import optimizers
from vizier.pyvizier import converters
from vizier.pyvizier.converters import padding
from vizier.testing import test_studies
from vizier.utils import profiler

from absl.testing import absltest
from absl.testing import parameterized

mt_type = multitask_tuned_gp_models.MultiTaskType

ard_optimizer = optimizers.default_optimizer()
vectorized_optimizer_factory = vb.VectorizedOptimizerFactory(
    strategy_factory=es.VectorizedEagleStrategyFactory(),
    max_evaluations=10,
)
lbfgsb_optimizer_factory = lo.LBFGSBOptimizerFactory()


def _build_mock_continuous_array_specs(n):
  continuous_spec = mock.create_autospec(converters.NumpyArraySpec)
  continuous_spec.type = converters.NumpyArraySpecType.CONTINUOUS
  continuous_spec.num_dimensions = 1
  return [continuous_spec] * n


def _setup_lambda_search(
    f: Callable[[float], float], num_trials: int = 100
) -> tuple[gp_bandit.VizierGPBandit, list[vz.Trial], vz.ProblemStatement]:
  """Sets up a GP designer and outputs completed studies for `f`.

  Args:
    f: 1D objective to be optimized, i.e. f(x), where x is a scalar in [-5., 5.)
    num_trials: Number of mock "evaluated" trials to return.

  Returns:
  A GP designer set up for the problem of optimizing the objective, without any
  data updated.
  Evaluated trials against `f`.
  """
  assert (
      num_trials > 0
  ), f'Must provide a positive number of trials. Got {num_trials}.'

  search_space = vz.SearchSpace()
  search_space.root.add_float_param('x0', -5.0, 5.0)
  problem = vz.ProblemStatement(
      search_space=search_space,
      metric_information=vz.MetricsConfig(
          metrics=[
              vz.MetricInformation('obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
          ]
      ),
  )

  suggestions = quasi_random.QuasiRandomDesigner(
      problem.search_space, seed=1
  ).suggest(num_trials)

  obs_trials = []
  for idx, suggestion in enumerate(suggestions):
    trial = suggestion.to_trial(idx)
    x = suggestions[idx].parameters['x0'].value
    trial.complete(vz.Measurement(metrics={'obj': f(x)}))
    obs_trials.append(trial)

  gp_designer = gp_bandit.VizierGPBandit(problem, ard_optimizer=ard_optimizer)
  return gp_designer, obs_trials, problem


def _compute_mse(
    designer: gp_bandit.VizierGPBandit,
    test_trials: list[vz.Trial],
    y_test: list[float],
) -> float:
  """Evaluate the designer's accuracy on the test set.

  Args:
    designer: The GP bandit designer to predict from.
    test_trials: The trials of the test set
    y_test: The results of the test set

  Returns:
    The MSE of `designer` on `test_trials` and `y_test`
  """
  preds = designer.predict(test_trials)
  return np.sum(np.square(preds.mean - y_test))


class GoogleGpBanditTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(iters=3, batch_size=2, num_seed_trials=1, ensemble_size=2),
      dict(
          iters=3,
          batch_size=1,
          num_seed_trials=1,
          ensemble_size=2,
          acquisition_optimizer_factory=lbfgsb_optimizer_factory,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          padding_schedule=padding.PaddingSchedule(
              num_trials=padding.PaddingType.MULTIPLES_OF_10,
              num_features=padding.PaddingType.POWERS_OF_2,
          ),
      ),
      dict(
          iters=5,
          batch_size=1,
          num_seed_trials=3,
          padding_schedule=padding.PaddingSchedule(
              num_trials=padding.PaddingType.POWERS_OF_2,
              num_features=padding.PaddingType.POWERS_OF_2,
          ),
          acquisition_optimizer_factory=lbfgsb_optimizer_factory,
      ),
      dict(
          padding_schedule=padding.PaddingSchedule(
              num_trials=padding.PaddingType.POWERS_OF_2,
              num_features=padding.PaddingType.POWERS_OF_2,
          ),
          ensemble_size=3,
      ),
  )
  def test_on_flat_continuous_space(
      self,
      *,
      iters: int = 5,
      batch_size: int = 1,
      num_seed_trials: int = 1,
      ensemble_size: int = 1,
      padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule(),
      use_trust_region: bool = False,
      acquisition_optimizer_factory: Union[
          vb.VectorizedOptimizerFactory, lo.LBFGSBOptimizerFactory
      ] = vectorized_optimizer_factory,
  ):
    # We use string names so that test case names are readable. Convert them
    # to objects.
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    designer = gp_bandit.VizierGPBandit(
        problem=problem,
        acquisition_optimizer_factory=acquisition_optimizer_factory,
        ard_optimizer=optimizers.JaxoptLbfgsB(
            optimizers.LbfgsBOptions(maxiter=5, num_line_search_steps=5)
        ),
        num_seed_trials=num_seed_trials,
        ensemble_size=ensemble_size,
        padding_schedule=padding_schedule,
        use_trust_region=use_trust_region,
        rng=jax.random.PRNGKey(0),
        linear_coef=0.1,
    )
    with profiler.collect_events() as events:
      self.assertLen(
          test_runners.RandomMetricsRunner(
              problem,
              iters=iters,
              batch_size=batch_size,
              verbose=1,
              validate_parameters=True,
              seed=1,
          ).run_designer(designer),
          iters * batch_size,
      )

    self.assertIn('VizierGPBandit.suggest', profiler.get_latencies_dict(events))

    quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        problem.search_space,
    )
    predict_trials = quasi_random_sampler.suggest(count=3)
    # test the sample method.
    samples = designer.sample(predict_trials, num_samples=5)
    self.assertSequenceEqual(samples.shape, (5, 3))
    self.assertFalse(np.isnan(samples).any())
    # test the sample method with a different rng.
    empty_samples = designer.sample(
        [], num_samples=5, rng=jax.random.PRNGKey(1)
    )
    self.assertSequenceEqual(empty_samples.shape, (5, 0))
    # test the predict method.
    prediction = designer.predict(predict_trials)
    self.assertLen(prediction.mean, 3)
    self.assertLen(prediction.stddev, 3)
    self.assertFalse(np.isnan(prediction.mean).any())
    self.assertFalse(np.isnan(prediction.stddev).any())

  @parameterized.parameters(
      dict(iters=3, batch_size=5, num_seed_trials=5),
      dict(iters=5, batch_size=1, num_seed_trials=2),
      dict(iters=5, batch_size=1, num_seed_trials=1),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          padding_schedule=padding.PaddingSchedule(
              num_trials=padding.PaddingType.MULTIPLES_OF_10,
              num_features=padding.PaddingType.POWERS_OF_2,
          ),
      ),
  )
  def test_on_flat_mixed_space(
      self,
      iters: int,
      batch_size: int,
      num_seed_trials: int,
      padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule(),
      use_trust_region: bool = True,
  ):
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    designer = gp_bandit.VizierGPBandit(
        problem=problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        num_seed_trials=num_seed_trials,
        padding_schedule=padding_schedule,
        use_trust_region=use_trust_region,
    )
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=iters,
            batch_size=batch_size,
            verbose=1,
            validate_parameters=True,
        ).run_designer(designer),
        iters * batch_size,
    )
    quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        problem.search_space
    )
    predict_trials = quasi_random_sampler.suggest(count=3)
    # Test the sample method.
    samples = designer.sample(predict_trials, num_samples=5)
    self.assertSequenceEqual(samples.shape, (5, 3))
    # Test the sample method with a different rng.
    samples = designer.sample(
        predict_trials, num_samples=5, rng=jax.random.PRNGKey(1)
    )
    self.assertSequenceEqual(samples.shape, (5, 3))
    self.assertFalse(np.isnan(samples).any())
    empty_samples = designer.sample([], num_samples=5)
    self.assertSequenceEqual(empty_samples.shape, (5, 0))
    # Test the predict method.
    prediction = designer.predict(predict_trials)
    self.assertLen(prediction.mean, 3)
    self.assertLen(prediction.stddev, 3)
    self.assertFalse(np.isnan(prediction.mean).any())
    self.assertFalse(np.isnan(prediction.stddev).any())

  def test_invariance_to_trials_padding_on_flat_mixed_space(
      self,
  ):
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    iters = 5
    num_seed_trials = 1
    acquisition_optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=es.VectorizedEagleStrategyFactory(),
        max_evaluations=100,
    )
    # TODO: The test fails with proper ARD. Fix that and then turn
    # on ARD again.
    noop_ard_optimizer = optimizers.default_optimizer(maxiter=0)
    desinger_rng = jax.random.PRNGKey(0)
    designer = gp_bandit.VizierGPBandit(
        problem=problem,
        acquisition_optimizer_factory=acquisition_optimizer_factory,
        ard_optimizer=noop_ard_optimizer,
        num_seed_trials=num_seed_trials,
        rng=desinger_rng,
    )
    padding_designer = gp_bandit.VizierGPBandit(
        problem=problem,
        acquisition_optimizer_factory=acquisition_optimizer_factory,
        ard_optimizer=noop_ard_optimizer,
        num_seed_trials=num_seed_trials,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10,
        ),
        rng=desinger_rng,
    )
    metrics_runner_seed = 1
    designer_suggestions = test_runners.RandomMetricsRunner(
        problem,
        iters=iters,
        verbose=1,
        seed=metrics_runner_seed,
        validate_parameters=True,
    ).run_designer(designer)

    padding_designer_suggestions = test_runners.RandomMetricsRunner(
        problem,
        iters=iters,
        verbose=1,
        seed=metrics_runner_seed,
        validate_parameters=True,
    ).run_designer(padding_designer)

    self.assertLen(designer_suggestions, iters)
    self.assertLen(padding_designer_suggestions, iters)
    for idx, (suggestion, padding_suggestion) in enumerate(
        zip(designer_suggestions, padding_designer_suggestions)
    ):
      params1 = suggestion.parameters.as_dict()
      params2 = padding_suggestion.parameters.as_dict()
      self.assertSameElements(params1.keys(), params2.keys())
      for key in params1.keys():
        self.assertAlmostEqual(
            params1[key],
            params2[key],
            places=5,
            msg=f'Mismatch in parameter: {key}, suggestions {idx}',
        )

  def test_prediction_accuracy(self):
    f = lambda x: -((x - 0.5) ** 2)
    gp_designer, obs_trials, _ = _setup_lambda_search(f)
    gp_designer.update(vza.CompletedTrials(obs_trials), vza.ActiveTrials())
    pred_trial = vz.Trial({'x0': 0.0})
    pred = gp_designer.predict([pred_trial])
    self.assertLess(np.abs(pred.mean[0] - f(0.0)), 2e-2)

  # TODO: Add assertions to this test. Ideally
  # create two designers with the same trial count (without padding)
  # padding is just an internal detail that should be tested separately.
  def test_jit_once(self, *args):
    del args
    jax.clear_caches()

    space = test_studies.flat_continuous_space_with_scaling()
    problem = vz.ProblemStatement(space)
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    def create_designer(problem):
      return gp_bandit.VizierGPBandit(
          problem=problem,
          acquisition_optimizer_factory=vectorized_optimizer_factory,
          num_seed_trials=3,
          ensemble_size=2,
          padding_schedule=padding.PaddingSchedule(
              num_trials=padding.PaddingType.MULTIPLES_OF_10,
              num_features=padding.PaddingType.MULTIPLES_OF_10,
          ),
      )

    def create_runner(problem):
      return test_runners.RandomMetricsRunner(
          problem,
          iters=5,
          batch_size=1,
          verbose=1,
          validate_parameters=True,
      )

    designer = create_designer(problem)
    # Padding schedule should avoid retracing every iteration.
    create_runner(problem).run_designer(designer)

    # Padding schedule should avoid retracing with one more feature.
    space.root.add_float_param('x0', -5.0, 5.0)
    designer1 = create_designer(problem)
    create_runner(problem).run_designer(designer1)

    # Retracing should not occur when a new VizierGPBandit instance is created.
    designer2 = create_designer(problem)
    create_runner(problem).run_designer(designer2)

  def test_parallel_acquisition(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    def _qei_factory(data: types.ModelData) -> acquisitions.AcquisitionFunction:
      best_labels = acquisitions.get_best_labels(data.labels)
      return acquisitions.QEI(best_labels=best_labels, num_samples=100)

    scoring_fn_factory = acquisitions.bayesian_scoring_function_factory(
        _qei_factory
    )

    n_parallel = 4
    iters = 3
    designer = gp_bandit.VizierGPBandit(
        problem=problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        ard_optimizer=optimizers.JaxoptLbfgsB(
            optimizers.LbfgsBOptions(maxiter=5, num_line_search_steps=5)
        ),
        scoring_function_factory=scoring_fn_factory,
        scoring_function_is_parallel=True,
        use_trust_region=False,
        num_seed_trials=n_parallel,
        ensemble_size=3,
        rng=jax.random.PRNGKey(0),
        linear_coef=0.1,
    )
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=iters,
            batch_size=n_parallel,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(designer),
        iters * n_parallel,
    )

  @parameterized.parameters(
      dict(multitask_type=mt_type.INDEPENDENT),
      dict(multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR),
      dict(multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR),
      dict(multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR),
  )
  def test_multi_metrics(self, multitask_type: mt_type):
    search_space = vz.SearchSpace()
    search_space.root.add_float_param('x0', -5.0, 5.0)
    problem = vz.ProblemStatement(
        search_space=search_space,
        metric_information=vz.MetricsConfig(
            metrics=[
                vz.MetricInformation(
                    'obj1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
                vz.MetricInformation(
                    'obj2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
            ]
        ),
    )

    iters = 2
    designer = gp_bandit.VizierGPBandit(problem, multitask_type=multitask_type)
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=iters,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(designer),
        iters,
    )


class GPBanditSimplekDTest(parameterized.TestCase):
  """Simplekd convergence tests for gp bandit designer."""

  @parameterized.parameters(
      dict(best_category='corner', max_relative_error=0.5),
      dict(best_category='center', max_relative_error=0.5),
      dict(best_category='mixed', max_relative_error=0.5),
  )
  def test_convergence(
      self,
      best_category: simplekd.SimpleKDCategory,
      *,
      max_relative_error: float,
  ) -> None:
    simplekd_runner.SimpleKDConvergenceTester(
        best_category=best_category,
        designer_factory=(
            # pylint: disable=g-long-lambda
            lambda problem, seed: gp_bandit.VizierGPBandit(
                problem,
                rng=jax.random.PRNGKey(seed),
                padding_schedule=padding.PaddingSchedule(
                    num_trials=padding.PaddingType.MULTIPLES_OF_10
                ),
            )
        ),
        num_trials=20,
        max_relative_error=max_relative_error,
        num_repeats=1,
        target_num_convergence=1,
    ).assert_convergence()


# TODO: Fix transfer learning and enable tests.
@unittest.skip('The current transfer learning seems broken and test failing.')
class GPBanditPriorsTest(parameterized.TestCase):

  def test_prior_warping(self):
    f = lambda x: -((x - 0.5) ** 2)
    transform_f = lambda x: -3 * ((x - 0.5) ** 2) + 10

    # X is in range of what is defined in `_setup_lambda_search`, [-5.0, 5.0)
    x_test = np.random.default_rng(1).uniform(-5.0, 5.0, 100)
    y_test = [transform_f(x) for x in x_test]
    test_trials = [vz.Trial({'x0': x}) for x in x_test]

    # Create the designer with a prior and the trials to train the prior.
    gp_designer_with_prior, obs_trials_for_prior, _ = _setup_lambda_search(
        f=f, num_trials=100
    )

    # Set priors to above trials.
    gp_designer_with_prior.set_priors(
        [vza.CompletedTrials(obs_trials_for_prior)]
    )

    # Create a no prior designer on the transformed function `transform_f`.
    # Also use the generated trials to update both the designer with prior and
    # the designer without. This tests that the prior designer is resilient
    # to linear transforms between the prior and the top level study.
    gp_designer_no_prior, obs_trials, _ = _setup_lambda_search(
        f=transform_f, num_trials=20
    )

    # Update both designers with the actual study.
    gp_designer_no_prior.update(
        vza.CompletedTrials(obs_trials), vza.ActiveTrials()
    )
    gp_designer_with_prior.update(
        vza.CompletedTrials(obs_trials), vza.ActiveTrials()
    )

    # Evaluate the no prior designer's accuracy on the test set.
    mse_no_prior = _compute_mse(gp_designer_no_prior, test_trials, y_test)

    # Evaluate the designer with prior's accuracy on the test set.
    mse_with_prior = _compute_mse(gp_designer_with_prior, test_trials, y_test)

    # The designer with a prior should predict better.
    self.assertLess(mse_with_prior, mse_no_prior)

  @parameterized.parameters(
      dict(iters=3, batch_size=5),
      dict(iters=5, batch_size=1),
  )
  def test_run_with_priors(self, *, iters, batch_size):
    f = lambda x: -((x - 0.5) ** 2)

    # Create the designer with a prior and the trials to train the prior.
    gp_designer_with_prior, obs_trials_for_prior, problem = (
        _setup_lambda_search(f=f, num_trials=100)
    )

    # Set priors to the above trials.
    gp_designer_with_prior.set_priors(
        [vza.CompletedTrials(obs_trials_for_prior)]
    )

    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=iters,
            batch_size=batch_size,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(gp_designer_with_prior),
        iters * batch_size,
    )


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  jax.config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/algorithms/designers/gp_ucb_pe.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Gaussian Process Bandit with Pure Exploration using a Flax model and a TFP Gaussian Process."""

# pylint: disable=logging-fstring-interpolation, g-long-lambda

import copy
import datetime
import enum
import random
from typing import Any, Callable, Mapping, Optional, Sequence, Union

from absl import logging
import attr
import chex
import equinox as eqx
import jax
from jax import numpy as jnp
import jaxtyping as jt
import numpy as np
from tensorflow_probability.substrates import jax as tfp  # pylint: disable=g-importing-member
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.designers.gp import acquisitions
from vizier._src.algorithms.designers.gp import output_warpers
from vizier._src.algorithms.optimizers import eagle_strategy as es
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier._src.jax.models import tuned_gp_models
from vizier.jax import optimizers
from vizier.pyvizier import converters
from vizier.pyvizier.converters import padding
from vizier.utils import profiler


tfd = tfp.distributions


# The maximum number of feasible values to use for the trust region. If a
# discrete or integer parameter has more than this many feasible values, it is
# considered continuous and always included in the trust region computation.
_MAX_NUM_FEASIBLE_VALUES_FOR_TRUST_REGION = 1000


class MultimetricPromisingRegionPenaltyType(enum.Enum):
  """The type of penalty to apply to the points outside the promising region.

  Configures the penalty term in `PEScoreFunction` for multimetric problems.
  """

  # The penalty is applied to the points outside the union of the promising
  # regions of all metrics.
  UNION = 'union'
  # The penalty is applied to the points outside the intersection of the
  # promising regions of all metrics.
  INTERSECTION = 'intersection'
  # The penalty applied to a point in the search space is the average of
  # the penalties with respect to the promising regions of all metrics.
  AVERAGE = 'average'


class UCBPEConfig(eqx.Module):
  """UCB-PE config parameters."""

  ucb_coefficient: jt.Float[jt.Array, ''] = eqx.field(
      default=1.8, converter=jnp.asarray
  )
  # A separate ucb coefficient defining the good region to explore.
  explore_region_ucb_coefficient: jt.Float[jt.Array, ''] = eqx.field(
      default=0.5, converter=jnp.asarray
  )
  # The constraint violation penalty is a linear function of the constraint
  # violation, whose slope is determined by this coefficient.
  cb_violation_penalty_coefficient: jt.Float[jt.Array, ''] = eqx.field(
      default=10.0, converter=jnp.asarray
  )
  # Probability of selecting the UCB acquisition function when there are no new
  # completed trials. No-op if `optimize_set_acquisition_for_exploration` below
  # is True.
  ucb_overwrite_probability: jt.Float[jt.Array, ''] = eqx.field(
      default=0.25, converter=jnp.asarray
  )
  # Probability of selecting the PE acquisition function when there are new
  # completed trials.
  pe_overwrite_probability: jt.Float[jt.Array, ''] = eqx.field(
      default=0.1, converter=jnp.asarray
  )
  # The same as `pe_overwrite_probability` but only applies when the noise is
  # estimated to be high.
  pe_overwrite_probability_in_high_noise: jt.Float[jt.Array, ''] = eqx.field(
      default=0.7, converter=jnp.asarray
  )
  # When the ratio between the estimated signal variance and the noise variance
  # is below this threshold, the designer considers the noise to be high and may
  # explore more aggressively. Set to 0.0 to disable this feature.
  signal_to_noise_threshold: jt.Float[jt.Array, ''] = eqx.field(
      default=0.7, converter=jnp.asarray
  )
  # Whether to optimize the set acquisition function for exploration.
  optimize_set_acquisition_for_exploration: bool = eqx.field(
      default=False, static=True
  )
  # The type of penalty to apply to the points outside the promising region for
  # multimetric problems.
  multimetric_promising_region_penalty_type: (
      MultimetricPromisingRegionPenaltyType
  ) = eqx.field(
      default=MultimetricPromisingRegionPenaltyType.AVERAGE, static=True
  )

  # The type of multitask kernel to use for multimetric problems.
  multitask_type: multitask_tuned_gp_models.MultiTaskType = eqx.field(
      default=multitask_tuned_gp_models.MultiTaskType.INDEPENDENT, static=True
  )

  def __repr__(self):
    return eqx.tree_pformat(self, short_arrays=False)


# A dummy loss for ARD when there are no completed trials.
_DUMMY_LOSS = -1.0


def _has_new_completed_trials(
    completed_trials: Sequence[vz.Trial], active_trials: Sequence[vz.Trial]
) -> bool:
  """Returns True iff there are newer completed trials than active trials.

  Args:
    completed_trials: Completed trials.
    active_trials: Active trials.

  Returns:
    True if `completed_trials` is non-empty and:
      - `active_trials` is empty, or
      - The latest `completion_time` among `completed_trials` is
        later than the latest `creation_time` among `active_trials`.
    False: otherwise.
  """

  if not completed_trials:
    return False
  if not active_trials:
    return True

  completed_completion_times = [t.completion_time for t in completed_trials]
  active_creation_times = [t.creation_time for t in active_trials]

  if not all(completed_completion_times):
    raise ValueError('All completed trials must have completion times.')
  if not all(active_creation_times):
    raise ValueError('All active trials must have creation times.')

  return max(completed_completion_times) > max(active_creation_times)  # pytype:disable=unsupported-operands


def _compute_ucb_threshold(
    gprm: tfd.Distribution,
    is_missing: jt.Bool[jt.Array, ''],
    ucb_coefficient: jt.Float[jt.Array, ''],
) -> jax.Array:
  """Computes a threshold on UCB values.

  A promising evaluation point has UCB value no less than the threshold
  computed here. The threshold is the predicted mean of the feature array
  with the maximum UCB value among the points `gprm.index_points`.

  Args:
    gprm: A GP regression model for a set of predictive index points.
    is_missing: A 1-d boolean array indicating whether the corresponding
      predictive index points are missing.
    ucb_coefficient: The UCB coefficient.

  Returns:
    The predicted mean of the feature array with the maximum UCB among `xs`.
  """
  pred_mean = gprm.mean()
  if pred_mean.ndim > 1:
    # In the multimetric case, the predicted mean and stddev are of shape
    # [num_points, num_metrics].
    ucb_values = jnp.where(
        jnp.tile(is_missing[:, jnp.newaxis], (1, pred_mean.shape[-1])),
        -jnp.inf,
        pred_mean + ucb_coefficient * gprm.stddev(),
    )
    # The indices of the points with the maximum UCB values for each metric.
    best_ucb_indices = jnp.argmax(ucb_values, axis=0)
    return jax.vmap(
        lambda pred_mean, best_ucb_idx: pred_mean[best_ucb_idx],
        in_axes=-1,
        out_axes=-1,
    )(pred_mean, best_ucb_indices)
  else:
    # In the single metric case, the predicted mean and stddev are of shape
    # [num_points].
    ucb_values = jnp.where(
        is_missing, -jnp.inf, pred_mean + ucb_coefficient * gprm.stddev()
    )
    return pred_mean[jnp.argmax(ucb_values)]


# TODO: Use acquisitions.TrustRegion instead.
def _apply_trust_region(
    tr: acquisitions.TrustRegion, xs: types.ModelInput, acq_values: jax.Array
) -> jax.Array:
  """Applies the trust region to acquisition function values.

  Args:
   tr: Trust region.
   xs: Predictive index points.
   acq_values: Acquisition function values at predictive index points.

  Returns:
    Acquisition function values with trust region applied.
  """
  distance = tr.min_linf_distance(xs)
  # Due to output normalization, acquisition values can't be as low as -1e4.
  # We use a bad value that decreases in the distance to trust region so that
  # acquisition optimizer can follow the gradient and escape untrustred regions.
  return jnp.where(
      (distance < tr.trust_radius) | (tr.trust_radius > 0.5),
      acq_values,
      -1e4 - distance,
  )


def _apply_trust_region_to_set(
    tr: acquisitions.TrustRegion, xs: types.ModelInput, acq_values: jax.Array
) -> jax.Array:
  """Applies the trust region to a batch of set acquisition function values.

  Args:
   tr: Trust region.
   xs: A batch of predictive index point sets of a fixed size.
   acq_values: A batch of acquisition function values at predictive index point
     sets, shaped as [batch_size].

  Returns:
    Acquisition function values with trust region applied, shaped as
    [batch_size].
  """
  distance = tr.min_linf_distance(xs)  # [batch_size, index_point_set_size]
  # Due to output normalization, acquisition values can't be as low as -1e4.
  # We penalize the acquisition values by an amount that decreases in the
  # total distances to the trust region so that acquisition optimizer can follow
  # the gradient and escape untrustred regions.
  return acq_values + jnp.sum(
      ((distance > tr.trust_radius) & (tr.trust_radius <= 0.5))
      * (-1e4 - distance),
      axis=1,
  )


def _get_features_shape(
    features: types.ModelInput,
) -> types.ContinuousAndCategorical:
  """Gets the shapes of continuous/categorical features for logging."""
  return types.ContinuousAndCategorical(
      features.continuous.shape,
      features.categorical.shape,
  )


class UCBScoreFunction(eqx.Module):
  """Computes the UCB acquisition value.

  The UCB acquisition value is the sum of the predicted mean based on completed
  trials and the predicted standard deviation based on all trials, completed and
  pending (scaled by the UCB coefficient). If `prior_acquisition` is not None,
  the return value is the sum of the prior acquisition value and the UCB
  acquisition value. This class follows the `acquisitions.ScoreFunction`
  protocol.

  Attributes:
    predictive: Predictive model with cached Cholesky conditioned on completed
      trials.
    predictive_all_features: Predictive model with cached Cholesky conditioned
      on completed and pending trials.
    ucb_coefficient: The UCB coefficient.
    trust_region: Trust region.
    prior_acquisition: An optional prior acquisition function.
    scalarization_weights_rng: Random key for scalarization.
    labels: Labels, shaped as [num_index_points, num_metrics].
    num_scalarizations: Number of scalarizations.
  """

  predictive: sp.UniformEnsemblePredictive
  predictive_all_features: sp.UniformEnsemblePredictive
  ucb_coefficient: jt.Float[jt.Array, '']
  trust_region: Optional[acquisitions.TrustRegion]
  prior_acquisition: Callable[[types.ModelInput], jax.Array] | None
  labels: types.PaddedArray
  scalarizer: scalarization.Scalarization

  def __init__(
      self,
      predictive: sp.UniformEnsemblePredictive,
      predictive_all_features: sp.UniformEnsemblePredictive,
      ucb_coefficient: jt.Float[jt.Array, ''],
      trust_region: Optional[acquisitions.TrustRegion],
      prior_acquisition: Callable[[types.ModelInput], jax.Array] | None,
      scalarization_weights_rng: jax.Array,
      labels: types.PaddedArray,
      num_scalarizations: int = 1000,
  ):
    self.predictive = predictive
    self.predictive_all_features = predictive_all_features
    self.ucb_coefficient = ucb_coefficient
    self.trust_region = trust_region
    self.prior_acquisition = prior_acquisition
    self.labels = labels
    self.scalarizer = acquisitions.create_hv_scalarization(
        num_scalarizations, labels, scalarization_weights_rng
    )

  def score(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> jax.Array:
    return self.score_with_aux(xs, seed=seed)[0]

  def aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> chex.ArrayTree:
    return self.score_with_aux(xs, seed=seed)[1]

  def score_with_aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> tuple[jax.Array, chex.ArrayTree]:
    del seed
    gprm = self.predictive.predict(xs)
    gprm_all_features = self.predictive_all_features.predict(xs)
    mean = gprm.mean()
    stddev_from_all = gprm_all_features.stddev()
    acq_values = mean + self.ucb_coefficient * stddev_from_all
    # `self.labels` is of shape [num_index_points, num_metrics].
    if self.labels.shape[1] > 1:
      scalarized = self.scalarizer(acq_values)
      padded_labels = self.labels.replace_fill_value(-np.inf).padded_array
      if padded_labels.shape[0] > 0:
        # Broadcast max_scalarized to the same shape as scalarized and take max.
        max_scalarized = jnp.max(self.scalarizer(padded_labels), axis=-1)
        shape_mismatch = len(scalarized.shape) - len(max_scalarized.shape)
        expand_max = jnp.expand_dims(
            max_scalarized, axis=range(-shape_mismatch, 0)
        )
        scalarized = jnp.maximum(scalarized, expand_max)
      scalarized_acq_values = jnp.mean(scalarized, axis=0)
    else:
      scalarized_acq_values = acq_values
    if self.trust_region is not None:
      scalarized_acq_values = _apply_trust_region(
          self.trust_region, xs, scalarized_acq_values
      )
    aux = {
        'mean': mean,
        'stddev': gprm.stddev(),
        'stddev_from_all': stddev_from_all,
    }
    if self.prior_acquisition is not None:
      prior_acq_values = self.prior_acquisition(xs)
      scalarized_acq_values = prior_acq_values + scalarized_acq_values
      aux['prior_acq_values'] = prior_acq_values
    return scalarized_acq_values, aux


class PEScoreFunction(eqx.Module):
  """Computes the Pure-Exploration acquisition value.

  The PE acquisition value is the predicted standard deviation (eq. (9)
  in https://arxiv.org/pdf/1304.5350) based on all completed and active trials,
  plus a penalty term that grows linearly in the amount of violation of the
  constraint `UCB(xs) >= threshold`. If `prior_acquisition` is not None, the
  returned value is the sum of the prior acquisition value and the PE
  acquisition value. This class follows the `acquisitions.ScoreFunction`
  protocol.

  Attributes:
    predictive: Predictive model with cached Cholesky conditioned on completed
      trials.
    predictive_all_features: Predictive model with cached Cholesky conditioned
      on completed and pending trials.
    ucb_coefficient: The UCB coefficient used to compute the threshold.
    explore_ucb_coefficient: The UCB coefficient used for computing the UCB
      values on `xs`.
    penalty_coefficient: Multiplier on the constraint violation penalty.
    trust_region:
    prior_acquisition: An optional prior acquisition function.
    multimetric_promising_region_penalty_type: The type of multimetric promising
      region penalty.

  Returns:
    The Pure-Exploration acquisition value.
  """

  predictive: sp.UniformEnsemblePredictive
  predictive_all_features: sp.UniformEnsemblePredictive
  ucb_coefficient: jt.Float[jt.Array, '']
  explore_ucb_coefficient: jt.Float[jt.Array, '']
  penalty_coefficient: jt.Float[jt.Array, '']
  trust_region: Optional[acquisitions.TrustRegion]
  prior_acquisition: Callable[[types.ModelInput], jax.Array] | None
  multimetric_promising_region_penalty_type: (
      MultimetricPromisingRegionPenaltyType
  )

  def score(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> jax.Array:
    return self.score_with_aux(xs, seed=seed)[0]

  def aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> chex.ArrayTree:
    return self.score_with_aux(xs, seed=seed)[1]

  def score_with_aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> tuple[jax.Array, chex.ArrayTree]:
    del seed
    features = self.predictive_all_features.predictives.observed_data.features
    is_missing = (
        features.continuous.is_missing[0] | features.categorical.is_missing[0]
    )
    gprm_threshold = self.predictive.predict(features)
    threshold = _compute_ucb_threshold(
        gprm_threshold, is_missing, self.ucb_coefficient
    )
    gprm = self.predictive.predict(xs)
    mean = gprm.mean()
    stddev = gprm.stddev()
    explore_ucb = mean + stddev * self.explore_ucb_coefficient

    gprm_all = self.predictive_all_features.predict(xs)
    stddev_from_all = gprm_all.stddev()
    penalty = self.penalty_coefficient * jnp.minimum(
        explore_ucb - threshold,
        0.0,
    )
    # `stddev_from_all` and `penalty` are of shape
    # [num_index_points, num_metrics] for multi-metric problems or
    # [num_index_points] for single-metric problems.
    if stddev_from_all.ndim > 1:
      if self.multimetric_promising_region_penalty_type == (
          MultimetricPromisingRegionPenaltyType.UNION
      ):
        scalarized_penalty = jnp.max(penalty, axis=-1)
      elif self.multimetric_promising_region_penalty_type == (
          MultimetricPromisingRegionPenaltyType.INTERSECTION
      ):
        scalarized_penalty = jnp.min(penalty, axis=-1)
      elif self.multimetric_promising_region_penalty_type == (
          MultimetricPromisingRegionPenaltyType.AVERAGE
      ):
        scalarized_penalty = jnp.mean(penalty, axis=-1)
      else:
        raise ValueError(
            'Unsupported multimetric promising region penalty type:'
            f' {self.multimetric_promising_region_penalty_type}'
        )
      acq_values = jnp.mean(stddev_from_all, axis=-1) + scalarized_penalty
    else:
      acq_values = stddev_from_all + penalty
    if self.trust_region is not None:
      acq_values = _apply_trust_region(self.trust_region, xs, acq_values)
    aux = {
        'mean': mean,
        'stddev': stddev,
        'stddev_from_all': stddev_from_all,
    }
    if self.prior_acquisition is not None:
      prior_acq_values = self.prior_acquisition(xs)
      acq_values += prior_acq_values
      aux['prior_acq_values'] = prior_acq_values
    return acq_values, aux


def _logdet(matrix: jax.Array):
  """Computes the log-determinant of a symmetric and positive-definite matrix.

  Args:
    matrix: A square matrix.

  Returns:
    The log-determinant of `matrix`. If `matrix` is not symmetric or not
    positive-definite, the result is invalid and may be -inf.
  """
  cholesky_matrix = jnp.linalg.cholesky(matrix)
  output = 2.0 * jnp.sum(jnp.log(jnp.linalg.diagonal(cholesky_matrix)), axis=-1)
  return jnp.where(jnp.isnan(output), -jnp.inf, output)


class SetPEScoreFunction(eqx.Module):
  """Computes the Pure-Exploration acquisition value over sets.

  The PE acquisition value over a set of points is the log-determinant of the
  predicted covariance matrix evaluated at the points (eq. (8) in
  https://arxiv.org/pdf/1304.5350) based on all completed and active trials,
  plus a penalty term that grows linearly in the amount of violation of the
  constraint `UCB(xs) >= threshold`. If `prior_acquisition` is not None, the
  returned value is the sum of the prior acquisition value and the PE
  acquisition value. This class follows the `acquisitions.ScoreFunction`
  protocol.

  Attributes:
    predictive: Predictive model with cached Cholesky conditioned on completed
      trials.
    predictive_all_features: Predictive model with cached Cholesky conditioned
      on completed and pending trials.
    ucb_coefficient: The UCB coefficient used to compute the threshold.
    explore_ucb_coefficient: The UCB coefficient used for computing the UCB
      values on `xs`.
    penalty_coefficient: Multiplier on the constraint violation penalty.
    trust_region:
    prior_acquisition: An optional prior acquisition function.

  Returns:
    The Pure-Exploration acquisition value.
  """

  predictive: sp.UniformEnsemblePredictive
  predictive_all_features: sp.UniformEnsemblePredictive
  ucb_coefficient: jt.Float[jt.Array, '']
  explore_ucb_coefficient: jt.Float[jt.Array, '']
  penalty_coefficient: jt.Float[jt.Array, '']
  trust_region: Optional[acquisitions.TrustRegion]
  prior_acquisition: Callable[[types.ModelInput], jax.Array] | None

  def score(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> jax.Array:
    return self.score_with_aux(xs, seed=seed)[0]

  def aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> chex.ArrayTree:
    return self.score_with_aux(xs, seed=seed)[1]

  def score_with_aux(
      self, xs: types.ModelInput, seed: Optional[jax.Array] = None
  ) -> tuple[jax.Array, chex.ArrayTree]:
    del seed
    features = self.predictive_all_features.predictives.observed_data.features
    is_missing = (
        features.continuous.is_missing[0] | features.categorical.is_missing[0]
    )
    gprm_threshold = self.predictive.predict(features)
    threshold = _compute_ucb_threshold(
        gprm_threshold, is_missing, self.ucb_coefficient
    )
    gprm = self.predictive.predict(xs)
    mean = gprm.mean()
    stddev = gprm.stddev()
    explore_ucb = mean + stddev * self.explore_ucb_coefficient

    gprm_all = self.predictive_all_features.predict(xs)
    cov = gprm_all.covariance()
    acq_values = _logdet(cov) + self.penalty_coefficient * jnp.sum(
        jnp.minimum(
            explore_ucb - threshold,
            0.0,
        ),
        axis=1,
    )
    if self.trust_region is not None:
      acq_values = _apply_trust_region_to_set(self.trust_region, xs, acq_values)
    aux = {
        'mean': mean,
        'stddev': stddev,
        'stddev_from_all': jnp.sqrt(jnp.diagonal(cov, axis1=1, axis2=2)),
    }
    if self.prior_acquisition is not None:
      prior_acq_values = self.prior_acquisition(xs)
      acq_values += prior_acq_values
      aux['prior_acq_values'] = prior_acq_values
    return acq_values, aux


def default_ard_optimizer() -> optimizers.Optimizer[types.ParameterDict]:
  return optimizers.JaxoptScipyLbfgsB(
      options=optimizers.LbfgsBOptions(
          num_line_search_steps=20,
          tol=1e-5,
          maxiter=500,
      ),
      max_duration=datetime.timedelta(minutes=40),
  )


# TODO: Remove excess use of copy.deepcopy()
@attr.define(auto_attribs=False)
class VizierGPUCBPEBandit(vza.Designer):
  """GP_UCB_PE with a flax model.

  Attributes:
    problem: Must be a flat study with a single metric.
    acquisition_optimizer:
    gp_model_class: The GP model class, which must implement a `build_model`
      class method that takes `ModelInput` and returns a
      `StochasticProcessModel`.
    metadata_ns: Metadata namespace that this designer writes to.
    use_trust_region: Uses trust region.
    ard_optimizer: An optimizer object, which should return a batch of
      hyperparameters to be ensembled.
    num_seed_trials: If greater than zero, first trial is the center of the
      search space. Afterwards, uses quasirandom until this number of trials are
      observed.
    rng: If not set, uses random numbers.
    clear_jax_cache: If True, every `suggest` call clears the Jax cache.
    padding_schedule: Configures what inputs (trials, features, labels) to pad
      with what schedule. Useful for reducing JIT compilation passes. (Default
      implies no padding.)
    prior_acquisition: An optional prior acquisition function. If provided, the
      suggestions will be generated by maximizing the sum of the prior
      acquisition value and the GP-based acquisition value (UCB or PE). Useful
      for biasing the suggestions towards a prior, e.g., being close to some
      known parameter values.
  """

  _problem: vz.ProblemStatement = attr.field(kw_only=False)
  _acquisition_optimizer_factory: Union[
      Callable[[Any], vza.GradientFreeOptimizer], vb.VectorizedOptimizerFactory
  ] = attr.field(
      kw_only=True,
      factory=lambda: VizierGPUCBPEBandit.default_acquisition_optimizer_factory,
  )
  _gp_model_class: sp.ModelCoroutine[tfd.GaussianProcess] = attr.field(
      kw_only=True,
      factory=lambda: tuned_gp_models.VizierGaussianProcess,
  )
  _metadata_ns: str = attr.field(
      default='google_gp_ucb_pe_bandit', kw_only=True
  )
  _ensemble_size: Optional[int] = attr.field(default=1, kw_only=True)
  _all_completed_trials: list[vz.Trial] = attr.field(factory=list, init=False)
  _all_active_trials: Sequence[vz.Trial] = attr.field(factory=list, init=False)
  _ard_optimizer: optimizers.Optimizer[types.ParameterDict] = attr.field(
      factory=default_ard_optimizer,
      kw_only=True,
  )
  _ard_random_restarts: int = attr.field(default=4, kw_only=True)
  _use_trust_region: bool = attr.field(default=True, kw_only=True)
  _num_seed_trials: int = attr.field(default=1, kw_only=True)
  _config: UCBPEConfig = attr.field(
      factory=UCBPEConfig,
      kw_only=True,
  )
  _rng: jax.Array = attr.field(
      factory=lambda: jax.random.PRNGKey(random.getrandbits(32)), kw_only=True
  )
  _clear_jax_cache: bool = attr.field(default=False, kw_only=True)
  # TODO: Check padding does not affect designer behavior.
  _padding_schedule: padding.PaddingSchedule = attr.field(
      factory=padding.PaddingSchedule, kw_only=True
  )
  _prior_acquisition: Callable[[types.ModelInput], jax.Array] | None = (
      attr.field(factory=lambda: None, kw_only=True)
  )
  _mixes_linear_kernel: bool = attr.field(default=False, kw_only=True)

  default_eagle_config = es.EagleStrategyConfig(
      visibility=3.6782451729470043,
      gravity=3.028167342024462,
      negative_gravity=0.03036267153343141,
      perturbation=0.23337470891647027,
      categorical_perturbation_factor=9.587350648631066,
      pure_categorical_perturbation_factor=28.636337967676518,
      prob_same_category_without_perturbation=0.9744882009359648,
      perturbation_lower_bound=7.376256294543107e-4,
      penalize_factor=0.7817632796830948,
      pool_size_exponent=2.0494446726436744,
      mutate_normalization_type=es.MutateNormalizationType.RANDOM,
      normalization_scale=1.9893618760239418,
      prior_trials_pool_pct=0.423499384081575,
  )
  default_acquisition_optimizer_factory = vb.VectorizedOptimizerFactory(
      strategy_factory=es.VectorizedEagleStrategyFactory(
          eagle_config=default_eagle_config
      ),
      max_evaluations=75000,
      suggestion_batch_size=25,
  )

  def __attrs_post_init__(self):
    # Extra validations
    if self._problem.search_space.is_conditional:
      raise ValueError(f'{type(self)} does not support conditional search.')
    elif len(self._problem.metric_information) != 1:
      if self._config.optimize_set_acquisition_for_exploration:
        raise ValueError(
            f'{type(self)} works with exactly one metric when'
            ' `optimize_set_acquisition_for_exploration` is enabled.'
        )
      empty_labels = jnp.array([[]])
      padded_labels = self._padding_schedule.pad_labels(empty_labels)
      if padded_labels.shape[0] != empty_labels.shape[0]:
        raise ValueError(
            f'{type(self)} does not support trial padding for multimetric'
            ' problems.'
        )
      if padded_labels.shape[1] != empty_labels.shape[1]:
        raise ValueError(
            f'{type(self)} does not support metric padding for multimetric'
            ' problems.'
        )

    # Extra initializations.
    # Discrete parameters are continuified to account for their actual values.
    self._converter = converters.TrialToModelInputConverter.from_problem(
        self._problem,
        scale=True,
        max_discrete_indices=0,
        flip_sign_for_minimization_metrics=True,
        padding_schedule=self._padding_schedule,
    )
    qrs_seed, self._rng = jax.random.split(self._rng)
    self._quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        self._problem.search_space,
        seed=int(jax.random.randint(qrs_seed, [], 0, 2**16)),
    )
    self._output_warpers: list[output_warpers.OutputWarper] = []

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    self._all_completed_trials.extend(copy.deepcopy(completed.trials))
    self._all_active_trials = copy.deepcopy(all_active.trials)

  @property
  def _metric_info(self) -> vz.MetricInformation:
    return self._problem.metric_information.item()

  def _generate_seed_trials(self, count: int) -> Sequence[vz.TrialSuggestion]:
    """Generate seed trials.

    The first seed trial is chosen as the search space center, the rest of the
    seed trials are chosen quasi-randomly.

    Arguments:
      count: The number of seed trials.

    Returns:
      The seed trials.
    """
    seed_suggestions = []
    if (not self._all_completed_trials) and (not self._all_active_trials):
      features = self._converter.to_features([])  # to extract shape.
      # NOTE: The code below assumes that a scaled value of 0.5 corresponds
      # to the center of the feasible range. This is true, but only by accident;
      # ideally, we should get the center from the converters.
      continuous = self._padding_schedule.pad_features(
          0.5 * np.ones([1, features.continuous.shape[1]])
      )
      categorical = self._padding_schedule.pad_features(
          np.zeros([1, features.categorical.shape[1]], dtype=types.INT_DTYPE)
      )
      model_input = types.ModelInput(continuous, categorical)
      parameters = self._converter.to_parameters(model_input)[0]
      suggestion = vz.TrialSuggestion(
          parameters, metadata=vz.Metadata({'seeded': 'center'})
      )
      seed_suggestions.append(suggestion)
    if (remaining_counts := count - len(seed_suggestions)) > 0:
      quasi_suggestions = self._quasi_random_sampler.suggest(remaining_counts)
      seed_suggestions.extend(quasi_suggestions)
    return seed_suggestions

  @profiler.record_runtime(
      name_prefix='VizierGPUCBPEBandit',
      name='build_gp_model_and_optimize_parameters',
  )
  def _build_gp_model_and_optimize_parameters(
      self, data: types.ModelData, rng: jax.Array
  ) -> sp.StochasticProcessWithCoroutine:
    """Builds a GP model and optimizes parameters.

    Args:
      data: Observed features and labels.
      rng: A key for random number generation.

    Returns:
      A tuple of GP model and its parameters optimized over `data.features` and
      `data.labels`. If `data.features` is empty, the returned parameters are
      initial values picked by the GP model.
    """
    # TODO: Creates a new abstract base class for GP models with a
    # `build_model` API to avoid disabling the pytype attribute-error.
    coroutine = self._gp_model_class.build_model(  # pytype: disable=attribute-error
        data,
        multitask_type=self._config.multitask_type,
        linear_coef=1.0 if self._mixes_linear_kernel else None,
    ).coroutine
    model = sp.CoroutineWithData(coroutine, data)

    if (data.features.continuous.padded_array.shape[0] == 0) and (
        data.features.categorical.padded_array.shape[0] == 0
    ):
      # This happens when `suggest` is called after the seed trials are
      # generated without any completed trials. In this case, the designer
      # uses the PE acquisition, but still needs a GP to do that. By using a
      # dummy loss here, the ARD optimizer is expected to return the initial
      # values it uses for the parameters.
      ard_loss_with_aux = lambda _: (_DUMMY_LOSS, dict())
    else:
      ard_loss_with_aux = model.loss_with_aux

    logging.info(
        'Optimizing the loss function on features with shape '
        f'{_get_features_shape(data.features)} and labels with shape '
        f'{data.labels.shape}...'
    )
    constraints = sp.get_constraints(model)
    rng, init_rng = jax.random.split(rng, 2)
    random_init_params = eqx.filter_jit(eqx.filter_vmap(model.setup))(
        jax.random.split(init_rng, self._ard_random_restarts)
    )
    fixed_init_params = {
        'signal_variance': jnp.array([0.039]),
        'observation_noise_variance': jnp.array([0.0039]),
        'continuous_length_scale_squared': jnp.array(
            [[1.0] * data.features.continuous.padded_array.shape[-1]]
        ),
        'categorical_length_scale_squared': jnp.array(
            [[1.0] * data.features.categorical.padded_array.shape[-1]]
        ),
    }
    if self._mixes_linear_kernel:
      fixed_init_params.update({
          'linear_slope_amplitude': jnp.array([0.0]),
          'linear_shift': jnp.array([0.0]),
          'mean_fn': jnp.zeros(
              [1, 1]
              + ([data.labels.shape[-1]] if data.labels.shape[-1] > 1 else [])
          ),
      })
    # Multitask GP models whose multitask type is not `INDEPENDENT` require
    # extra parameters for the task kernel priors, which are randomly sampled
    # and added to the fixed initialization parameters.
    if (
        data.labels.shape[-1] > 1
        and self._config.multitask_type
        != multitask_tuned_gp_models.MultiTaskType.INDEPENDENT
    ):
      rng, extra_params_rng = jax.random.split(rng, 2)
      extra_random_init_params = eqx.filter_jit(model.setup)(extra_params_rng)
      for p_name, p_value in extra_random_init_params.items():
        if p_name not in fixed_init_params:
          fixed_init_params[p_name] = jnp.array([p_value])

    best_n = self._ensemble_size or 1
    optimal_params, metrics = self._ard_optimizer(
        init_params=jax.tree.map(
            lambda x, y: jnp.concatenate([x, y]),
            fixed_init_params,
            random_init_params,
        ),
        loss_fn=ard_loss_with_aux,
        rng=rng,
        constraints=constraints,
        best_n=best_n,
    )
    # The `"loss"` field of the `metrics` output of ARD optimizers contains an
    # array of losses of shape `[num_steps, num_random_restarts]` (or
    # `[1, num_random_restarts]` if only the final loss is recorded).
    if jnp.any(metrics['loss'][-1, :].argsort()[:best_n] == 0):
      logging.info(
          'Parameters found by fixed initialization are among the best'
          f' {best_n} parameters.'
      )
    else:
      logging.info(
          f'The best {best_n} parameters were all found by random'
          ' initialization.'
      )

    logging.info('Optimal parameters: %s', optimal_params)
    return sp.StochasticProcessWithCoroutine(coroutine, optimal_params)

  def get_score_fn_on_trials(
      self, score_fn: Callable[[types.ModelInput], jax.Array]
  ) -> Callable[[Sequence[vz.Trial]], Mapping[str, jax.Array]]:
    """Builds a callable that evaluates the score function on trials.

    Args:
      score_fn: Score function that takes arrays as input.

    Returns:
      Score function that takes trials as input.
    """

    def acquisition(trials: Sequence[vz.Trial]) -> Mapping[str, jax.Array]:
      jax_acquisitions = eqx.filter_jit(score_fn)(
          self._converter.to_features(trials)
      )
      return {'acquisition': jax_acquisitions}

    return acquisition

  @profiler.record_runtime
  def _trials_to_data(self, trials: Sequence[vz.Trial]) -> types.ModelData:
    """Convert trials to scaled features and warped labels."""
    # TrialToArrayConverter returns floating arrays.
    data = self._converter.to_xy(trials)
    logging.info(
        'Transforming the labels of shape %s. Features has shape: %s',
        data.labels.shape,
        _get_features_shape(data.features),
    )
    unpadded_labels = np.asarray(data.labels.unpad())
    warped_labels = []
    self._output_warpers = []
    for i in range(data.labels.shape[1]):
      output_warper = output_warpers.create_default_warper()
      warped_labels.append(output_warper.warp(unpadded_labels[:, i : i + 1]))
      self._output_warpers.append(output_warper)
    labels = types.PaddedArray.from_array(
        np.concatenate(warped_labels, axis=-1),
        data.labels.padded_array.shape,
        fill_value=data.labels.fill_value,
    )
    logging.info('Transformed the labels. Now has shape: %s', labels.shape)
    return types.ModelData(features=data.features, labels=labels)

  @profiler.record_runtime(
      name_prefix='VizierGPUCBPEBandit', name='get_predictive_all_features'
  )
  def _get_predictive_all_features(
      self,
      pending_features: types.ModelInput,
      data: types.ModelData,
      model: sp.StochasticProcessWithCoroutine,
      noise_is_high: bool,
  ) -> sp.UniformEnsemblePredictive:
    """Builds the predictive model conditioned on observed and pending features.

    Args:
      pending_features: Pending features.
      data: Features/labels for completed trials.
      model: The GP model.
      noise_is_high: Whether the noise is estimated to be high.

    Returns:
      Predictive model with cached Cholesky conditioned on observed and pending
      features.
    """
    # TODO: Use `PaddedArray.concatenate` when implemented.
    all_features_continuous = jnp.concatenate(
        [
            data.features.continuous.unpad(),
            pending_features.continuous.unpad(),
        ],
        axis=0,
    )
    all_features_categorical = jnp.concatenate(
        [
            data.features.categorical.unpad(),
            pending_features.categorical.unpad(),
        ],
        axis=0,
    )
    all_features = types.ModelInput(
        continuous=self._padding_schedule.pad_features(all_features_continuous),
        categorical=self._padding_schedule.pad_features(
            all_features_categorical
        ),
    )
    # Pending features are only used to predict standard deviation, so their
    # labels do not matter, and we simply set them to 0.
    dummy_labels = jnp.zeros(
        shape=(
            pending_features.continuous.unpad().shape[0],
            data.labels.shape[-1],
        ),
        dtype=data.labels.padded_array.dtype,
    )
    all_labels = jnp.concatenate([data.labels.unpad(), dummy_labels], axis=0)
    all_labels = self._padding_schedule.pad_labels(all_labels)
    all_data = types.ModelData(features=all_features, labels=all_labels)
    if noise_is_high:
      pe_params = dict(copy.deepcopy(model.params))
      pe_params['observation_noise_variance'] = jnp.array([1e-10])
      pe_model = sp.StochasticProcessWithCoroutine(model.coroutine, pe_params)
    else:
      pe_model = model
    return sp.UniformEnsemblePredictive(
        predictives=eqx.filter_jit(pe_model.precompute_predictive)(all_data)
    )

  def _suggest_one(
      self,
      active_trials: Sequence[vz.Trial],
      data: types.ModelData,
      model: sp.StochasticProcessWithCoroutine,
      predictive: sp.UniformEnsemblePredictive,
      tr: acquisitions.TrustRegion,
      acquisition_problem: vz.ProblemStatement,
  ) -> vz.TrialSuggestion:
    """Generates one suggestion."""
    start_time = datetime.datetime.now()
    self._rng, rng = jax.random.split(self._rng, 2)
    snr = model.params['signal_variance'] / jnp.maximum(
        model.params['observation_noise_variance'], 1e-12
    )
    noise_is_high = (snr < self._config.signal_to_noise_threshold).all()
    pe_overwrite_probability = (
        self._config.pe_overwrite_probability_in_high_noise
        if noise_is_high
        else self._config.pe_overwrite_probability
    )
    if _has_new_completed_trials(
        completed_trials=self._all_completed_trials,
        active_trials=active_trials,
    ):
      # When there are trials completed after all active trials were created,
      # we optimize the UCB acquisition function except with a small
      # probability the PE acquisition function to ensure exploration.
      use_ucb = not jax.random.bernoulli(key=rng, p=pe_overwrite_probability)
    else:
      has_completed_trials = len(self._all_completed_trials) > 0  # pylint:disable=g-explicit-length-test
      # When there are no trials completed after all active trials were
      # created, we optimize the PE acquisition function except with a small
      # probability the UCB acquisition function, in case the UCB acquisition
      # function is not well optimized.
      use_ucb = has_completed_trials and jax.random.bernoulli(
          key=rng, p=self._config.ucb_overwrite_probability
      )

    # TODO: Feed the eagle strategy with completed trials.
    # TODO: Change budget based on requested suggestion count.
    acquisition_optimizer = self._acquisition_optimizer_factory(self._converter)

    pending_features = self._converter.to_features(active_trials)
    predictive_all_features = self._get_predictive_all_features(
        pending_features, data, model, noise_is_high
    )

    # When `use_ucb` is true, the acquisition function computes the UCB
    # values. Otherwise, it computes the Pure-Exploration acquisition values.
    if use_ucb:
      scalarization_weights_rng, self._rng = jax.random.split(self._rng)
      scoring_fn = UCBScoreFunction(
          predictive,
          predictive_all_features,
          ucb_coefficient=self._config.ucb_coefficient,
          trust_region=tr if self._use_trust_region else None,
          prior_acquisition=self._prior_acquisition,
          scalarization_weights_rng=scalarization_weights_rng,
          labels=data.labels,
      )
    else:
      scoring_fn = PEScoreFunction(
          predictive,
          predictive_all_features,
          penalty_coefficient=self._config.cb_violation_penalty_coefficient,
          ucb_coefficient=self._config.ucb_coefficient,
          explore_ucb_coefficient=self._config.explore_region_ucb_coefficient,
          trust_region=tr if self._use_trust_region else None,
          prior_acquisition=self._prior_acquisition,
          multimetric_promising_region_penalty_type=(
              self._config.multimetric_promising_region_penalty_type
          ),
      )

    if isinstance(acquisition_optimizer, vb.VectorizedOptimizer):
      acq_rng, self._rng = jax.random.split(self._rng)
      with profiler.timeit('acquisition_optimizer', also_log=True):
        best_candidates = eqx.filter_jit(acquisition_optimizer)(
            scoring_fn.score,
            prior_features=vb.trials_to_sorted_array(
                self._all_completed_trials, self._converter
            ),
            count=1,
            seed=acq_rng,
            score_with_aux_fn=scoring_fn.score_with_aux,
        )
        jax.block_until_ready(best_candidates)
      with profiler.timeit('best_candidates_to_trials', also_log=True):
        best_candidate = vb.best_candidates_to_trials(
            best_candidates, self._converter
        )[0]
    elif isinstance(acquisition_optimizer, vza.GradientFreeOptimizer):
      # Seed the optimizer with previous trials.
      acquisition = self.get_score_fn_on_trials(scoring_fn.score)
      best_candidate = acquisition_optimizer.optimize(
          acquisition,
          acquisition_problem,
          count=1,
          seed_candidates=copy.deepcopy(self._all_completed_trials),
      )[0]
    else:
      raise ValueError(
          f'Unrecognized acquisition_optimizer: {type(acquisition_optimizer)}'
      )

    # Make predictions (in the warped space).
    logging.info('Converting the optimization result into suggestion...')
    optimal_features = self._converter.to_features([best_candidate])  # [1, D]
    aux = eqx.filter_jit(scoring_fn.aux)(optimal_features)
    predict_mean = aux['mean']  # [1,]
    predict_stddev = aux['stddev']  # [1,]
    predict_stddev_from_all = aux['stddev_from_all']  # [1,]
    acquisition = best_candidate.final_measurement_or_die.metrics.get_value(
        'acquisition', float('nan')
    )
    logging.info(
        'Created predictions for the best candidates which were converted to'
        f' an array of shape: {_get_features_shape(optimal_features)}. mean'
        f' has shape {predict_mean.shape}. stddev has shape'
        f' {predict_stddev.shape}.stddev_from_all has shape'
        f' {predict_stddev_from_all.shape}. acquisition value of'
        f' best_candidate: {acquisition}, use_ucb: {use_ucb}'
    )

    # Create a suggestion, injecting the predictions as metadata for
    # debugging needs.
    metadata = best_candidate.metadata.ns(self._metadata_ns)
    metadata.ns('prediction_in_warped_y_space').update({
        'mean': np.array2string(np.asarray(predict_mean[0]), separator=','),
        'stddev': np.array2string(np.asarray(predict_stddev[0]), separator=','),
        'stddev_from_all': np.array2string(
            np.asarray(predict_stddev_from_all[0]), separator=','
        ),
        'acquisition': f'{acquisition}',
        'use_ucb': f'{use_ucb}',
        'trust_radius': f'{tr.trust_radius}',
        'params': f'{model.params}',
    })
    if 'prior_acq_values' in aux:
      # Take the first element of the array because `aux` is computed only for
      # the best candidate.
      prior_acq_value = aux['prior_acq_values'][0]
      metadata.ns('prior_acquisition').update({'value': f'{prior_acq_value}'})
    metadata.ns('timing').update(
        {'time': f'{datetime.datetime.now() - start_time}'}
    )
    return vz.TrialSuggestion(
        best_candidate.parameters, metadata=best_candidate.metadata
    )

  def _suggest_batch_with_exploration(
      self,
      count: int,
      active_trials: Sequence[vz.Trial],
      data: types.ModelData,
      model: sp.StochasticProcessWithCoroutine,
      predictive: sp.UniformEnsemblePredictive,
      tr: acquisitions.TrustRegion,
  ):
    """Generates a batch of suggestions with exploration."""
    start_time = datetime.datetime.now()
    snr = model.params['signal_variance'] / jnp.maximum(
        model.params['observation_noise_variance'], 1e-12
    )
    pending_features = self._converter.to_features(active_trials)
    predictive_all_features = self._get_predictive_all_features(
        pending_features,
        data,
        model,
        noise_is_high=(snr < self._config.signal_to_noise_threshold),
    )
    scoring_fn = SetPEScoreFunction(
        predictive,
        predictive_all_features,
        penalty_coefficient=self._config.cb_violation_penalty_coefficient,
        ucb_coefficient=self._config.ucb_coefficient,
        explore_ucb_coefficient=self._config.explore_region_ucb_coefficient,
        trust_region=tr if self._use_trust_region else None,
        prior_acquisition=self._prior_acquisition,
    )

    acquisition_optimizer = self._acquisition_optimizer_factory(self._converter)

    acq_rng, self._rng = jax.random.split(self._rng)
    with profiler.timeit('acquisition_optimizer', also_log=True):
      best_candidates = eqx.filter_jit(acquisition_optimizer)(
          scoring_fn.score,
          prior_features=vb.trials_to_sorted_array(
              self._all_completed_trials, self._converter
          ),
          count=1,
          seed=acq_rng,
          score_with_aux_fn=scoring_fn.score_with_aux,
          n_parallel=count,
      )
      jax.block_until_ready(best_candidates)
    with profiler.timeit('best_candidates_to_trials', also_log=True):
      trials = vb.best_candidates_to_trials(best_candidates, self._converter)[
          :count
      ]

    optimal_features = self._converter.to_features(trials)  # [count, D]
    aux = eqx.filter_jit(scoring_fn.aux)(
        jax.tree_util.tree_map(
            lambda x: jnp.expand_dims(x, axis=0), optimal_features
        )
    )
    predict_mean = aux['mean']  # [1, count]
    predict_stddev = aux['stddev']  # [1, count]
    predict_stddev_from_all = aux['stddev_from_all']  # [1, count]
    acquisition = trials[0].final_measurement_or_die.metrics.get_value(
        'acquisition', float('nan')
    )
    logging.info(
        'Created predictions for the best candidates which were converted to'
        f' an array of shape: {_get_features_shape(optimal_features)}. mean'
        f' has shape {predict_mean.shape}. stddev has shape'
        f' {predict_stddev.shape}.stddev_from_all has shape'
        f' {predict_stddev_from_all.shape}. acquisition value of'
        f' best_candidate: {acquisition}, use_ucb: False'
    )

    logging.info(
        'Converting the optimization result into %d suggestions...', count
    )
    suggestions = []
    end_time = datetime.datetime.now()
    for idx, best_candidate in enumerate(trials):
      # Make predictions (in the warped space).
      # Create suggestions, injecting the predictions as metadata for
      # debugging needs.
      metadata = best_candidate.metadata.ns(self._metadata_ns)
      metadata.ns('prediction_in_warped_y_space').update({
          'mean': f'{predict_mean[0, idx]}',
          'stddev': f'{predict_stddev[0, idx]}',
          'stddev_from_all': f'{predict_stddev_from_all[0, idx]}',
          'acquisition': f'{acquisition}',
          'use_ucb': 'False',
          'trust_radius': f'{tr.trust_radius}',
          'params': f'{model.params}',
      })
      if 'prior_acq_values' in aux:
        # Take the first element of the array because `aux` is computed only for
        # the best candidate.
        prior_acq_value = aux['prior_acq_values'][0]
        metadata.ns('prior_acquisition').update({'value': f'{prior_acq_value}'})
      metadata.ns('timing').update({'time': f'{end_time - start_time}'})
      suggestions.append(
          vz.TrialSuggestion(
              best_candidate.parameters, metadata=best_candidate.metadata
          )
      )
    return suggestions

  @profiler.record_runtime
  def sample(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: int = 1000,
  ) -> types.Array:
    """Returns unwarped samples from the model for any given trials.

    Arguments:
      trials: The trials where the predictions will be made.
      rng: The sampling random key.
      num_samples: The number of samples per trial.

    Returns:
      The samples in the specified trials. shape: (num_samples, num_trials)
    """
    if rng is None:
      rng = jax.random.PRNGKey(0)

    if not trials:
      return np.zeros((num_samples, 0))

    data = self._trials_to_data(self._all_completed_trials)
    self._rng, ard_rng = jax.random.split(self._rng, 2)
    model = self._build_gp_model_and_optimize_parameters(data, ard_rng)
    predictive = sp.UniformEnsemblePredictive(
        predictives=eqx.filter_jit(model.precompute_predictive)(data)
    )

    xs = self._converter.to_features(trials)
    xs = types.ModelInput(
        continuous=xs.continuous.replace_fill_value(0.0),
        categorical=xs.categorical.replace_fill_value(0),
    )
    samples = eqx.filter_jit(acquisitions.sample_from_predictive)(
        predictive, xs, num_samples, key=rng
    )
    # Scope `samples` to non-padded only (there's a single padded dimension).
    # `samples` has shape: [num_samples, num_trials] for single metric or
    # [num_samples, num_trials, num_metrics] for multi-metric problems.
    if samples.ndim == 2:
      samples = jnp.expand_dims(samples, axis=-1)
    samples = samples[
        :, ~(xs.continuous.is_missing[0] | xs.categorical.is_missing[0]), :
    ]
    # TODO: vectorize output warping.
    if self._output_warpers:
      unwarped_samples = []
      for metric_idx, output_warper in enumerate(self._output_warpers):
        unwarped_samples.append(
            np.vstack([
                output_warper.unwarp(
                    samples[i][:, metric_idx : metric_idx + 1]
                ).reshape(-1)
                for i in range(samples.shape[0])
            ])
        )
      unwarped_samples = np.stack(unwarped_samples, axis=-1)
      if unwarped_samples.shape[-1] > 1:
        return unwarped_samples
      else:
        return np.squeeze(unwarped_samples, axis=-1)
    else:
      raise TypeError(
          'Output warpers are expected to be set, but found to be'
          f' {self._output_warpers}.'
      )

  @profiler.record_runtime
  def predict(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: Optional[int] = 1000,
  ) -> vza.Prediction:
    """Returns the mean and stddev for any given trials.

    The method performs sampling of the warped GP model, unwarp the samples and
    compute the empirical mean and standard deviation as an apprixmation.

    Arguments:
      trials: The trials where the predictions will be made.
      rng: The sampling random key used for approximation.
      num_samples: The number of samples used for the approximation.

    Returns:
      The predictions in the specified trials.
    """
    unwarped_samples = self.sample(trials, rng, num_samples)
    mean = np.mean(unwarped_samples, axis=0)
    stddev = np.std(unwarped_samples, axis=0)
    return vza.Prediction(mean=mean, stddev=stddev)

  @profiler.record_runtime(name_prefix='VizierGPUCBPEBandit', name='suggest')
  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    count = count or 1
    num_total = len(self._all_completed_trials) + len(self._all_active_trials)
    if num_total < self._num_seed_trials:
      return self._generate_seed_trials(count)

    if self._clear_jax_cache:
      jax.clear_caches()

    self._rng, rng = jax.random.split(self._rng, 2)
    data = self._trials_to_data(self._all_completed_trials)
    model = self._build_gp_model_and_optimize_parameters(data, rng)
    predictive = sp.UniformEnsemblePredictive(
        predictives=eqx.filter_jit(model.precompute_predictive)(data)
    )

    # Optimize acquisition.
    active_trial_features = self._converter.to_features(self._all_active_trials)

    tr_features = types.ModelInput(
        continuous=self._padding_schedule.pad_features(
            jnp.concatenate(
                [
                    data.features.continuous.unpad(),
                    active_trial_features.continuous.unpad(),
                ],
                axis=0,
            )
        ),
        categorical=self._padding_schedule.pad_features(
            jnp.concatenate(
                [
                    data.features.categorical.unpad(),
                    active_trial_features.categorical.unpad(),
                ],
                axis=0,
            ),
        ),
    )
    tr = acquisitions.TrustRegion(
        trusted=tr_features,
        continuous_feasible_values=self._converter.continuous_feasible_values(
            max_num_feasible_values=_MAX_NUM_FEASIBLE_VALUES_FOR_TRUST_REGION
        ),
    )

    acquisition_problem = copy.deepcopy(self._problem)
    acquisition_problem.metric_information = [
        vz.MetricInformation(
            name='acquisition', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    ]
    logging.info('Optimizing acquisition...')

    # TODO: Feed the eagle strategy with completed trials.
    # TODO: Change budget based on requested suggestion count.
    active_trials = list(self._all_active_trials)
    if count <= 1:
      return [
          self._suggest_one(
              active_trials, data, model, predictive, tr, acquisition_problem
          )
      ]

    suggestions = []
    if self._config.optimize_set_acquisition_for_exploration:
      if _has_new_completed_trials(
          completed_trials=self._all_completed_trials,
          active_trials=active_trials,
      ):
        suggestions.append(
            self._suggest_one(
                active_trials, data, model, predictive, tr, acquisition_problem
            )
        )
        active_trials.append(suggestions[-1].to_trial())
      return suggestions + self._suggest_batch_with_exploration(
          count - len(suggestions), active_trials, data, model, predictive, tr
      )
    else:
      for _ in range(count):
        suggestions.append(
            self._suggest_one(
                active_trials, data, model, predictive, tr, acquisition_problem
            )
        )
        active_trials.append(suggestions[-1].to_trial())
      return suggestions


--- vizier/_src/algorithms/designers/gp_ucb_pe_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for gp_ucb_pe."""

import ast
import copy
from typing import Any, Tuple

import jax
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.core import abstractions
from vizier._src.algorithms.designers import gp_ucb_pe
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.optimizers import eagle_strategy as es
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier.jax import optimizers
from vizier.pyvizier.converters import padding
from vizier.testing import test_studies

from absl.testing import absltest
from absl.testing import parameterized

ensemble_ard_optimizer = optimizers.default_optimizer()

mt_type = multitask_tuned_gp_models.MultiTaskType


def _extract_predictions(
    metadata: Any,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float, bool]:
  pred = metadata.ns('prediction_in_warped_y_space')
  return (
      np.asarray(ast.literal_eval(pred['mean'])),
      np.asarray(ast.literal_eval(pred['stddev'])),
      np.asarray(ast.literal_eval(pred['stddev_from_all'])),
      float(pred['acquisition']),
      bool(pred['use_ucb'] == 'True'),
  )


class GpUcbPeTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(iters=3, batch_size=5, num_seed_trials=5, mixes_linear_kernel=True),
      dict(iters=5, batch_size=1, num_seed_trials=2),
      dict(iters=5, batch_size=3, num_seed_trials=2, ensemble_size=3),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          applies_padding=True,
          mixes_linear_kernel=True,
      ),
      dict(iters=5, batch_size=1, num_seed_trials=2, pe_overwrite=True),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          applies_padding=True,
          optimize_set_acquisition_for_exploration=True,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          applies_padding=True,
          optimize_set_acquisition_for_exploration=True,
          search_space=test_studies.flat_categorical_space(),
          mixes_linear_kernel=True,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          applies_padding=True,
          ensemble_size=3,
          turns_on_high_noise_mode=True,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          num_metrics=2,
          mixes_linear_kernel=True,
      ),
      dict(
          iters=3,
          batch_size=3,
          num_metrics=2,
          multimetric_promising_region_penalty_type=(
              gp_ucb_pe.MultimetricPromisingRegionPenaltyType.UNION
          ),
      ),
      dict(
          iters=3,
          batch_size=3,
          num_metrics=2,
          ensemble_size=4,
          multimetric_promising_region_penalty_type=(
              gp_ucb_pe.MultimetricPromisingRegionPenaltyType.INTERSECTION
          ),
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          num_metrics=2,
          multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR,
          mixes_linear_kernel=True,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          num_metrics=2,
          multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR,
      ),
      dict(
          iters=3,
          batch_size=5,
          num_seed_trials=5,
          num_metrics=2,
          multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR,
          mixes_linear_kernel=True,
      ),
  )
  def test_on_flat_space(
      self,
      iters: int = 5,
      batch_size: int = 1,
      num_seed_trials: int = 1,
      ard_optimizer: str = 'default',
      ensemble_size: int = 1,
      applies_padding: bool = False,
      pe_overwrite: bool = False,
      optimize_set_acquisition_for_exploration: bool = False,
      search_space: vz.SearchSpace = (
          test_studies.flat_continuous_space_with_scaling()
      ),
      turns_on_high_noise_mode: bool = False,
      num_metrics: int = 1,
      multimetric_promising_region_penalty_type: (
          gp_ucb_pe.MultimetricPromisingRegionPenaltyType
      ) = gp_ucb_pe.MultimetricPromisingRegionPenaltyType.AVERAGE,
      multitask_type: mt_type = mt_type.INDEPENDENT,
      mixes_linear_kernel: bool = False,
  ):
    # We use string names so that test case names are readable. Convert them
    # to objects.
    if ard_optimizer == 'default':
      ard_optimizer = optimizers.default_optimizer()
    problem = vz.ProblemStatement(search_space)
    for metric_idx in range(num_metrics):
      problem.metric_information.append(
          vz.MetricInformation(
              name=f'metric{metric_idx}',
              goal=vz.ObjectiveMetricGoal.MAXIMIZE
              if metric_idx % 2 == 0
              else vz.ObjectiveMetricGoal.MINIMIZE,
          )
      )
    vectorized_optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=es.VectorizedEagleStrategyFactory(),
        max_evaluations=100,
    )
    designer = gp_ucb_pe.VizierGPUCBPEBandit(
        problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        num_seed_trials=num_seed_trials,
        ard_optimizer=ard_optimizer,
        metadata_ns='gp_ucb_pe_bandit_test',
        config=gp_ucb_pe.UCBPEConfig(
            ucb_coefficient=10.0,
            explore_region_ucb_coefficient=0.5,
            # Sets the penalty coefficient to 0.0 so that the PE aquisition
            # value is exactly the standard deviation prediction based on all
            # trials.
            cb_violation_penalty_coefficient=0.0,
            ucb_overwrite_probability=0.0,
            pe_overwrite_probability=1.0 if pe_overwrite else 0.0,
            # In high noise mode, the PE acquisition function is always used.
            pe_overwrite_probability_in_high_noise=1.0,
            optimize_set_acquisition_for_exploration=(
                optimize_set_acquisition_for_exploration
            ),
            signal_to_noise_threshold=np.inf
            if turns_on_high_noise_mode
            else 0.0,
            multimetric_promising_region_penalty_type=(
                multimetric_promising_region_penalty_type
            ),
            multitask_type=multitask_type,
        ),
        ensemble_size=ensemble_size,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10
            if applies_padding
            else padding.PaddingType.NONE,
        ),
        rng=jax.random.PRNGKey(1),
        mixes_linear_kernel=mixes_linear_kernel,
    )

    quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        problem.search_space,
    )
    test_trials = quasi_random_sampler.suggest(count=3)

    all_active_trials = []
    all_trials = []
    trial_id = 1
    last_prediction = None
    last_samples = None
    label_rng = jax.random.PRNGKey(1)
    # Simulates batch suggestions with delayed feedback: the first two batches
    # are generated by the designer without any completed trials (but all with
    # active trials). Starting from the third batch, the oldest batch gets
    # completed and updated to the new designer with all the active trials, and
    # the designer then makes a new batch of suggestions. The last two batches
    # of suggestions are again made with only active trials being updated to
    # the designer.
    for idx in range(iters + 2):
      suggestions = designer.suggest(batch_size)
      self.assertLen(suggestions, batch_size)
      for suggestion in suggestions:
        problem.search_space.assert_contains(suggestion.parameters)
        all_active_trials.append(suggestion.to_trial(trial_id))
        all_trials.append(copy.deepcopy(all_active_trials[-1]))
        trial_id += 1
      completed_trials = []
      # Starting from the second until the last but two batch, complete the
      # oldest batch of suggestions.
      if idx > 0 and idx < iters:
        for _ in range(batch_size):
          measurement = vz.Measurement()
          for mi in problem.metric_information:
            label_rng, rng = jax.random.split(label_rng, 2)
            measurement.metrics[mi.name] = float(
                jax.random.uniform(
                    rng,
                    minval=mi.min_value_or(lambda: -10.0),
                    maxval=mi.max_value_or(lambda: 10.0),
                )
            )
          completed_trials.append(
              all_active_trials.pop(0).complete(measurement)
          )
      designer.update(
          completed=abstractions.CompletedTrials(completed_trials),
          all_active=abstractions.ActiveTrials(all_active_trials),
      )
      # After the designer is updated with completed trials, prediction and
      # sampling results are expected to change.
      if len(completed_trials) > 1:
        # test the sample method.
        samples = designer.sample(test_trials, num_samples=5)
        self.assertSequenceEqual(
            samples.shape, (5, 3) if num_metrics == 1 else (5, 3, num_metrics)
        )
        self.assertFalse(np.isnan(samples).any())
        # test the sample method with a different rng.
        samples_rng = designer.sample(
            test_trials, num_samples=5, rng=jax.random.PRNGKey(1)
        )
        self.assertFalse(np.isnan(samples_rng).any())
        self.assertFalse((np.abs(samples - samples_rng) <= 1e-6).all())
        # test the predict method.
        prediction = designer.predict(test_trials)
        self.assertSequenceEqual(
            prediction.mean.shape,
            (3,) if num_metrics == 1 else (3, num_metrics),
        )
        self.assertSequenceEqual(
            prediction.stddev.shape,
            (3,) if num_metrics == 1 else (3, num_metrics),
        )
        self.assertFalse(np.isnan(prediction.mean).any())
        self.assertFalse(np.isnan(prediction.stddev).any())
        if last_prediction is None:
          last_prediction = prediction
          last_samples = samples
        else:
          self.assertFalse(
              (np.abs(last_prediction.mean - prediction.mean) <= 1e-6).all()
          )
          self.assertFalse(
              (np.abs(last_prediction.stddev - prediction.stddev) <= 1e-6).all()
          )
          self.assertFalse((np.abs(last_samples - samples) <= 1e-6).all())

    self.assertLen(all_trials, (iters + 2) * batch_size)

    # The suggestions after the seeds up to the first two batches are expected
    # to be generated by the PE acquisition function.
    for jdx in range(2 * batch_size):
      # Before the designer was updated with enough trials, the suggested
      # batches were seeds, not from acquisition optimization.
      if (jdx // batch_size) * batch_size >= num_seed_trials:
        _, _, _, acq, use_ucb = _extract_predictions(
            all_trials[jdx].metadata.ns('gp_ucb_pe_bandit_test')
        )
        self.assertFalse(use_ucb)
        if not optimize_set_acquisition_for_exploration:
          self.assertGreaterEqual(acq, 0.0, msg=f'suggestion: {jdx}')

    for idx in range(2, iters + 2):
      # Skips seed trials, which are not generated by acquisition function
      # optimization.
      if idx * batch_size < num_seed_trials:
        continue
      set_acq_value = None
      stddev_from_all_list = []
      for jdx in range(batch_size):
        mean, _, stddev_from_all, acq, use_ucb = _extract_predictions(
            all_trials[idx * batch_size + jdx].metadata.ns(
                'gp_ucb_pe_bandit_test'
            )
        )
        if (
            jdx == 0
            and idx < (iters + 1)
            and not pe_overwrite
            and not turns_on_high_noise_mode
        ):
          # Except for the last batch of suggestions, the acquisition value of
          # the first suggestion in a batch is expected to be UCB, which
          # combines the predicted mean based only on completed trials and the
          # predicted standard deviation based on all trials. Only checks the
          # single-metric case because the acquisition value in the multi-metric
          # case is randomly scalarized.
          if num_metrics == 1:
            expected_acq = mean + 10.0 * stddev_from_all
            self.assertAlmostEqual(expected_acq, acq, delta=1e-6)
          self.assertTrue(use_ucb)
          continue

        self.assertFalse(use_ucb)
        if optimize_set_acquisition_for_exploration:
          stddev_from_all_list.append(stddev_from_all)
          if set_acq_value is None:
            set_acq_value = acq
          else:
            self.assertAlmostEqual(set_acq_value, acq)
        else:
          # Because `ucb_overwrite_probability` is set to 0.0, when the designer
          # makes suggestions without seeing newer completed trials, it uses the
          # Pure-Exploration acquisition function. In this test, that happens
          # on the entire last batch and the second until the last suggestions
          # in every batch. The Pure-Exploration acquisition values are standard
          # deviation predictions based on all trials (completed and pending).
          self.assertAlmostEqual(
              acq,
              np.mean(stddev_from_all),
              msg=f'batch: {idx}, suggestion: {jdx}',
          )
      if optimize_set_acquisition_for_exploration:
        geometric_mean_of_pred_cov_eigs = np.exp(
            set_acq_value / (batch_size - 1)
        )
        arithmetic_mean_of_pred_cov_eigs = np.mean(
            np.square(stddev_from_all_list)
        )
        self.assertLessEqual(
            geometric_mean_of_pred_cov_eigs, arithmetic_mean_of_pred_cov_eigs
        )

  def test_ucb_overwrite(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    vectorized_optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=es.VectorizedEagleStrategyFactory(),
        max_evaluations=100,
    )
    designer = gp_ucb_pe.VizierGPUCBPEBandit(
        problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        metadata_ns='gp_ucb_pe_bandit_test',
        num_seed_trials=1,
        config=gp_ucb_pe.UCBPEConfig(
            ucb_coefficient=10.0,
            explore_region_ucb_coefficient=0.5,
            cb_violation_penalty_coefficient=10.0,
            ucb_overwrite_probability=1.0,
            pe_overwrite_probability=0.0,
            signal_to_noise_threshold=0.0,
        ),
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10
        ),
        rng=jax.random.PRNGKey(1),
    )

    trial_id = 1
    batch_size = 5
    iters = 3
    rng = jax.random.PRNGKey(1)
    all_trials = []
    # Simulates a batch suggestion loop that completes a full batch of
    # suggestions before asking for the next batch.
    for _ in range(iters):
      suggestions = designer.suggest(count=batch_size)
      self.assertLen(suggestions, batch_size)
      completed_trials = []
      for suggestion in suggestions:
        problem.search_space.assert_contains(suggestion.parameters)
        trial_id += 1
        measurement = vz.Measurement()
        for mi in problem.metric_information:
          measurement.metrics[mi.name] = float(
              jax.random.uniform(
                  rng,
                  minval=mi.min_value_or(lambda: -10.0),
                  maxval=mi.max_value_or(lambda: 10.0),
              )
          )
          rng, _ = jax.random.split(rng)
        completed_trials.append(
            suggestion.to_trial(trial_id).complete(measurement)
        )
      all_trials.extend(completed_trials)
      designer.update(
          completed=abstractions.CompletedTrials(completed_trials),
          all_active=abstractions.ActiveTrials(),
      )

    self.assertLen(all_trials, iters * batch_size)

    for idx, trial in enumerate(all_trials):
      if idx < batch_size:
        # Skips the first batch of suggestions, which are generated by the
        # seeding designer, not acquisition function optimization.
        continue
      # Because `ucb_overwrite_probability` is 1 and `pe_overwrite_probability`
      # is 0, all suggestions after the first batch are expected to be generated
      # by UCB. Within a batch, the first suggestion's UCB value is expected to
      # use predicted standard deviation based only on completed trials, while
      # the UCB values of the second to the last suggestions are expected to use
      # the predicted standard deviations based on completed and active trials.
      mean, stddev, stddev_from_all, acq, use_ucb = _extract_predictions(
          trial.metadata.ns('gp_ucb_pe_bandit_test')
      )
      self.assertAlmostEqual(
          mean + 10.0 * (stddev_from_all if idx % batch_size > 0 else stddev),
          acq,
          places=6,
      )
      self.assertTrue(use_ucb)

  @parameterized.parameters(
      dict(optimize_set_acquisition_for_exploration=False),
      dict(optimize_set_acquisition_for_exploration=True),
  )
  def test_prior_acquisition(
      self, optimize_set_acquisition_for_exploration: bool
  ):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    vectorized_optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=es.VectorizedEagleStrategyFactory(),
        max_evaluations=100,
    )

    def dummy_prior_acquisition(xs: types.ModelInput):
      return np.ones(xs.continuous.shape[0]) * 12345.0

    designer = gp_ucb_pe.VizierGPUCBPEBandit(
        problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        metadata_ns='gp_ucb_pe_bandit_test',
        num_seed_trials=1,
        config=gp_ucb_pe.UCBPEConfig(
            ucb_coefficient=10.0,
            explore_region_ucb_coefficient=0.5,
            cb_violation_penalty_coefficient=10.0,
            ucb_overwrite_probability=0.0,
            pe_overwrite_probability=0.0,
            signal_to_noise_threshold=0.0,
            optimize_set_acquisition_for_exploration=(
                optimize_set_acquisition_for_exploration
            ),
        ),
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10
        ),
        prior_acquisition=dummy_prior_acquisition,
        rng=jax.random.PRNGKey(1),
    )

    trial_id = 1
    batch_size = 3
    iters = 2
    rng = jax.random.PRNGKey(1)
    all_trials = []
    # Simulates a batch suggestion loop that completes a full batch of
    # suggestions before asking for the next batch.
    for _ in range(iters):
      suggestions = designer.suggest(count=batch_size)
      self.assertLen(suggestions, batch_size)
      completed_trials = []
      for suggestion in suggestions:
        problem.search_space.assert_contains(suggestion.parameters)
        trial_id += 1
        measurement = vz.Measurement()
        for mi in problem.metric_information:
          measurement.metrics[mi.name] = float(
              jax.random.uniform(
                  rng,
                  minval=mi.min_value_or(lambda: -10.0),
                  maxval=mi.max_value_or(lambda: 10.0),
              )
          )
          rng, _ = jax.random.split(rng)
        completed_trials.append(
            suggestion.to_trial(trial_id).complete(measurement)
        )
      all_trials.extend(completed_trials)
      designer.update(
          completed=abstractions.CompletedTrials(completed_trials),
          all_active=abstractions.ActiveTrials(),
      )

    self.assertLen(all_trials, iters * batch_size)

    set_acq_value = None
    stddev_from_all_list = []
    for idx, trial in enumerate(all_trials):
      if idx < batch_size:
        # Skips the first batch of suggestions, which are generated by the
        # seeding designer, not acquisition function optimization.
        continue
      mean, stddev, stddev_from_all, acq, use_ucb = _extract_predictions(
          trial.metadata.ns('gp_ucb_pe_bandit_test')
      )
      prior_acq_value = float(
          trial.metadata.ns('gp_ucb_pe_bandit_test')
          .ns('prior_acquisition')
          .get('value')
      )
      self.assertEqual(prior_acq_value, 12345.0)
      if idx % batch_size == 0:
        # The first suggestion in a batch is expected to be generated by UCB,
        # and the acquisition value is expected to be the sum of UCB and the
        # prior acquisition value.
        self.assertTrue(use_ucb)
        self.assertAlmostEqual(
            mean + 10.0 * stddev + prior_acq_value,
            acq,
        )
      else:
        # Later suggestions in a batch are expected to be generated by PE,
        # and the acquisition value is expected to be the sum of PE and the
        # prior acquisition value.
        self.assertFalse(use_ucb)
        if optimize_set_acquisition_for_exploration:
          # The acquisition value is expected to be the sum of the
          # log-determinant of the predicted covariance matrix and the prior
          # acquisition value (12345.0), so it should be greater than 10000.0.
          self.assertGreater(acq, 10000.0)
          stddev_from_all_list.append(stddev_from_all)
          if set_acq_value is None:
            set_acq_value = acq - prior_acq_value
          else:
            self.assertAlmostEqual(set_acq_value, acq - prior_acq_value)
        else:
          self.assertAlmostEqual(stddev_from_all + prior_acq_value, acq)

    if optimize_set_acquisition_for_exploration:
      geometric_mean_of_pred_cov_eigs = np.exp(set_acq_value / (batch_size - 1))
      arithmetic_mean_of_pred_cov_eigs = np.mean(
          np.square(stddev_from_all_list)
      )
      self.assertLessEqual(
          geometric_mean_of_pred_cov_eigs, arithmetic_mean_of_pred_cov_eigs
      )

  @parameterized.parameters(
      dict(num_iters=10, batch_size=1),
      dict(num_iters=5, batch_size=2),
  )
  def test_discrete_parameters_are_explored(
      self, num_iters: int, batch_size: int
  ):
    space = vz.SearchSpace()
    root = space.root
    root.add_discrete_param('discrete_double_0', [-1.0, 1.0])
    root.add_discrete_param('discrete_double_1', [-3.0, 7.0])
    root.add_discrete_param('discrete_double_single_feasible_value', [0.37])
    root.add_float_param('double', min_value=-5.0, max_value=5.0)
    root.add_int_param('integer_with_many_feasible_values', -1, 6 * 10**7)
    root.add_discrete_param(
        'discrete_double_many_feasible_values', np.linspace(-10.0, 10.0, 1000)
    )
    problem = vz.ProblemStatement(space)
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    vectorized_optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=es.VectorizedEagleStrategyFactory(),
        max_evaluations=100,
    )
    designer = gp_ucb_pe.VizierGPUCBPEBandit(
        problem,
        acquisition_optimizer_factory=vectorized_optimizer_factory,
        metadata_ns='gp_ucb_pe_bandit_test',
        num_seed_trials=1,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10
        ),
        rng=jax.random.PRNGKey(1),
    )
    all_trials = []
    trial_id = 1
    discrete_double_0_count = {'-1.0': 0, '1.0': 0}
    discrete_double_1_count = {'-3.0': 0, '7.0': 0}
    for _ in range(num_iters):
      suggestions = designer.suggest(count=batch_size)
      self.assertLen(suggestions, batch_size)
      completed_trials = []
      for suggestion in suggestions:
        problem.search_space.assert_contains(suggestion.parameters)
        discrete_double_0_count[
            str(suggestion.parameters['discrete_double_0'])
        ] += 1
        discrete_double_1_count[
            str(suggestion.parameters['discrete_double_1'])
        ] += 1
        trial_id += 1
        measurement = vz.Measurement()
        for mi in problem.metric_information:
          measurement.metrics[mi.name] = np.sum(
              [val.as_float for val in suggestion.parameters.values()]
          )
        completed_trials.append(
            suggestion.to_trial(trial_id).complete(measurement)
        )
      all_trials.extend(completed_trials)
      designer.update(
          completed=abstractions.CompletedTrials(completed_trials),
          all_active=abstractions.ActiveTrials(),
      )

    for val, count in discrete_double_0_count.items():
      self.assertGreater(count, 0, msg=f'discrete_double_0: {val}')
    for val, count in discrete_double_1_count.items():
      self.assertGreater(count, 0, msg=f'discrete_double_1: {val}')


if __name__ == '__main__':
  jax.config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/algorithms/designers/grid.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Grid Search Designer which searches over a discretized grid of Trial parameter values."""

import copy
import random
from typing import Dict, List, Optional, Sequence

from absl import logging
import attrs
import numpy as np
from vizier import algorithms
from vizier import pyvizier
from vizier.interfaces import serializable
from vizier.pyvizier import converters


GridValues = Dict[str, List[pyvizier.ParameterValue]]


@attrs.define(auto_attribs=False, init=False)
class GridSearchDesigner(algorithms.PartiallySerializableDesigner):
  """Grid Search designer.

  This designer searches over a grid of hyper-parameter values.

  NOTE: The grid search index (i.e. which grid point to output) is based the
  number of suggestions created so far (regardless of completion or not). This
  means the class must be wrapped via `PartiallySerializableDesignerPolicy` for
  use in Pythia, thus requiring load/dump implementations.
  """

  _unshuffled_grid_values: GridValues = attrs.field()
  _grid_values: GridValues = attrs.field()
  _current_index: int = attrs.field()
  _shuffle_seed: Optional[int] = attrs.field()
  _double_grid_resolution: int = attrs.field()

  _metadata_ns: str = 'grid'  # class-level constant.

  def __init__(
      self,
      search_space: pyvizier.SearchSpace,
      shuffle_seed: Optional[int] = None,
      *,
      double_grid_resolution: int = 10,
  ):
    """Init.

    Args:
      search_space: Must be a flat search space.
      shuffle_seed: Whether to shuffle the grid ordering. If None, uses the
        given ordering from original search space.
      double_grid_resolution: Number of grid points for DOUBLE parameters.
    """
    if search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.'
      )
    self._search_space = search_space
    self._shuffle_seed = shuffle_seed
    self._double_grid_resolution = double_grid_resolution
    self._current_index = 0

    # Creates unshuffled grid values for every parameter. This is just a
    # template for creating the potentially shuffled self._grid_values to be
    # used in suggest().
    self._unshuffled_grid_values = {}
    for parameter_config in self._search_space.parameters:
      self._unshuffled_grid_values[parameter_config.name] = (
          self._grid_points_from_parameter_config(parameter_config)
      )

    # Set true grid values to be used during suggest calls.
    self._grid_values = self._maybe_shuffled_grid_values(self._shuffle_seed)

  @classmethod
  def from_problem(
      cls,
      problem: pyvizier.ProblemStatement,
      seed: Optional[int] = None,
  ):
    """For wrapping via `PartiallySerializableDesignerPolicy`."""
    return GridSearchDesigner(problem.search_space, seed)

  def update(
      self,
      completed: algorithms.CompletedTrials,
      all_active: algorithms.ActiveTrials,
  ) -> None:
    pass

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[pyvizier.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1

    parameter_dicts = []
    for index in range(self._current_index, self._current_index + count):
      # Use index to select parameters via Cartesian Product ordering.
      # Effectively equivalent to itertools.product(list_of_lists)[index]`,
      # without the memory blowup.
      parameter_dict = pyvizier.ParameterDict()
      temp_index = index
      for p_name in self._grid_values:
        p_length = len(self._grid_values[p_name])
        p_index = temp_index % p_length
        parameter_dict[p_name] = self._grid_values[p_name][p_index]
        temp_index = temp_index // p_length
      parameter_dicts.append(parameter_dict)

    self._current_index += len(parameter_dicts)
    return [pyvizier.TrialSuggestion(parameters=p) for p in parameter_dicts]

  def load(self, metadata: pyvizier.Metadata) -> None:
    """Load the current index."""
    metadata = metadata.ns(self._metadata_ns)
    try:
      current_index = int(metadata['current_index'])
      none_or_int = metadata['shuffle_seed']
      shuffle_seed = None if (none_or_int == 'None') else int(none_or_int)
      logging.info('Restored shuffle seed: %s', shuffle_seed)
    except (KeyError, ValueError) as e:
      raise serializable.HarmlessDecodeError() from e

    self._current_index = current_index
    self._shuffle_seed = shuffle_seed
    self._grid_values = self._maybe_shuffled_grid_values(self._shuffle_seed)

  def dump(self) -> pyvizier.Metadata:
    """Dump the current index."""
    metadata = pyvizier.Metadata()
    metadata.ns(self._metadata_ns)['current_index'] = str(self._current_index)
    metadata.ns(self._metadata_ns)['shuffle_seed'] = str(self._shuffle_seed)
    return metadata

  def _grid_points_from_parameter_config(
      self,
      parameter_config: pyvizier.ParameterConfig,
  ) -> List[pyvizier.ParameterValue]:
    """Produces grid points from a parameter_config."""
    if parameter_config.type == pyvizier.ParameterType.DOUBLE:
      min_value, max_value = parameter_config.bounds
      if min_value == max_value:
        return [pyvizier.ParameterValue(value=min_value)]

      converter = converters.DefaultModelInputConverter(
          parameter_config, scale=True
      )
      grid_scalars = np.linspace(0.0, 1.0, num=self._double_grid_resolution)
      return converter.to_parameter_values(grid_scalars)  # pytype:disable=bad-return-type

    elif parameter_config.type == pyvizier.ParameterType.INTEGER:
      min_value, max_value = parameter_config.bounds
      return [
          pyvizier.ParameterValue(value=value)
          for value in range(min_value, max_value + 1)
      ]

    elif parameter_config.type in [
        pyvizier.ParameterType.DISCRETE,
        pyvizier.ParameterType.CATEGORICAL,
    ]:
      return [
          pyvizier.ParameterValue(value=value)
          for value in parameter_config.feasible_values
      ]

    else:
      raise ValueError(
          'ParameterConfig type is not one of the supported primitives for'
          f' ParameterConfig: {parameter_config}'
      )

  def _maybe_shuffled_grid_values(
      self, shuffle_seed: Optional[int]
  ) -> GridValues:
    grid_values = copy.deepcopy(self._unshuffled_grid_values)

    # Shuffle the grid if specified.
    if shuffle_seed is not None:
      rng = random.Random(shuffle_seed)
      # Shuffle dict keys.
      shuffled_items = list(grid_values.items())
      rng.shuffle(shuffled_items)
      grid_values = dict(shuffled_items)
      # Shuffle dict value lists.
      for param_name in grid_values:
        rng.shuffle(grid_values[param_name])

    return grid_values


--- vizier/_src/algorithms/designers/grid_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for grid."""
import functools

from vizier import pythia
from vizier import pyvizier
from vizier._src.algorithms.designers import grid
from vizier._src.algorithms.policies import designer_policy
from vizier._src.algorithms.testing import test_runners
from vizier.interfaces import serializable

from absl.testing import absltest
from absl.testing import parameterized


class GridSearchTest(parameterized.TestCase):
  # TODO: Add conditional test case.

  def setUp(self):
    """Setups up search space."""
    self.search_space = pyvizier.SearchSpace()
    self.search_space.root.add_float_param(
        'double', min_value=-1.0, max_value=1.0
    )
    self.search_space.root.add_float_param(
        'double_logscaled',
        min_value=0.001,
        max_value=1.0,
        scale_type=pyvizier.ScaleType.LOG,
    )
    self.search_space.root.add_float_param(
        'double_same_bounds', min_value=30.0, max_value=30.0
    )
    self.search_space.root.add_categorical_param(
        name='categorical', feasible_values=['a', 'b', 'c']
    )
    self.search_space.root.add_discrete_param(
        name='discrete', feasible_values=[0.1, 0.3, 0.5]
    )
    self.search_space.root.add_int_param(name='int', min_value=1, max_value=5)
    self.search_space.root.add_int_param(
        name='int_same_bounds', min_value=150, max_value=150
    )
    self._designer = grid.GridSearchDesigner(self.search_space)
    self.search_space_size = (
        self._designer._double_grid_resolution
        * self._designer._double_grid_resolution
        * 3
        * 3
        * 5
    )
    super().setUp()

  def test_make_grid_values(self):
    grid_values = self._designer._grid_values
    self.assertLen(
        grid_values['double'], self._designer._double_grid_resolution
    )
    double_list = [p.value for p in grid_values['double']]
    for d in double_list:
      self.assertBetween(d, -1.0, 1.0)
    self.assertContainsSubset([-1.0, 1.0], double_list)
    self.assertLen(grid_values['categorical'], 3)
    self.assertLen(grid_values['discrete'], 3)
    self.assertLen(grid_values['int'], 5)

  def test_load_failure(self):
    problem = pyvizier.ProblemStatement(self.search_space)
    test_runners.RandomMetricsRunner(
        problem, iters=20, validate_parameters=True
    ).run_designer(self._designer)

    with self.assertRaises(serializable.HarmlessDecodeError):
      self._designer.load(pyvizier.Metadata())

    # After the load failure, the grid designer is still in a valid state
    # and can be used to generate suggestions.
    test_runners.RandomMetricsRunner(
        problem, iters=10, validate_parameters=True
    ).run_designer(self._designer)

  @parameterized.parameters((None,), (0,), (1,), (2,))
  def test_make_suggestions(self, shuffle_seed):
    """Tests designer suggestion generation."""
    designer = grid.GridSearchDesigner(self.search_space, shuffle_seed)
    suggestions = designer.suggest(self.search_space_size)
    self.assertLen(suggestions, self.search_space_size)
    print(suggestions)
    for suggestion in suggestions:
      self.assertLen(suggestion.parameters, len(self.search_space.parameters))

    # Make sure we covered entire search space.
    distinct_suggestions = set(
        [
            tuple(suggestion.parameters.as_dict().values())
            for suggestion in suggestions
        ]
    )
    self.assertLen(distinct_suggestions, self.search_space_size)

  @parameterized.parameters((None,), (0,), (1,), (2,))
  def test_policy_wrapping(self, shuffle_seed):
    problem = pyvizier.ProblemStatement()
    problem.search_space = self.search_space
    policy_supporter = pythia.InRamPolicySupporter(problem)
    designer_factory = functools.partial(
        grid.GridSearchDesigner.from_problem, seed=shuffle_seed
    )
    policy = designer_policy.PartiallySerializableDesignerPolicy(
        problem, policy_supporter, designer_factory
    )

    # Make sure we covered entire search space.
    all_suggestions = []
    for _ in range(self.search_space_size):
      request = pythia.SuggestRequest(
          study_descriptor=policy_supporter.study_descriptor(), count=1
      )
      decisions = policy.suggest(request)
      all_suggestions.extend(decisions.suggestions)

    distinct_suggestions = set(
        [
            tuple(suggestion.parameters.as_dict().values())
            for suggestion in all_suggestions
        ]
    )
    self.assertLen(distinct_suggestions, self.search_space_size)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/harmonica.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Harmonica algorithm for boolean search spaces from 'Hyperparameter Optimization: A Spectral Approach' (https://arxiv.org/abs/1706.00764).

This is a faithful re-implementation based off
https://github.com/callowbird/Harmonica.
"""
# pylint:disable=invalid-name
import abc
import itertools
from typing import Optional, Sequence, Set
import attrs
import numpy as np
from sklearn import linear_model
from sklearn import preprocessing

from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random


class _Surrogate(abc.ABC):
  """Utility class to organize code."""

  @abc.abstractmethod
  def reset(self) -> None:
    """Resets internal state."""

  @abc.abstractmethod
  def regress(self, X: np.ndarray, Y: np.ndarray) -> None:
    """Performs regression and modifies internal state."""

  @abc.abstractmethod
  def predict(self, x: np.ndarray) -> float:
    """Performs prediction. Should NOT modify internal state."""


@attrs.define
class PolynomialSparseRecovery(_Surrogate):
  """Performs LASSO regression over low (d) degree polynomial coefficients."""

  # Maximum degree of monomials to use during polynomial regression.
  _d: int = attrs.field(
      default=3, validator=attrs.validators.ge(1), kw_only=True
  )
  # Number of top monomial coefficients to use.
  _num_top_monomials: int = attrs.field(
      default=5, validator=attrs.validators.ge(1), kw_only=True
  )

  # LASSO regularization coefficient.
  _alpha = attrs.field(
      default=3.0, validator=attrs.validators.ge(0.0), kw_only=True
  )

  # Internal state after performing regression.
  _poly_transformer: preprocessing.PolynomialFeatures = attrs.field(init=False)
  _top_poly_indices: np.ndarray = attrs.field(init=False)
  _top_poly_coefficients: np.ndarray = attrs.field(init=False)

  def __attrs_post_init__(self):
    self.reset()

  def reset(self) -> None:
    self._poly_transformer = preprocessing.PolynomialFeatures(
        self._d, interaction_only=True
    )
    self._top_poly_indices = np.empty(0)
    self._top_poly_coefficients = np.empty(0)

  def regress(self, X: np.ndarray, Y: np.ndarray) -> None:
    """Performs LASSO regression to obtain top monomial coefficients."""

    # Computes monomial features for every vector in X.
    X_poly_features = self._poly_transformer.fit_transform(X)

    # Find optimial coefficients on monomials to the data.
    lasso_solver = linear_model.Lasso(fit_intercept=True, alpha=self._alpha)
    lasso_solver.fit(X_poly_features, Y)

    # Sort and obtain top coefficients.
    lasso_coefficients = lasso_solver.coef_
    poly_indices = np.argsort(-np.abs(lasso_coefficients))
    self._top_poly_indices = poly_indices[: self._num_top_monomials]
    self._top_poly_coefficients = lasso_coefficients[self._top_poly_indices]

  def predict(self, x: np.ndarray) -> float:
    """Predicts using polynomial features."""
    x_poly_features = self._poly_transformer.transform(x.reshape(1, -1))
    x_poly_features = x_poly_features[0]

    top_x_poly_features = np.take_along_axis(
        x_poly_features, self._top_poly_indices, axis=0
    )
    return np.dot(top_x_poly_features, self._top_poly_coefficients)

  def index_set(self) -> Set[int]:
    """Returns parameter index set J from top coefficients.

    For example, if our monomials corresponding to top cofficients were x0*x1,
    x3, x1*x2*x3, then the output would be Union({0, 1}, {3}, {1, 2, 3}).
    """
    indices = set()
    poly_feature_one_hots = self._poly_transformer.powers_
    for top_poly_index in self._top_poly_indices:
      active_indices = np.nonzero(poly_feature_one_hots[top_poly_index])
      active_indices = active_indices[0].tolist()
      indices = indices | set(active_indices)
    return indices


@attrs.define
class RestrictedSurrogate(_Surrogate):
  """New surrogate with input x's positions replaced from X_restrictor values."""

  X_restrictors: np.ndarray = attrs.field(init=True, kw_only=True)
  replacement_indices: Sequence[int] = attrs.field(init=True, kw_only=True)
  psr: PolynomialSparseRecovery = attrs.field(
      init=True, kw_only=True, factory=PolynomialSparseRecovery
  )

  def reset(self) -> None:
    raise NotImplementedError('Should not be used.')

  def regress(self, X: np.ndarray, Y: np.ndarray) -> None:
    raise NotImplementedError('Should not be used.')

  def predict(self, x: np.ndarray) -> float:
    objectives = []
    for x_restrictor in self.X_restrictors:
      x_copy = np.copy(x)
      x_copy[self.replacement_indices] = x_restrictor[self.replacement_indices]
      objectives.append(self.psr.predict(x_copy))
    return np.mean(objectives)


def _binary_subset_enumeration(
    dim: int, indices: Sequence[int], default_value: float = 1.0
) -> np.ndarray:
  """Outputs all possible binary vectors from {-1, 1}^{dim} where only positions from `indices` are changed."""
  output = default_value * np.ones(
      shape=(2 ** len(indices), dim), dtype=np.float32
  )
  for i, binary in enumerate(
      itertools.product([-1.0, 1.0], repeat=len(indices))
  ):
    output[i, indices] = binary
  return output


@attrs.define
class HarmonicaQ(_Surrogate):
  """Q-stage Harmonica.

  At each stage:
  1. Invoke PSR on the previous data (X,Y).
  2. Obtain t maximizers of the surrogate of the PSR.
  3. Redefine a 'restricted surrogate' using the t maximizers.
  4. Produce a new (X', Y') dataset via random search on this 'restricted
  surrogate'.
  """

  # PolynomialSparseRecovery regressor.
  _psr: PolynomialSparseRecovery = attrs.field(
      init=True, kw_only=True, factory=PolynomialSparseRecovery
  )
  # Number of stages.
  _q: int = attrs.field(
      default=10, validator=attrs.validators.ge(0), kw_only=True
  )

  # Number of maximizers on the surrogate to use.
  _t: int = attrs.field(
      default=1, validator=attrs.validators.ge(1), kw_only=True
  )
  # Number of data samples to collect on the restricted surrogate.
  _T: int = attrs.field(
      default=300, validator=attrs.validators.ge(1), kw_only=True
  )

  _restricted_surrogate: Optional[RestrictedSurrogate] = None

  def __attrs_post_init__(self):
    self.reset()

  def reset(self) -> None:
    self._restricted_surrogate = None
    self._psr.reset()

  def regress(self, X: np.ndarray, Y: np.ndarray) -> None:
    """Performs q-stage Harmonica."""
    num_vars = X.shape[-1]

    X_temp = X
    Y_temp = Y
    for _ in range(self._q):
      # Invoke PSR on data.
      self._psr.reset()
      self._psr.regress(X_temp, Y_temp)
      J = self._psr.index_set()

      # Perform brute force maximization to optain top t optimizers.
      all_X_in_J = _binary_subset_enumeration(num_vars, list(J))
      all_Y_in_J = np.array([self._psr.predict(x) for x in all_X_in_J])
      maximizer_idxs = np.argsort(all_Y_in_J)[-self._t :]
      X_maximizers = all_X_in_J[maximizer_idxs]

      # Define restricted surrogate and obtain data from it.
      self._restricted_surrogate = RestrictedSurrogate(
          X_restrictors=X_maximizers,
          replacement_indices=list(J),
          psr=self._psr,
      )
      X_temp = np.random.choice([-1.0, 1.0], size=(self._T, num_vars))
      Y_temp = np.array([self._restricted_surrogate.predict(x) for x in X_temp])

  def predict(self, x: np.ndarray) -> float:
    if self._restricted_surrogate is None:
      raise ValueError('You must call regress() first.')
    return self._restricted_surrogate.predict(x)


class HarmonicaDesigner(vza.Designer):
  """Harmonica Designer.

  The summary of the current implementation is as follows:

  1. Use previous trials for data collection.
  2. Perform Polynomial Lasso over the data and obtain a predictor function
  based on filtering only the highest coefficients.
  3. Obtain the "J"-set (i.e. set of input indices that only affect the
  predictor function).
  4. Find the global optimizer over the J-set domain on the predictor function,
  and return it as a suggestion.
  """

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      harmonica_q: Optional[HarmonicaQ] = None,
      acquisition_samples: int = 100,
      num_init_samples: int = 10,
  ):
    """Init.

    Args:
      problem_statement: Must use a boolean search space.
      harmonica_q: HarmonicaQ class. If None, will use default class with
        default kwargs.
      acquisition_samples: Number of trial samples to optimize final acquisition
        function.
      num_init_samples: Number of initial random suggestions for seeding the
        model.
    """

    if problem_statement.search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.'
      )
    for p_config in problem_statement.search_space.parameters:
      if p_config.external_type != vz.ExternalType.BOOLEAN:
        raise ValueError('Only boolean search spaces are supported.')

    self._problem_statement = problem_statement
    self._metric_name = self._problem_statement.metric_information.item().name
    self._search_space = problem_statement.search_space
    self._num_vars = len(self._search_space.parameters)

    self._harmonica_q = harmonica_q or HarmonicaQ()
    self._acquisition_samples = acquisition_samples
    self._num_init_samples = num_init_samples

    self._trials = []

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    self._trials += tuple(completed.trials)

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    """Performs entire q-stage Harmonica using previous trials for regression data.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1
    if count > 1:
      raise ValueError('This designer does not support batched suggestions.')

    if len(self._trials) < self._num_init_samples:
      random_designer = random.RandomDesigner(self._search_space)
      return random_designer.suggest(count)

    # Convert previous trial data into regression data.
    X = []
    Y = []
    for t in self._trials:
      single_x = [
          1.0 if t.parameters.get_value(p.name) == 'True' else -1.0
          for p in self._search_space.parameters
      ]
      single_y = t.final_measurement.metrics[self._metric_name].value
      X.append(single_x)
      Y.append(single_y)
    X = np.array(X)
    Y = np.array(Y)

    if (
        self._problem_statement.metric_information.item().goal
        == vz.ObjectiveMetricGoal.MINIMIZE
    ):
      Y = -Y

    # Perform q-stage Harmonica.
    self._harmonica_q.reset()
    self._harmonica_q.regress(X, Y)

    # Optimize final acquisition function.
    # TODO: Allow any designer instead of just random search.
    X_temp = np.random.choice(
        [-1.0, 1.0], size=(self._acquisition_samples, self._num_vars)
    )
    Y_temp = np.array([self._harmonica_q.predict(x) for x in X_temp])
    x_new = X_temp[np.argmax(Y_temp)]

    parameters = vz.ParameterDict()
    for i, p in enumerate(self._search_space.parameters):
      parameters[p.name] = 'True' if x_new[i] == 1.0 else 'False'
    return [vz.TrialSuggestion(parameters=parameters)]


--- vizier/_src/algorithms/designers/harmonica_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for bocs."""
from vizier._src.algorithms.designers import harmonica
from vizier._src.algorithms.testing import test_runners
from vizier._src.benchmarks.experimenters import combo_experimenter

from absl.testing import absltest


class HarmonicaTest(absltest.TestCase):

  def test_make_suggestions(self):
    experimenter = combo_experimenter.IsingExperimenter(lamda=0.01)
    designer = harmonica.HarmonicaDesigner(
        experimenter.problem_statement(), num_init_samples=1)

    num_trials = 10
    trials = test_runners.run_with_random_metrics(
        designer,
        experimenter.problem_statement(),
        iters=num_trials,
        batch_size=1,
        verbose=1,
        validate_parameters=True)
    self.assertLen(trials, num_trials)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/meta_learning/eagle_meta_learning.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Eagle meta learner search space.
"""

from vizier import pyvizier as vz


def meta_eagle_search_space() -> vz.SearchSpace:
  """Returns the meta eagle search space."""
  search_space = vz.SearchSpace()
  # Perturbation
  search_space.root.add_float_param(
      name='perturbation',
      min_value=1e-4,
      max_value=1e2,
      default_value=1e-1,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='perturbation_lower_bound',
      min_value=1e-5,
      max_value=1e-1,
      default_value=1e-3,
      scale_type=vz.ScaleType.LOG,
  )
  # Gravity
  search_space.root.add_float_param(
      name='gravity',
      min_value=1e-2,
      max_value=1e2,
      default_value=1.0,
      scale_type=vz.ScaleType.LOG,
  )
  # Visibility
  search_space.root.add_float_param(
      name='visibility',
      min_value=3 * 1e-2,
      max_value=3 * 1e2,
      default_value=3.0,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='categorical_visibility',
      min_value=2.0 * 1e-3,
      max_value=2.0 * 1e1,
      default_value=2.0 * 1e-1,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='discrete_visibility',
      min_value=1e-2,
      max_value=1e2,
      default_value=1.0,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='categorical_perturbation_factor',
      min_value=2.5 * 1e-1,
      max_value=2.5 * 1e3,
      default_value=2.5 * 1e1,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='discrete_perturbation_factor',
      min_value=1e-1,
      max_value=1e3,
      default_value=1e1,
      scale_type=vz.ScaleType.LOG,
  )
  # Pool size.
  search_space.root.add_float_param(
      name='pool_size_factor',
      min_value=1.0,
      max_value=2.0,
      default_value=1.2,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='negative_gravity',
      min_value=2.0 * 1e-4,
      max_value=2.0,
      default_value=2 * 1e-2,
      scale_type=vz.ScaleType.LOG,
  )
  search_space.root.add_float_param(
      name='pure_categorical_perturbation',
      min_value=1e-3,
      max_value=1e1,
      default_value=1e-1,
      scale_type=vz.ScaleType.LOG,
  )

  return search_space


--- vizier/_src/algorithms/designers/meta_learning/eagle_meta_learning_convergence_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Convergence test for Eagle Auto Tuner."""

from typing import Optional
import attrs
from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.designers.eagle_strategy import testing
from vizier._src.algorithms.designers.meta_learning import eagle_meta_learning
from vizier._src.algorithms.designers.meta_learning import meta_learning
from vizier._src.algorithms.testing import comparator_runner
from vizier._src.benchmarks.experimenters.synthetic import bbob
from absl.testing import absltest
from absl.testing import parameterized


def _eagle_designer_factory(
    problem: vz.ProblemStatement, seed: Optional[int], **kwargs
):
  """Creates an EagleStrategyDesigner with hyper-parameters and seed."""
  config = eagle_strategy.FireflyAlgorithmConfig()
  # Unpack the hyperparameters into the Eagle config class.
  for param_name, param_value in kwargs.items():
    if param_name not in attrs.asdict(config):
      raise ValueError(f"'{param_name}' is not in FireflyAlgorithmConfig!")
    setattr(config, param_name, param_value)
  return eagle_strategy.EagleStrategyDesigner(
      problem_statement=problem,
      seed=seed,
      config=config,
  )


class EagleEagleMetaLearningConvergenceTest(parameterized.TestCase):
  """Convergence test for meta Eagle-Eagle designer.

  Note that all optimization problems are MINIMIZATION.
  """

  @parameterized.parameters(
      testing.create_continuous_exptr(bbob.Rastrigin),
      testing.create_categorical_exptr(),
  )
  def test_convergence(self, exptr):
    def _random_designer_factory(problem, seed):
      return random.RandomDesigner(problem.search_space, seed=seed)

    def _meta_eagle_eagle_designer_factory(problem, seed):
      meta_config = meta_learning.MetaLearningConfig(
          num_trials_per_tuning=100,
          tuning_max_num_trials=1000,
          tuning_min_num_trials=200,
      )
      return meta_learning.MetaLearningDesigner(
          problem=problem,
          tuned_designer_factory=_eagle_designer_factory,
          meta_designer_factory=_eagle_designer_factory,
          tuning_hyperparams=eagle_meta_learning.meta_eagle_search_space(),
          config=meta_config,
          seed=seed,
      )

    random_benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        designer_factory=_random_designer_factory, experimenter=exptr
    )

    meta_benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        designer_factory=_meta_eagle_eagle_designer_factory,
        experimenter=exptr,
    )
    evaluations = 1_500
    # Random designer batch size is large to expedite run time.
    comparator_runner.SimpleRegretComparisonTester(
        baseline_num_trials=2 * evaluations,
        candidate_num_trials=evaluations,
        baseline_suggestion_batch_size=2 * evaluations,
        candidate_suggestion_batch_size=5,
        baseline_num_repeats=5,
        candidate_num_repeats=1,
        alpha=0.05,
        goal=vz.ObjectiveMetricGoal.MINIMIZE,
    ).assert_benchmark_state_better_simple_regret(
        baseline_benchmark_state_factory=random_benchmark_state_factory,
        candidate_benchmark_state_factory=meta_benchmark_state_factory,
    )


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/designers/meta_learning/meta_learning.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Meta Learning Designer.

The 'meta-learning' designer attempts to find the optimal hyper-parameters
values of the 'tuned' designer through a meta-learning process. In essensce, it
searches for the hyper-parameters values that yield the best result for the
given problem as oppose to blindly using pre-defined fixed values.

Notes
-----

1) The 'meta-learning' designer's search space is the hyper-parameters of
the 'tuned' designer which we seek to tune.

2) Before tuning starts, a tuned designer instantiated with the default
parameter values defined by the search space is used. This means that
default configuration defined in the designer level will be overridden.

3) The 'tuned_designer_factory' should accept the hyper-parameters as arguments.
This means for example, that if the 'tuned_designer' relies on an internal
configuration class the 'tuned_designer_factory' function would have to handle
the instantiation of that class (see 'eagle_meta_learning_convergence_test.py'
for an example).

4) Each instance of 'tuned' designer is updated with all trials seen thus-far,
therefore hyper-parameters that were created later in the process will benefit
from being instantiated with larger trajectory. This violates the assumption
of the meta-learning designer which the meta-trial metrics are derived from the
same objective function. For now we don't address this issue directly, though
in the future we could consider applying techniques such as RegEvo.
"""

import enum
from typing import Optional, Sequence
import attrs
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.meta_learning import meta_learning_utils as utils


@attrs.define
class MetaLearningConfig:
  """Meta learning configuration (with pre-defined default values).

  To balance exploration and exploitation in the meta-learning process it's
  recommended to stop the meta-learner after a certain number of iterations so
  to take advantage of the thus-far best hyper-parameters and not to waste
  suggestion trials on further exploration.

  With the default configuration the number of meta-learning iterations is
  (10000-3000) / 100 = 70. In order to not terminate the meta-learning process,
  set the `tuning_max_num_trials` to sufficiently large value (e.g. 1e6).
  """

  # Number of trials to use per tuning iteration.
  num_trials_per_tuning: int = 100

  # Tuning starts when number of completed trials is at least this threshold.
  tuning_min_num_trials: int = 3000

  # Once the number of trials exceeds this threshold meta-learning stops.
  tuning_max_num_trials: int = 10000


class MetaLearningState(enum.Enum):
  """Meta learning state."""

  # The meta-learning process hasn't started yet, accumulating a sufficient
  # number of trials.
  INITIALIZE = 1

  # The meta-learning progress is performed to tune the designer.
  TUNE = 2

  # A tuned designer with the best hyper-params is used to generate suggestons.
  # Instantiate a tuned designer
  USE_BEST_PARAMS = 3


# TODO: support serialization.
@attrs.define
class MetaLearningDesigner(vza.Designer):
  """Meta learning designer."""

  # The problem associated with the 'tuned' designer.
  problem: vz.ProblemStatement

  # Factory of the 'tuned' designer.
  tuned_designer_factory: vza.DesignerFactory[vza.Designer]

  # Factory of the 'meta-learning' designer.
  meta_designer_factory: vza.DesignerFactory[vza.Designer]

  # The 'tuned' designer hyper-parameters to apply meta-learning on (which
  # constitute the search space of the 'meta-learning' designer).
  tuning_hyperparams: vz.SearchSpace

  # The meta-learner configuration.
  config: Optional[MetaLearningConfig] = None

  # A random seed used for instantiating the designers.
  seed: Optional[int] = None

  # ----------------------------------------------------------------------------
  # Internal Attributes
  # ----------------------------------------------------------------------------

  # The metric name used by the meta-learning designer.
  _meta_designer_metric_name: str = attrs.field(default='score', init=False)

  # The current 'tuned' designer instance with assigned hyper-parameter.
  _curr_tuned_designer: vza.Designer = attrs.field(init=False)

  # The 'meta' designer instance (only instantiated once).
  _meta_designer: vza.Designer = attrs.field(init=False)

  # The current state of the meta-learner.
  _state: MetaLearningState = attrs.field(init=False)

  # All the completed meta-trials suggested by the 'meta' designer.
  _meta_trials: list[vz.Trial] = attrs.field(factory=list, init=False)

  # The current 'tuned' hyper-parameter suggested by the meta-learner designer.
  _curr_tuned_hyperparams: vz.TrialSuggestion = attrs.field(init=False)

  # All the completed trials, suggested by the 'tuned' designers, seen since the
  # beginning of the meta-learner run.
  _trials: list[vz.Trial] = attrs.field(factory=list, init=False)

  # The completed trials suggested by the current 'tuned' designer which was
  # instantiated with the current hyper-parameter values.
  _curr_trials: list[vz.Trial] = attrs.field(factory=list, init=False)

  def __attrs_post_init__(self):
    """Initializes the meta learning desiger."""
    if len(self.problem.metric_information) != 1:
      raise ValueError(f'Expected exactly one metric. {self.problem}')

    if self.seed is None:
      # JAX random seed doesn't accept None, so generating random integer.
      self.seed = np.random.randint(low=0, high=1e6)

    # Instantiate an MetaLearningUtils.
    self._utils = utils.MetaLearningUtils(
        goal=self.problem.metric_information.item().goal,
        tuned_metric_name=self.problem.metric_information.item().name,
        meta_metric_name=self._meta_designer_metric_name,
        tuning_params=self.tuning_hyperparams,
    )
    # Instantiated 'tuned' designer the with default hyper-parameters.
    self._curr_tuned_hyperparams = self._utils.get_default_hyperparameters()
    self._curr_tuned_designer = self.tuned_designer_factory(
        self.problem,
        seed=self.seed,
        **self._curr_tuned_hyperparams.parameters.as_dict(),
    )
    self._meta_designer = self.meta_designer_factory(
        self._utils.meta_problem, seed=self.seed
    )
    self.config = self.config or MetaLearningConfig()
    self._state = MetaLearningState.INITIALIZE

  def suggest(self, count: int = 1) -> Sequence[vz.TrialSuggestion]:
    """Suggests trials."""
    return self._curr_tuned_designer.suggest(count)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Incorporates completed trials into the meta-learner designer's state.

    1) At the beginning of the run the state is INITIALIZE, during which a
    'tune' designer with default hyper-parameter is used to suggest trials.

    2) After accumulating `tuning_min_num_trials` completed trials the state
    transitions to TUNE, and then the tuning process starts.

    3) After accumulating `num_trials_per_tuning` completd trials, the meta
    iteration is summarized, during which the best trial is selected to be used
    to update the 'meta' desinger.

    4) Once reaching maximum number of total completed meta-learn trials
    (`tuning_max_num_trials`), the meta-learning process is finalized during
    which the best hyper-parameter values are selected to instantiate a new
    'tuned' designer which is then updated with all seen trials.

    Args:
      completed:
      all_active:
    """
    self._trials.extend(completed.trials)
    self._curr_trials.extend(completed.trials)
    # Update curr_designer with newly completed trials (applies to all states).
    self._curr_tuned_designer.update(
        vza.CompletedTrials(completed.trials), vza.ActiveTrials()
    )

    # Check if enough trials already accumulated to start meta learning.
    if len(self._trials) < self.config.tuning_min_num_trials:
      return

    # Check if the meta-learning process should be terminated.
    elif len(self._trials) >= self.config.tuning_max_num_trials:
      # Check if the meta-learning has just terminated. If so, finalize it.
      if self._state == MetaLearningState.TUNE:
        # Find the best meta-learn result.
        best_meta_trial = self._utils.get_best_meta_trial(self._meta_trials)
        self._curr_tuned_designer = self.tuned_designer_factory(
            self.problem, seed=self.seed, **best_meta_trial.parameters.as_dict()
        )
        # Update the newly created designer with all completed trials.
        self._curr_tuned_designer.update(
            vza.CompletedTrials(self._trials), vza.ActiveTrials()
        )
        self._state = MetaLearningState.USE_BEST_PARAMS

    else:
      self._state = MetaLearningState.TUNE
      # Check if there's enough trials to summarize meta iteration.
      if len(self._curr_trials) >= self.config.num_trials_per_tuning:
        # Get best score for the current iteration.
        meta_trial = self._utils.complete_meta_suggestion(
            meta_suggestion=self._curr_tuned_hyperparams,
            score=self._utils.get_best_tuned_trial_score(self._curr_trials),
        )
        self._meta_designer.update(
            vza.CompletedTrials([meta_trial]), vza.ActiveTrials()
        )
        # Store the iteration results to be used during meta-learn finalization.
        self._meta_trials.append(meta_trial)
        # Get new tuned params suggestion and initialize a new curr_designer.
        self._curr_tuned_hyperparams = self._meta_designer.suggest(1)[0]
        self._curr_tuned_designer = self.tuned_designer_factory(
            self.problem,
            seed=self.seed,
            **self._curr_tuned_hyperparams.parameters.as_dict(),
        )
        # Update the newly created designer with all completed trials.
        self._curr_tuned_designer.update(
            vza.CompletedTrials(self._trials), vza.ActiveTrials()
        )
        # Reset the trials associated with current hyper-parameters.
        self._curr_trials = []


--- vizier/_src/algorithms/designers/meta_learning/meta_learning_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for meta learning designer."""
from typing import Optional

import attrs
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.designers.meta_learning import meta_learning

from absl.testing import absltest
from absl.testing import parameterized


MetaLearningDesigner = meta_learning.MetaLearningDesigner
MetaLearningConfig = meta_learning.MetaLearningConfig
MetaLearningState = meta_learning.MetaLearningState


def _eagle_designer_factory(
    problem: vz.ProblemStatement, seed: Optional[int], **kwargs
):
  """Creates an EagleStrategyDesigner with hyper-parameters and seed."""
  config = eagle_strategy.FireflyAlgorithmConfig()
  # Unpack the hyperparameters into the Eagle config class.
  for param_name, param_value in kwargs.items():
    if param_name not in attrs.asdict(config):
      raise ValueError(f"'{param_name}' is not in FireflyAlgorithmConfig!")
    setattr(config, param_name, param_value)
  return eagle_strategy.EagleStrategyDesigner(
      problem_statement=problem,
      seed=seed,
      config=config,
  )


def _quasirandom_designer_factory(
    problem: vz.ProblemStatement, seed: Optional[int] = None
):
  """Creates a QuasiRandomDesigner with seed."""
  return quasi_random.QuasiRandomDesigner(problem.search_space, seed=seed)


def meta_learning_suggest_update_loop(
    meta_learning_designer: MetaLearningDesigner,
    num_suggestions: int,
    batch_size: int,
):
  trial_id = 0
  for _ in range(num_suggestions // batch_size):
    suggestions = meta_learning_designer.suggest(batch_size)
    trials = []
    for i in range(batch_size):
      trial = suggestions[i].to_trial(trial_id)
      trial_id += 1
      # Complete the trial with additional metric to test 'tuned_metric'.
      trial.complete(
          vz.Measurement(
              metrics={
                  'objective': np.random.random(),
                  'secondary_metric': np.random.random(),
              }
          )
      )
      trials.append(trial)
    meta_learning_designer.update(
        vza.CompletedTrials(trials), vza.ActiveTrials()
    )


class MetaLearningDesignerTest(parameterized.TestCase):
  """Tests for AutoTunerDesinger."""

  def setUp(self):
    super().setUp()
    self.problem = vz.ProblemStatement()
    self.problem.search_space.root.add_float_param('x', 0.0, 15.0)
    self.problem.search_space.root.add_float_param('y', -5.0, 10.0)
    self.problem.search_space.root.add_categorical_param('c', ['a', 'b', 'c'])
    self.problem.metric_information.append(
        vz.MetricInformation(
            name='objective',
            goal=vz.ObjectiveMetricGoal.MAXIMIZE,
        )
    )
    self.meta_config = MetaLearningConfig(
        num_trials_per_tuning=10,
        tuning_min_num_trials=100,
        tuning_max_num_trials=500,
    )
    self.tuning_params = vz.SearchSpace()
    self.tuning_params.root.add_float_param(
        'visibility', 0.0, 10.0, default_value=2.22
    )
    self.tuning_params.root.add_float_param(
        'gravity', 0.0, 10.0, default_value=3.33
    )

  def test_initialize_designer(self):
    meta_learning_designer = MetaLearningDesigner(
        problem=self.problem,
        tuning_hyperparams=self.tuning_params,
        tuned_designer_factory=_eagle_designer_factory,
        meta_designer_factory=_quasirandom_designer_factory,
        config=self.meta_config,
    )
    self.assertEqual(
        meta_learning_designer._state, MetaLearningState.INITIALIZE
    )
    # type: ignore[attribute-error]  # pylint: disable=protected-access
    self.assertEqual(
        meta_learning_designer._curr_tuned_designer._config.visibility, 2.22
    )
    self.assertEqual(
        meta_learning_designer._curr_tuned_designer._config.gravity, 3.33
    )

  @parameterized.parameters([1, 5])
  def test_state_transitions(self, batch_size):
    meta_learning_designer = MetaLearningDesigner(
        problem=self.problem,
        tuning_hyperparams=self.tuning_params,
        tuned_designer_factory=_eagle_designer_factory,
        meta_designer_factory=_quasirandom_designer_factory,
        config=self.meta_config,
    )
    self.assertEqual(
        meta_learning_designer._state,
        MetaLearningState.INITIALIZE,
    )
    meta_learning_suggest_update_loop(meta_learning_designer, 50, batch_size)
    self.assertEqual(
        meta_learning_designer._state,
        MetaLearningState.INITIALIZE,
    )
    meta_learning_suggest_update_loop(meta_learning_designer, 200, batch_size)
    self.assertEqual(
        meta_learning_designer._state, meta_learning.MetaLearningState.TUNE
    )
    meta_learning_suggest_update_loop(meta_learning_designer, 600, batch_size)
    self.assertEqual(
        meta_learning_designer._state,
        MetaLearningState.USE_BEST_PARAMS,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/meta_learning/meta_learning_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Meta learning helper functionality."""

import attrs
from vizier import pyvizier as vz


@attrs.define(kw_only=True)
class MetaLearningUtils:
  """Meta learning utils class."""

  # The objective goal of 'meta' and 'tuned' algorithms.
  _goal: vz.ObjectiveMetricGoal
  # The metric name associated with the 'tuned' designer trials.
  _tuned_metric_name: str
  # The metric name associated with the 'meta' designer trials.
  _meta_metric_name: str
  # The 'tuned' algorithms params to be tuned (meta designer's search space).
  _tuning_params: vz.SearchSpace

  @property
  def meta_problem(self) -> vz.ProblemStatement:
    """Create meta problem."""
    problem = vz.ProblemStatement(search_space=self._tuning_params)
    problem.metric_information = vz.MetricsConfig(
        metrics=[vz.MetricInformation(self._meta_metric_name, goal=self._goal)]
    )
    return problem

  def complete_meta_suggestion(
      self,
      meta_suggestion: vz.TrialSuggestion,
      score: float,
  ) -> vz.Trial:
    """Complete meta trial suggestion and assign score."""
    meta_trial = meta_suggestion.to_trial()
    meta_trial.complete(vz.Measurement(metrics={self._meta_metric_name: score}))
    return meta_trial

  def get_default_hyperparameters(self) -> vz.TrialSuggestion:
    """Generate trial suggestion with default tuned designer parameter values."""
    suggestion = vz.TrialSuggestion()
    for param in self.meta_problem.search_space.parameters:
      if param.default_value is None:
        raise ValueError(
            "Hyper-param (%s) doesn't have default value!" % param.name
        )
      else:
        suggestion.parameters[param.name] = param.default_value
    return suggestion

  def get_best_meta_trial(self, trials: list[vz.Trial]) -> vz.Trial:
    """Return the best meta trial."""
    return self._get_best_trial(trials, self._meta_metric_name)

  def get_best_tuned_trial(self, trials: list[vz.Trial]) -> vz.Trial:
    """Return the best tuned trial."""
    return self._get_best_trial(trials, self._tuned_metric_name)

  def get_best_meta_trial_score(self, trials: list[vz.Trial]) -> float:
    """Return the best meta trial score."""
    return self._get_best_trial_score(trials, self._meta_metric_name)

  def get_best_tuned_trial_score(self, trials: list[vz.Trial]) -> float:
    """Return the best tuned trial score."""
    return self._get_best_trial_score(trials, self._tuned_metric_name)

  def _get_best_trial(
      self, trials: list[vz.Trial], metric_name: str
  ) -> vz.Trial:
    """Return the best trial with the metric specified by `trial_type`."""
    best_trial = trials[0]
    for trial in trials:
      if self._is_trial_better(trial, best_trial, metric_name):
        best_trial = trial
    return best_trial

  def _get_best_trial_score(
      self, trials: list[vz.Trial], metric_name: str
  ) -> float:
    """Return the best trial score."""
    best_trial = self._get_best_trial(trials, metric_name)
    if best_trial.final_measurement is None:
      raise ValueError("'final_measurement' shouldn't be None.")
    return best_trial.final_measurement.metrics[metric_name].value

  def _is_trial_better(
      self,
      trial1: vz.Trial,
      trial2: vz.Trial,
      metric_name: str,
  ) -> bool:
    """Returns whether trial1 is better than trial2 in terms of `metric_name`."""
    if metric_name not in trial1.final_measurement_or_die.metrics:
      raise ValueError(
          f"Metric name ({metric_name}) not found in trial1. {trial1}"
      )
    if metric_name not in trial2.final_measurement_or_die.metrics:
      raise ValueError(
          f"Metric name ({metric_name}) not found in trial2. {trial2}"
      )
    if trial1.infeasible:
      return False
    if trial2.infeasible:
      return True
    val1 = trial1.final_measurement_or_die.metrics[metric_name].value
    val2 = trial2.final_measurement_or_die.metrics[metric_name].value
    if self._goal == vz.ObjectiveMetricGoal.MAXIMIZE and val1 > val2:
      return True
    elif self._goal == vz.ObjectiveMetricGoal.MINIMIZE and val1 < val2:
      return True
    else:
      return False


--- vizier/_src/algorithms/designers/meta_learning/meta_learning_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for meta_learning_utils."""

import random
from vizier import pyvizier as vz
from vizier._src.algorithms.designers.meta_learning import meta_learning_utils
from absl.testing import absltest
from absl.testing import parameterized


class MetaLearningUtilsTest(parameterized.TestCase):

  def setUp(self):
    super().setUp()
    space = vz.SearchSpace()
    space.root.add_int_param('tuned_param', 0, 100, default_value=55)
    self.utils = meta_learning_utils.MetaLearningUtils(
        goal=vz.ObjectiveMetricGoal.MAXIMIZE,
        tuned_metric_name='tuned_obj',
        meta_metric_name='meta_obj',
        tuning_params=space,
    )
    self.meta_trials = []
    for i in range(10):
      meta_trial = vz.Trial({'meta_param': i})
      meta_trial.complete(vz.Measurement(metrics={'meta_obj': float(i)}))
      self.meta_trials.append(meta_trial)
    random.shuffle(self.meta_trials)

    self.tuned_trials = []
    for i in range(50):
      tuned_trial = vz.Trial({'tuned_param': i})
      tuned_trial.complete(vz.Measurement(metrics={'tuned_obj': float(i)}))
      self.tuned_trials.append(tuned_trial)
    random.shuffle(self.tuned_trials)

  def test_best_trial(self):
    # meta trial
    best_meta_trial = self.utils.get_best_meta_trial(self.meta_trials)
    self.assertEqual(best_meta_trial.parameters['meta_param'].value, 9)
    # tuned trial
    best_tuned_trial = self.utils.get_best_tuned_trial(self.tuned_trials)
    self.assertEqual(best_tuned_trial.parameters['tuned_param'].value, 49)

  def test_best_trial_score(self):
    # meta trial score
    best_meta_trial_score = self.utils.get_best_meta_trial_score(
        self.meta_trials
    )
    self.assertEqual(best_meta_trial_score, 9.0)
    # tuned trial score
    best_tuned_trial_score = self.utils.get_best_tuned_trial_score(
        self.tuned_trials
    )
    self.assertEqual(best_tuned_trial_score, 49.0)

  def test_generate_default_tuned_parameters(self):
    tuned_suggestion = self.utils.get_default_hyperparameters()
    self.assertEqual(tuned_suggestion.parameters['tuned_param'].value, 55)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/pycmaes.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""CMA-ES designer using pycma https://github.com/CMA-ES/pycma."""

from typing import Optional, Sequence

import cma
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.pyvizier import converters


class PyCMAESDesigner(vza.Designer):
  """CMA-ES designer wrapping pycma."""

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      sigma0: float = 0.1,
      popsize: int | None = None,
  ):
    """Init.

    Args:
      problem_statement: Must use a flat DOUBLE-only search space.
      sigma0: The initial standard deviation of the CMA-ES algorithm.
      popsize: The size of the population to use in each CMA-ES update. If None,
        the default popsize is used.
    """
    self._problem_statement = problem_statement
    self._metric_name = self._problem_statement.metric_information.item().name

    if popsize is not None and popsize < 2:
      raise ValueError(f'Popsize must be at least 2. Got {popsize}.')

    self._search_space = self._problem_statement.search_space
    if self._search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.')
    elif len(self._problem_statement.metric_information) != 1:
      raise ValueError(f'{type(self)} works with exactly one metric.')

    init_params = vz.ParameterDict()
    for parameter_config in self._search_space.parameters:
      if not parameter_config.type.is_continuous():
        raise ValueError(
            f'This designer {self} only supports continuous parameters.')
      if parameter_config.default_value is not None:
        init_params[parameter_config.name] = parameter_config.default_value
      elif parameter_config.bounds is not None:
        init_params[parameter_config.name] = (
            parameter_config.bounds[0] + parameter_config.bounds[1]
        ) / 2.0
      else:
        raise ValueError(
            f'The continuous parameter: {parameter_config.name} is missing'
            ' bounds.'
        )

    self._converter = converters.TrialToArrayConverter.from_study_config(
        self._problem_statement,
        scale=True,
        flip_sign_for_minimization_metrics=True,
    )
    self._x0 = self._converter.to_features([vz.TrialSuggestion(init_params)])[0]
    self._sigma0 = sigma0
    self._popsize = popsize
    self._all_completed_trials: list[vz.Trial] = []

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    self._all_completed_trials.extend(completed.trials)

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1
    num_completed_trials = len(self._all_completed_trials)
    # The trial converter scales the parameters to [0, 1] range.
    if self._popsize is None:
      options = {'bounds': [0.0, 1.0]}
    else:
      options = {'bounds': [0.0, 1.0], 'popsize': self._popsize}
    cma_evolution = cma.CMAEvolutionStrategy(
        self._x0,
        self._sigma0,
        options,
    )
    # Ensures that the number of completed trials fed to CMA-ES is a multiple
    # of the popsize as required.
    feed_size = int(
        (num_completed_trials // cma_evolution.popsize) * cma_evolution.popsize
    )
    if feed_size > 0:
      features, labels = self._converter.to_xy(
          self._all_completed_trials[-feed_size:]
      )
      # CMA-ES expects a minimization problem by default, but the converter
      # outputs maximization metrics, so we sign-flip the converted labels.
      cma_evolution.feed_for_resume(features, -labels)
    cma_suggestions = np.array(cma_evolution.ask(count))
    return [
        vz.TrialSuggestion(params)
        for params in self._converter.to_parameters(cma_suggestions)
    ]


--- vizier/_src/algorithms/designers/pycmaes_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier._src.algorithms.designers import pycmaes
from vizier._src.algorithms.testing import test_runners
from vizier.benchmarks import experimenters

from absl.testing import absltest
from absl.testing import parameterized


class PycmaesTest(parameterized.TestCase):

  def setUp(self):
    self.experimenter = experimenters.BBOBExperimenterFactory("Sphere", 2)()
    super().setUp()

  @parameterized.parameters(
      dict(batch_size=1),
      dict(batch_size=1, popsize=7),
      dict(batch_size=3),
      dict(batch_size=3, popsize=2),
      dict(batch_size=3, popsize=5),
  )
  def test_e2e(self, batch_size: int, popsize: int | None = None):
    designer = pycmaes.PyCMAESDesigner(
        self.experimenter.problem_statement(), popsize=popsize
    )

    trials = test_runners.run_with_random_metrics(
        designer,
        self.experimenter.problem_statement(),
        iters=10,
        batch_size=batch_size,
        verbose=1,
        validate_parameters=True,
    )
    self.assertLen(trials, batch_size * 10)

  def test_invalid_popsize(self):
    with self.assertRaisesRegex(ValueError, "Popsize must be at least 2"):
      pycmaes.PyCMAESDesigner(self.experimenter.problem_statement(), popsize=1)


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/designers/quasi_random.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Quasi-random designer."""

import collections
import math
import sys
import time
from typing import Optional, Sequence

import numpy as np
from scipy.stats import qmc
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.pyvizier import converters


class QuasiRandomDesigner(vza.PartiallySerializableDesigner):
  """Sample points using quasi-random Halton algorithm."""

  def __init__(
      self,
      search_space: vz.SearchSpace,
      *,
      skip_points: int = 1000,
      seed: Optional[int] = None,
  ):
    """Init.

    Args:
      search_space: Must be a flat search space.
      skip_points: If positive, then these first points in the sequence are
        discarded in order to avoid unwanted correlations.
      seed: Random seed.
    """
    if search_space.is_conditional:
      raise ValueError(
          f'This designer {self} does not support conditional search.'
      )

    def _create_input_converter(pc):
      return converters.DefaultModelInputConverter(
          pc,
          scale=True,
          max_discrete_indices=sys.maxsize,
          float_dtype=np.float64,
      )

    self._converter = converters.DefaultTrialConverter(
        [_create_input_converter(pc) for pc in search_space.parameters]
    )

    for spec in self._converter.output_specs.values():
      if spec.type not in [
          converters.NumpyArraySpecType.CONTINUOUS,
          converters.NumpyArraySpecType.DISCRETE,
      ]:
        raise ValueError(f'Unsupported type: {spec.type} in {spec}')
      if spec.num_dimensions != 1:
        raise ValueError(
            'Multi-dimensional discrete types are unsuppored. Received spec: %s'
            % spec
        )
    self._seed = seed if seed is not None else np.int32(time.time())
    self._halton = qmc.Halton(
        d=len(self._converter.output_specs),
        seed=self._seed,
    )
    self._halton.fast_forward(skip_points)
    self._skip_points = skip_points
    self._output_specs = tuple(self._converter.output_specs.values())

  @classmethod
  def from_problem(
      cls, problem: vz.ProblemStatement, seed: Optional[int] = None
  ):
    """For wrapping via `PartiallySerializableDesignerPolicy`."""
    return QuasiRandomDesigner(problem.search_space, seed=seed)

  def load(self, metadata: vz.Metadata) -> None:
    """Loads designer's state from metadata."""
    self._seed = int(metadata.ns('quasi_random')['seed'])
    self._halton = qmc.Halton(
        d=len(self._converter.output_specs),
        seed=int(metadata.ns('quasi_random')['seed']),
    )
    self._skip_points = int(metadata.ns('quasi_random')['skip_points'])
    # Skip forward to where the previous Halton sequence stopped.
    self._halton.fast_forward(self._skip_points)

  def dump(self) -> vz.Metadata:
    """Dumps the designer's state."""
    metadata = vz.Metadata()
    metadata.ns('quasi_random')['skip_points'] = str(self._skip_points)
    metadata.ns('quasi_random')['seed'] = str(self._seed)
    return metadata

  def _generate_discrete_point(
      self, spec: converters.NumpyArraySpec, halton_value: float
  ) -> int:
    """Generate a discrete parameter value from a Halton value."""
    # +1 because the bounds are inclusive on both ends.
    num_discrete_options = spec.bounds[1] - spec.bounds[0] + 1 - spec.num_oovs
    # Get a number in [0,  num_discrete_options].
    halton_value *= num_discrete_options
    # Get an integer between 0 and num_discrete_options-1 (inclusive).
    halton_value = int(math.floor(halton_value))
    return halton_value + int(spec.bounds[0])

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    pass

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    """Suggest new suggestions, taking into account `count`."""
    count = count or 1
    sample = collections.defaultdict(list)
    for _ in range(count):
      # Generate one random sample in [0.0, 1.0]^d (d was specified in the
      # constructor).
      halton_values = self._halton.random(n=1)[0]
      # Increment 'skip_points' to reflect designer's state.
      self._skip_points += 1
      for dimension_index, spec in enumerate(self._output_specs):
        halton_value = halton_values[dimension_index]
        if spec.type == converters.NumpyArraySpecType.CONTINUOUS:
          # Samples are in [0,1]; Relies on the converter to scale back to
          # original scale once parameters are created.
          sample[spec.name].append(np.float64(halton_value))
        elif spec.type == converters.NumpyArraySpecType.DISCRETE:
          # Converter expects an integer for discrete/categorical parameters.
          sample[spec.name].append(
              np.int64(self._generate_discrete_point(spec, halton_value))
          )
        else:
          # Only CONTINUOUS and DISCRETE are supported (doesn't support ONEHOT).
          raise ValueError(
              f'Unsupported spec type: {spec.type}. {self._converter} should be'
              ' configured to return CONTINUOUS or DISCRETE specs only.'
          )
    sample = {
        name: np.expand_dims(np.asarray(elements), axis=-1)
        for (name, elements) in sample.items()
    }
    # Convert the samples back to parameters in the original search space.
    return [
        vz.TrialSuggestion(p) for p in self._converter.to_parameters(sample)
    ]


--- vizier/_src/algorithms/designers/quasi_random_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import random

from scipy import stats
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.policies import designer_policy
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies

from absl.testing import absltest


class QuasiRandomTest(absltest.TestCase):

  def test_on_flat_space(self):
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    designer = quasi_random.QuasiRandomDesigner(problem.search_space)
    self.assertLen(
        test_runners.run_with_random_metrics(
            designer, problem, iters=50, batch_size=5, validate_parameters=True
        ),
        250,
    )

  def test_dump_and_load(self):
    # Check metadata checkpointing.
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    designer = quasi_random.QuasiRandomDesigner(problem.search_space)
    designer.suggest(random.randint(1, 50))
    metadata = designer.dump()
    designer2 = quasi_random.QuasiRandomDesigner(problem.search_space)
    designer2.load(metadata)
    self.assertEqual(designer.suggest(10), designer2.suggest(10))

  def test_distribution1d(self):
    # Make sure output distribution makes sense.
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('float', 0.0, 1.0)
    designer = quasi_random.QuasiRandomDesigner(problem.search_space)

    suggestions = designer.suggest(2000)
    float_points = [
        suggestion.parameters['float'].value for suggestion in suggestions
    ]

    # Test uniformity of end-to-end parameter values.
    # p_value greater than 0.9 roughly means we're very certain it's uniform.
    # Unfortunately KS-test doesn't work for discrete/categorical distributions.
    _, float_p_value = stats.kstest(
        float_points, stats.uniform(loc=0.0, scale=1.0).cdf
    )
    self.assertGreater(float_p_value, 0.9)

  def test_distribution4d(self):
    # Make sure output distribution makes sense.
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('x0', 0.0, 1.0)
    problem.search_space.root.add_float_param('x1', 0.0, 1.0)
    problem.search_space.root.add_float_param('x2', 0.0, 1.0)
    problem.search_space.root.add_float_param('x3', 0.0, 1.0)
    designer = quasi_random.QuasiRandomDesigner(problem.search_space)

    suggestions = designer.suggest(2000)
    for i_dim in ['x0', 'x1', 'x2', 'x3']:
      float_points = [
          suggestion.parameters[i_dim].value for suggestion in suggestions
      ]
      # Test that in every dimension, we cover the full range.
      self.assertGreater(max(float_points), 0.998)
      self.assertLess(min(float_points), 0.002)

  def test_equal_seeds(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('float', 0.0, 1.0)
    designer_1 = quasi_random.QuasiRandomDesigner(problem.search_space, seed=1)
    suggestions_1 = designer_1.suggest(10)
    designer_2 = quasi_random.QuasiRandomDesigner(problem.search_space, seed=1)
    suggestions_2 = designer_2.suggest(10)
    self.assertEqual(suggestions_1, suggestions_2)

  def test_distinct_seeds(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('float', 0.0, 1.0)
    designer_1 = quasi_random.QuasiRandomDesigner(problem.search_space, seed=0)
    suggestions_1 = designer_1.suggest(10)
    designer_2 = quasi_random.QuasiRandomDesigner(problem.search_space, seed=1)
    suggestions_2 = designer_2.suggest(10)
    self.assertNotEqual(suggestions_1, suggestions_2)

  def test_policy_wrapping(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('float', 0.0, 1.0)
    policy_supporter = pythia.InRamPolicySupporter(problem)
    policy = designer_policy.PartiallySerializableDesignerPolicy(
        problem,
        policy_supporter,
        quasi_random.QuasiRandomDesigner.from_problem,
    )

    # Make sure outputs are distinct.
    all_suggestions = []
    for _ in range(1000):
      request = pythia.SuggestRequest(
          study_descriptor=policy_supporter.study_descriptor(), count=1
      )
      decisions = policy.suggest(request)
      all_suggestions.extend(decisions.suggestions)

    distinct_suggestions = set(
        [
            tuple(suggestion.parameters.as_dict().values())
            for suggestion in all_suggestions
        ]
    )
    self.assertLen(distinct_suggestions, 1000)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/random.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Random designer."""

from typing import Any, Optional, Sequence

import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.pyvizier import converters


class RandomDesigner(vza.Designer):
  """Random designer.

  This designer uniformly samples from the scaled search space.
  """

  def __init__(self,
               search_space: vz.SearchSpace,
               *,
               dtype: Any = np.float64,
               seed: Optional[int] = None):
    """Init.

    Args:
      search_space: Must be a flat search space.
      dtype:
      seed: Any valid seed for np.random.RandomState.
    """
    if search_space.is_conditional:
      # TODO: Add conditional sampling case.
      raise ValueError(
          f'This designer {self} does not support conditional search.')

    def create_input_converter(pc):
      # Setting 'max_discrete_indices' to not continuify DISCRETE parameters.
      return converters.DefaultModelInputConverter(
          pc, scale=True, max_discrete_indices=np.inf, float_dtype=dtype
      )

    self._converter = converters.DefaultTrialConverter(
        [create_input_converter(pc) for pc in search_space.parameters])

    self._rng = np.random.RandomState(seed)

  @classmethod
  def from_problem(
      cls, problem: vz.ProblemStatement, seed: Optional[int] = None
  ):
    """For wrapping via `PartiallySerializableDesignerPolicy`."""
    return RandomDesigner(problem.search_space, seed=seed)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    pass

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    count = count or 1
    sample = dict()
    for name, spec in self._converter.output_specs.items():
      if spec.type == converters.NumpyArraySpecType.DISCRETE:
        sample[name] = self._rng.randint(
            spec.bounds[0], spec.bounds[1] - spec.num_oovs + 1, size=[count, 1]
        )
      elif spec.type == converters.NumpyArraySpecType.CONTINUOUS:
        # Trial-Numpy converter was configured to scale values to [0, 1].
        # Sample uniformly in that range and convert back to Trials.
        sample[name] = self._rng.random([count, 1])
      else:
        raise ValueError(f'Unsupported type: {spec.type} in {spec}')
    return [
        vz.TrialSuggestion(p) for p in self._converter.to_parameters(sample)
    ]


--- vizier/_src/algorithms/designers/random_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for random."""

import collections
import math

from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies

from absl.testing import absltest


class RandomTest(absltest.TestCase):

  def test_on_flat_space(self):
    config = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    designer = random.RandomDesigner(config.search_space, seed=None)
    self.assertLen(
        test_runners.run_with_random_metrics(
            designer, config, iters=50, batch_size=1), 50)

  def test_reproducible_random(self):
    config = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    designer = random.RandomDesigner(config.search_space, seed=5)
    t1 = designer.suggest(10)

    designer = random.RandomDesigner(config.search_space, seed=5)
    t2 = designer.suggest(10)
    self.assertEqual(t1, t2)

  def test_log_scaling(self):
    """Confirm that LINEAR and LOG scaling give the right distribution."""
    config = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    designer = random.RandomDesigner(config.search_space, seed=None)
    trials = test_runners.run_with_random_metrics(
        designer, config, iters=1000, batch_size=1
    )
    # LINEAR scaling.
    sum_lineardouble = 0.0
    for t in trials:
      sum_lineardouble += t.parameters['lineardouble'].value
    avg_lineardouble = sum_lineardouble / len(trials)
    # Delta=0.16 corresponds to ~6 standard deviations, so the probability of
    # accidental test failure is ~1e-6.
    self.assertAlmostEqual(avg_lineardouble, 0.5 * (-1 + 2), delta=0.16)
    # LOG scaling
    sum_log_logdouble = 0.0
    sum_logdouble = 0.0
    for t in trials:
      sum_logdouble += t.parameters['logdouble'].value
      sum_log_logdouble += math.log(t.parameters['logdouble'].value)
    avg_logdouble = sum_logdouble / len(trials)
    avg_log_logdouble = sum_log_logdouble / len(trials)
    # If the distribution were LINEAR, we'd expect avg_logdouble = 50;
    # if the distribution were LOG, we'd expect avg_logdouble = 7.2, so
    # we check that avg_logdouble is closer to the LOG expected value.
    self.assertLess(abs(avg_logdouble - 7.2), abs(avg_logdouble - 50))
    # The average of log(value) should be close to the average of
    # the logs of the endpoints.  $delta is set at 6 standard deviations to
    # yield a false failure rate of about 1e-6.
    self.assertAlmostEqual(
        avg_log_logdouble, 0.5 * (math.log(1e-4) + math.log(1e2)), delta=0.76
    )

  def test_discrete_parameter(self):
    """Confirm that DISCRETE parameters are sampled uniformally."""
    n_samples = 10_000
    search_space = vz.SearchSpace()
    search_space.root.add_discrete_param(
        'discrete1', feasible_values=[0, 70, 71, 72, 100]
    )
    search_space.root.add_discrete_param('discrete2', feasible_values=[0, 8, 9])
    random_designer = random.RandomDesigner(search_space, seed=1)
    values = [
        (sg.parameters['discrete1'].value, sg.parameters['discrete2'].value)
        for sg in random_designer.suggest(n_samples)
    ]
    counts = collections.defaultdict(int)
    for value in values:
      counts[str(value[0]) + str(value[1])] += 1
    for count in counts.values():
      # Check that there's no more than 1% deviation.
      self.assertLess(abs(count / n_samples - 1 / 15), 0.01)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/scalarization.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Scalarizations that weights multiple metrics into a scalar."""

import abc
from typing import Callable, Optional

import equinox as eqx
import jax
from jax import numpy as jnp
import jaxtyping as jt
import typeguard


def _broadcast_multiply(
    weights: jt.Float[jax.Array, '*Num #Obj'],
    objs: jt.Float[jax.Array, '*Batch #Obj'],
) -> jt.Float[jax.Array, '*NumBatch #Obj']:
  # [*Num, #Obj] -> [*Num, 1, ..., 1, #Obj]
  broadcasted_weights = jnp.expand_dims(
      weights, axis=range(-2, -1 - len(objs.shape), -1)
  )
  return broadcasted_weights * objs


class Scalarization(abc.ABC, eqx.Module):
  """Reduces an array of objectives to a single float.

  Assumes all objectives are for MAXIMIZATION.
  """

  @abc.abstractmethod
  def __call__(
      self, objectives: jt.Float[jax.Array, '*Batch Obj']
  ) -> jt.Float[jax.Array, '*NumBatch']:
    """Computes the scalarization."""


# Scalarization factory from weights.
ScalarizationFromWeights = Callable[
    [jt.Float[jax.Array, '*Num #Obj']], Scalarization
]


class LinearScalarization(Scalarization):
  """Linear Scalarization."""

  weights: jt.Float[jax.Array, '*Num #Obj'] = eqx.field(converter=jnp.asarray)

  @jt.jaxtyped(typechecker=typeguard.typechecked)
  def __call__(
      self, objectives: jt.Float[jax.Array, '*Batch Obj']
  ) -> jt.Float[jax.Array, '*NumBatch']:
    product = _broadcast_multiply(self.weights, objectives)
    return jnp.sum(product, axis=-1)


class ChebyshevScalarization(Scalarization):
  """Chebyshev Scalarization."""

  weights: jt.Float[jax.Array, '*Num #Obj'] = eqx.field(converter=jnp.asarray)

  @jt.jaxtyped(typechecker=typeguard.typechecked)
  def __call__(
      self, objectives: jt.Float[jax.Array, '*Batch Obj']
  ) -> jt.Float[jax.Array, '*NumBatch']:
    product = _broadcast_multiply(self.weights, objectives)
    return jnp.min(product, axis=-1)


class HyperVolumeScalarization(Scalarization):
  """HyperVolume Scalarization."""

  weights: jt.Float[jax.Array, '*Num #Obj'] = eqx.field(converter=jnp.asarray)
  reference_point: Optional[jt.Float[jax.Array, '* #Obj']] = eqx.field(
      default=None
  )
  enforce_nonnegativity: bool = eqx.field(static=True, default=True)

  @jt.jaxtyped(typechecker=typeguard.typechecked)
  def __call__(
      self, objectives: jt.Float[jax.Array, '*Batch Obj']
  ) -> jt.Float[jax.Array, '*NumBatch']:
    # Uses scalarizations in https://arxiv.org/abs/2006.04655 for
    # non-convex multiobjective optimization. Removes the non-negativity
    # constraint as objectives shifted by reference point should be
    # non-negative.
    if self.reference_point is not None:
      objectives = objectives - self.reference_point

    if self.enforce_nonnegativity:
      # Sometimes shifted objectives are still negative. This enforces the
      # non-negativity constraint.
      objectives = jnp.maximum(objectives, 0.0)

    product = _broadcast_multiply(1.0 / self.weights, objectives)
    return jnp.pow(jnp.min(product, axis=-1), objectives.shape[-1])


class LinearAugmentedScalarization(Scalarization):
  """Scalarization augmented with a linear sum.

  See https://arxiv.org/pdf/1904.05760.pdf.
  """

  weights: jt.Float[jax.Array, '*Num #Obj'] = eqx.field(converter=jnp.asarray)

  scalarization_factory: ScalarizationFromWeights = eqx.field(static=True)
  augment_weight: jt.Float[jax.Array, ''] = eqx.field(
      default=1.0, converter=jnp.asarray
  )

  @jt.jaxtyped(typechecker=typeguard.typechecked)
  def __call__(
      self, objectives: jt.Float[jax.Array, '*Batch Obj']
  ) -> jt.Float[jax.Array, '*NumBatch']:
    return self.scalarization_factory(self.weights)(
        objectives
    ) + self.augment_weight * LinearScalarization(weights=self.weights)(
        objectives
    )


--- vizier/_src/algorithms/designers/scalarization_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from jax import numpy as jnp
from vizier._src.algorithms.designers import scalarization
from absl.testing import absltest


class ScalarizationsTest(absltest.TestCase):

  def test_linear_scalarizer(self):
    scalarizer = scalarization.LinearScalarization(
        weights=jnp.array([0.1, 0.2])
    )
    self.assertAlmostEqual(scalarizer(jnp.array([3.0, 4.5])), 1.2)

  def test_chebyshev_scalarizer(self):
    scalarizer = scalarization.ChebyshevScalarization(
        weights=jnp.array([0.1, 0.2])
    )
    self.assertAlmostEqual(scalarizer(jnp.array([3.0, 4.5])), 0.3)

  def test_hypervolume_scalarizer(self):
    scalarizer = scalarization.HyperVolumeScalarization(
        weights=jnp.array([0.1, 0.2])
    )
    self.assertAlmostEqual(scalarizer(jnp.array([3.0, 4.5])), 506.25)

  def test_hypervolume_scalarizer_with_reference(self):
    scalarizer = scalarization.HyperVolumeScalarization(
        weights=jnp.array([0.1, 0.2]), reference_point=jnp.array([-1])
    )
    self.assertAlmostEqual(scalarizer(jnp.array([3.0, 4.5])), 756.25)

  def test_augmented_scalarizer(self):
    scalarizer = scalarization.LinearAugmentedScalarization(
        weights=jnp.array([0.1, 0.2]),
        scalarization_factory=scalarization.HyperVolumeScalarization,
    )
    # Should be the sum of hypervolume and linear scalarizations.
    self.assertAlmostEqual(scalarizer(jnp.array([3.0, 4.5])), 507.45)

if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/designers/scalarizing_designer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Scalarizing Designer for MOO via reducing to a scalarized objective."""
import copy
import random
from typing import Optional, Sequence

from jax import numpy as jnp
from jax import random as jax_random
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.ensemble import ensemble_designer


class ScalarizingDesigner(vza.Designer):
  """Multiobjective Scalarizing Designer.

  This designer applies multiobjective optimization by reducing to
  single objective optimization via scalarization.
  """

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      designer_factory: vza.DesignerFactory[vza.Designer],
      scalarizer: scalarization.Scalarization,
      *,
      seed: Optional[int] = None,
  ):
    """Init.

    Args:
      problem_statement: Must be a mulitobjective search space.
      designer_factory: Factory to create the single-objective designer.
      scalarizer: Scalarization to be applied to objective metrics.
      seed: Any valid seed for factory.
    """
    self._scalarizer = scalarizer
    self._objectives = problem_statement.metric_information.of_type(
        vz.MetricType.OBJECTIVE
    )
    if len(self._objectives) <= 1:
      raise ValueError(f'Problem should be multi-objective {self._objectives}')

    # Create a single-objective designer.
    self._scalarized_metric_name = 'scalarized'
    single_objective_metric = problem_statement.metric_information.exclude_type(
        vz.MetricType.OBJECTIVE
    )
    single_objective_metric.append(
        vz.MetricInformation(
            name=self._scalarized_metric_name,
            goal=vz.ObjectiveMetricGoal.MAXIMIZE,
        )
    )
    self._problem_statement = copy.deepcopy(problem_statement)
    self._problem_statement.metric_information = single_objective_metric
    self._designer = designer_factory(self._problem_statement, seed=seed)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    for trial in completed.trials:
      objectives = [
          trial.final_measurement_or_die.metrics.get_value(
              config.name, default=jnp.nan
          )
          for config in self._objectives
      ]
      # Simply append the scalarized value.
      trial.final_measurement_or_die.metrics[self._scalarized_metric_name] = (
          self._scalarizer(jnp.array(objectives))
      )

    self._designer.update(completed, all_active)

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    return self._designer.suggest(count)


def create_gaussian_scalarizing_designer(
    problem_statement: vz.ProblemStatement,
    designer_factory: vza.DesignerFactory[vza.Designer],
    scalarization_factory: scalarization.ScalarizationFromWeights,
    num_ensemble: int,
    *,
    seed: Optional[int] = None,
) -> vza.Designer:
  """Factory for an Ensemble of Gaussian weighted scalarized Designers."""
  objectives = problem_statement.metric_information.of_type(
      vz.MetricType.OBJECTIVE
  )
  if len(objectives) <= 1:
    raise ValueError(
        'Problem should be multi-objective for applying '
        f'scalarization ensembling {objectives}'
    )

  key = jax_random.PRNGKey(seed or random.getrandbits(32))
  weights = abs(
      jax_random.normal(key=key, shape=(num_ensemble, len(objectives)))
  )
  weights /= jnp.linalg.norm(weights, axis=1)[..., jnp.newaxis]
  ensemble_dict = {}
  for weight in weights:
    ensemble_dict[f'scalarized_weight: {weight}'] = ScalarizingDesigner(
        problem_statement=problem_statement,
        designer_factory=designer_factory,
        scalarizer=scalarization_factory(weight),
        seed=seed,
    )
  return ensemble_designer.EnsembleDesigner(ensemble_dict)


--- vizier/_src/algorithms/designers/scalarizing_designer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from jax import numpy as jnp
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.designers import scalarizing_designer
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies

from absl.testing import absltest


class ScalarizingDesignerTest(absltest.TestCase):

  def test_scalarizing_eagle(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.extend([
        vz.MetricInformation(
            name='metric1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='metric2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
    ])

    def eagle_designer_factory(ps, seed):
      return eagle_strategy.EagleStrategyDesigner(
          problem_statement=ps, seed=seed
      )

    scalarized_designer = scalarizing_designer.ScalarizingDesigner(
        problem,
        eagle_designer_factory,
        scalarizer=scalarization.HyperVolumeScalarization(
            weights=jnp.ones(len(problem.metric_information))
        ),
    )
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=3,
            batch_size=5,
            verbose=1,
            validate_parameters=True,
        ).run_designer(scalarized_designer),
        15,
    )

  def test_ensemble_scalarizing_eagle(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.extend([
        vz.MetricInformation(
            name='metric1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='metric2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
    ])

    def eagle_designer_factory(ps, seed):
      return eagle_strategy.EagleStrategyDesigner(
          problem_statement=ps, seed=seed
      )

    scalarized_designer = (
        scalarizing_designer.create_gaussian_scalarizing_designer(
            problem,
            eagle_designer_factory,
            scalarization.HyperVolumeScalarization,
            num_ensemble=10,
        )
    )
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=3,
            batch_size=5,
            verbose=1,
            validate_parameters=True,
        ).run_designer(scalarized_designer),
        15,
    )

  def test_missing_metrics(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.extend([
        vz.MetricInformation(
            name='metric1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='metric2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
    ])

    def eagle_designer_factory(ps, seed):
      return eagle_strategy.EagleStrategyDesigner(
          problem_statement=ps, seed=seed
      )

    scalarized_designer = scalarizing_designer.ScalarizingDesigner(
        problem,
        eagle_designer_factory,
        scalarizer=scalarization.HyperVolumeScalarization(
            weights=jnp.ones(len(problem.metric_information))
        ),
    )

    suggestions = scalarized_designer.suggest(1)
    trial = list(suggestions)[0].to_trial(1)
    trial.complete(vz.Measurement(metrics={'metric1': 0.4}))
    scalarized_designer.update(
        vza.CompletedTrials(trials=[trial]),
        all_active=vza.ActiveTrials(trials=[]),
    )
    self.assertTrue(
        jnp.isnan(trial.final_measurement_or_die.metrics['scalarized'].value)
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/designers/scheduled_designer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Scheduled designer for budget-aware optimization.
"""

import abc
import logging
from typing import Protocol, Sequence
import attrs
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.pyvizier.shared import common
from vizier.interfaces import serializable

ParamsValues = dict[str, float]


@attrs.define
class ScheduledParam(abc.ABC):
  """Represents a scheduled parameter.

  The scheduling starts from `init_value` (the first step) and ends at
  `final_value` (last step).

  The `value` method returns the value of the scheduled param in the current
  step, accounting for the total number of steps in the scheduling process.

  Note that some types of scheduling (e.g. random, cyclic) are not supported.
  """

  init_value: float
  final_value: float

  def __attrs_post_init__(self):
    self._validate()

  def _validate(self) -> None:
    """Validate the scheduling param initial and final values."""
    total_steps = 10  # arbitrarily choose the total steps for validation.
    tol = 1e-5

    if abs(self.value(total_steps, 0) - self.init_value) > tol:
      raise ValueError("Initial value mismatch (step 0).")

    if abs(self.value(total_steps, total_steps - 1) - self.final_value) > tol:
      raise ValueError("Final value mismatch (last step).")

  @abc.abstractmethod
  def value(self, total_steps: int, step: int) -> float:
    """Compute the scheduling param value in the current step.

    Args:
      total_steps: The total number of steps for which the scheduling applied to
        (e.g. total number of study trials).
      step: The current step in the scheduling process (e.g. current study
        trial).
    """


@attrs.define
class LinearScheduledParam(ScheduledParam):
  """Linear scheduler."""

  def value(self, total_steps: int, step: int) -> float:
    slope = (self.final_value - self.init_value) / (total_steps - 1)
    return self.init_value + slope * step


@attrs.define
class ExponentialScheduledParam(ScheduledParam):
  """Exponential scheduler."""

  rate: float

  def value(self, total_steps: int, step: int) -> float:
    alpha = 1 / self.rate
    beta = -np.log(self.final_value / self.init_value) / (
        (total_steps - 1) ** alpha
    )
    return self.init_value * np.exp(-beta * step**alpha)


class DesignerStateUpdater(Protocol):
  """Update designer state based on params.

  Allows to efficiently modify the designer state without re-instantiating it.
  """

  def __call__(self, designer: vza.Designer, params: ParamsValues) -> None:
    """Update the designer state (inplace) based on parameter values.

    Note: The `params` dictionary does not need to directly correspond to
      `designer` attributes. The protocol implementations have the flexibility
      to determine how to use the `params`.

    Args:
      designer: The designer to update its state inplace.
      params: A dictionary of params to be used to update the designer state.
    """
    pass


@attrs.define
class ScheduledDesigner(vza.PartiallySerializableDesigner):
  """Scheduled designer."""

  _problem: vz.ProblemStatement = attrs.field(kw_only=False)
  _designer_factory: vza.DesignerFactory = attrs.field(kw_only=True)
  _designer_state_updater: DesignerStateUpdater = attrs.field(kw_only=True)
  _scheduled_params: dict[str, ScheduledParam] = attrs.field(kw_only=True)
  # The total number of study trials the designer is expected to generate. This
  # is used to determine the scheduling rate of change across the study.
  _expected_total_num_trials: int = attrs.field(
      kw_only=True, validator=attrs.validators.ge(0)
  )
  # ------------------------------------------------------------------
  # Internal attributes which should not be set by callers.
  # ------------------------------------------------------------------
  _designer: vza.Designer = attrs.field(init=False)
  _num_incorporated_suggested_trials: int = attrs.field(init=False, default=0)
  _metadata_ns: str = attrs.field(default="scheduled_designer", init=False)

  def __attrs_post_init__(self):
    self._designer = self._designer_factory(self._problem)
    self._update_designer_state()

  def dump(self) -> vz.Metadata:
    """Dumps the current state of the designer.

    Note that the state only contains the number of suggested trials. The other
    attributes are provided during designer instantiation.

    Returns:
      Metadata with the current designer's state.
    """
    metadata = vz.Metadata()
    metadata.ns(self._metadata_ns)["suggested_num_trials"] = str(
        self._num_incorporated_suggested_trials
    )
    return metadata

  def load(self, metadata: common.Metadata) -> None:
    """Loads the designer state from the metadata."""
    if (
        metadata.ns(self._metadata_ns).get("suggested_num_trials", default=None)
        is None
    ):
      # When the designer is called for the first time, or if the algorithm has
      # changed in the middle of the study.
      raise serializable.FatalDecodeError(
          "The metadata doesn't contain a state to be recovered."
      )
    try:
      self._num_incorporated_suggested_trials = int(
          metadata.ns(self._metadata_ns)["suggested_num_trials"]
      )
    except Exception as e:
      raise serializable.FatalDecodeError(
          "Couldn't load state ('suggested_num_trials') from metadata."
      ) from e

  @property
  def designer(self) -> vza.Designer:
    return self._designer

  @property
  def scheduled_params(self) -> dict[str, ScheduledParam]:
    return self._scheduled_params

  @property
  def num_incorporated_suggested_trials(self) -> int:
    """Returns the total number of suggested trials the designer generated (regardless of their current status)."""
    return self._num_incorporated_suggested_trials

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Update the underlying designer based on completed and pending trials."""
    self._designer.update(completed, all_active)
    self._validate_num_incorporated_suggested_trials(completed, all_active)

  def _validate_num_incorporated_suggested_trials(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Validate (and if needed update) the designer's suggested number of trials."""
    num_completed = len(completed.trials)
    num_active = len(all_active.trials)
    if self._num_incorporated_suggested_trials < num_completed + num_active:
      # If the designer is updated with more trials than what it observed, we
      # fast forward and update the state. Would happen if the designer is
      # updated with trials it didn't generate.
      logging.warning(
          "The scheduled designer's `num_incorporated_suggested_trials` (%s)"
          " doesn't match the actual number of trials in the study (%s).",
          self._num_incorporated_suggested_trials,
          num_completed + num_active,
      )
      self._num_incorporated_suggested_trials = num_completed + num_active

  def _update_designer_state(self) -> ParamsValues:
    """Efficiently update the underlying designer state (inplace).

    Returns:
      The current scheduled parameters values, which is used for logging.
    """
    params_values = {}
    for name, scheduled_param in self._scheduled_params.items():
      params_values[name] = scheduled_param.value(
          total_steps=self._expected_total_num_trials,
          step=self._num_incorporated_suggested_trials,
      )
    self._designer_state_updater(self._designer, params_values)
    logging.info("Updated designer state with params: %s", params_values)
    return params_values

  def suggest(self, count: int = 1) -> Sequence[vz.TrialSuggestion]:
    """Suggest trials."""
    # Compute the scheduled param values and update the designer state.
    params_values = self._update_designer_state()
    # Suggest 'count' trials in batch, using the same scheduled params values.
    suggest_trials = self._designer.suggest(count)
    self._num_incorporated_suggested_trials += len(suggest_trials)
    # Log the parameter values in the suggested trials for debugging.
    for trial in suggest_trials:
      metadata = trial.metadata.ns(self._metadata_ns).ns("devinfo")
      for name in self._scheduled_params.keys():
        metadata[name] = str(params_values[name])
    # Check that the maximum number of trials hasn't surpassed.
    if (
        self._num_incorporated_suggested_trials
        >= self._expected_total_num_trials
    ):
      logging.info(
          "Suggested trial count (%s) exceeded the configured maximum (%s).",
          self._num_incorporated_suggested_trials,
          self._expected_total_num_trials,
      )
    return suggest_trials


--- vizier/_src/algorithms/designers/scheduled_designer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for scheduled designers."""

from typing import Sequence

import attrs
import jax
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import gp_bandit
from vizier._src.algorithms.designers import gp_ucb_pe
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.designers import scheduled_designer
from vizier._src.algorithms.designers import scheduled_gp_bandit
from vizier._src.algorithms.designers import scheduled_gp_ucb_pe
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies

from absl.testing import absltest


@attrs.define
class MockParameterizedDesigner(vza.Designer):
  """Mock parameterized designer."""

  problem: vz.ProblemStatement
  parameter1: float = 0.0
  parameter2: float = 0.0

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    """Update the designer based on completed and pending trials."""
    del completed
    del all_active

  def suggest(self, count: int = 1) -> Sequence[vz.TrialSuggestion]:
    """Suggest trials."""
    return random.RandomDesigner(self.problem.search_space).suggest(count)


class DirectDesignerStateUpdater(scheduled_designer.DesignerStateUpdater):
  """Direct designer state updater."""

  def __call__(
      self, designer: vza.Designer, params: scheduled_designer.ParamsValues
  ) -> None:
    """Update the designer state assuming the params are class attributes."""
    for param_name, param_value in params.items():
      setattr(designer, param_name, param_value)


class ScheduledDesignerTest(absltest.TestCase):

  def test_schedule_designer(self):
    expected_total_num_trials = 10
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name="metric", goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    param1 = scheduled_designer.LinearScheduledParam(
        init_value=10.5, final_value=2.1
    )
    param2 = scheduled_designer.ExponentialScheduledParam(
        init_value=1.5, final_value=20.1, rate=1.7
    )
    mock_scheduled_designer = scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=MockParameterizedDesigner,
        designer_state_updater=DirectDesignerStateUpdater(),
        scheduled_params={"parameter1": param1, "parameter2": param2},
        expected_total_num_trials=expected_total_num_trials,
    )
    # Check initial values.
    self.assertEqual(mock_scheduled_designer.designer.parameter1, 10.5)  # pytype: disable=attribute-error
    self.assertEqual(mock_scheduled_designer.designer.parameter2, 1.5)  # pytype: disable=attribute-error
    # Check suggestions.
    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=expected_total_num_trials,
            batch_size=1,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(mock_scheduled_designer),
        expected_total_num_trials,
    )
    # Check final values.
    self.assertAlmostEqual(mock_scheduled_designer.designer.parameter1, 2.1)  # pytype: disable=attribute-error
    self.assertAlmostEqual(mock_scheduled_designer.designer.parameter2, 20.1)  # pytype: disable=attribute-error

  def test_validate_suggested_num_trials(self):
    # Test that updating the designer with trials changes the state.
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name="metric", goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    param1 = scheduled_designer.LinearScheduledParam(
        init_value=10.5, final_value=2.1
    )
    param2 = scheduled_designer.ExponentialScheduledParam(
        init_value=1.5, final_value=20.1, rate=1.7
    )
    mock_scheduled_designer = scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=MockParameterizedDesigner,
        designer_state_updater=DirectDesignerStateUpdater(),
        scheduled_params={"parameter1": param1, "parameter2": param2},
        expected_total_num_trials=10,
    )
    # Generate active and completed trials.
    active_trials = test_studies.flat_continuous_space_with_scaling_trials(2)
    completed_trials = [
        s.to_trial()
        for s in test_studies.flat_continuous_space_with_scaling_trials(4)
    ]
    for trial in completed_trials:
      trial.complete(vz.Measurement({"metric": 1.2}), inplace=True)
    # Update the scheduled designer.
    mock_scheduled_designer.update(
        vza.CompletedTrials(completed_trials), vza.ActiveTrials(active_trials)
    )
    # Validate that the state was updated.
    self.assertEqual(
        mock_scheduled_designer.num_incorporated_suggested_trials, 4 + 2
    )

  def test_scheduled_designer_serialization(self):
    expected_total_num_trials = 10
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name="metric", goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    param1 = scheduled_designer.LinearScheduledParam(
        init_value=10.5, final_value=2.1
    )
    param2 = scheduled_designer.ExponentialScheduledParam(
        init_value=1.5, final_value=20.1, rate=1.7
    )
    mock_scheduled_designer = scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=MockParameterizedDesigner,
        designer_state_updater=DirectDesignerStateUpdater(),
        scheduled_params={"parameter1": param1, "parameter2": param2},
        expected_total_num_trials=expected_total_num_trials,
    )
    # Making several suggestions so the state would change.
    mock_scheduled_designer.suggest(count=1)
    mock_scheduled_designer.suggest(count=3)
    # Store the state in metadata.
    state = mock_scheduled_designer.dump()
    # Create a new designer and load state.
    new_mock_scheduled_designer = scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=MockParameterizedDesigner,
        designer_state_updater=DirectDesignerStateUpdater(),
        scheduled_params={"parameter1": param1, "parameter2": param2},
        expected_total_num_trials=expected_total_num_trials,
    )
    new_mock_scheduled_designer.load(state)
    self.assertEqual(
        new_mock_scheduled_designer._num_incorporated_suggested_trials, 4
    )


class ScheduledGpBanditTest(absltest.TestCase):

  def test_scheduled_gp_bandit(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name="metric", goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    def _gp_bandit_factory(problem):
      return gp_bandit.VizierGPBandit(problem)

    scheduled_desinger = scheduled_gp_bandit.ScheduledGPBanditFactory(
        gp_bandit_factory=_gp_bandit_factory,
        expected_total_num_trials=2,
        init_ucb_coefficient=4.0,
        final_ucb_coefficient=1.0,
        decay_ucb_coefficient=1.2,
    )(problem)

    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=2,
            batch_size=1,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(scheduled_desinger),
        2,
    )


class ScheduledGpUcbPeTest(absltest.TestCase):

  def test_scheduled_gp_ucb_pe(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.append(
        vz.MetricInformation(
            name="metric", goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    def _gp_ucb_pe_factory(
        problem: vz.ProblemStatement,
    ) -> gp_ucb_pe.VizierGPUCBPEBandit:
      return gp_ucb_pe.VizierGPUCBPEBandit(problem)

    scheduled_desinger = scheduled_gp_ucb_pe.ScheduledGPUCBPEFactory(
        gp_ucb_pe_factory=_gp_ucb_pe_factory,
        expected_total_num_trials=10,
        init_ucb_coefficient=4.0,
        final_ucb_coefficient=1.0,
        decay_ucb_coefficient=1.2,
        init_explore_region_ucb_coefficient=1.0,
        final_explore_region_ucb_coefficient=0.5,
        decay_explore_region_ucb_coefficient=1.2,
        init_ucb_overwrite_probability=0.25,
        final_ucb_overwrite_probability=0.0,
        decay_ucb_overwrite_probability=1.0,
    )(problem)

    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=10,
            batch_size=1,
            verbose=1,
            validate_parameters=True,
            seed=1,
        ).run_designer(scheduled_desinger),
        10,
    )


if __name__ == "__main__":
  jax.config.update("jax_enable_x64", True)
  absltest.main()


--- vizier/_src/algorithms/designers/scheduled_gp_bandit.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Scheduled GP-Bandit for budget-aware optimization.
"""

from typing import Callable
import attrs
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import gp_bandit
from vizier._src.algorithms.designers import scheduled_designer
from vizier._src.algorithms.designers.gp import acquisitions


@attrs.define(kw_only=True)
class ScheduledGPBanditFactory:
  """Scheduled GP-Bandit factory."""

  _gp_bandit_factory: Callable[[vz.ProblemStatement], gp_bandit.VizierGPBandit]
  _init_ucb_coefficient: float
  _final_ucb_coefficient: float
  _decay_ucb_coefficient: float
  _expected_total_num_trials: int

  def __call__(
      self,
      problem: vz.ProblemStatement,
  ) -> scheduled_designer.ScheduledDesigner:
    """Creates a scheduled GP-Bandit designer."""

    def _gp_bandit_state_updater(designer, params):
      designer._scoring_function_factory = (  # pylint: disable=protected-access
          acquisitions.bayesian_scoring_function_factory(
              lambda _: acquisitions.UCB(params['ucb_coefficient'])
          )
      )

    ucb_coef_param = scheduled_designer.ExponentialScheduledParam(
        init_value=self._init_ucb_coefficient,
        final_value=self._final_ucb_coefficient,
        rate=self._decay_ucb_coefficient,
    )

    return scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=self._gp_bandit_factory,
        designer_state_updater=_gp_bandit_state_updater,
        scheduled_params={'ucb_coefficient': ucb_coef_param},
        expected_total_num_trials=self._expected_total_num_trials,
    )


--- vizier/_src/algorithms/designers/scheduled_gp_ucb_pe.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Scheduled GP-UCB-PE for budget-aware optimization.
"""

import dataclasses as dc
from typing import Callable
import attrs
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import gp_ucb_pe
from vizier._src.algorithms.designers import scheduled_designer


@attrs.define(kw_only=True)
class ScheduledGPUCBPEFactory:
  """Scheduled GP-UCB-PE factory."""

  _gp_ucb_pe_factory: Callable[
      [vz.ProblemStatement], gp_ucb_pe.VizierGPUCBPEBandit
  ]
  _expected_total_num_trials: int
  # ucb_coefficient
  _init_ucb_coefficient: float
  _final_ucb_coefficient: float
  _decay_ucb_coefficient: float

  # explore_region_ucb_coefficient
  _init_explore_region_ucb_coefficient: float
  _final_explore_region_ucb_coefficient: float
  _decay_explore_region_ucb_coefficient: float

  # ucb_overwrite_probability
  _init_ucb_overwrite_probability: float
  _final_ucb_overwrite_probability: float
  _decay_ucb_overwrite_probability: float

  def __call__(
      self,
      problem: vz.ProblemStatement,
  ) -> scheduled_designer.ScheduledDesigner:
    """Creates a scheduled GP-Bandit designer."""

    def _gp_ucb_pe_state_updater(designer, params):
      # Create a new copy of the config and replace the ucb coefficient value.
      updated_config = dc.replace(
          designer._config,  # pylint: disable=protected-access
          ucb_coefficient=params['ucb_coefficient'],
          explore_region_ucb_coefficient=params[
              'explore_region_ucb_coefficient'
          ],
          ucb_overwrite_probability=params['ucb_overwrite_probability'],
      )
      # Update the designer config. As the GP_UCB_PE state is recomputed every
      # suggest() call based on the config, this effectively updates the state.
      # TODO: Instead of changing internal state, use public method.
      designer._config = updated_config  # pylint: disable=protected-access

    ucb_coefficient_param = scheduled_designer.ExponentialScheduledParam(
        init_value=self._init_ucb_coefficient,
        final_value=self._final_ucb_coefficient,
        rate=self._decay_ucb_coefficient,
    )

    explore_region_ucb_coefficient_param = (
        scheduled_designer.ExponentialScheduledParam(
            init_value=self._init_explore_region_ucb_coefficient,
            final_value=self._final_explore_region_ucb_coefficient,
            rate=self._decay_explore_region_ucb_coefficient,
        )
    )

    ucb_overwrite_probability_param = (
        scheduled_designer.ExponentialScheduledParam(
            init_value=self._init_ucb_overwrite_probability,
            final_value=self._final_ucb_overwrite_probability,
            rate=self._decay_ucb_overwrite_probability,
        )
    )

    scheduled_params = {
        'ucb_coefficient': ucb_coefficient_param,
        'explore_region_ucb_coefficient': explore_region_ucb_coefficient_param,
        'ucb_overwrite_probability': ucb_overwrite_probability_param,
    }

    return scheduled_designer.ScheduledDesigner(
        problem,
        designer_factory=self._gp_ucb_pe_factory,
        designer_state_updater=_gp_ucb_pe_state_updater,
        scheduled_params=scheduled_params,
        expected_total_num_trials=self._expected_total_num_trials,
    )


--- vizier/_src/algorithms/designers/unsafe_as_infeasible_designer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Designer that maps unsafe Trials as infeasible."""

import copy
from typing import Optional, Sequence
from absl import logging
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.pyvizier import multimetric


class UnsafeAsInfeasibleDesigner(vza.Designer):
  """Designer that maps unsafe Trials as infeasible."""

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      designer_factory: vza.DesignerFactory[vza.Designer],
      *,
      seed: Optional[int] = None,
  ):
    """Init.

    Args:
      problem_statement:
      designer_factory:
      seed:
    """
    self._safety_checker = multimetric.SafetyChecker(
        problem_statement.metric_information
    )
    problem_statement_safety_removed = copy.deepcopy(problem_statement)
    problem_statement_safety_removed.metric_information = (
        problem_statement.metric_information.exclude_type(vz.MetricType.SAFETY)
    )
    self._designer = designer_factory(
        problem_statement_safety_removed, seed=seed
    )

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    are_safe = self._safety_checker.are_trials_safe(completed.trials)
    mutated_trials = []
    mutated_trial_count = 0
    for is_safe, trial in zip(are_safe, completed.trials):
      if is_safe:
        mutated_trials.append(trial)
      else:
        measurement = trial.final_measurement or vz.Measurement()
        mutated_trial = trial.complete(
            measurement, infeasibility_reason='unsafe', inplace=False
        )
        mutated_trial_count += 1
        mutated_trials.append(mutated_trial)

    logging.info(
        'Update designer with %d completed trials (%d converted from safe to'
        ' infeasible)',
        len(mutated_trials),
        mutated_trial_count,
    )
    self._designer.update(vza.CompletedTrials(mutated_trials), all_active)

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    """Make new suggestions.

    Args:
      count: Makes best effort to generate this many suggestions. If None,
        suggests as many as the algorithm wants.

    Returns:
      New suggestions.
    """
    return self._designer.suggest(count)


--- vizier/_src/algorithms/designers/unsafe_as_infeasible_designer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import pyvizier as vz
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.designers import scalarizing_designer
from vizier._src.algorithms.designers import unsafe_as_infeasible_designer
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies
from absl.testing import absltest


class UnsafeAsInfeasibleDesignerTest(absltest.TestCase):

  def test_unsafe_eagle(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.extend([
        vz.MetricInformation(
            name='metric1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='metric_safe',
            goal=vz.ObjectiveMetricGoal.MAXIMIZE,
            safety_threshold=0.2,
        ),
    ])

    def eagle_designer_factory(ps, seed):
      return eagle_strategy.EagleStrategyDesigner(
          problem_statement=ps, seed=seed
      )

    safe_eagle = unsafe_as_infeasible_designer.UnsafeAsInfeasibleDesigner(
        problem, eagle_designer_factory
    )

    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=6,
            batch_size=5,
            verbose=1,
            validate_parameters=True,
        ).run_designer(safe_eagle),
        30,
    )

  def test_unsafe_multi_objective_eagle(self):
    problem = vz.ProblemStatement(
        test_studies.flat_continuous_space_with_scaling()
    )
    problem.metric_information.extend([
        vz.MetricInformation(
            name='metric1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='metric2', goal=vz.ObjectiveMetricGoal.MINIMIZE
        ),
        vz.MetricInformation(
            name='metric_safe',
            goal=vz.ObjectiveMetricGoal.MAXIMIZE,
            safety_threshold=0.2,
        ),
    ])

    def eagle_designer_factory(ps, seed):
      return eagle_strategy.EagleStrategyDesigner(
          problem_statement=ps, seed=seed
      )

    def scalarized_eagle_factory(ps, seed):
      return scalarizing_designer.create_gaussian_scalarizing_designer(
          ps,
          eagle_designer_factory,
          scalarization.HyperVolumeScalarization,
          num_ensemble=3,
          seed=seed,
      )

    safe_eagle = unsafe_as_infeasible_designer.UnsafeAsInfeasibleDesigner(
        problem, scalarized_eagle_factory
    )

    self.assertLen(
        test_runners.RandomMetricsRunner(
            problem,
            iters=6,
            batch_size=5,
            verbose=1,
            validate_parameters=True,
        ).run_designer(safe_eagle),
        30,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/ensemble/ensemble_design.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Ensembling design algorithms for choosing between algorithms/experts."""

import abc
import attrs
import numpy as np

# An observation, that is a expert index (int) along with its reward.
IndexWithReward = tuple[int, float]


class EnsembleDesign(abc.ABC):
  """Strategy that samples which index/expert to choose according to rewards."""

  @property
  @abc.abstractmethod
  def ensemble_probs(self) -> np.ndarray:
    """Computes the probability distribution used to choose the expert.

    The returned shape is (num_experts,) and is a probability distribution.
    """
    pass

  @abc.abstractmethod
  def update(self, observation: IndexWithReward, **kwargs):
    """Updates the strategy with observation."""
    pass


@attrs.define
class RandomEnsembleDesign(EnsembleDesign):
  indices: list[int] = attrs.field()

  @property
  def ensemble_probs(self) -> np.ndarray:
    """The probability distribution used to choose the expert."""
    return np.ones(shape=len(self.indices)) / len(self.indices)

  def update(self, observation: IndexWithReward, **kwargs):
    """Updates the strategy with observation."""
    pass


def softmax(x: np.ndarray) -> np.ndarray:
  """Compute softmax values for x."""
  e_x = np.exp(x - np.max(x))
  return e_x / np.sum(e_x)


# https://bjpcjp.github.io/pdfs/math/bandits-exp3-IX-BA.pdf
@attrs.define
class EXP3IXEnsembleDesign(EnsembleDesign):
  """The EXP3-IX Algorithm that is robust against small probabilities."""

  indices: list[int] = attrs.field()
  stepsize: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  max_reward: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )

  def __attrs_post_init__(self):
    self._log_weights = np.zeros(shape=len(self.indices))
    self._history: list[IndexWithReward] = []

  @property
  def ensemble_probs(self) -> np.ndarray:
    return softmax(self._log_weights)

  def update(self, observation: IndexWithReward):
    expert_idx, reward = observation
    reward = min(self.max_reward, reward)
    gamma = 1.0 / np.sqrt(1.0 + len(self._history))
    loss_estimator = (self.max_reward - reward) / (
        self.ensemble_probs[expert_idx] + gamma
    )

    self._log_weights[expert_idx] += self.stepsize * (-loss_estimator)
    self._history.append(observation)


# pytype: disable=attribute-error
# https://www.cs.princeton.edu/courses/archive/fall16/cos402/lectures/402-lec22.pdf.
@attrs.define
class EXP3UniformEnsembleDesign(EnsembleDesign):
  """The EXP3 algorithm with uniform exploration."""

  indices: list[int] = attrs.field()
  stepsize: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  max_reward: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  use_loss_formulation: bool = attrs.field(default=False)
  # If the fed-in rewards are estimators, no reward clipping or
  # importance weighting logic are done.
  use_reward_estimator: bool = attrs.field(default=False)

  def __attrs_post_init__(self):
    self._log_weights = np.zeros(shape=len(self.indices))
    self._history: list[IndexWithReward] = []

  @property
  def ensemble_probs(self) -> np.ndarray:
    n_arms = len(self.indices)
    uniform = np.ones(n_arms) * 1.0 / n_arms
    gamma = 1.0 / np.sqrt(1 + len(self._history))
    probs = (1 - gamma) * softmax(self._log_weights) + gamma * uniform
    return probs

  def update(self, observation: IndexWithReward):
    """Update history and weights."""
    expert_idx, reward = observation
    if not self.use_reward_estimator:
      reward = min(self.max_reward, reward)

    gamma = 1.0 / np.sqrt(1 + len(self._history))
    if self.use_loss_formulation:
      if self.use_reward_estimator:
        loss_estimator = self.max_reward - reward
      else:
        loss_estimator = (self.max_reward - reward) / (
            self.ensemble_probs[expert_idx]
        )
      self._log_weights[expert_idx] += -self.stepsize * gamma * loss_estimator
    else:
      if self.use_reward_estimator:
        reward_estimator = reward
      else:
        reward_estimator = reward / (self.ensemble_probs[expert_idx])
      self._log_weights[expert_idx] += self.stepsize * gamma * reward_estimator

    self._history.append(observation)

  @property
  def history(self) -> list[IndexWithReward]:
    return self._history


# Adaptive metalearner that minimizes adaptive regret by ensembling over history
# lengths. See https://arxiv.org/abs/2401.09278 for more details on adaptive
# regret and algorithmic details on time horizon ensembling.
@attrs.define(kw_only=True)
class AdaptiveEnsembleDesign(EnsembleDesign):
  """An EnsembleStrategy that minimizes adaptive regret."""

  indices: list[int] = attrs.field()
  # List of max history lengths.
  max_lengths: list[int] = attrs.field()
  # Base stepsize and meta_stepsize should theoretically be 1/sqrt(n) where
  # n = # of indices (or underlying arms) when using a reward estimator.
  base_stepsize: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  # Stepsize of the meta-learner.
  meta_stepsize: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  max_reward: float = attrs.field(
      default=1.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.gt(0)],
  )
  # Whether to use naive sampling for loss estimate.
  naive_sampling: bool = attrs.field(default=False)

  def __attrs_post_init__(self):
    self._log_weights = {}
    self._base_algos = {}
    self._history = []
    for max_length in self.max_lengths:
      # Initialize log_weight = log(1/sqrt(max_length * len(indices))).
      self._base_algos[max_length] = EXP3UniformEnsembleDesign(
          indices=self.indices,
          stepsize=self.base_stepsize,
          use_loss_formulation=False,
          use_reward_estimator=True,
      )
      self._log_weights[max_length] = (
          -np.log(max_length * len(self.indices)) / 2.0
      )

  @property
  def ensemble_probs(self) -> np.ndarray:
    # Return observation probabilities every other observation.
    if len(self._history) % 2 == 1:
      return self.observation_probs

    return self.play_probs

  @property
  def play_probs(self) -> np.ndarray:
    """Returns the probability of playing an expert."""
    ensemble_probs = [algo.ensemble_probs for algo in self._base_algos.values()]
    weights = softmax(np.array(list(self._log_weights.values())))
    final_probs = np.zeros(len(self.indices))
    for weight, probs in zip(weights, ensemble_probs):
      final_probs += weight * probs

    return final_probs

  @property
  def observation_probs(self) -> np.ndarray:
    """Returns the probability of observing an expert."""
    num_experts = len(self.indices)
    uniform = np.ones(num_experts) * 1.0 / num_experts
    if self.naive_sampling:
      return uniform

    algo_prob_max = np.zeros(num_experts)
    algo_prob_sum = np.zeros(num_experts)
    ensemble_probs = [algo.ensemble_probs for algo in self._base_algos.values()]
    for i in range(num_experts):
      algo_prob_max[i] = np.amax(np.array([p[i] ** 2 for p in ensemble_probs]))
      algo_prob_sum[i] = np.sum(np.array([p[i] for p in ensemble_probs]))

    # Take the average of the two max and sum distributions.
    observation_prob = algo_prob_max / (
        2 * np.sum(algo_prob_max)
    ) + algo_prob_sum / (2 * np.sum(algo_prob_sum))
    return observation_prob

  def update(self, observation: IndexWithReward):
    expert_idx, reward = observation
    reward = min(reward, self.max_reward)
    reward_estimator = reward * 1.0 / self.ensemble_probs[expert_idx]

    # Updates the states of base algorithms.
    for max_length, base_algo in self._base_algos.items():
      # If the history is about to be filled, restart.
      if len(base_algo.history) + 1 >= max_length:
        # Initialize log_weight = log(1/sqrt(max_length * len(indices))).
        self._log_weights[max_length] = (
            -np.log(max_length * len(self.indices)) / 2.0
        )
        self._base_algos[max_length] = EXP3UniformEnsembleDesign(
            indices=self.indices,
            stepsize=self.base_stepsize,
            use_loss_formulation=False,
            use_reward_estimator=True,
        )

        continue

      # Otherwise, update meta-weights via unbiased reward estimation.
      reward_estimator_base = reward_estimator * (
          base_algo.ensemble_probs[expert_idx] - self.play_probs[expert_idx]
      )
      # Stepsize is 1/sqrt(max_length).
      gamma = 1.0 / np.sqrt(max_length)
      self._log_weights[max_length] += (
          self.meta_stepsize * gamma * reward_estimator_base
      )

      # Propagate observed reward estimator.
      base_algo.update((expert_idx, reward_estimator))

    self._history.append(observation)


# pytype: enable=attribute-error


--- vizier/_src/algorithms/ensemble/ensemble_design_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier._src.algorithms.ensemble import ensemble_design

from absl.testing import absltest


class EnsembleDesignTest(absltest.TestCase):

  def testRandom(self):
    indices = [0, 1]
    rewards = [(1, 1), (0, 0), (1, 1)]
    strategy = ensemble_design.RandomEnsembleDesign(indices=indices)

    for reward in rewards:
      strategy.update(reward)
      np.testing.assert_array_equal(strategy.ensemble_probs, [0.5, 0.5])

  def testEXP3IX(self):
    indices = [0, 1]
    rewards = [(1, 1), (0, 0), (1, 1)]
    strategy = ensemble_design.EXP3IXEnsembleDesign(
        indices=indices, stepsize=1.0
    )

    np.testing.assert_array_equal(strategy.ensemble_probs, [0.5, 0.5])

    for reward in rewards:
      strategy.update(reward)

    probs = strategy.ensemble_probs
    self.assertLen(probs, 2)
    self.assertGreater(probs[1], 0.55)
    self.assertGreater(probs[0], 0.3)

  def testEXP3Uniform(self):
    indices = [0, 1]
    rewards = [(1, 1), (0, 0), (1, 1)]
    strategy = ensemble_design.EXP3UniformEnsembleDesign(
        indices=indices, stepsize=1.0
    )

    np.testing.assert_array_equal(strategy.ensemble_probs, [0.5, 0.5])

    for reward in rewards:
      strategy.update(reward)

    probs = strategy.ensemble_probs
    self.assertLen(probs, 2)
    self.assertGreater(probs[1], 0.7)
    self.assertGreater(probs[0], 0.25)

  def testAdaptiveStrategyFixed(self):
    indices = [0, 1]
    rewards = [(1, 1), (0, 0.0), (1, 1)] * 3

    strategy = ensemble_design.AdaptiveEnsembleDesign(
        indices=indices,
        max_lengths=[5, 10],
    )

    np.testing.assert_array_equal(strategy.ensemble_probs, [0.5, 0.5])

    for reward in rewards:
      strategy.update(reward)

    # We should still favor 1 greatly.
    self.assertGreater(strategy.ensemble_probs[1], 0.7)
    self.assertGreater(strategy.observation_probs[1], 0.7)

  def testAdaptiveStrategy(self):
    indices = [0, 1]
    rewards = [(0, 0), (1, 1), (1, 0), (0, 1)]

    strategy = ensemble_design.AdaptiveEnsembleDesign(
        indices=indices,
        max_lengths=[2, 4],
        base_stepsize=1 / np.sqrt(2),
        meta_stepsize=1 / np.sqrt(2),
    )

    np.testing.assert_array_equal(strategy.ensemble_probs, [0.5, 0.5])

    for reward in rewards:
      strategy.update(reward)

    probs = strategy.ensemble_probs
    self.assertLen(probs, 2)
    # We have adapted to favor index 0 quickly.
    self.assertGreater(probs[0], 0.45)


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/ensemble/ensemble_designer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""A meta-Designer to adaptively ensemble multiple Designers."""

from typing import Callable, Optional, Sequence

from absl import logging
import attrs
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.ensemble import ensemble_design
from vizier.benchmarks import analyzers

ENSEMBLE_NS = 'ens'
EXPERT_KEY = 'expert'


@attrs.define
class ObjectiveRewardGenerator:
  """Stateful rewards generator using the objective curves of Trials."""

  problem: vz.ProblemStatement = attrs.field()
  all_trials: list[vz.Trial] = attrs.field(factory=list)
  reward_regularization: float = attrs.field(
      default=0.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.ge(0)],
  )
  min_reward: float = attrs.field(
      default=0.0,
      validator=[attrs.validators.instance_of(float), attrs.validators.ge(0)],
  )
  # Arguments passed to the hypervolume converter.
  reference_value: Optional[np.ndarray] = attrs.field(
      default=None,
      kw_only=True,
      validator=[
          attrs.validators.optional(attrs.validators.instance_of(np.ndarray))
      ],
  )
  num_vectors: int = attrs.field(
      default=100, kw_only=True, validator=[attrs.validators.ge(0)]
  )

  def __call__(self, trials: list[vz.Trial]) -> list[float]:
    """Generate rewards from trials."""
    # Using MultiMetricConverter for safety understanding.
    if self.problem.is_single_objective:

      def curve_generator() -> analyzers.StatefulCurveConverter:
        return analyzers.MultiMetricCurveConverter.from_metrics_config(
            self.problem.metric_information, flip_signs_for_min=True
        )

    else:

      def curve_generator() -> analyzers.StatefulCurveConverter:
        return analyzers.MultiMetricCurveConverter.from_metrics_config(
            self.problem.metric_information,
            reference_value=self.reference_value,
            num_vectors=self.num_vectors,
            infer_origin_factor=0.1,
        )

    stateful_curve_generator = analyzers.RestartingCurveConverter(
        curve_generator, restart_min_trials=10, restart_rate=1.5
    )

    original_num_trials = len(self.all_trials) - len(trials)
    if original_num_trials <= 0:
      # First round so there is no reward signal.
      rewards = [self.min_reward] * len(trials)
    else:
      # Objective curve is a 1 x (1+len(trials)) array.
      objective_curve = stateful_curve_generator.convert(
          self.all_trials[-1:] + trials
      )
      rewards = []
      for idx in range(len(trials)):
        obj_reward = objective_curve.ys[:, idx + 1] - objective_curve.ys[:, idx]
        regularized_reward = (
            obj_reward
            + self.reward_regularization * objective_curve.ys[:, idx + 1]
        )
        if np.isfinite(regularized_reward):
          rewards.append(
              max(self.min_reward, float(np.squeeze(regularized_reward)))
          )
        else:
          rewards.append(self.min_reward)

    self.all_trials.extend(trials)
    return rewards


class EnsembleDesigner(vza.Designer):
  """Ensembling of Designers."""

  def __init__(
      self,
      designers: dict[str, vza.Designer],
      ensemble_design_factory: Callable[
          [list[int]], ensemble_design.EnsembleDesign
      ] = ensemble_design.RandomEnsembleDesign,
      reward_generator: Optional[ObjectiveRewardGenerator] = None,
      *,
      use_diverse_suggest: bool = False,
      use_separate_update: bool = False,
  ):
    """Creates a wrapper that uses an ensemble of pydesigners.

    Args:
      designers: Dictionary of PyDesigners to be ensembled.
      ensemble_design_factory: EnsembleDesign factory that takes in list of
        indices.
      reward_generator: Generates rewards for use in ensembling strategy.
      use_diverse_suggest: Whether to use a diverse set of designers for batched
        Suggests by repeatedly calling Suggest(1) in round robin fashion.
      use_separate_update: If false, update all Designers with Trials that were
        Suggested by other Designers.

    Raises:
      ValueError when designers is empty.
    """
    if not designers:
      raise ValueError('Empty list of Designers for ensembling.')
    self._designers = designers
    self._ensemble_design_factory = ensemble_design_factory
    # Each index corresponds to the designer order in the dictionary.
    self._strategy = ensemble_design_factory(list(range(len(self._designers))))
    self._use_diverse_suggest = use_diverse_suggest
    self._reward_generator = reward_generator
    self._use_separate_update = use_separate_update

  def suggest(self, num_suggestions: int) -> Sequence[vz.TrialSuggestion]:
    """Randomly chooses a designer and Suggests from the chosen designer.

    Args:
      num_suggestions: Number of suggested trials desired.

    Returns:
      Batch of suggested trials.
    """

    if num_suggestions == 1 or not self._use_diverse_suggest:
      probs = self._strategy.ensemble_probs
      logging.info(
          'Choosing ensemble with probabilities %s over %s',
          probs,
          self._designers.keys(),
      )
      designer_key = np.random.choice(list(self._designers.keys()), p=probs)
      trials = self._designers[designer_key].suggest(num_suggestions)
      for t in trials:
        t.metadata.ns(ENSEMBLE_NS)[EXPERT_KEY] = f'{designer_key}'
      return trials
    else:
      # Apply diverse Suggestions.
      diverse_suggestions = []
      for _ in range(num_suggestions):
        diverse_suggestions.extend(self.suggest(1))
      return diverse_suggestions

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    # With no completed Trials, simply update with reward calcuations.
    if not completed.trials:
      for designer in self._designers.values():
        designer.update(completed, all_active)
      return

    if self._reward_generator is None:
      rewards = []
      for t in completed.trials:
        if t.infeasible:
          rewards.append(0.0)
        elif t.final_measurement is not None and t.final_measurement.metrics:
          metrics = t.final_measurement.metrics
          first_key = list(metrics.keys())[0]
          rewards.append(metrics.get_value(first_key, default=0.0))
        else:
          raise ValueError(
              'A completed Trial is feasible but has no final measurement'
              f' metrics: {t}.'
          )
    else:
      rewards = self._reward_generator(list(completed.trials))

    if len(completed.trials) != len(rewards):
      # If lengths are different zip(..) would silently truncate the lists.
      raise RuntimeError(
          'Internal error: Mismatched completed trials count'
          f' (len(completed.trials)) and rewards counts {(len(rewards))}.'
      )
    # Update the EnsembleDesign strategy.
    for trial, reward in zip(completed.trials, rewards):
      designer_key = trial.metadata.ns(ENSEMBLE_NS).get(EXPERT_KEY)
      if designer_key is None:
        logging.warning(
            'Trial does not contain required metadata with key %s: %s',
            EXPERT_KEY,
            trial,
        )
        continue

      all_keys = list(self._designers.keys())
      if designer_key not in all_keys:
        logging.warning(
            'Trial does not contain designer-specific metadata for designer key'
            ' %s in all designer keys (skipping update as default): \n %s',
            designer_key,
            all_keys,
        )
        if self._use_separate_update:
          raise ValueError(
              'Separate update algorithms require designer-specific metadata'
              f'{designer_key} in all designer keys: \n {all_keys}'
          )
        continue

      observation = (all_keys.index(designer_key), reward)
      logging.info('Updating with expert, rewards: %s', observation)
      self._strategy.update(observation)

    # Update the underlying designers.
    if self._use_separate_update:
      for designer_key, designer in self._designers.items():
        filtered_completed = [
            t
            for t in completed.trials
            if t.metadata.ns(ENSEMBLE_NS).get(EXPERT_KEY) == designer_key
        ]
        filtered_active = [
            t
            for t in all_active.trials
            if t.metadata.ns(ENSEMBLE_NS).get(EXPERT_KEY) == designer_key
        ]
        designer.update(
            vza.CompletedTrials(filtered_completed),
            vza.ActiveTrials(filtered_active),
        )
    else:
      for designer in self._designers.values():
        designer.update(completed, all_active)


--- vizier/_src/algorithms/ensemble/ensemble_designer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import jax.numpy as jnp
from vizier import algorithms as vza
from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.designers import scalarization
from vizier._src.algorithms.designers import scalarizing_designer
from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy
from vizier._src.algorithms.ensemble import ensemble_design
from vizier._src.algorithms.ensemble import ensemble_designer
from vizier.benchmarks import experimenters

from absl.testing import absltest
from absl.testing import parameterized


class EnsembleDesignerTest(parameterized.TestCase):

  # pylint: disable=g-long-lambda
  @parameterized.parameters(
      (ensemble_design.EXP3UniformEnsembleDesign),
      (ensemble_design.EXP3IXEnsembleDesign),
      (
          lambda ind: ensemble_design.AdaptiveEnsembleDesign(
              indices=ind, max_lengths=[50, 100, 200]
          )
      ),
  )
  def testSmartEnsemblingWithGenerator(self, ensemble_design_factory):
    dim = 6
    bbob_factory = experimenters.BBOBExperimenterFactory('Sphere', dim)
    exptr_factory = experimenters.SingleObjectiveExperimenterFactory(
        bbob_factory
    )

    def ensemble_designer_factory(config: vz.ProblemStatement, seed: int):
      random_designer = random.RandomDesigner(config.search_space, seed=seed)
      eagle = eagle_strategy.EagleStrategyDesigner(config, seed=seed)
      reward_generator = ensemble_designer.ObjectiveRewardGenerator(
          config, reward_regularization=0.1
      )
      return ensemble_designer.EnsembleDesigner(
          {'random': random_designer, 'eagle': eagle},
          ensemble_design_factory=ensemble_design_factory,
          reward_generator=reward_generator,
      )

    benchmark_state_factory = (
        benchmarks.ExperimenterDesignerBenchmarkStateFactory(
            designer_factory=ensemble_designer_factory,
            experimenter_factory=exptr_factory,
        )
    )
    bench_state = benchmark_state_factory()
    runner = benchmarks.BenchmarkRunner(
        benchmark_subroutines=[benchmarks.GenerateAndEvaluate(5)],
        num_repeats=50,
    )
    runner.run(bench_state)
    self.assertEmpty(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        )
    )
    all_trials = bench_state.algorithm.supporter.GetTrials()
    self.assertLen(all_trials, 250)

    num_random_trials = 0
    num_eagle_trials = 0
    for t in all_trials:
      if (
          t.metadata.ns(ensemble_designer.ENSEMBLE_NS).get(
              ensemble_designer.EXPERT_KEY
          )
          == 'random'
      ):
        num_random_trials += 1
      elif (
          t.metadata.ns(ensemble_designer.ENSEMBLE_NS).get(
              ensemble_designer.EXPERT_KEY
          )
          == 'eagle'
      ):
        num_eagle_trials += 1
    self.assertEqual(num_random_trials + num_eagle_trials, 250)

  @parameterized.parameters(
      (ensemble_design.EXP3UniformEnsembleDesign),
      (ensemble_design.EXP3IXEnsembleDesign),
      (
          lambda ind: ensemble_design.AdaptiveEnsembleDesign(
              indices=ind, max_lengths=[50, 100, 200]
          )
      ),
  )
  def testPendingMulitobjectiveUpdate(self, ensemble_design_factory):
    dim = 2
    func1 = experimenters.bbob.Sphere
    func2 = experimenters.bbob.Rastrigin
    exptr1 = experimenters.NumpyExperimenter(
        func1, experimenters.bbob.DefaultBBOBProblemStatement(dim)
    )
    exptr2 = experimenters.NumpyExperimenter(
        func2, experimenters.bbob.DefaultBBOBProblemStatement(dim)
    )
    exptr = experimenters.MultiObjectiveExperimenter(
        {'m1': exptr1, 'm2': exptr2}
    )

    def ensemble_designer_factory(config: vz.ProblemStatement, seed: int):
      random_designer = random.RandomDesigner(config.search_space, seed=seed)

      def eagle_designer_factory(ps, seed):
        return eagle_strategy.EagleStrategyDesigner(
            problem_statement=ps, seed=seed
        )

      scalarized_eagle = scalarizing_designer.ScalarizingDesigner(
          config,
          eagle_designer_factory,
          scalarizer=scalarization.HyperVolumeScalarization(
              weights=jnp.ones(len(config.metric_information))
          ),
      )

      reward_generator = ensemble_designer.ObjectiveRewardGenerator(
          config, reward_regularization=0.1
      )
      return ensemble_designer.EnsembleDesigner(
          {'random': random_designer, 'eagle': scalarized_eagle},
          ensemble_design_factory=ensemble_design_factory,
          reward_generator=reward_generator,
      )

    benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        designer_factory=ensemble_designer_factory,
        experimenter=exptr,
    )
    bench_state = benchmark_state_factory()
    runner = benchmarks.BenchmarkRunner(
        benchmark_subroutines=[benchmarks.GenerateSuggestions(1)],
        num_repeats=5,
    )
    runner.run(bench_state)
    self.assertLen(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        ),
        5,
    )

  def testInfeasibleTrials(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('x1', 0.0, 1.0)
    problem.metric_information = [
        vz.MetricInformation(name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
    ]
    random_designer = random.RandomDesigner(problem.search_space)
    eagle = eagle_strategy.EagleStrategyDesigner(problem)
    ens_designer = ensemble_designer.EnsembleDesigner(
        {'random': random_designer, 'eagle': eagle},
    )
    trial1 = ens_designer.suggest(num_suggestions=1)[0].to_trial()
    trial2 = ens_designer.suggest(num_suggestions=1)[0].to_trial()
    # Complete trial1 with feasible value.
    trial1.complete(vz.Measurement({'obj': 1240.5}), inplace=True)
    # Complete trial2 with infeasible value.
    trial2.complete(
        vz.Measurement(), infeasibility_reason='infeasible', inplace=True
    )
    ens_designer.update(
        completed=vza.CompletedTrials([trial1, trial2]),
        all_active=vza.ActiveTrials([]),
    )
    ens_designer.suggest(num_suggestions=1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/evolution/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/evolution/nsga2.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""NSGA-II algorithm: https://ieeexplore.ieee.org/document/996017."""

from typing import Callable, Optional, Sequence, Tuple

from absl import logging
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.evolution import numpy_populations
from vizier._src.algorithms.evolution import templates

Population = numpy_populations.Population
Offspring = numpy_populations.Offspring
Mutation = templates.Mutation


def pareto_rank(ys: np.ndarray) -> np.ndarray:
  """Pareto rank, which is the number of points dominating it.

  Args:
    ys: (number of population) x (number of metrics) array.

  Returns:
    (number of population) integer array.
  """
  if ys.shape[0] == 0:
    return np.zeros([0])
  dominated = [np.all(ys <= r, axis=-1) & np.any(r > ys, axis=-1) for r in ys]
  return np.sum(np.stack(dominated), axis=0)


def crowding_distance(
    ys: np.ndarray, *, extra_tiebreakers: Sequence[np.ndarray] = tuple()
) -> np.ndarray:
  """Crowding distance.

  Reference:
  https://medium.com/@rossleecooloh/optimization-algorithm-nsga-ii-and-python-package-deap-fca0be6b2ffc
  except that the lower boundary does not get infinity crowding score.

  Args:
    ys: (number of population) x (number of metrics) array.
    extra_tiebreakers: A sequence of (number of population) array of floating
      numbers. If specified, they are used to break ties when sorting the
      population. By default, a random number is always used to break ties
      consistently across all metrics.

  Returns:
    (number of population) float32 array. Higher numbers mean less crowding
    and more desirable.
  """
  scores = np.zeros([ys.shape[0]], dtype=np.float32)

  if ys.shape[0] <= 1:
    return scores

  rng = np.random.default_rng()
  # Use a random number to break ties. But the same random number is used for
  # all metrics.
  random_tiebreaker = rng.random(ys.shape[0])
  tiebreakers = list(extra_tiebreakers) + [random_tiebreaker]

  for m in range(ys.shape[1]):
    # Sort by the m-th metric.
    sid = sorted(
        np.arange(ys.shape[0]),
        key=lambda i, m=m: (ys[i, m],)
        + tuple(tiebreaker[i] for tiebreaker in tiebreakers),
    )

    # Compute the range of the m-th metric.
    yy = ys[:, m]  # Shape: (num_population,)
    yrange = yy[sid[-1]] - yy[sid[0]] + np.finfo(np.float32).eps

    # Lower boundary is assigned a one-sided score and does not automatically
    # get infinity. This is different from the paper. The lower boundary means
    # it's dominated by all other points in one dimension. There's no reason to
    # favor it over other points.
    scores[sid[0]] += (yy[sid[1]] - yy[sid[0]]) / yrange
    # Upper boundary is assigned infinity. This point will survive anyways
    # because it's pareto-optimal. But in case there are ties, it's useful to
    # make only one of them stand out.
    scores[sid[-1]] += np.inf

    scores[sid[1:-1]] += (yy[sid[2:]] - yy[sid[:-2]]) / yrange
  # Normalize the score to [0, 1].
  return scores / ys.shape[1]


def _constraint_violation(ys: np.ndarray) -> np.ndarray:
  """Counts the constraints violated.

  Args:
    ys: (number of population) x (number of metrics) array.

  Returns:
    (number of population) array of integers.
  """
  return np.sum(ys < 0, axis=1)


def _select_by(ys: np.ndarray, target: int) -> Tuple[np.ndarray, np.ndarray]:
  """Returns a boolean index array for the top `target` elements of ys.

  This method is tough to parse. Please improve the API if you see a better
  design!

  Args:
    ys: Array of shape [M]. Entries are expected to have a small set of unique
      values.
    target: Count to return.

  Returns:
    A tuple of two bolean index arrays `top` and `border`.
     * `ys[top]` has length less than or equal to `target`. They are within
       top `target`.
     * `ys[top | border]` has length greater than or equal to `target`.
     * `ys[border]` have all-identical entries. Callers should break ties
       among them.
     * `top & border` is all False.
  """
  if ys.shape[0] <= target:
    return (
        np.ones(ys.shape[:1], dtype=np.bool_),
        np.zeros(ys.shape[:1], dtype=np.bool_),
    )
  unique, counts = np.unique(ys, return_counts=True)
  cutoffidx = np.argmax(np.cumsum(counts) > target)
  cutoffnumber = unique[cutoffidx]
  return ys < cutoffnumber, ys == cutoffnumber


class NSGA2Survival(templates.Survival):
  """NSGA2 Survival step.

  Reference: https://ieeexplore.ieee.org/document/996017
  """

  def __init__(
      self,
      target_size: int,
      *,
      ranking_fn: Callable[[np.ndarray], np.ndarray] = pareto_rank,
      eviction_limit: Optional[int] = None
  ):
    """Init.

    Args:
      target_size: select() method reduces the population to this size, unless
        the population is already at or below this size in which case select()
        is a no-op.
      ranking_fn: Takes (number of population) x (number of metrics) array of
        floating numbers and returns (number of population) array of integers,
        representing the pareto rank aka number of points it is dominated by.
        For performance-sensitive applications, plug in an XLA-compiled
        function.
      eviction_limit: Evict genes that were alive for this many generations.
    """
    self._target_size = target_size
    self._ranking_fn = ranking_fn
    self._eviction_limit = eviction_limit or float('inf')

  def select(self, population: Population) -> Population:
    """Applies survival process.

    Sorted all points by 3-tuple
    1. Descending order of safety constraint violation score. Zero
      means no violations.
    2. Ascending order of how many points it's dominated by.
    3. Descending order of crowding distance.

    Args:
      population:

    Returns:
      Population of size self._population_size.
    """
    population = population[population.ages < self._eviction_limit]
    if not population:
      # return empty.
      return population

    logging.info('Selecting %s from %s', self._target_size, len(population))
    selected = population.empty_like()
    # Sort by the safety constraint.
    if selected.cs.shape[1]:
      top, border = _select_by(
          _constraint_violation(population.cs), target=self._target_size
      )
      selected += population[top]
      population = population[border]
      logging.info(
          'Selected %s by safety constraints, and will break ties among: %s',
          len(selected),
          len(population),
      )
    # Sort by the pareto rank.
    pareto_ranks = self._ranking_fn(population.ys)
    top, border = _select_by(
        pareto_ranks, target=self._target_size - len(selected)
    )
    considered_pareto_ranks = np.concatenate(
        [-np.ones(len(selected)), pareto_ranks[top], pareto_ranks[border]]
    )
    selected += population[top]
    population = population[border]
    logging.info(
        'Selected %s by pareto rank, and will break ties among: %s',
        len(selected),
        len(population),
    )
    # Sort by the distance. Include the points that are already selected for
    # the computation.
    # Flip the sign so it works with ascending sort.
    distance = -crowding_distance(
        (selected + population).ys,
        extra_tiebreakers=[considered_pareto_ranks],
    )
    sids = np.argsort(distance)
    # Selected points have fewer constraint violations or better pareto rank.
    # Regardless of the distance, they remain selected. Rank the remainder only.
    sids = sids[sids >= len(selected)] - len(selected)
    selected += population[sids[: self._target_size - len(selected)]]

    return attr.evolve(selected, ages=selected.ages + 1)


class NSGA2Designer(
    templates.CanonicalEvolutionDesigner[Population, Offspring]
):
  """NSGA2 Designer.

  Reference: https://ieeexplore.ieee.org/document/996017
  """

  def __init__(
      self,
      problem: vz.ProblemStatement,
      population_size: int = 50,
      first_survival_after: Optional[int] = None,
      *,
      ranking_fn: Callable[[np.ndarray], np.ndarray] = pareto_rank,
      eviction_limit: Optional[int] = None,
      adaptation: Optional[Mutation[Population, Offspring]] = None,
      adaptation_callable: Optional[
          Callable[[int], Mutation[Population, Offspring]]
      ] = None,
      metadata_namespace: str = 'nsga2',
      seed: Optional[int] = None
  ):
    """Creates NSGA2 Designer.

    Args:
      problem:
      population_size: Survival steps reduce the population to this size.
      first_survival_after: Apply the survival step after observing this many
        trials. Leave it unset to use the default behavior.
      ranking_fn: Takes (number of population) x (number of metrics) array of
        floating numbers and returns (number of population) array of integers,
        representing the pareto rank aka number of points it is dominated by.
        The default implementation is reasonably fast for hundreds of trials,
        but if you want to improve performance, your own implementation can be
        injected.
      eviction_limit: Evict a gene that has been alive for this many
        generations.
      adaptation: Fixed mutation used to evolve population.
      adaptation_callable: If specified, adaptation callable is a function of
        num_trials that returns the trial-dependent adaptation.
      metadata_namespace: Metadata namespace to use.
      seed: Random seed.

    Returns:
      NSGA2 Designer.
    """
    super().__init__(
        numpy_populations.PopulationConverter(
            problem.search_space,
            problem.metric_information,
            metadata_ns=metadata_namespace,
        ),
        numpy_populations.UniformRandomSampler(problem.search_space, seed=seed),
        NSGA2Survival(
            population_size,
            ranking_fn=ranking_fn,
            eviction_limit=eviction_limit,
        ),
        adaptation=adaptation
        or numpy_populations.LinfMutation(seed=seed, norm=0.001),
        first_survival_after=first_survival_after,
        adaptation_callable=adaptation_callable,
        population_size=population_size,
    )


--- vizier/_src/algorithms/evolution/nsga2_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for nsga2."""

import datetime
from typing import Optional

from absl import logging

import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz

from vizier._src.algorithms.evolution import nsga2
from vizier._src.algorithms.evolution import templates
from vizier.testing import test_studies

from absl.testing import absltest

np.set_printoptions(precision=3)


class CrowdingDistanceTest(absltest.TestCase):

  def test_crowding_distance(self):
    ys = np.array([
        [0.5, 4.0],
        [0.5, 4.0],
        [0.5, 2.0],
        [1.0, 2.0],
        [1.0, 2.0],
    ])
    result = nsga2.crowding_distance(
        ys, extra_tiebreakers=[nsga2.pareto_rank(ys)]
    )
    np.testing.assert_allclose(
        # Only one of the duplicate pareto-optimal points should get
        # infinity score.
        np.isinf(result).sum(),
        2,
    )


def nsga2_on_all_types(
    population_size: int = 50, eviction_limit: Optional[int] = None
) -> templates.CanonicalEvolutionDesigner[nsga2.Population, nsga2.Offspring]:
  problem = vz.ProblemStatement(
      search_space=test_studies.flat_space_with_all_types()
  )
  problem.metric_information.extend([
      vz.MetricInformation(name='m1', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
      vz.MetricInformation(name='m2', goal=vz.ObjectiveMetricGoal.MINIMIZE),
      vz.MetricInformation(
          name='s1',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          safety_threshold=1.0,
      ),
      vz.MetricInformation(
          name='s2',
          goal=vz.ObjectiveMetricGoal.MINIMIZE,
          safety_threshold=1.0,
      ),
  ])

  algorithm = nsga2.NSGA2Designer(
      problem,
      population_size,
      first_survival_after=population_size,
      eviction_limit=eviction_limit,
      seed=0,
  )
  return algorithm


class Nsga2Test(absltest.TestCase):

  def test_pareto_rank_empty(self):
    ys = np.array([]).reshape(0, 2)
    ranks = nsga2.pareto_rank(ys)
    self.assertEqual(ranks.shape, (0,))

  def test_pareto_rank_single(self):
    ys = np.array([[1.0, 2.0]])
    ranks = nsga2.pareto_rank(ys)
    np.testing.assert_array_equal(ranks, [0])

  def test_pareto_rank_simple_dominance(self):
    # P0 dominates P1
    # P0 dominates P2
    # P1 and P2 don't dominate each other
    ys = np.array([[2.0, 3.0], [1.0, 3.0], [2.0, 2.0]])
    ranks = nsga2.pareto_rank(ys)
    np.testing.assert_array_equal(ranks, [0, 1, 1])

  def test_pareto_rank_no_dominance(self):
    ys = np.array([[1.0, 5.0], [2.0, 4.0], [3.0, 3.0]])
    ranks = nsga2.pareto_rank(ys)
    np.testing.assert_array_equal(ranks, [0, 0, 0])

  def test_pareto_rank_duplicate_points_do_not_dominate_each_other(self):
    ys = np.array([[2.0, 3.0], [1.0, 2.0], [2.0, 3.0]])
    ranks = nsga2.pareto_rank(ys)
    np.testing.assert_array_equal(ranks, [0, 2, 0])

  def test_pareto_rank_larger_case(self):
    ys = np.array([
        [10, 5],  # 0
        [8, 5],  # 1: (dominated by [10, 5])
        [9, 4],  # 1: (dominated by [10, 5])
        [8, 4],  # 3: (dominated by [10, 5], [8, 5], [9, 4])
        [1, 10],  # 0
    ])
    ranks = nsga2.pareto_rank(ys)
    np.testing.assert_array_equal(ranks, [0, 1, 1, 3, 0])

  def test_survival_by_pareto_rank(self):
    algorithm = nsga2_on_all_types(3)
    # Trial 0 is the only point on the frontier.
    trial0 = vz.Trial(id=0)
    trial0.complete(
        vz.Measurement({'m1': 1.0, 'm2': 0.0, 's1': 2.0, 's2': 0.0})
    )

    # 4 safe trials with the same pareto rank. Crowding distance is computed
    # among them to break ties. Trial 3 is less "crowded" than Trial 2.
    trial1 = vz.Trial(id=1)
    trial1.complete(
        vz.Measurement({'m1': 0.0, 'm2': -1.0, 's1': 2.0, 's2': 0.0})
    )
    trial2 = vz.Trial(id=2)
    trial2.complete(
        vz.Measurement({'m1': 0.5, 'm2': -0.5, 's1': 2.0, 's2': 0.0})
    )
    trial3 = vz.Trial(id=3)
    trial3.complete(
        vz.Measurement({'m1': 0.2, 'm2': -0.2, 's1': 2.0, 's2': 0.0})
    )
    trial4 = vz.Trial(id=4)
    trial4.complete(
        vz.Measurement({'m1': 0.3, 'm2': -0.3, 's1': 2.0, 's2': 0.0})
    )

    trials = vza.CompletedTrials([trial0, trial1, trial2, trial3, trial4])
    algorithm.update(trials, vza.ActiveTrials())
    self.assertSetEqual(set(algorithm.population.trial_ids), {0, 1, 2})

  def test_survival_by_crowding_distance(self):
    algorithm = nsga2_on_all_types(4)
    # Trial 0 is the only point on the frontier.
    trial0 = vz.Trial(id=0)
    trial0.complete(
        vz.Measurement({'m1': 1.001, 'm2': -1.001, 's1': 2.0, 's2': 0.0})
    )

    # 4 safe trials with the same pareto rank. Ties are broken by their crowding
    # distances, computed *with* the pareto optimal trials. Trial 1 is most
    # crowded because of its proximity to trial 0.
    trial1 = vz.Trial(id=1)
    trial1.complete(
        vz.Measurement({'m1': 1.0, 'm2': 0.0, 's1': 2.0, 's2': 0.9})
    )
    trial2 = vz.Trial(id=2)
    trial2.complete(
        vz.Measurement({'m1': 0.9, 'm2': -0.1, 's1': 2.0, 's2': 0.9})
    )
    trial3 = vz.Trial(id=3)
    trial3.complete(
        vz.Measurement({'m1': 0.5, 'm2': -0.5, 's1': 2.0, 's2': 0.9})
    )
    trial4 = vz.Trial(id=4)
    trial4.complete(
        vz.Measurement({'m1': 0.0, 'm2': -1.0, 's1': 2.0, 's2': 0.9})
    )

    trials = vza.CompletedTrials([trial0, trial1, trial2, trial3, trial4])
    algorithm.update(trials, vza.ActiveTrials())
    self.assertSetEqual(set(algorithm.population.trial_ids), {0, 2, 3, 4})

  def test_survival_by_safety(self):
    algorithm = nsga2_on_all_types(3)

    # Trial 1 violates 's1'.
    trial1 = vz.Trial(id=1)
    trial1.complete(
        vz.Measurement({'m1': 1.0, 'm2': 1.0, 's1': 0.0, 's2': 0.0})
    )
    # Trial 2 violates no constraints.
    trial2 = vz.Trial(id=2)
    trial2.complete(
        vz.Measurement({'m1': 0.9, 'm2': 0.9, 's1': 2.0, 's2': 0.0})
    )
    # Trial 3 and 4 violate both 's1' and 's2'.
    trial3 = vz.Trial(id=3)
    trial3.complete(
        vz.Measurement({'m1': 0.5, 'm2': 0.5, 's1': 0.0, 's2': 2.0})
    )
    trial4 = vz.Trial(id=4)
    trial4.complete(
        vz.Measurement({'m1': 0.0, 'm2': 0.0, 's1': 0.0, 's2': 2.0})
    )

    trials = vza.CompletedTrials([trial1, trial2, trial3, trial4])
    algorithm.update(trials, vza.ActiveTrials())
    self.assertSetEqual(set(algorithm.population.trial_ids), {1, 2, 4})

  def test_comprehensive_sanity_check(self):
    algorithm = nsga2_on_all_types(5, eviction_limit=3)

    tid = 1
    for i in range(10):
      if i == 8:
        dumped = algorithm.dump()
      tick = datetime.datetime.now()
      suggestions = algorithm.suggest()
      tock = datetime.datetime.now()
      logging.info('Iteration %s: Suggestion took %s.', i, tock - tick)
      trials = []
      for t in suggestions:
        trials.append(
            t.to_trial(tid).complete(
                vz.Measurement(
                    metrics={
                        'm1': np.random.random(),
                        'm2': np.random.random(),
                        's1': np.random.uniform(1.0, 2.0),
                        's2': np.random.uniform(
                            0.0, 1.0
                        ),  # fails with 25% probability
                    }
                )
            )
        )
        tid += 1
      tick = datetime.datetime.now()
      logging.info(
          'Suggesitons evaluated: %s', '\n'.join(repr(t) for t in trials)
      )
      algorithm.update(vza.CompletedTrials(trials), vza.ActiveTrials())
      tock = datetime.datetime.now()
      logging.info(
          (
              'Iteration %s: Update took %s.\nPopulation(in array'
              ' format):%s\nAges:%s'
          ),
          i,
          tock - tick,
          algorithm.population.xs,
          algorithm.population.ages,
      )
      tick = tock

    self.assertTrue(np.all(algorithm.population.ages <= 3))

    ys = algorithm.population.ys
    pareto = algorithm.population[nsga2.pareto_rank(ys) == 0]
    logging.info('Pareto frontier %s %s', pareto.xs, pareto.ys)

    # Smoke test dump-load.
    algorithm.load(dumped)

  def test_seeding(self):
    algorithm_1 = nsga2_on_all_types(10)
    algorithm_2 = nsga2_on_all_types(10)

    self.assertEqual(algorithm_1.suggest(10), algorithm_2.suggest(10))

    completed_trials = []

    for tid in range(1, 11):
      trial = vz.Trial(id=tid)

      trial.complete(
          vz.Measurement({
              'm1': np.random.random(),
              'm2': np.random.random(),
              's1': np.random.random(),
              's2': np.random.random(),
          })
      )

      completed_trials.append(trial)

    trials = vza.CompletedTrials(completed_trials)
    algorithm_1.update(trials, vza.ActiveTrials())
    algorithm_2.update(trials, vza.ActiveTrials())

    self.assertEqual(algorithm_1.suggest(10), algorithm_2.suggest(10))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/evolution/numpy_populations.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Core population utilities."""
import collections
import json
from typing import Any, Callable, Collection, List, Optional, Sequence, Tuple, Type
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.evolution import templates
from vizier.interfaces import serializable
from vizier.pyvizier import converters
from vizier.utils import json_utils

# TODO: Use a byte encoding instead of JSON.


def _filter_and_split(
    metrics: Collection[vz.MetricInformation],
) -> Tuple[List[vz.MetricInformation], List[vz.MetricInformation]]:
  """Choose objective and safety metrics and split.

  Args:
    metrics:

  Returns:
    Tuple of objective and safety metrics.
  """
  metrics_by_type = collections.defaultdict(list)
  for metric in metrics:
    metrics_by_type[metric.type].append(metric)

  return (
      metrics_by_type[vz.MetricType.OBJECTIVE],
      metrics_by_type[vz.MetricType.SAFETY],
  )


def _concat(a1: np.ndarray, a2: np.ndarray) -> np.ndarray:
  return np.concatenate([a1, a2], axis=0)


def _shape_equals(
    instance_to_shape: Callable[[Any], Collection[Optional[int]]]
):
  """Creates a shape validator for attr.

  For example, _shape_equals(lambda s : [3, None]) validates that the shape has
  length 2 and its first element is 3.

  Args:
    instance_to_shape: Takes instance as input and returns the desired shape for
      the instance. `None` is treated as "any number".

  Returns:
    A validator that can be passed into attr.ib or attr.field.
  """

  def validator(instance, attribute, value) -> None:
    shape = instance_to_shape(instance)

    def _validator_boolean():
      if len(value.shape) != len(shape):
        return False
      for s1, s2 in zip(value.shape, shape):
        if (s2 is not None) and (s1 != s2):
          return False
      return True

    if not _validator_boolean():
      raise ValueError(
          f'{attribute.name} has shape {value.shape} '
          f'which does not match the expected shape {shape}'
      )

  return validator


@attr.define(frozen=True, init=False)
class Offspring(serializable.Serializable):
  """Offspring which corresponds to trial suggestions.

  Attributes:
    xs: Encoding of trial parameters.
    generations: The number of changes this gene went through.
    ids: Identifier for the gene. Can be used to track the "family tree" created
      from the execution of an evolutionary algorithm. For example, when a new
      offspring is sampled, it is assigned a new id and its successors all carry
      the same id.
  """

  xs: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s), None]),
      ]
  )
  ids: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )

  generations: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )

  def __init__(
      self,
      xs: np.ndarray,
      ids: Optional[np.ndarray] = None,
      generations: Optional[np.ndarray] = None,
  ):
    if generations is None:
      generations = np.zeros([xs.shape[0]])
    if ids is None:
      ids = np.zeros([xs.shape[0]])

    self.__attrs_init__(xs, ids, generations)

  def __len__(self) -> int:
    return self.generations.shape[0]

  def __getitem__(self, index: Any) -> 'Offspring':
    return Offspring(self.xs[index], self.ids[index], self.generations[index])

  def __add__(self, other: 'Offspring') -> 'Offspring':
    return Offspring(
        _concat(self.xs, other.xs),
        _concat(self.ids, other.ids),
        _concat(self.generations, other.generations),
    )

  @classmethod
  def load(cls: Type['Offspring'], metadata: vz.Metadata) -> 'Offspring':
    encoded = metadata.get('values', cls=str)
    try:
      decoded = json.loads(encoded, object_hook=json_utils.numpy_hook)
    except Exception as e:
      raise serializable.DecodeError('Failed to decode') from e
    return cls(**decoded)

  def dump(self) -> vz.Metadata:
    encoded = json.dumps(attr.asdict(self), cls=json_utils.NumpyEncoder)
    return vz.Metadata({'values': encoded})


@attr.define(frozen=True)
class Population(templates.Population):
  """Population.

  No validations are done.

  Attributes:
    xs: [len(trials), D] array where D is the number of features. The values are
      in [0, 1] range.
    ys: [len(trials), M_1] array where M_1 is the number of objective metrics.
      The values should be pre-processed as maximization metrics.
    cs: [len(trials), M_2] array where M_2 is the number of soft constraint
      metrics. The values should be pre-processed such that the constraint is to
      get them greater than zero.
    ages: The number of selection phases that each entity survived for.
    generations: The number of ancestors of each entity.
    ids: Identifier for the gene aka "family name".
    trial_ids: Trial ids. For debugging and testing only.
  """

  xs: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s), None]),
      ]
  )
  ys: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s), None]),
      ]
  )
  cs: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s), None]),
      ]
  )
  ages: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )
  generations: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )
  ids: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )
  trial_ids: np.ndarray = attr.field(
      validator=[
          attr.validators.instance_of(np.ndarray),
          _shape_equals(lambda s: [len(s)]),
      ]
  )

  def __len__(self) -> int:
    return self.ys.shape[0]

  def __getitem__(
      self,
      index: Any,
     
  ) -> 'Population':
    return Population(**{k: v[index] for k, v in attr.asdict(self).items()})

  def __add__(self, other: 'Population') -> 'Population':
    return Population(
        _concat(self.xs, other.xs),
        _concat(self.ys, other.ys),
        _concat(self.cs, other.cs),
        _concat(self.ages, other.ages),
        _concat(self.generations, other.generations),
        _concat(self.ids, other.ids),
        _concat(self.trial_ids, other.trial_ids),
    )

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> 'Population':
    encoded = metadata.get('values', default='', cls=str)
    try:
      decoded = json.loads(encoded, object_hook=json_utils.numpy_hook)
    except json.JSONDecodeError as e:
      raise serializable.DecodeError('Failed to recover state.') from e
    return cls(**decoded)

  def dump(self) -> vz.Metadata:
    encoded = json.dumps(attr.asdict(self), cls=json_utils.NumpyEncoder)
    return vz.Metadata({'values': encoded})

  def empty_like(self) -> 'Population':
    """Creates an empty population that has the same shape as this."""

    return Population(
        **{
            k: np.zeros([0] + list(v.shape[1:]))
            for k, v in attr.asdict(self).items()
        }
    )


def _create_parameter_converters(
    search_space: vz.SearchSpace,
) -> Collection[converters.DefaultModelInputConverter]:
  """Returns parameter converters."""
  if search_space.is_conditional:
    raise ValueError('Cannot handle conditional search space!')

  def create_input_converter(
      pc: vz.ParameterConfig,
  ) -> converters.DefaultModelInputConverter:
    return converters.DefaultModelInputConverter(
        pc, scale=True, max_discrete_indices=0, onehot_embed=True
    )

  return [create_input_converter(pc) for pc in search_space.parameters]


def _create_metric_converter(
    mc: vz.MetricInformation,
) -> converters.DefaultModelOutputConverter:
  # TODO: Do something other than raising an error
  return converters.DefaultModelOutputConverter(
      mc,
      flip_sign_for_minimization_metrics=True,
      shift_safe_metrics=True,
      raise_errors_for_missing_metrics=True,
  )


class PopulationConverter(templates.PopulationConverter):
  """Population converter."""

  def __init__(
      self,
      search_space: vz.SearchSpace,
      metrics: Collection[vz.MetricInformation],
      *,
      metadata_ns: str = 'population',
      trial_converter: Optional[converters.DefaultTrialConverter] = None,
  ):
    self._objective_metrics, self._safe_metrics = _filter_and_split(metrics)
    self._num_objective_metrics = len(self._objective_metrics)
    self._num_safe_metrics = len(self._safe_metrics)
    self._metrics = self._objective_metrics + self._safe_metrics
    self._metadata_ns = metadata_ns

    self._trial_converter = trial_converter or converters.DefaultTrialConverter(
        _create_parameter_converters(search_space),
        [_create_metric_converter(mc) for mc in self._metrics],
    )
    self._empty_feature_dict = converters.DictOf2DArrays(
        self._trial_converter.to_features([])
    )

  def to_suggestions(  # pytype: disable=signature-mismatch  # overriding-parameter-type-checks
      self, offsprings: Offspring
  ) -> Collection[vz.TrialSuggestion]:
    parameters_list = self._trial_converter.to_parameters(
        self._empty_feature_dict.dict_like(offsprings.xs)
    )
    suggestions = [vz.TrialSuggestion(p) for p in parameters_list]
    for idx, t in enumerate(suggestions):
      t.metadata.ns(self._metadata_ns).update(offsprings[idx : idx + 1].dump())
    return suggestions

  def _empty_offsprings(self) -> Offspring:
    return Offspring(
        self._empty_feature_dict.asarray(), np.zeros([0]), np.zeros([0])
    )

  def to_population(self, completed: Sequence[vz.CompletedTrial]) -> Population:
    """Converts trials into population. Accepts an empty list."""
    offsprings = self._empty_offsprings()  # create empty
    # Each Trial should contain its genes as metadata. (Note that
    # genes-to-trial mapping is many-to-one). We try to load the genes.
    for t in completed:
      metadata = t.metadata.ns(self._metadata_ns)
      try:
        offsprings += Offspring.load(metadata)
      except serializable.DecodeError:
        # Upon failure, arbitrarily choose one set of genes that map to the
        # current trial.
        offsprings += Offspring(
            converters.DictOf2DArrays(
                self._trial_converter.to_features([t])
            ).asarray(),
            np.zeros([1]),
            np.zeros([1]),
        )

    ys = self._trial_converter.to_labels_array(completed)
    return Population(
        offsprings.xs,
        ys[:, : self._num_objective_metrics],
        ys[:, self._num_objective_metrics :],
        np.zeros([ys.shape[0]]),
        offsprings.generations,
        offsprings.ids,
        np.asarray([t.id for t in completed], dtype=np.int32),
    )


class UniformRandomSampler(templates.Sampler[Offspring]):
  """Generates uniformly random samples."""

  def __init__(self, search_space: vz.SearchSpace, seed: Optional[int] = None):
    if search_space.is_conditional:
      raise ValueError(f'{type(self)} does not support conditional spaces.')
    self._trial_converter = converters.DefaultTrialConverter(
        _create_parameter_converters(search_space)
    )
    self._dimension = sum(
        v[-1] for v in self._trial_converter.features_shape.values()
    )
    self._num_samples = 0
    self._rng = np.random.RandomState(seed)

  def sample(self, count: int) -> Offspring:
    self._num_samples += count
    return Offspring(
        self._rng.random([count, self._dimension]),
        ids=np.arange(self._num_samples - count, self._num_samples),
    )


class LinfMutation(templates.Mutation):
  """L-inf Mutations. Values are assumed to be in [0,1] range."""

  def __init__(self, norm: float = 0.1, seed: Optional[int] = None):
    self._norm = norm
    self._rng = np.random.RandomState(seed)

  def mutate(self, population: Population, count: int) -> Offspring:
    """Perturb by a uniform sample from l-inf ball.

    If the perturbation pushes a coordinate out of [0, 1] range, it "wraps
    around". This is likely suboptimal if optimum is at or near the boundary.

    Args:
      population:
      count:

    Returns:
      Offsprings.
    """
    # Mutate and truncate to [-5, 1.5] range. Perturbations should generally
    # NOT generate values outside this range.
    arr = population.xs.copy()
    arr += self._rng.uniform(-self._norm, self._norm, arr.shape)
    arr = np.maximum(np.minimum(arr, 1.5), -0.5)

    # When the value is outside [0, 1] range, wrap around the limits.
    # -0.2 becomes 0.8, 1.2 becomes 0.2, etc.
    arr = np.where(arr < 0, 1 + arr, np.where(arr > 1, arr - 1, arr))
    return Offspring(arr, population.ids, population.generations + 1)


--- vizier/_src/algorithms/evolution/numpy_populations_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import copy

from absl import logging
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.evolution import numpy_populations
from vizier._src.algorithms.testing import test_runners
from vizier.testing import test_studies

from absl.testing import absltest


class NumpyPopulationsTest(absltest.TestCase):

  def setUp(self):
    super().setUp()
    problem = vz.ProblemStatement(
        test_studies.flat_space_with_all_types(),
        metric_information=test_studies.metrics_all_unconstrained())
    converter = numpy_populations.PopulationConverter(
        problem.search_space, problem.metric_information)

    trials = test_runners.run_with_random_metrics(
        random.RandomDesigner(problem.search_space, seed=0),
        problem,
        iters=1,
        batch_size=10,
        validate_parameters=False)
    population = converter.to_population(trials)

    self._problem = problem
    self._converter = converter
    self._trials = trials
    self._population = population

  def test_convert_to_population(self):
    logging.info('Population: %s', self._population)
    self.assertLen(self._population, 10)

  def test_population_serialization(self):
    dumped = self._population.dump()
    redumped = self._population.recover(dumped).dump()
    self.assertEqual(dumped, redumped)

  def test_linf_mutation_no_state_modification(self):
    population_copy = copy.deepcopy(self._population)

    mutation = numpy_populations.LinfMutation()
    mutation.mutate(self._population, 0)

    population_copy_dict = attr.asdict(population_copy)
    population_dict = attr.asdict(self._population)
    for k, v in population_copy_dict.items():
      np.testing.assert_array_equal(v, population_dict[k], err_msg=k)

  def test_convert_from_offspring(self):
    """TODO."""

  def test_offspring_serialization(self):
    """TODO."""

  def test_uniform_random_sampler(self):
    """TODO."""

  def test_linf_mutation(self):
    """TODO."""


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/evolution/templates.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Generic interfaces and template for evolutionary algorithms.

The evolutionary algorithm and Vizier interaction workflow is:
  * Algorithm generates `Offsprings`, which contains information about the
  "genes". `PopulationConverter` converts them to `TrialSuggestions`.
  * User evaluates `TrialSuggestions` and obtains completed `Trial` objects.
  `PopulationConverter` converts them to `Population`, which is what the
  algorithm receives.
  * This template does not dictate the nature of these conversions. They can be
  stochastic, deterministic, bijective, injective, inverse of each other or not.

Why are Offsprings and Population serializable?
  * `Offsprings` is serializable so that the original genes can be preserved
  through the conversions.
  * `Population` is serializable so that the algorithm can serialize its state.

A canonical evolutionary algorithm consists of the following operations:
  * `Sampler` generates fresh `Offsprings` independent of `Population`.
    It is generally used for sampling an initial batch of `Offsprings`.
  * `Survival` selects a subset of `Population` to keep.
  * `Mutation` mutates `Population` to generate new `Offsprings`.

To use the provided template, one should implement a subclass of `Offsprings`,
`Population`, `PopulationConverter`, `Sampler`, `Survival`, and `Mutation`.
"""

import abc
from typing import Callable, Generic, Optional, Sequence, TypeVar, Union

from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier.interfaces import serializable

_OffspringsType = TypeVar('_OffspringsType', bound=serializable.Serializable)


class Sampler(Generic[_OffspringsType], abc.ABC):
  """Creates new offsprings, typically at the start of evolutionary strategy."""

  @abc.abstractmethod
  def sample(self, count: int) -> _OffspringsType:
    """Generate offsprings."""


# Temporary typevar, used for defininig Population interface.
_P = TypeVar('_P', bound=serializable.Serializable)


class Population(serializable.Serializable, abc.ABC):
  """Typically contains genes (x-values) and scores (y-values).

  The supported operations are similar to python sequence, except that slicing
  always results in a `Population` of length 1.
  """

  @abc.abstractmethod
  def __len__(self) -> int:
    pass

  @abc.abstractmethod
  def __getitem__(
      self: _P,
      index: Union[int, slice],
     
  ) -> _P:
    pass

  @abc.abstractmethod
  def __add__(self: _P, other: _P) -> _P:
    pass


# Actual Typevar. Covariant with `Population`.
_PopulationType = TypeVar('_PopulationType')


class PopulationConverter(abc.ABC, Generic[_PopulationType, _OffspringsType]):

  @abc.abstractmethod
  def to_population(self,
                    completed: Sequence[vz.CompletedTrial]) -> _PopulationType:
    """Adds trials to the population."""

  @abc.abstractmethod
  def to_suggestions(
      self, offsprings: _OffspringsType) -> Sequence[vz.TrialSuggestion]:
    """Converts offsprings to suggestions."""


class Survival(abc.ABC, Generic[_PopulationType]):

  @abc.abstractmethod
  def select(self, population: _PopulationType) -> _PopulationType:
    """Survival mechanism."""


class Mutation(abc.ABC, Generic[_PopulationType, _OffspringsType]):

  @abc.abstractmethod
  def mutate(self, population: _PopulationType, count: int) -> _OffspringsType:
    """Generate offsprings, whose contain count many offsprings."""


class CanonicalEvolutionDesigner(vza.PartiallySerializableDesigner,
                                 Generic[_PopulationType, _OffspringsType]):
  """Evolution algorithm template."""

  def __init__(
      self,
      converter: PopulationConverter[_PopulationType, _OffspringsType],
      sampler: Sampler[_OffspringsType],
      survival: Survival[_PopulationType],
      *,
      adaptation: Mutation[_PopulationType, _OffspringsType],
      adaptation_callable: Optional[
          Callable[[int], Mutation[_PopulationType, _OffspringsType]]
      ] = None,
      initial_population: Optional[_PopulationType] = None,
      first_survival_after: Optional[int] = None,
      population_size: int = 50,
  ):
    """Init.

    Args:
      converter:
      sampler:
      survival:
      adaptation: Default adaptation. Will be overwrote if adaptation_callable
        is specified.
      adaptation_callable: Adapation as a function of number of Trials seen.
      initial_population: The initial population to seed the evolution.
      first_survival_after: Apply the survival step after observing this many
        trials. If unset, it defaults to twice the `population_size`.
      population_size: Survival steps reduce the population to this size.
    """
    self._survival = survival
    self._adaptation = adaptation
    self._converter = converter
    self._sampler = sampler
    self._population_size = population_size
    self._adaptation_callable = adaptation_callable
    self._first_survival_after = (
        first_survival_after or self._population_size * 2
    )
    self._num_trials_seen = 0
    self._population = initial_population or converter.to_population([])

  @property
  def converter(self) -> PopulationConverter[_PopulationType, _OffspringsType]:
    return self._converter

  @property
  def population(self) -> _PopulationType:
    return self._population

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    count = count or self._population_size
    if self._num_trials_seen < self._first_survival_after:
      return self._converter.to_suggestions(self._sampler.sample(count))

    if self._adaptation_callable is not None:
      adaptation = self._adaptation_callable(self._num_trials_seen)
    else:
      adaptation = self._adaptation

    suggestions = self._converter.to_suggestions(
        adaptation.mutate(self._population, count)
    )
    return suggestions

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    completed = completed.trials
    self._num_trials_seen += len(completed)
    candidates = self._population + self._converter.to_population(completed)
    self._population = self._survival.select(candidates)

  def load(self, metadata: vz.Metadata):
    self._population = type(self._population).recover(metadata)

  def dump(self) -> vz.Metadata:
    return self._population.dump()


--- vizier/_src/algorithms/optimizers/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

# Optimizers that directly act on Vizier search spaces and trials.
# For jax optimizers that can be jit compiled, see vizier/_src/jax.


--- vizier/_src/algorithms/optimizers/base.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Base class for acquisition optimizers."""

import abc
from typing import Any, Callable, Optional, Protocol, Sequence

import attr
from vizier import pyvizier as vz

Array = Any


def _is_positive(instance, attribute, value):
  del instance, attribute
  if value < 0:
    raise ValueError(f'value must be positive! given: {value}')


class BatchTrialScoreFunction(Protocol):
  """Protocol (https://peps.python.org/pep-0544/) for scoring trials."""

  # TODO: Decide what to do with NaNs.
  def __call__(self, trials: Sequence[vz.Trial]) -> dict[str, Array]:
    """Evaluates the trials.

    Args:
      trials: A sequence of N trials

    Returns:
      A dict of shape (N, 1) arrays.
    """


@attr.s(frozen=False, init=True, slots=True)
class BranchSelection:
  """Branch Selection.

  Decision of a branch selector that contains the flat subspace and number
  of suggestions to be generated in that subspace.

  Instead of creating N suggestions on a conditional space S, we create
  N_1, ..., N_k suggestions on flat spaces S_1, .., S_k such that
  N_1 + ... + N_k = N and S_1, ..., S_k are contained in S. Each BranchSelection
  object represents (N_i, S_i) tuple.

  Attributres:
    search_space: Search space that does not contain any conditional parameters.
    num_suggestions: Number of suggestions to be made in the search space.
  """
  search_space: vz.SearchSpace = attr.ib(
      validator=attr.validators.instance_of(vz.SearchSpace),
      on_setattr=attr.setters.validate)
  num_suggestions: int = attr.ib(
      validator=[attr.validators.instance_of(int), _is_positive],
      on_setattr=attr.setters.validate)


class BranchSelector(abc.ABC):

  @abc.abstractmethod
  def select_branches(self, num_suggestions: int) -> list[BranchSelection]:
    pass


class GradientFreeOptimizer(abc.ABC):
  """Optimizes a function on Vizier search space.

  Typically used for optimizing acquisition functions.
  """

  @abc.abstractmethod
  def optimize(
      self,
      score_fn: BatchTrialScoreFunction,
      problem: vz.ProblemStatement,
      *,
      count: int = 1,
      budget_factor: float = 1.0,
      seed_candidates: Sequence[vz.TrialSuggestion] = tuple()
  ) -> list[vz.Trial]:
    """Optimizes a function.

    Args:
      score_fn: Should return a dict whose keys contain the metric names of
        "problem.metric_information".
      problem:
      count: Optimizer tries to return this many trials.
      budget_factor:  For optimizers with a notion of a budget, use this much
        fraction of the standard budget for the call.
      seed_candidates: Seed suggestions to be used as initial batch for
        optimization.

    Returns:
      Trials, of length less than or equal to max_num_suggestions.
      Trials are COMPLETED with score_fn results.
    """
    pass


@attr.frozen
class BranchThenOptimizer(GradientFreeOptimizer):
  """Optimizes a function by first choosing a branch and then apply Optimizer.

  Attributes:
    _branch_selector: Selects all conditional parent values
    _optimizer_factory: Creates an optimizer in the flat (non-conditional) after
      branch selector fixing all conditional parent values.
    max_num_suggestions_per_branch: Limits the number of suggestions per branch.
      This is useful when acquisition function isn't well-suited for batch
      suggestions.
  """
  _branch_selector: BranchSelector
  _optimizer_factory: Callable[[], GradientFreeOptimizer]
  max_num_suggestions_per_branch: Optional[int] = None

  def _num_suggestions_for_branch(self, branch: BranchSelection) -> int:
    if self.max_num_suggestions_per_branch is None:
      return branch.num_suggestions
    else:
      return min(self.max_num_suggestions_per_branch, branch.num_suggestions)

  def optimize(
      self,
      score_fn: BatchTrialScoreFunction,
      problem: vz.ProblemStatement,
      *,
      count: int = 1,
      budget_factor: float = 1.0,
      seed_candidates: Sequence[vz.TrialSuggestion] = tuple(),
  ) -> list[vz.Trial]:
    # If there are conditional branches, use Vizier's default branch
    # selection mechanism.
    branches = self._branch_selector.select_branches(count)
    suggestions = []
    optimizer = self._optimizer_factory()
    for branch in branches:
      subproblem = attr.evolve(problem, search_space=branch.search_space)
      suggestions.extend(
          optimizer.optimize(
              score_fn,
              subproblem,
              count=self._num_suggestions_for_branch(branch),
              budget_factor=budget_factor * (branch.num_suggestions / count)))
    return suggestions


--- vizier/_src/algorithms/optimizers/designer_optimizer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Wraps Designer as a gradient-free optimizer."""

from typing import Callable, List, TypeVar, Sequence

from absl import logging
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.core import abstractions
from vizier._src.algorithms.optimizers import base

_Features = TypeVar('_Features')


class DesignerAsOptimizer(base.GradientFreeOptimizer):
  """Wraps a Designer into GradientFreeOptimizer."""

  def __init__(self,
               designer_factory: Callable[[vz.ProblemStatement],
                                          abstractions.Designer],
               *,
               batch_size: int = 100,
               num_evaluations: int = 15000):
    """Init.

    Args:
      designer_factory:
      batch_size: In each iteration, ask the designer to generate this many
        candidate trials.
      num_evaluations: Total number of trials to be evaluated on `score_fn`.
    """
    self._designer_factory = designer_factory
    self._batch_size = batch_size
    self._num_evaluations = num_evaluations

  def optimize(
      self,
      score_fn: base.BatchTrialScoreFunction,
      problem: vz.ProblemStatement,
      *,
      count: int = 1,
      budget_factor: float = 1.0,
      seed_candidates: Sequence[vz.TrialSuggestion] = tuple(),
  ) -> List[vz.Trial]:
    # Use the in-ram supporter as a pseudo-client for running a study in RAM.
    study = pythia.InRamPolicySupporter(problem)

    # Save the designer for debugging purposes only.
    self._designer = self._designer_factory(problem)
    num_iterations = max(
        int(self._num_evaluations * budget_factor) // self._batch_size, 1)
    logging.info(
        'Optimizing the acquisition for %s iterations of %s trials each',
        num_iterations, self._batch_size)

    for _ in range(num_iterations):
      trials = study.AddSuggestions(self._designer.suggest(self._batch_size))
      if not trials:
        break
      scores = score_fn(trials)
      # Check that scores are (N, 1) arrays as in BatchTrialScoreFunction.
      for k, v in scores.items():
        if v.shape != (len(trials), 1):
          raise ValueError(
              f'Incorrect shape {v.shape} in scores {scores[k]}\n'
              f'Expected shape is {(len(trials), 1)}'
          )
      for i, trial in enumerate(trials):
        # TODO: Decide what to do with NaNs scores.
        trial.complete(
            vz.Measurement({k: v[i].item() for k, v in scores.items()}))
      self._designer.update(
          abstractions.CompletedTrials(trials), abstractions.ActiveTrials()
      )
    logging.info(
        'Finished running the optimization study. Extracting the best trials...'
    )
    return study.GetBestTrials(count=count)


--- vizier/_src/algorithms/optimizers/designer_optimizer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for designer_optimizer."""

from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.algorithms.evolution import nsga2
from vizier._src.algorithms.optimizers import designer_optimizer
from vizier._src.algorithms.testing import optimizer_test_utils

from absl.testing import absltest


class DesignerOptimizerTest(absltest.TestCase):

  def test_smoke(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('a', 0, 1)

    designer_factory = quasi_random.QuasiRandomDesigner.from_problem
    optimizer_test_utils.assert_passes_on_random_single_metric_function(
        self,
        problem.search_space,
        designer_optimizer.DesignerAsOptimizer(designer_factory),
        np_random_seed=1)

  def test_bi_objective(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('a', 0, 1)

    designer_factory = nsga2.NSGA2Designer
    optimizer_test_utils.assert_passes_on_random_multi_metric_function(
        self,
        problem.search_space,
        designer_optimizer.DesignerAsOptimizer(designer_factory),
        np_random_seed=1,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/optimizers/eagle_optimizer_convergence_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for eagle_optimizer."""

import logging
from typing import Optional

import jax
from jax import numpy as jnp
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import eagle_strategy
from vizier._src.algorithms.optimizers import random_vectorized_optimizer as rvo
from vizier._src.algorithms.testing import comparator_runner
from vizier._src.jax import types
from vizier.pyvizier import converters

from absl.testing import absltest
from absl.testing import parameterized


def create_continuous_problem(
    n_features: int,
    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:
  if not problem:
    problem = vz.ProblemStatement()
  root = problem.search_space.select_root()
  for i in range(n_features):
    root.add_float_param('x%d' % i, -5.0, 5.0)
  return problem


def create_categorical_problem(
    n_features: int,
    categorical_dim: int = 6,
    problem: Optional[vz.ProblemStatement] = None) -> vz.ProblemStatement:
  if not problem:
    problem = vz.ProblemStatement()
  root = problem.search_space.select_root()
  for i in range(n_features):
    root.add_categorical_param(
        'c%d' % i, feasible_values=[str(i) for i in range(categorical_dim)])
  return problem


def create_mix_problem(n_features: int,
                       categorical_dim: int = 8) -> vz.ProblemStatement:
  problem = create_continuous_problem(n_features // 2)
  return create_categorical_problem(n_features // 2, categorical_dim, problem)


# TODO: Change to bbob functions when they can support batching.
def sphere(x: types.ModelInput) -> jax.Array:
  return -(
      jnp.sum(jnp.square(x.continuous.padded_array), axis=-1)
      + 0.1 * jnp.sum(jnp.square(x.categorical.padded_array), axis=-1)
  )


def _rastrigin_d10_part(x: types.Array) -> jax.Array:
  return 10 * jnp.sum(jnp.cos(2 * np.pi * x), axis=-1) - jnp.sum(
      jnp.square(x), axis=-1
  )


def rastrigin_d10(x: types.ModelInput) -> jax.Array:
  return _rastrigin_d10_part(x.continuous.padded_array) + _rastrigin_d10_part(
      0.01 * x.categorical.padded_array
  )


class EagleOptimizerConvegenceTest(parameterized.TestCase):
  """Test optimizing an acquisition functions using vectorized Eagle Strategy.
  """

  @absltest.skip("Test takes too long externally.")
  @parameterized.product(
      create_problem_fn=[
          create_continuous_problem,
          create_categorical_problem,
          create_mix_problem,
      ],
      n_features=[10, 20],
      score_fn=[sphere, rastrigin_d10],
  )
  def test_converges(self, create_problem_fn, n_features, score_fn):
    logging.info('Starting a new convergence test (n_features: %s)', n_features)
    evaluations = 20_000
    problem = create_problem_fn(n_features)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    eagle_strategy_factory = eagle_strategy.VectorizedEagleStrategyFactory(
        eagle_config=eagle_strategy.EagleStrategyConfig())
    shifted_score_fn = score_fn
    random_strategy_factory = rvo.random_strategy_factory
    # Run simple regret convergence test.
    comparator_runner.SimpleRegretComparisonTester(
        baseline_num_trials=2 * evaluations,
        candidate_num_trials=evaluations,
        baseline_suggestion_batch_size=5,
        candidate_suggestion_batch_size=5,
        baseline_num_repeats=5,
        candidate_num_repeats=3,
        alpha=0.05,
        goal=vz.ObjectiveMetricGoal.MAXIMIZE,
    ).assert_optimizer_better_simple_regret(
        converter=converter,
        score_fn=shifted_score_fn,
        baseline_strategy_factory=random_strategy_factory,
        candidate_strategy_factory=eagle_strategy_factory,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/optimizers/eagle_strategy.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Eagle Strategy Optimizer.

Implements a variation of Eagle Strategy without the Levy random walk, aka
Firefly Algorithm (FA) for the purpose of optimizing a given objective function.

Reference: Yang XS. (2009) Firefly Algorithms for Multimodal Optimization.
In: Stochastic Algorithms: Foundations and Applications (SAGA) 2009.
DOI: https://doi.org/10.1007/978-3-642-04944-6_14

Firefly Algorithm Summary
=========================
FA is a genetic algorithm that maintains a pool of fireflies. Each
firefly emits a light whose intensity is non-decreasing in (or simply equal
to) the objective value. Each iteration, a firefly chases after a brighter
firefly, but the brightness it perceives decreases in distance. This allows
multiple "clusters" to form, as opposed to all fireflies collapsing to a
single point. Not included in the original algorithm, we added "repulsion" which
in addition to the "attraction" forces, meaning fireflies move towards the
bright spots as well as away from the dark spots. We also support non-decimal
parameter types (categorical, discrete, integer), and treat them uniquely when
computing distance, applying pertrubation, and mutating fireflies.

For more details, see the linked paper.

OSS Vizier Implementation Summary
=================================
The fireflies are stored in three JAX arrays: features, rewards, perturbations.
Each iteration we mutate 'batch_size' fireflies to generate new features. The
new features are evaluated on the objective function to obtain their associated
rewards and update the pool where improvement was obtained, and decrease the
perturbation factor otherwise.

If the firefly's perturbation reaches the `perturbation_lower_bound` threshold
it's removed and replaced with new random features.

For performance consideration, the 'pool size' is a multiplier of the
'batch size', and so each iteration the pool is sliced to obtain the current
fireflies to be mutated.

Passing prior trials is supported for knowledge-transfering from previous runs
of the optimizer. Prior trials are used to populate the pool with fireflies
which are closer to the optimum.

Note that the strategy assumes that the search space is not conditional.

Example
=======
# Construct the optimizer with eagle strategy.
optimizer = VectorizedOptimizerFactory(
    VectorizedEagleStrategyFactory(),
)
# Run the optimization.
trials = optimizer.optimize(problem_statement, objective_function)
"""

import dataclasses
# pylint: disable=g-long-lambda

import enum
import logging
import math
from typing import Callable, Optional, Tuple

import attr
from flax import struct
import jax
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import types
from vizier.pyvizier import converters

tfd = tfp.distributions


@enum.unique
class MutateNormalizationType(enum.IntEnum):
  """The force normalization mode. Use IntEnum for JIT compatibility."""

  MEAN = 0
  RANDOM = 1
  UNNORMALIZED = 2


@enum.unique
class ContinuousFeaturePerturbationType(enum.IntEnum):
  """The type of perturbation to apply to continuous features."""

  # Add the perturbation to the feature value.
  ADDITIVE = 0
  # Multiply the feature value by exp(perturbation).
  MULTIPLICATIVE = 1


@struct.dataclass
class EagleStrategyConfig:
  """Eagle Strategy optimizer config.

  Attributes:
    visibility: The sensetivity to distance between flies when computing pulls.
    gravity: The maximum amount of attraction pull.
    negative_gravity: The maximum amount of repulsion pull.
    perturbation: The default amount of noise for perturbation.
    continuous_feature_perturbation_type: The type of perturbation to apply to
      continuous features.
    categorical_perturbation_factor: A factor to apply on categorical params.
    pure_categorical_perturbation_factor: A factor on purely categorical space.
    prob_same_category_without_perturbation: Baseline probability of selecting
      the same category.
    perturbation_lower_bound: The threshold below flies are removed from pool.
    penalize_factor: The perturbation decrease for unsuccessful flies.
    pool_size_exponent: An exponent for computing pool size based on search
      space size.
    pool_size: An optional way to set the pool size. If not 0, this takes
      precedent over the programatic pool size computation.
    max_pool_size: Ceiling on pool size.
    mutate_normalization_type: The type of force mutation normalizatoin used for
      damping down the force intensity to stay within reasonable bounds.
    normalization_scale: A mutation force scale-factor to control intensity.
    prior_trials_pool_pct: The percentage of the pool populated by prior trials.
  """

  # Visibility
  visibility: float = 0.45
  # Gravity
  gravity: float = 1.5
  negative_gravity: float = 0.008
  # Perturbation
  perturbation: float = 0.16
  continuous_feature_perturbation_type: ContinuousFeaturePerturbationType = (
      ContinuousFeaturePerturbationType.ADDITIVE
  )
  categorical_perturbation_factor: float = 1.0
  pure_categorical_perturbation_factor: float = 30
  prob_same_category_without_perturbation: float = 0.98
  # Penalty
  perturbation_lower_bound: float = 7e-5
  penalize_factor: float = 7e-1
  # Pool size
  pool_size_exponent: float = struct.field(pytree_node=False, default=1.2)
  pool_size: int = struct.field(pytree_node=False, default=0)
  max_pool_size: int = struct.field(pytree_node=False, default=100)
  # Force normalization mode
  mutate_normalization_type: MutateNormalizationType = struct.field(
      pytree_node=False,
      default=MutateNormalizationType.MEAN,
  )
  # Multiplier factor when using normalized modes
  normalization_scale: float = 0.5
  # The percentage of the firefly pool to be populated with prior trials
  prior_trials_pool_pct: float = struct.field(pytree_node=False, default=0.96)


@dataclasses.dataclass(frozen=True)
class FeatureDimensions:
  """Dimensions of the features used by Eagle."""

  # The number of categories for each categorical feature.
  categorical_sizes: list[int]
  # The number of feature dimensions with feature padding.
  n_feature_dimensions_with_padding: types.ContinuousAndCategorical[int]
  # The number of feature dimensions without feature padding.
  n_feature_dimensions: types.ContinuousAndCategorical[int]

  @property
  def max_categorical_size(self) -> int:
    """The maximum number of categories across the categorical features."""
    return max(self.categorical_sizes) if self.categorical_sizes else 0


def compute_feature_dimensions_from_converter(
    converter: converters.TrialToModelInputConverter,
) -> FeatureDimensions:
  """Computes feature dimensions for Eagle from a converter.

  Args:
    converter: A converter from trials to the input to the acquisition function.

  Returns:
    A FeatureDimensions object that contains the feature dimensions information
    for Eagle.
  """
  empty_features = converter.to_features([])
  n_feature_dimensions_with_padding = types.ContinuousAndCategorical[int](
      continuous=empty_features.continuous.shape[-1],
      categorical=empty_features.categorical.shape[-1],
  )
  n_feature_dimensions = types.ContinuousAndCategorical(
      continuous=len(converter.output_specs.continuous),
      categorical=len(converter.output_specs.categorical),
  )

  categorical_sizes = []
  for spec in converter.output_specs.categorical:
    categorical_sizes.append(spec.bounds[1])
  extra_categories = (
      n_feature_dimensions_with_padding.categorical
      - n_feature_dimensions.categorical
  )
  # Pads the categorical sizes with 0s to match the number of categorical
  # features with padding.
  categorical_sizes = categorical_sizes + [0] * extra_categories
  return FeatureDimensions(
      categorical_sizes=categorical_sizes,
      n_feature_dimensions_with_padding=n_feature_dimensions_with_padding,
      n_feature_dimensions=n_feature_dimensions,
  )


class DefaultProjection(vb.Projection):
  """Default projection for vectorized optimizer."""

  def __init__(
      self,
      converter: converters.TrialToModelInputConverter,
  ):
    # Extracts the bounds of the continuous features from the converter.
    continuous_features_min = [
        float(spec.bounds[0]) for spec in converter.output_specs.continuous
    ]
    continuous_features_max = [
        float(spec.bounds[1]) for spec in converter.output_specs.continuous
    ]
    empty_features = converter.to_features([])
    continuous_feature_dimensions_with_padding = (
        empty_features.continuous.shape[-1]
    )
    bounds = []
    # Pad the bounds with fill-values to the length of the feature dimension
    # with padding.
    for bound, fill_value in (
        (continuous_features_min, 0.0),
        (continuous_features_max, 1.0),
    ):
      padded_bound = bound + [fill_value] * (
          continuous_feature_dimensions_with_padding - len(bound)
      )
      bounds.append(jnp.array(padded_bound))
    self._bounds = tuple(bounds)

  def __call__(
      self,
      x: vb.VectorizedOptimizerInput,
  ) -> vb.VectorizedOptimizerInput:
    return vb.VectorizedOptimizerInput(
        continuous=jnp.clip(
            x.continuous,
            jnp.broadcast_to(self._bounds[0], x.continuous.shape),
            jnp.broadcast_to(self._bounds[1], x.continuous.shape),
        ),
        categorical=x.categorical,
    )


class DefaultRandomSampler(vb.RandomSampler):
  """Default random sampler for vectorized optimizer."""

  def __init__(
      self,
      converter: converters.TrialToModelInputConverter,
  ):
    feature_dimensions = compute_feature_dimensions_from_converter(converter)
    self._max_categorical_size = feature_dimensions.max_categorical_size
    self._categorical_sizes = feature_dimensions.categorical_sizes
    self._continuous_padded_dim = (
        feature_dimensions.n_feature_dimensions_with_padding.continuous
    )

  def __call__(
      self,
      num_samples: int,
      n_parallel: int,
      seed: jax.Array,
  ) -> vb.VectorizedOptimizerInput:
    cont_seed, cat_seed = jax.random.split(seed)

    if self._max_categorical_size > 0:
      # Appends a new axis to `self._categorical_sizes` to make it broadcastable
      # with the shape of jnp.arange(self._max_categorical_size) in the
      # comparison below.
      sizes = jnp.array(self._categorical_sizes)[:, jnp.newaxis]
      logits = jnp.where(
          jnp.arange(self._max_categorical_size) < sizes,
          0.0,
          -jnp.inf,
      )  # shape (sizes.shape[0], max_categorical_size)
      random_categorical_features = (
          tfd.Categorical(logits=logits)
          .sample((num_samples, n_parallel), seed=cat_seed)
          .astype(types.INT_DTYPE)
      )
    else:
      random_categorical_features = jnp.zeros(
          [num_samples, n_parallel, 0], types.INT_DTYPE
      )
    return types.ContinuousAndCategoricalArray(
        continuous=jax.random.uniform(
            cont_seed,
            shape=(
                num_samples,
                n_parallel,
                self._continuous_padded_dim,
            ),
        ),
        categorical=random_categorical_features,
    )


@attr.define(frozen=True)
class VectorizedEagleStrategyFactory(vb.VectorizedStrategyFactory):
  """Eagle strategy factory."""

  eagle_config: EagleStrategyConfig = attr.field(factory=EagleStrategyConfig)
  random_sampler_factory: Callable[
      [converters.TrialToModelInputConverter], vb.RandomSampler
  ] = attr.field(default=DefaultRandomSampler)
  projection_factory: Callable[
      [converters.TrialToModelInputConverter], vb.Projection
  ] = attr.field(default=DefaultProjection)

  def __call__(
      self,
      converter: converters.TrialToModelInputConverter,
      suggestion_batch_size: Optional[int] = None,
  ) -> "VectorizedEagleStrategy":
    """Create a new vectorized eagle strategy.

    In order to create the strategy a converter has to be passed, which the
    strategy will then store a pointer to. The strategy uses the converter to
    get information about the original Vizier Parameters and their relation to
    the array indices (which index belong to which Vizier Parameter). This is
    useful for example for sampling CATEGORICAL features.

    Arguments:
      converter: TrialToArrayConverter that matches the converter used in
        computing the objective / acuisition function.
      suggestion_batch_size: The batch_size of the returned suggestion array.

    Returns:
      A new instance of VectorizedEagleStrategy.
    """
    valid_types = [
        converters.NumpyArraySpecType.DISCRETE,
        converters.NumpyArraySpecType.CONTINUOUS,
    ]
    if any(
        spec.type not in valid_types
        for spec in (
            list(converter.output_specs.continuous)
            + list(converter.output_specs.categorical)
        )
    ):
      raise ValueError("Only DISCRETE/CONTINUOUS parameters are supported!")

    feature_dimensions = compute_feature_dimensions_from_converter(converter)
    n_features = (
        feature_dimensions.n_feature_dimensions.continuous
        + feature_dimensions.n_feature_dimensions.categorical
    )
    pool_size = self.eagle_config.pool_size
    if pool_size == 0:
      pool_size = 10 + int(
          0.5 * n_features + n_features**self.eagle_config.pool_size_exponent
      )
      pool_size = min(pool_size, self.eagle_config.max_pool_size)
      if suggestion_batch_size is not None:
        # If the batch_size was set, ensure pool_size is multiply of batch_size.
        pool_size = int(
            math.ceil(pool_size / suggestion_batch_size) * suggestion_batch_size
        )
    logging.info("Pool size: %d", pool_size)
    if suggestion_batch_size is None:
      # This configuration updates all the fireflies in each iteration.
      suggestion_batch_size = pool_size
    # Use priors to populate Eagle state
    # pytype: disable=wrong-arg-types  # jnp-type
    return VectorizedEagleStrategy(
        n_feature_dimensions=feature_dimensions.n_feature_dimensions,
        n_feature_dimensions_with_padding=(
            feature_dimensions.n_feature_dimensions_with_padding
        ),
        random_sampler=self.random_sampler_factory(converter),
        projection=self.projection_factory(converter),
        batch_size=suggestion_batch_size,
        config=self.eagle_config,
        pool_size=pool_size,
        categorical_sizes=jnp.array(feature_dimensions.categorical_sizes),
        max_categorical_size=feature_dimensions.max_categorical_size,
        dtype=converter._impl.dtype,
    )
    # pytype: enable=wrong-arg-types


@struct.dataclass
class VectorizedEagleStrategyState:
  """Container for Eagle strategy state."""

  iterations: jax.Array  # Scalar integer.
  features: vb.VectorizedOptimizerInput  # (pool_size, n_parallel, n_features).
  rewards: jax.Array  # Shape (pool_size,).
  best_reward: jax.Array  # Scalar float.
  perturbations: jax.Array  # Shape (pool_size,).


def _compute_features_dist_squared(
    x_batch: vb.VectorizedOptimizerInput, x_pool: vb.VectorizedOptimizerInput
) -> jax.Array:
  """Computes squared distance between features (or parallel feature batches).

  The distance computed is the squared Euclidean distance between each feature
  in the batch and each feature in the pool.

  To avoid materializing an array of size (batch_size, pool_size, n_features),
  the distances are computed efficiently using the formula (a-b)**2 = a**2 +
  b**2 - 2ab and broadcasting.

  Reference:
  https://github.com/tensorflow/probability/blob/r0.24/tensorflow_probability/python/math/psd_kernels/internal/util.py#L191

  Args:
    x_batch: A batch of features to be compared to the pool.
    x_pool: The pool of features to be compared to the batch.

  Returns:
    Array of size (batch_size, pool_size) with the squared distances.
  """
  dist = jnp.zeros([], dtype=x_batch.continuous.dtype)
  if x_batch.continuous.size > 0:
    x_batch_cont = jnp.reshape(
        x_batch.continuous, (x_batch.continuous.shape[0], -1)
    )
    x_pool_cont = jnp.reshape(
        x_pool.continuous, (x_pool.continuous.shape[0], -1)
    )
    continuous_dists = (
        jnp.sum(x_batch_cont**2, axis=-1, keepdims=True)
        + jnp.sum(x_pool_cont**2, axis=-1)
        - 2.0 * jnp.matmul(x_batch_cont, x_pool_cont.T)
    )  # shape (batch_size, pool_size)
    dist = dist + continuous_dists

  if x_batch.categorical.size > 0:
    x_batch_cat = jnp.reshape(
        x_batch.categorical, (x_batch.categorical.shape[0], -1)
    )
    x_pool_cat = jnp.reshape(
        x_pool.categorical, (x_pool.categorical.shape[0], -1)
    )
    categorical_diffs = (x_batch_cat[..., jnp.newaxis, :] != x_pool_cat).astype(
        x_batch.continuous.dtype
    )
    dist = dist + jnp.sum(categorical_diffs, axis=-1)
  return dist


def _mask_flip(
    prior_features: vb.VectorizedOptimizerInput, prior_rewards: types.Array
) -> Tuple[vb.VectorizedOptimizerInput, types.Array]:
  """Flips the ordering of the elements in `prior_rewards` and `prior_features`.

  Args:
    prior_features: Prior features to be flipped.
    prior_rewards: Prior rewards to be flipped.

  Returns:
    A tuple of flipped prior features and prior rewards such that all elements
    corresponding to -inf entries in `prior_rewards` are at the end, while all
    other elements have the opposite order. For example, if `prior_rewards` is
    [1, -jnp.inf, 3, -jnp.inf ,2],  `flipped_prior_rewards` will be
    [2, 3, 1, -jnp.inf, -jnp.inf].
  """
  mask = jnp.invert(jnp.isneginf(prior_rewards))
  indices = jnp.flip(
      jnp.argsort(jnp.where(mask, jnp.arange(prior_rewards.shape[0]), -1))
  )
  flipped_prior_features = jax.tree_util.tree_map(
      lambda x: x[indices], prior_features
  )
  flipped_prior_rewards = prior_rewards[indices]
  return flipped_prior_features, flipped_prior_rewards


@struct.dataclass
class VectorizedEagleStrategy(
    vb.VectorizedStrategy[VectorizedEagleStrategyState]
):
  """Eagle strategy implementation for maximization problem based on Numpy.

  Attributes:
    config: The Eagle strategy configuration.
    n_features: The number of features.
    batch_size: The number of suggestions generated at each suggestion call.
    pool_size: The total number of flies in the pool.
  """

  n_feature_dimensions: types.ContinuousAndCategorical[int]
  categorical_sizes: jax.Array
  n_feature_dimensions_with_padding: types.ContinuousAndCategorical[int] = (
      struct.field(pytree_node=False)
  )
  max_categorical_size: int = struct.field(pytree_node=False)
  pool_size: int = struct.field(pytree_node=False)
  dtype: jnp.dtype = struct.field(pytree_node=False)
  random_sampler: vb.RandomSampler = struct.field(pytree_node=False)
  projection: vb.Projection = struct.field(pytree_node=False)
  batch_size: Optional[int] = struct.field(pytree_node=False, default=None)
  config: EagleStrategyConfig = struct.field(
      default_factory=EagleStrategyConfig
  )

  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[vb.VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ) -> VectorizedEagleStrategyState:
    """Initializes the state."""
    if prior_features is not None and prior_rewards is not None:
      if prior_features.continuous.shape[1] != n_parallel:
        raise ValueError(
            "`prior_features.continuous` dimension 1 "
            f"({prior_features.continuous.shape[1]}) "
            f"doesn't match n_parallel ({n_parallel})!"
        )
      if prior_features.categorical.shape[1] != n_parallel:
        raise ValueError(
            "`prior_features.categorical` dimension 1 "
            f"({prior_features.categorical.shape[1]}) "
            f"doesn't match n_parallel ({n_parallel})!"
        )
      init_features = self._populate_pool_with_prior_trials(
          seed, prior_features, prior_rewards
      )
    else:
      init_features = self.random_sampler(
          self.pool_size,
          n_parallel=n_parallel,
          seed=seed,
      )
    # pytype: disable=wrong-arg-types  # jnp-type
    return VectorizedEagleStrategyState(
        iterations=jnp.array(0),
        features=init_features,
        rewards=jnp.ones(self.pool_size) * -jnp.inf,
        best_reward=-jnp.inf,
        perturbations=jnp.ones(self.pool_size) * self.config.perturbation,
    )
    # pytype: enable=wrong-arg-types

  def _populate_pool_with_prior_trials(
      self,
      seed: jax.Array,
      prior_features: types.ContinuousAndCategoricalArray,
      prior_rewards: types.Array,
  ) -> types.ContinuousAndCategoricalArray:
    """Populate the pool with prior trials.

    A portion of the pool is first populated with random features based on
    'prior_trials_pool_pct', then the rest of the flies are populated by
    sequentially iterate over the prior trials, finding the cloest firefly in
    the pool and replace it if the reward is better.

    Args:
      seed: Random seed.
      prior_features: (n_prior_features, n_parallel, features_count)
      prior_rewards: (n_prior_features, )

    Returns:
      initial_features
    """
    if prior_features is None or prior_rewards is None:
      raise ValueError("One of prior features / prior rewards wasn't provided!")

    if prior_features.continuous is not None:
      continuous_obs, _, continuous_dim = prior_features.continuous.shape
      if continuous_obs != prior_rewards.shape[0]:
        raise ValueError(
            f"prior continuous features shape ({continuous_obs}) doesn't match"
            f" prior rewards shape ({prior_rewards.shape[0]})!"
        )
      expected_dim = self.n_feature_dimensions_with_padding.continuous
      if continuous_dim != expected_dim:
        raise ValueError(
            f"prior continuous features shape ({continuous_dim}) doesn't match "
            f"n_features {expected_dim}!"
        )
    if prior_features.categorical is not None:
      categorical_obs, _, categorical_dim = prior_features.categorical.shape
      if categorical_obs != prior_rewards.shape[0]:
        raise ValueError(
            f"prior categorical features shape ({categorical_obs}) doesn't "
            f"match prior rewards shape ({prior_rewards.shape[0]})!"
        )
      expected_dim = self.n_feature_dimensions_with_padding.categorical
      if categorical_dim != expected_dim:
        raise ValueError(
            f"prior categorical features shape ({categorical_dim}) doesn't"
            f" match n_features {expected_dim}!"
        )
    if len(prior_rewards.shape) > 1:
      raise ValueError("prior rewards is expected to be 1D array!")

    # Reverse the order of prior trials to assign more weight to recent trials.
    flipped_prior_features, flipped_prior_rewards = _mask_flip(
        prior_features, prior_rewards
    )

    n_parallel = flipped_prior_features.continuous.shape[1]

    pool_random_features = self.random_sampler(
        self.pool_size,
        n_parallel,
        seed,
    )
    n_random_flies = int(
        self.pool_size * (1 - self.config.prior_trials_pool_pct)
    )
    # The pool is configured to have at least this many random trials.
    init_features = jax.tree_util.tree_map(
        lambda x: x[:n_random_flies, :, :], pool_random_features
    )
    pool_left_space = self.pool_size - n_random_flies
    # When the number of prior trials is smaller than configured, we fill in
    # random trials.
    random_features = jax.tree_util.tree_map(
        lambda x: x[n_random_flies:, :, :], pool_random_features
    )

    # Starts with the most recent `pool_left_space` prior trials as the chosen
    # set of trials, and loops through the remaining prior trials. At each
    # iteration, if the remaining trial has a better reward than its closest
    # neighbor in the chosen set, it replaces the closest neighbor. Note that
    # `pool_left_space` can be larger than the number of prior trials, in which
    # case the chosen set contains all prior trials.
    features = jax.tree_util.tree_map(
        lambda x: x[:pool_left_space], flipped_prior_features
    )
    rewards = flipped_prior_rewards[:pool_left_space]

    def _loop_body(i, args):
      features, rewards = args
      ind = jnp.argmin(
          _compute_features_dist_squared(
              jax.tree_util.tree_map(
                  lambda x: x[i][jnp.newaxis],
                  flipped_prior_features,
              ),
              features,
          ),
          axis=-1,
      )[0]
      return jax.lax.cond(
          rewards[ind] < flipped_prior_rewards[i],
          lambda: (
              jax.tree_util.tree_map(
                  lambda f, pf: f.at[ind].set(pf[i]),
                  features,
                  flipped_prior_features,
              ),
              rewards.at[ind].set(flipped_prior_rewards[i]),
          ),
          lambda: (features, rewards),
      )

    # TODO: Use a vectorized method to populate the pool and avoid
    # the for-loop.
    features, _ = jax.lax.fori_loop(
        lower=pool_left_space,
        upper=prior_rewards.shape[0],
        body_fun=_loop_body,
        init_val=(features, rewards),
    )

    num_chosen_trials = rewards.shape[0]
    # Replaces padded trials in the chosen trials with random trials.
    features = jax.tree_util.tree_map(
        lambda x, y: jnp.where(
            jnp.isneginf(rewards)[:, jnp.newaxis, jnp.newaxis],
            x[:num_chosen_trials, :, :],
            y,
        ),
        random_features,
        features,
    )
    # Then ensures the chosen set of trials has exactly `pool_left_space` trials
    # by filling in random trials.
    features = jax.tree_util.tree_map(
        lambda x, y: jnp.concatenate([x, y[num_chosen_trials:, :, :]]),
        features,
        random_features,
    )

    return jax.tree_util.tree_map(
        lambda x, y: jnp.concatenate([x, y]), init_features, features
    )

  @property
  def suggestion_batch_size(self) -> int | None:
    """The number of suggestions returned at each call of 'suggest'."""
    return self.batch_size

  def suggest(
      self,
      seed: jax.Array,
      state: VectorizedEagleStrategyState,
      n_parallel: int = 1,
  ) -> vb.VectorizedOptimizerInput:
    """Suggest new mutated and perturbed features.

    After initializing, at each call a `batch_size` of firefiles is taken
    from the pool and mutated to generate new features. The batch of fireflies
    is chosen based on the current iteration number. The mutation is done
    by applying pulling/pushing forces from all the fireflies in the pool.

    The update rule is:
    x' <-- x' + gravity * sum_i exp(-visibility * ||x'-x_i||^2) (x_i-x') + noise

    Args:
      seed: Random seed.
      state: Current strategy state.
      n_parallel: Number of points that the acquisition function maps to a
        single value. This arg may be greater than 1 if a parallel acquisition
        function (qEI, qUCB) is used; otherwise it should be 1.

    Returns:
      suggested batch features: (batch_size, n_parallel, n_features)
    """
    # Take the batch of fireflies based on which new features are generated.
    batch_id = state.iterations % (self.pool_size // self.batch_size)
    start = batch_id * self.batch_size
    features_batch = jax.tree_util.tree_map(
        lambda f: jax.lax.dynamic_slice_in_dim(f, start, self.batch_size),
        state.features,
    )
    rewards_batch = jax.lax.dynamic_slice_in_dim(
        state.rewards, start, self.batch_size
    )
    perturbations_batch = jax.lax.dynamic_slice_in_dim(
        state.perturbations, start, self.batch_size
    )
    features_seed, perturbations_seed = jax.random.split(seed)

    def _mutate_features(features_batch_):
      perturbations = self._create_random_perturbations(
          perturbations_batch, n_parallel, perturbations_seed
      )
      return self._create_features(
          state.features,
          state.rewards,
          features_batch_,
          rewards_batch,
          perturbations,
          features_seed,
      )

    # If the strategy is still initializing, return the random/prior features.
    new_features = jax.lax.cond(
        state.iterations < self.pool_size // self.batch_size,
        lambda x: x,
        _mutate_features,
        features_batch,
    )

    return self.projection(new_features)

  def _create_features(
      self,
      features: vb.VectorizedOptimizerInput,
      rewards: jax.Array,
      features_batch: vb.VectorizedOptimizerInput,
      rewards_batch: jax.Array,
      perturbations_batch: types.ContinuousAndCategoricalArray,
      seed: jax.Array,
  ) -> vb.VectorizedOptimizerInput:
    """Create new batch of mutated and perturbed features.

    The combined forces (pull/push) induced by the firefly pool is normalized to
    ensure it's contained and doesn't throw the firefly too far. Mathematically,
    the normalization guarantees that the combined normalized force is within
    the simplex spanned by the unnormalized forces and therefore within bounds.

    Args:
      features: (pool_size, n_parallel, n_features)
      rewards: (pool_size,)
      features_batch: (batch_size, n_parallel, n_features)
      rewards_batch: (batch_size,)
      perturbations_batch: (batch_size,)
      seed: Random seed.

    Returns:
      batch features: (batch_size, n_parallel, n_features)
    """
    # Compute the pairwise squared distances between the batch and the
    # pool features.
    dists = _compute_features_dist_squared(
        features_batch, features
    )  # shape: (batch_size, pool_size)

    # Compute the scaled direction for applying pull between two flies.
    # scaled_directions[i,j] := direction of force applied by fly 'j' on fly
    # 'i'. Note that to compute 'directions' we might perform subtract with
    # removed flies having value of -np.inf. We might even subtract between two
    # removed flies which will result in np.nan. Both cases are handled when
    # computing the actual feature changes by applying a relevant mask.
    directions = rewards - rewards_batch[:, jnp.newaxis]
    scaled_directions = jnp.where(
        directions >= 0.0, self.config.gravity, -self.config.negative_gravity
    )  # shape: (batch_size, pool_size)

    # Get the number of non-padded features.
    n_feature_dimensions = sum(
        jax.tree_util.tree_leaves(self.n_feature_dimensions)
    )
    # Normalize the distance by the number of feature dimensions.
    force = jnp.exp(
        -self.config.visibility * dists / n_feature_dimensions * 10.0
    )
    # Compute the directed force each pool firefly applies on batch fireflies.
    scaled_force = scaled_directions * force  # shape: (batch_size, pool_size)
    # Handle removed fireflies without updated rewards.
    finite_ind = jnp.isfinite(rewards).astype(scaled_force.dtype)
    # Ignore fireflies that were removed from the pool.
    scaled_force = scaled_force * finite_ind
    # Separate forces to pull and push so to normalize them separately.
    scaled_pulls = jnp.maximum(scaled_force, 0.0)
    scaled_push = jnp.minimum(scaled_force, 0.0)

    seed, categorical_seed = jax.random.split(seed)
    if self.config.mutate_normalization_type == MutateNormalizationType.MEAN:
      # Divide the push / pull forces by the number of participating fireflies.
      # Also multiply by normalization_scale.
      # pytype: disable=wrong-arg-types  # jnp-type
      norm_scaled_pulls = self.config.normalization_scale * jnp.nan_to_num(
          scaled_pulls / jnp.sum(scaled_pulls > 0.0, axis=1, keepdims=True), 0
      )
      norm_scaled_push = self.config.normalization_scale * jnp.nan_to_num(
          scaled_push / jnp.sum(scaled_push < 0.0, axis=1, keepdims=True), 0
      )
      # pytype: enable=wrong-arg-types
    elif self.config.mutate_normalization_type == (
        MutateNormalizationType.RANDOM
    ):
      # Create random matrices and normalize each row, s.t. the sum is 1.
      pull_seed, push_seed = jax.random.split(seed)
      scaled_pulls_pos = scaled_pulls > 0
      pull_rand_matrix = (
          jax.random.uniform(pull_seed, shape=scaled_pulls.shape)
          * scaled_pulls_pos
      )
      pull_weight_matrix = pull_rand_matrix / jnp.sum(
          pull_rand_matrix, axis=1, keepdims=True
      )
      push_rand_matrix = (
          jax.random.uniform(push_seed, shape=scaled_pulls.shape)
          * scaled_pulls_pos
      )
      push_weight_matrix = push_rand_matrix / jnp.sum(
          push_rand_matrix, axis=1, keepdims=True
      )
      # Normalize pulls/pulls by the weight matrices and multiply by
      # normalization_scale.
      norm_scaled_pulls = (
          self.config.normalization_scale * scaled_pulls * pull_weight_matrix
      )
      norm_scaled_push = (
          self.config.normalization_scale * scaled_push * push_weight_matrix
      )
    elif self.config.mutate_normalization_type == (
        MutateNormalizationType.UNNORMALIZED
    ):
      # Doesn't normalize the forces. Use this option with caution.
      norm_scaled_pulls = scaled_pulls
      norm_scaled_push = scaled_push
    else:
      raise ValueError(
          "Unsupported mutate normalization type:"
          f" {self.config.mutate_normalization_type}"
      )

    # Sums the normalized forces (pull/push) of all fireflies.
    scale = norm_scaled_pulls + norm_scaled_push
    flat_features = jnp.reshape(
        features.continuous, (features.continuous.shape[0], -1)
    )  # shape: (pool_size, n_features)
    flat_features_batch = jnp.reshape(
        features_batch.continuous, (features_batch.continuous.shape[0], -1)
    )  # shape: (batch_size, n_features)

    # Compute the feature changes without materializing a large matrix:
    # batch_features_i += sum_j (scale_{ij} * (features_j - batch_features_i))
    # shape: (batch_size, pool_size) @ (pool_size, n_features) - (batch_size,
    # n_features) * (batch_size, 1).
    features_changes_continuous = jnp.matmul(
        scale, flat_features
    ) - flat_features_batch * jnp.sum(scale, axis=-1, keepdims=True)
    moved_features_continuous = features_batch.continuous + jnp.reshape(
        features_changes_continuous, features_batch.continuous.shape
    )
    if (
        self.config.continuous_feature_perturbation_type
        == ContinuousFeaturePerturbationType.ADDITIVE
    ):
      new_features_continuous = (
          moved_features_continuous + perturbations_batch.continuous
      )
    elif (
        self.config.continuous_feature_perturbation_type
        == ContinuousFeaturePerturbationType.MULTIPLICATIVE
    ):
      new_features_continuous = moved_features_continuous * jnp.exp(
          perturbations_batch.continuous
      )
    else:
      raise ValueError(
          "Unsupported continuous feature perturbation type:"
          f" {self.config.continuous_feature_perturbation_type}"
      )
    if self.max_categorical_size > 0:
      features_categorical_logits = (
          self._create_categorical_feature_logits(
              features.categorical, features_batch.categorical, scale
          )
          + perturbations_batch.categorical
      )
      new_features_categorical = tfd.Categorical(
          logits=features_categorical_logits
      ).sample(seed=categorical_seed)
    else:
      new_features_categorical = jnp.zeros(
          features_batch.continuous.shape[:2] + (0,), dtype=types.INT_DTYPE
      )
    return vb.VectorizedOptimizerInput(
        continuous=new_features_continuous, categorical=new_features_categorical
    )

  def _create_logits_vector(
      self,
      features_one_category: jax.Array,  # [pool_size]
      feature_batch_member_one_category: jax.Array,  # scalar integer
      scale_batch_member: jax.Array,  # [pool_size]
      feature_size: jax.Array,
  ):  # scalar
    categories = jnp.arange(self.max_categorical_size)
    logit_same_category = jnp.log(
        self.config.prob_same_category_without_perturbation
    )
    logit_different_category = jnp.log(
        (1.0 - self.config.prob_same_category_without_perturbation)
        / (feature_size - 1.0)
    )
    logits = (
        jnp.sum(
            jnp.where(
                categories[:, jnp.newaxis] == features_one_category,
                scale_batch_member,
                0.0,
            ),
            axis=-1,
        )
        + logit_different_category
    )
    logits = jnp.where(categories < feature_size, logits, -jnp.inf)
    return logits.at[feature_batch_member_one_category].add(
        -jnp.sum(scale_batch_member)
        + logit_same_category
        - logit_different_category
    )  # [num_categories]

  def _create_logits_one_feature(
      self,
      features_one_category: jax.Array,  # [pool_size, num_parallel]
      features_batch_one_category: jax.Array,  # [batch_size, num_parallel]
      scale: jax.Array,  # [batch_size, pool_size]
      feature_size: jax.Array,  # scalar
  ):
    return jax.vmap(  # map over batch
        jax.vmap(self._create_logits_vector, in_axes=(-1, -1, None, None)),
        in_axes=(None, 0, 0, None),
    )(
        features_one_category, features_batch_one_category, scale, feature_size
    )  # [batch_size, num_parallel, max_num_categories]

  def _create_categorical_feature_logits(
      self,
      features: jax.Array,  # [pool_size, num_parallel, num_features]
      features_batch: jax.Array,  # [batch_size, num_parallel, num_features]
      scale: jax.Array,  # [batch_size, pool_size]
  ):
    return jax.vmap(
        self._create_logits_one_feature, in_axes=(-1, -1, None, 0), out_axes=2
    )(
        features, features_batch, scale, jnp.array(self.categorical_sizes)
    )  # [batch_size, num_parallel, num_features, num_categories]

  def _create_random_perturbations(
      self,
      perturbations_batch: jax.Array,
      n_parallel: int,
      seed: jax.Array,
  ) -> types.ContinuousAndCategoricalArray:
    """Create random perturbations for the newly created batch.

    Args:
      perturbations_batch: (batch_size,)
      n_parallel: Number of points that the acquisition function maps to a
        single value. This arg may be greater than 1 if a parallel acquisition
        function (qEI, qUCB) is used; otherwise it should be 1.
      seed: Random seed.

    Returns:
      perturbations: (batch_size, n_parallel, n_features)
    """
    cont_seed, cat_seed = jax.random.split(seed)
    # Generate normalized noise for each batch.
    batch_noise_continuous = jax.random.laplace(
        cont_seed,
        shape=(
            self.batch_size,
            n_parallel,
            self.n_feature_dimensions_with_padding.continuous,
        ),
    )
    if self.n_feature_dimensions_with_padding.continuous > 0:
      batch_noise_continuous /= jnp.max(
          jnp.abs(batch_noise_continuous), axis=1, keepdims=True
      )

    if self.n_feature_dimensions_with_padding.continuous == 0:
      categorical_perturbation = (
          self.config.pure_categorical_perturbation_factor
      )
    else:
      categorical_perturbation = self.config.categorical_perturbation_factor
    batch_noise_categorical = (
        jax.random.laplace(
            cat_seed,
            shape=(
                self.batch_size,
                n_parallel,
                self.n_feature_dimensions_with_padding.categorical,
                self.max_categorical_size,
            ),
        )
        * categorical_perturbation
    )
    return types.ContinuousAndCategoricalArray(
        continuous=(
            batch_noise_continuous
            * perturbations_batch[:, jnp.newaxis, jnp.newaxis]
        ),
        categorical=(
            batch_noise_categorical
            * perturbations_batch[:, jnp.newaxis, jnp.newaxis, jnp.newaxis]
        ),
    )

  def update(
      self,
      seed: jax.Array,
      state: VectorizedEagleStrategyState,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> VectorizedEagleStrategyState:
    """Update the firefly pool based on the new batch of results.

    Arguments:
      seed: Random seed.
      state: Current state.
      batch_features: (batch_size, n_parallel, n_features)
      batch_rewards: (batch_size, )

    Returns:
      new_state: Updated state.
    """
    new_best_reward = jnp.maximum(state.best_reward, jnp.max(batch_rewards))
    batch_id = state.iterations % (self.pool_size // self.batch_size)
    batch_start_ind = batch_id * self.batch_size
    batch_perturbations = jax.lax.dynamic_slice_in_dim(
        state.perturbations, batch_start_ind, self.batch_size
    )

    def _update(batch_features, batch_rewards, batch_perturbations):
      # Pass the new batch rewards and the associated last suggested features.
      new_batch_features, new_batch_rewards, new_batch_perturbations = (
          self._update_pool_features_and_rewards(
              batch_features,
              batch_rewards,
              jax.tree_util.tree_map(
                  lambda f: jax.lax.dynamic_slice_in_dim(
                      f, batch_start_ind, self.batch_size
                  ),
                  state.features,
              ),
              jax.lax.dynamic_slice_in_dim(
                  state.rewards, batch_start_ind, self.batch_size
              ),
              batch_perturbations,
          )
      )
      return self._trim_pool(
          new_batch_features,
          new_batch_rewards,
          new_batch_perturbations,
          new_best_reward,
          seed,
      )

    # If the strategy is still initializing, return the random/prior values.
    (new_batch_features, new_batch_rewards, new_batch_perturbations) = (
        jax.lax.cond(
            state.iterations < self.pool_size // self.batch_size,
            lambda *args: args,
            _update,
            batch_features,
            batch_rewards,
            batch_perturbations,
        )
    )

    return VectorizedEagleStrategyState(
        iterations=state.iterations + 1,
        features=jax.tree_util.tree_map(
            lambda sf, nbf: jax.lax.dynamic_update_slice_in_dim(
                sf, nbf, batch_start_ind, axis=0
            ),
            state.features,
            new_batch_features,
        ),
        rewards=jax.lax.dynamic_update_slice_in_dim(
            state.rewards, new_batch_rewards, batch_start_ind, axis=0
        ),
        best_reward=new_best_reward,
        perturbations=jax.lax.dynamic_update_slice_in_dim(
            state.perturbations,
            new_batch_perturbations,
            batch_start_ind,
            axis=0,
        ),
    )

  def _update_pool_features_and_rewards(
      self,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: jax.Array,
      prev_batch_features: vb.VectorizedOptimizerInput,
      prev_batch_rewards: jax.Array,
      perturbations: jax.Array,
  ) -> Tuple[vb.VectorizedOptimizerInput, jax.Array, jax.Array]:
    """Update the features and rewards for fireflies with improved rewards.

    Arguments:
      batch_features: (batch_size, n_parallel, n_features), new proposed
        features batch.
      batch_rewards: (batch_size,), rewards for new proposed features batch.
      prev_batch_features: (batch_size, n_parallel, n_features), previous
        features batch.
      prev_batch_rewards: (batch_size,), rewards for previous features batch.
      perturbations: (batch_size,)

    Returns:
      sliced features, sliced rewards, sliced perturbations
    """
    # Find indices of flies that their generated features made an improvement.
    improve_indx = batch_rewards > prev_batch_rewards
    # Update successful flies' with the associated last features and rewards.
    new_batch_features = jax.tree_util.tree_map(
        lambda bf, pbf: jnp.where(
            improve_indx[..., jnp.newaxis, jnp.newaxis], bf, pbf
        ),
        batch_features,
        prev_batch_features,
    )
    new_batch_rewards = jnp.where(
        improve_indx, batch_rewards, prev_batch_rewards
    )
    # Penalize unsuccessful flies.
    new_batch_perturbations = jnp.where(
        improve_indx, perturbations, perturbations * self.config.penalize_factor
    )
    return new_batch_features, new_batch_rewards, new_batch_perturbations

  def _trim_pool(
      self,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: jax.Array,
      batch_perturbations: jax.Array,
      best_reward: jax.Array,
      seed: jax.Array,
  ) -> Tuple[vb.VectorizedOptimizerInput, jax.Array, jax.Array]:
    """Trim the pool by replacing unsuccessful fireflies with new random ones.

    A firefly is considered unsuccessful if its current perturbation is below
    'perturbation_lower_bound' and it's not the best fly seen thus far.
    Random features are created to replace the existing ones, and rewards
    are set to -np.inf to indicate that we don't yet have values for those
    features and they shouldn't be used during suggest.

    Args:
      batch_features: (batch_size, n_parallel, n_features)
      batch_rewards: (batch_size,)
      batch_perturbations: (batch_size,)
      best_reward: Best reward seen so far.
      seed: Random seed.

    Returns:
      updated feature, reward, and perturbation batches.
    """
    indx = batch_perturbations < self.config.perturbation_lower_bound
    # Ensure the best firefly is never removed. For optimization purposes,
    # this logic is inside the if statement to be peformed only if needed.
    indx = indx & (batch_rewards != best_reward)

    # Replace fireflies with random features and evaluate rewards.
    random_features = self.random_sampler(
        self.batch_size,
        n_parallel=batch_features.continuous.shape[1],
        seed=seed,
    )
    new_batch_features = jax.tree_util.tree_map(
        lambda rf, bf: jnp.where(indx[..., jnp.newaxis, jnp.newaxis], rf, bf),
        random_features,
        batch_features,
    )
    new_batch_perturbations = jnp.where(
        indx, self.config.perturbation, batch_perturbations
    )
    # Setting rewards to -inf to filter out those fireflies during suggest.
    new_batch_rewards = jnp.where(indx, -jnp.inf, batch_rewards)
    return new_batch_features, new_batch_rewards, new_batch_perturbations


--- vizier/_src/algorithms/optimizers/eagle_strategy_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for eagle strategy."""

from absl.testing import parameterized
import jax
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import eagle_strategy
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import types
from vizier.pyvizier import converters
from vizier.pyvizier.converters import padding

from absl.testing import absltest

tfd = tfp.distributions
ContinuousFeaturePerturbationType = (
    eagle_strategy.ContinuousFeaturePerturbationType
)


def _create_logits_vector_simple(
    categorical_features,
    categorical_features_batch,
    scale,
    categorical_sizes,
    max_categorical_size,
    config,
):
  n_batch = categorical_features_batch.shape[0]
  n_feat = categorical_features.shape[0]
  logits = np.zeros((n_batch, len(categorical_sizes), max_categorical_size))
  for i, s in enumerate(categorical_sizes):
    one_hot_features = np.zeros([n_feat, s])
    one_hot_features[np.arange(n_feat), categorical_features[:, i]] = 1

    one_hot_batch = np.zeros([n_batch, s])
    one_hot_batch[np.arange(n_batch), categorical_features_batch[:, i]] = 1

    features_change = np.matmul(
        scale, one_hot_features
    ) - one_hot_batch * np.sum(scale, axis=-1, keepdims=True)

    diff_category_logit = np.log(
        (1.0 - config.prob_same_category_without_perturbation) / (s - 1)
    )
    logits_i = np.zeros((n_batch, max_categorical_size)) + diff_category_logit
    logits_i[:, s:] = -np.inf
    logits_i[np.arange(n_batch), categorical_features_batch[:, i]] = np.log(
        config.prob_same_category_without_perturbation
    )
    logits_i[:, :s] = logits_i[:, :s] + features_change
    logits[:, i, :] = logits_i
  return logits


def _create_features_simple(
    features,
    rewards,
    features_batch,
    rewards_batch,
    config,
    n_features,
    categorical_sizes,
    max_categorical_size,
    seed,
):
  """A version of `_create_features` that materializes large intermediates."""
  # Only works with no parallel batch dimension.
  continuous_features_diffs = (
      features.continuous - features_batch.continuous[:, jnp.newaxis, :]
  )
  categorical_features_diffs = (
      features.categorical != features_batch.categorical[:, jnp.newaxis, :]
  )
  features_diffs = vb.VectorizedOptimizerInput(
      continuous=continuous_features_diffs,
      categorical=categorical_features_diffs,
  )
  dists = jax.tree_util.tree_map(
      lambda x: jnp.sum(jnp.square(x), axis=-1), features_diffs
  )
  directions = rewards - rewards_batch[:, jnp.newaxis]
  scaled_directions = jnp.where(
      directions >= 0.0, config.gravity, -config.negative_gravity
  )

  # Handle removed fireflies without updated rewards.
  finite_ind = jnp.isfinite(rewards).astype(directions.dtype)

  # Ignore fireflies that were removed from the pool.
  scale = jax.tree_util.tree_map(
      lambda x: finite_ind  # pylint: disable=g-long-lambda
      * scaled_directions
      * jnp.exp(-config.visibility * x / n_features * 10.0),
      dists,
  )

  # Separate forces to pull and push so to normalize them separately.
  new_continuous_features = features_batch.continuous + jnp.sum(
      features_diffs.continuous * scale.continuous[..., jnp.newaxis], axis=1
  )
  categorical_features_logits = _create_logits_vector_simple(
      features.categorical,
      features_batch.categorical,
      scale.categorical,
      categorical_sizes,
      max_categorical_size,
      config,
  )
  new_categorical_features = tfd.Categorical(
      logits=categorical_features_logits
  ).sample(seed=seed)

  return vb.VectorizedOptimizerInput(
      new_continuous_features, new_categorical_features
  )


class VectorizedEagleStrategyContinuousTest(parameterized.TestCase):

  def setUp(self):
    super(VectorizedEagleStrategyContinuousTest, self).setUp()
    self.config = eagle_strategy.EagleStrategyConfig(
        visibility=1, gravity=1, pool_size=4
    )
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_categorical_param('c1', ['a', 'b'])
    root.add_categorical_param('c2', ['a', 'b', 'c'])
    self.converter = converters.TrialToModelInputConverter.from_problem(problem)
    self.eagle = eagle_strategy.VectorizedEagleStrategyFactory(
        eagle_config=self.config
    )(converter=self.converter, suggestion_batch_size=2)

  def test_create_features_and_logits(self):
    features_continuous = jnp.array(
        [[[1.0, 2.0]], [[3.0, 4.0]], [[7.0, 7.0]], [[8.0, 8.0]]]
    )
    features_categorical = jnp.array([[1, 2], [0, 0], [0, 1], [1, 1]])[
        :, jnp.newaxis, :
    ]
    rewards = jnp.array([2, 3, 4, 1])
    seed = jax.random.PRNGKey(0)
    features_continuous_batch = features_continuous[: self.eagle.batch_size]
    features_categorical_batch = features_categorical[: self.eagle.batch_size]
    features = vb.VectorizedOptimizerInput(
        continuous=features_continuous, categorical=features_categorical
    )
    features_batch = vb.VectorizedOptimizerInput(
        continuous=features_continuous_batch,
        categorical=features_categorical_batch,
    )
    rewards_batch = rewards[: self.eagle.batch_size]
    created_features = self.eagle._create_features(
        features,
        rewards,
        features_batch,
        rewards_batch,
        vb.VectorizedOptimizerInput(
            jnp.zeros_like(features_continuous_batch),
            jnp.zeros(features_categorical_batch.shape + (3,)),
        ),
        seed=seed,
    )
    self.assertEqual(created_features.continuous.shape, (2, 1, 2))
    self.assertEqual(created_features.categorical.shape, (2, 1, 2))

    features_2d = vb.VectorizedOptimizerInput(
        features.continuous[:, 0, :], features.categorical[:, 0, :]
    )
    features_batch_2d = vb.VectorizedOptimizerInput(
        features_batch.continuous[:, 0, :], features_batch.categorical[:, 0, :]
    )
    expected = _create_features_simple(
        features_2d,
        rewards,
        features_batch_2d,
        rewards_batch,
        self.config.replace(
            mutate_normalization_type=(
                eagle_strategy.MutateNormalizationType.UNNORMALIZED
            )
        ),
        (
            self.eagle.n_feature_dimensions.continuous
            + self.eagle.n_feature_dimensions.categorical
        ),
        self.eagle.categorical_sizes,
        self.eagle.max_categorical_size,
        seed,
    )
    actual = self.eagle._create_features(
        features,
        rewards,
        features_batch,
        rewards_batch,
        vb.VectorizedOptimizerInput(
            jnp.zeros_like(features_continuous_batch),
            jnp.zeros(features_categorical_batch.shape + (3,)),
        ),
        seed=seed,
    )
    np.testing.assert_array_equal(
        expected.continuous, actual.continuous[:, 0, :]
    )
    np.testing.assert_array_equal(
        expected.categorical, actual.categorical[:, 0, :]
    )

    scale = np.random.normal(size=[self.eagle.batch_size, 4])
    expected_logits = _create_logits_vector_simple(
        features_2d.categorical,
        features_batch_2d.categorical,
        scale,
        self.eagle.categorical_sizes,
        self.eagle.max_categorical_size,
        self.config,
    )
    actual_logits = self.eagle._create_categorical_feature_logits(
        features.categorical, features_batch.categorical, scale
    )
    np.testing.assert_allclose(
        expected_logits, actual_logits[:, 0, :, :], rtol=1e-5
    )

  @parameterized.parameters(1, 5)
  def test_create_random_perturbations(self, n_parallel):
    seed = jax.random.PRNGKey(0)
    perturbations_batch = jnp.ones(self.eagle.batch_size)
    perturbations = self.eagle._create_random_perturbations(
        perturbations_batch,
        n_parallel=n_parallel,
        seed=seed,
    )
    self.assertEqual(perturbations.continuous.shape, (2, n_parallel, 2))
    self.assertEqual(perturbations.categorical.shape, (2, n_parallel, 2, 3))

  def test_update_pool_features_and_rewards(self):
    features = vb.VectorizedOptimizerInput(
        continuous=jnp.array(
            [[[1, 2]], [[3, 4]], [[7, 7]], [[8, 8]]], dtype=jnp.float64
        ),
        categorical=jnp.array(
            [[[1, 2]], [[3, 0]], [[0, 1]], [[2, 1]]], dtype=jnp.int32
        ),
    )
    rewards = jnp.array([2, 3, 4, 1], dtype=jnp.float64)
    perturbations = jnp.array([1, 1, 1, 1], dtype=jnp.float64)

    batch_features = vb.VectorizedOptimizerInput(
        continuous=jnp.array([[[9, 9]], [[10, 10]]], dtype=jnp.float64),
        categorical=jnp.array([[[0, 0]], [[1, 1]]], dtype=jnp.int32),
    )
    batch_rewards = jnp.array([5, 0.5], dtype=jnp.float64)

    new_features, new_rewards, new_perturbations = (
        self.eagle._update_pool_features_and_rewards(
            batch_features,
            batch_rewards,
            jax.tree_util.tree_map(
                lambda f: f[: self.eagle.batch_size], features
            ),
            rewards[: self.eagle.batch_size],
            perturbations[: self.eagle.batch_size],
        )
    )
    np.testing.assert_array_equal(
        new_features.continuous,
        np.array([[[9, 9]], [[3, 4]]], dtype=np.float64),
        err_msg='Features are not equal.',
    )
    np.testing.assert_array_equal(
        new_features.categorical,
        np.array([[[0, 0]], [[3, 0]]], dtype=np.int32),
        err_msg='Features are not equal.',
    )

    np.testing.assert_array_equal(
        new_rewards,
        np.array([5, 3], dtype=np.float64),
        err_msg='rewards are not equal.',
    )

    pc = self.config.penalize_factor
    np.testing.assert_array_almost_equal(
        new_perturbations,
        np.array([1, pc], dtype=np.float64),
        err_msg='Perturbations are not equal.',
    )

  def test_update_best_reward(self):
    # Test replacing the best reward.
    features = vb.VectorizedOptimizerInput(
        continuous=jnp.array(
            [[[1, 2]], [[3, 4]], [[7, 7]], [[8, 8]]], dtype=jnp.float64
        ),
        categorical=jnp.array(
            [[[1, 2]], [[3, 0]], [[0, 1]], [[2, 1]]], dtype=jnp.int32
        ),
    )
    rewards = jnp.array([2, 3, 4, 1], dtype=jnp.float64)
    state = eagle_strategy.VectorizedEagleStrategyState(
        iterations=jnp.array(0),
        features=features,
        rewards=rewards,
        best_reward=jnp.max(rewards),
        perturbations=jnp.ones_like(rewards),
    )
    batch_features = vb.VectorizedOptimizerInput(
        continuous=jnp.array([[[9, 9]], [[10, 10]]], dtype=jnp.float64),
        categorical=jnp.array([[[0, 0]], [[1, 1]]], dtype=jnp.int32),
    )
    batch_rewards = jnp.array([5, 0.5], dtype=jnp.float64)
    seed = jax.random.PRNGKey(0)
    new_state = self.eagle.update(seed, state, batch_features, batch_rewards)
    self.assertEqual(new_state.best_reward, 5.0)
    # Test not replacing the best reward.
    batch_rewards = jnp.array([2, 4], dtype=jnp.float64)
    new_new_state = self.eagle.update(
        seed, new_state, batch_features, batch_rewards
    )
    self.assertEqual(new_new_state.best_reward, 5.0)

  @parameterized.parameters(
      {'batch_size': 5, 'expected_batch_size': 5, 'max_pool_size': 50},
      {'batch_size': None, 'expected_batch_size': 50, 'max_pool_size': 50},
      {'batch_size': 5, 'expected_batch_size': 5, 'max_pool_size': 10},
      {'batch_size': None, 'expected_batch_size': 10, 'max_pool_size': 10},
  )
  def test_batch_size_and_pool_size(
      self, batch_size, expected_batch_size, max_pool_size
  ):
    problem = vz.ProblemStatement()
    root = problem.search_space.root
    for i in range(100):
      root.add_float_param(f'x{i}', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    config = eagle_strategy.EagleStrategyConfig(max_pool_size=max_pool_size)
    eagle = eagle_strategy.VectorizedEagleStrategyFactory(eagle_config=config)(
        converter=converter, suggestion_batch_size=batch_size
    )
    self.assertEqual(eagle.pool_size, max_pool_size)
    self.assertEqual(eagle.batch_size, expected_batch_size)

  def test_trim_pool(self):
    pc = self.config.perturbation
    features_batch = vb.VectorizedOptimizerInput(
        continuous=jnp.array([[[1, 2]], [[3, 4]]], dtype=jnp.float64),
        categorical=jnp.array([[[1, 2]], [[3, 0]]], dtype=jnp.int32),
    )
    rewards_batch = jnp.array([2, 3], dtype=jnp.float64)
    perturbations = jnp.array([pc, 0], dtype=jnp.float64)
    seed = jax.random.PRNGKey(0)
    new_features, new_rewards, new_perturbations = self.eagle._trim_pool(
        features_batch,
        rewards_batch,
        perturbations,
        best_reward=jnp.array(4.0),
        seed=seed,
    )

    np.testing.assert_array_almost_equal(
        new_features.continuous[0],
        features_batch.continuous[0],
        err_msg='Continuous features are not equal.',
    )
    np.testing.assert_array_almost_equal(
        new_features.categorical[0],
        features_batch.categorical[0],
        err_msg='Categorical features are not equal.',
    )
    self.assertTrue(
        np.all(
            np.not_equal(
                new_features.continuous[1], features_batch.continuous[1]
            )
        ),
        msg='Continuous features are not equal.',
    )
    self.assertTrue(
        np.all(
            np.not_equal(
                new_features.categorical[1], features_batch.categorical[1]
            )
        ),
        msg='Categorical features are not equal.',
    )

    np.testing.assert_array_equal(
        new_rewards,
        np.array([2, -np.inf], dtype=np.float64),
        err_msg='rewards are not equal.',
    )
    # The best firefly is never removed.
    np.testing.assert_array_almost_equal(
        new_perturbations,
        np.array([pc, pc], dtype=np.float64),
        err_msg='Perturbations are not equal.',
    )

  def test_create_strategy_from_factory(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_float_param('x3', 0.0, 1.0)
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    eagle = eagle_factory(converter)
    self.assertEqual(eagle.n_feature_dimensions.continuous, 3)
    self.assertEqual(eagle.n_feature_dimensions.categorical, 0)

  def test_optimize_with_eagle(self):

    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        self.converter
    )
    optimizer(
        score_fn=lambda x, _: -jnp.sum(x.continuous.padded_array, 1), count=1
    )

  def test_continuous_feature_perturbation_type(self):
    optimizer_additive_perturbation = vb.VectorizedOptimizerFactory(
        strategy_factory=eagle_strategy.VectorizedEagleStrategyFactory(
            eagle_config=eagle_strategy.EagleStrategyConfig(
                continuous_feature_perturbation_type=(
                    ContinuousFeaturePerturbationType.ADDITIVE
                )
            )
        ),
        max_evaluations=50,
    )(self.converter)
    expected_count = 4
    new_features_additive_perturbation = optimizer_additive_perturbation(
        score_fn=lambda x, _: -jnp.sum(x.continuous.padded_array, 1),
        count=expected_count,
    ).features
    self.assertSequenceEqual(
        new_features_additive_perturbation.continuous.shape,
        (expected_count, 1, 2),
    )
    self.assertSequenceEqual(
        new_features_additive_perturbation.categorical.shape,
        (expected_count, 1, 2),
    )
    optimizer_mult_perturbation = vb.VectorizedOptimizerFactory(
        strategy_factory=eagle_strategy.VectorizedEagleStrategyFactory(
            eagle_config=eagle_strategy.EagleStrategyConfig(
                continuous_feature_perturbation_type=(
                    ContinuousFeaturePerturbationType.MULTIPLICATIVE
                )
            )
        ),
        max_evaluations=50,
    )(self.converter)
    new_features_mult_perturbation = optimizer_mult_perturbation(
        score_fn=lambda x, _: -jnp.sum(x.continuous.padded_array, 1),
        count=expected_count,
    ).features
    self.assertSequenceEqual(
        new_features_mult_perturbation.continuous.shape,
        (expected_count, 1, 2),
    )
    self.assertSequenceEqual(
        new_features_mult_perturbation.categorical.shape,
        (expected_count, 1, 2),
    )
    self.assertGreater(
        np.abs(
            new_features_additive_perturbation.continuous
            - new_features_mult_perturbation.continuous
        ).max(),
        1e-1,
    )

  def test_optimize_with_eagle_continuous_only(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_float_param('x3', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        converter
    )
    n_parallel = 5
    results = optimizer(
        score_fn=lambda x, _: -jnp.sum(x.continuous.padded_array, axis=(1, 2)),
        count=1,
        n_parallel=n_parallel,
    )
    self.assertSequenceEqual(
        results.features.continuous.shape, (1, n_parallel, 3)
    )

  def test_singleton_constraints_are_respected(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 10.0)
    root.add_float_param('x2', 0.0, 10.0)
    root.add_float_param('x3', 0.35, 0.35)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        converter
    )
    score_fn = lambda x, _: -jnp.sum(x.continuous.padded_array, axis=(1,))
    results = optimizer(
        score_fn=score_fn,
        count=1,
    )
    self.assertSequenceEqual(results.features.continuous.shape, (1, 1, 3))
    best_candidates = vb.best_candidates_to_trials(results, converter)
    # Evaluating the score function being optimized on the best candidate should
    # result in the same value as the output of the optimizer.
    self.assertSequenceAlmostEqual(
        score_fn(converter.to_features(best_candidates), None), results.rewards
    )

  def test_optimize_with_eagle_padding(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_float_param('x3', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(
        problem,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.POWERS_OF_2,
            num_features=padding.PaddingType.POWERS_OF_2,
        ),
    )
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        converter
    )
    n_parallel = 2
    results = optimizer(
        score_fn=lambda x, _: -jnp.sum(x.continuous.padded_array, axis=(1, 2)),
        count=1,
        n_parallel=n_parallel,
    )
    self.assertSequenceEqual(
        results.features.continuous.shape, (1, n_parallel, 4)
    )

  def test_singleton_constraints_are_respected_with_padding(self):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 10.0)
    root.add_float_param('x2', 0.0, 10.0)
    root.add_float_param('x3', 7.5, 7.5)
    converter = converters.TrialToModelInputConverter.from_problem(
        problem,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.POWERS_OF_2,
            num_features=padding.PaddingType.POWERS_OF_2,
        ),
    )
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        converter
    )
    score_fn = lambda x, _: -jnp.sum(
        x.continuous.replace_fill_value(0).padded_array, axis=(1,)
    )
    results = optimizer(
        score_fn=score_fn,
        count=1,
    )
    self.assertSequenceEqual(results.features.continuous.shape, (1, 1, 4))
    best_candidates = vb.best_candidates_to_trials(results, converter)
    # Evaluating the score function being optimized on the best candidate should
    # result in the same value as the output of the optimizer.
    self.assertSequenceAlmostEqual(
        score_fn(converter.to_features(best_candidates), None), results.rewards
    )

  @parameterized.parameters(
      dict(add_float_params=True, add_categorical_params=True),
      dict(add_float_params=True, add_categorical_params=False),
      dict(add_float_params=False, add_categorical_params=True),
  )
  def test_compute_feature_dimensions_from_converter(
      self, add_float_params: bool, add_categorical_params: bool
  ):
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    if add_float_params:
      root.add_float_param('x1', 0.0, 1.0)
      root.add_float_param('x2', 0.0, 1.0)
      root.add_float_param('x3', 0.0, 1.0)
    if add_categorical_params:
      root.add_categorical_param('c1', ['a', 'b', 'c'])
      root.add_categorical_param('c2', ['d', 'e', 'f', 'g'])
    converter = converters.TrialToModelInputConverter.from_problem(
        problem,
        padding_schedule=padding.PaddingSchedule(
            num_trials=padding.PaddingType.MULTIPLES_OF_10,
            num_features=padding.PaddingType.POWERS_OF_2,
        ),
    )
    feature_dimensions = (
        eagle_strategy.compute_feature_dimensions_from_converter(converter)
    )
    if add_float_params:
      self.assertEqual(feature_dimensions.n_feature_dimensions.continuous, 3)
      self.assertEqual(
          feature_dimensions.n_feature_dimensions_with_padding.continuous, 4
      )
    if add_categorical_params:
      self.assertEqual(feature_dimensions.n_feature_dimensions.categorical, 2)
      self.assertEqual(
          feature_dimensions.n_feature_dimensions_with_padding.categorical, 2
      )
      self.assertEqual(feature_dimensions.max_categorical_size, 4)
      self.assertEqual(feature_dimensions.categorical_sizes, [3, 4])

  @parameterized.parameters(
      {'num_prior_trials': 0},
      {'num_prior_trials': 3},
      {'num_prior_trials': 53},
  )
  def test_optimize_with_eagle_trials_padding(self, num_prior_trials):
    eagle_factory = eagle_strategy.VectorizedEagleStrategyFactory()
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_float_param('x3', 0.0, 1.0)
    root.add_categorical_param('c1', ['a', 'b', 'c'])
    root.add_categorical_param('c2', ['d', 'e', 'f', 'g'])

    rng = np.random.default_rng(seed=1)
    prior_features_continuous = rng.uniform(size=(num_prior_trials, 3))
    prior_features_categorical = rng.integers(
        low=0,
        high=[[3, 4]],
        size=(num_prior_trials, 2),
    )

    def score_fn(x, _):
      return jnp.sum(
          x.continuous.replace_fill_value(0).padded_array, axis=(1,)
      ) + jnp.sum(x.categorical.replace_fill_value(0).padded_array, axis=(1,))

    count = 3

    converter = converters.TrialToModelInputConverter.from_problem(
        problem,
    )
    optimizer = vb.VectorizedOptimizerFactory(strategy_factory=eagle_factory)(
        converter
    )
    prior_features = types.ModelInput(
        continuous=types.PaddedArray.as_padded(prior_features_continuous),
        categorical=types.PaddedArray.as_padded(prior_features_categorical),
    )
    results = optimizer(
        score_fn=score_fn,
        count=count,
        prior_features=None if num_prior_trials == 0 else prior_features,
    )

    padding_schedule = padding.PaddingSchedule(
        num_trials=padding.PaddingType.MULTIPLES_OF_10,
    )
    padding_converter = converters.TrialToModelInputConverter.from_problem(
        problem,
        padding_schedule=padding_schedule,
    )
    padding_optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=eagle_factory
    )(padding_converter)
    padded_prior_features = types.ModelInput(
        continuous=padding_schedule.pad_features(prior_features_continuous),
        categorical=padding_schedule.pad_features(prior_features_categorical),
    )
    padding_results = padding_optimizer(
        score_fn=score_fn,
        count=count,
        prior_features=None if num_prior_trials == 0 else padded_prior_features,
    )

    self.assertSequenceEqual(results.features.continuous.shape, (count, 1, 3))
    self.assertSequenceEqual(results.features.categorical.shape, (count, 1, 2))
    np.testing.assert_array_almost_equal(
        results.features.continuous,
        padding_results.features.continuous,
        decimal=5,
    )
    np.testing.assert_array_almost_equal(
        results.features.categorical,
        padding_results.features.categorical,
        decimal=5,
    )

  def test_factory(self):
    self.assertEqual(self.eagle.n_feature_dimensions.continuous, 2)
    self.assertEqual(self.eagle.n_feature_dimensions.categorical, 2)
    self.assertLen(self.eagle.categorical_sizes, 2)

  def test_sample_categorical_features(self):
    # features shouldn't have values in oov_mask, and have the structure of:
    # [c1,c1,c1,f1,c2,c2,c2,c2,d1]
    num_parallel = 3
    random_sampler = eagle_strategy.DefaultRandomSampler(self.converter)
    sampled_features = random_sampler(
        20,
        n_parallel=num_parallel,
        seed=jax.random.PRNGKey(0),
    )
    self.assertTrue(
        np.all(
            (sampled_features.continuous >= 0.0)
            & (sampled_features.continuous <= 1.0)
        )
    )
    self.assertTrue(
        np.all(
            sampled_features.categorical
            < np.array(self.eagle.categorical_sizes)
        )
    )

  def test_prior_trials(self):
    config = eagle_strategy.EagleStrategyConfig(
        visibility=1, gravity=1, pool_size=4, prior_trials_pool_pct=1.0
    )
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_categorical_param('c1', ['a', 'b', 'c'])
    converter = converters.TrialToModelInputConverter.from_problem(problem)

    prior_features_continuous = jnp.array(
        [[[1, -1]], [[2, 1]], [[3, 2]], [[4, 5]]], dtype=jnp.float32
    )
    prior_features = vb.VectorizedOptimizerInput(
        continuous=prior_features_continuous,
        categorical=jnp.array([[0], [2], [1], [1]])[:, jnp.newaxis],
    )
    prior_rewards = jnp.array([1, 2, 3, 4])
    eagle = eagle_strategy.VectorizedEagleStrategyFactory(
        eagle_config=config,
    )(converter=converter, suggestion_batch_size=2)
    init_state = eagle.init_state(
        jax.random.PRNGKey(0),
        prior_features=prior_features,
        prior_rewards=prior_rewards,
    )
    jax.tree_util.tree_map(
        lambda x, y: np.testing.assert_array_equal(y, jnp.flip(x, axis=0)),
        init_state.features,
        prior_features,
    )

  @parameterized.parameters(2, 10)
  def test_prior_trials_with_too_few_or_many_trials(self, n_prior_trials):
    # Test proper populating of the pool in case the number of prior trials is
    # greater or lesser than the pool size.
    config = eagle_strategy.EagleStrategyConfig(
        visibility=1, gravity=1, pool_size=4, prior_trials_pool_pct=1.0
    )
    problem = vz.ProblemStatement()
    root = problem.search_space.select_root()
    root.add_float_param('x1', 0.0, 1.0)
    root.add_float_param('x2', 0.0, 1.0)
    root.add_categorical_param('c1', ['a', 'b', 'c'])
    converter = converters.TrialToModelInputConverter.from_problem(problem)

    n_parallel = 3
    prior_features = vb.VectorizedOptimizerInput(
        continuous=jnp.asarray(np.random.randn(n_prior_trials, n_parallel, 2)),
        categorical=jnp.asarray(
            np.random.randint(3, size=(n_prior_trials, n_parallel, 1))
        ),
    )
    prior_rewards = jnp.asarray(np.random.randn(n_prior_trials))

    eagle = eagle_strategy.VectorizedEagleStrategyFactory(
        eagle_config=config,
    )(converter=converter, suggestion_batch_size=2)
    init_state = eagle.init_state(
        jax.random.PRNGKey(0),
        n_parallel=n_parallel,
        prior_features=prior_features,
        prior_rewards=prior_rewards,
    )
    self.assertEqual(
        init_state.features.continuous.shape, (config.pool_size, n_parallel, 2)
    )
    self.assertEqual(
        init_state.features.categorical.shape, (config.pool_size, n_parallel, 1)
    )


if __name__ == '__main__':
  jax.config.update('jax_threefry_partitionable', False)
  absltest.main()


--- vizier/_src/algorithms/optimizers/lbfgsb_optimizer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""L-BFGS-B Strategy optimizer."""

import itertools
from typing import Callable, Optional, Sequence, Union

import attr
from flax import struct
import jax
import jax.numpy as jnp
import numpy as np
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier.jax import optimizers
from vizier.pyvizier import converters


def _optimizer_to_model_input(
    x: jax.Array,
    n_features: jax.Array,
) -> types.ModelInput:
  x_cat = jnp.zeros(x.shape[:-1] + (0,), dtype=types.INT_DTYPE)
  return types.ModelInput(
      continuous=vb.optimizer_to_model_input_single_array(x, n_features),
      categorical=vb.optimizer_to_model_input_single_array(
          x_cat, jnp.zeros([])
      ),
  )


@struct.dataclass
class LBFGSBOptimizer:
  """L-BFGS-B optimizer."""

  n_feature_dimensions: types.ContinuousAndCategorical[jax.Array]
  n_feature_dimensions_with_padding: types.ContinuousAndCategorical[int] = (
      struct.field(pytree_node=False)
  )
  continuous_features_bounds: Sequence[tuple[float, float]] = struct.field(
      pytree_node=False
  )
  # Number of parallel runs of L-BFGS-B.
  random_restarts: int = struct.field(pytree_node=False, default=25)
  # Number of iterations for each L-BFGS-B run.
  maxiter: int = struct.field(pytree_node=False, default=50)

  def __post_init__(self):
    if self.n_feature_dimensions_with_padding.categorical > 0:
      raise ValueError("LBFGSBOptimizer doesn't support categorical features.")

  # TODO: Remove score_fn argument.
  # pylint: disable=g-bare-generic
  def __call__(
      self,
      score_fn: Union[vb.ArrayScoreFunction, vb.ParallelArrayScoreFunction],
      *,
      score_with_aux_fn: Optional[Callable] = None,
      count: Optional[int] = 1,
      prior_features: Optional[types.ModelInput] = None,
      n_parallel: Optional[int] = None,
      seed: Optional[int] = None,
  ) -> vb.VectorizedStrategyResults:
    """Optimize a continuous objective function using L-BFGS-B.

    Arguments:
      score_fn: A callback that expects 2D Array with dimensions (batch_size,
        features_count) or a 3D array with dimensions (batch_size, n_parallel,
        features_count), and a random seed, and returns a 1D Array
        (batch_size,).
      score_with_aux_fn: A callback similar to score_fn but additionally returns
        an array tree.
      count: The number of suggestions to generate.
      prior_features: Completed trials to be used for knowledge transfer.
        Currently ignored.
      n_parallel: This arg should be specified if a parallel acquisition
        function (e.g. qEI, qUCB) is used, which evaluates a set of points of
        this size to a single value. Pass `None` when optimizing acquisition
        functions that evaluate a single point. (Note that `num_parallel=1` and
        `num_parallel=None` will each return a single suggestion, though they
        are different in that `num_parallel=1` indicates that the acquisition
        function consumes the two rightmost dimensions of the input while
        `num_parallel=None` indicates that only the rightmost dimension is
        consumed.)
      seed: The seed to use in the random generator.

    Returns:
      The best trials found in the optimization.
    """
    del prior_features
    seed = jax.random.PRNGKey(0) if seed is None else seed
    if n_parallel and (count or 0) > 1:
      # Note that we can't distinguish between 'BatchArrayScoreFunction' and
      # 'ParallelArrayScoreFunction' using 'isinstance' as they both have the
      # same function names.
      raise ValueError(
          "LBFGSBOptimizer doesn't support batch of batches (count > 1 is"
          " disallowed when num_parallel_candidates is set)."
      )

    parallel_dim = n_parallel or 1
    optimize = optimizers.JaxoptScipyLbfgsB(
        optimizers.LbfgsBOptions(maxiter=self.maxiter)
    )

    score_rng, init_seed, optim_seed = jax.random.split(seed, num=3)

    score_with_aux = score_with_aux_fn
    if score_with_aux is None:
      score_with_aux = lambda *args: (score_fn(*args), dict())

    # We'll assume score_fn is differentiable and use the same acquisition
    # function seed at every call.
    # We also wrap the score function because the convention for optimizers is
    # to maximize, while L-BFGS-B always minimizes.
    def _opt_score_fn(x):
      score, aux = score_with_aux(
          _optimizer_to_model_input(
              x,
              self.n_feature_dimensions.continuous,
          ),
          score_rng,
      )
      if n_parallel is None:
        score = jnp.squeeze(score, axis=-1)
      return -score, aux

    feature_shape = (
        parallel_dim,
        self.n_feature_dimensions_with_padding.continuous,
    )

    def setup(rng):
      return jax.random.uniform(rng, shape=feature_shape)

    continuous_features_min, continuous_features_max = itertools.zip_longest(
        *self.continuous_features_bounds
    )
    bounds = []
    for bound, fill_value in (
        (continuous_features_min, 0.0),
        (continuous_features_max, 1.0),
    ):
      # Pad the bound with fill_value to the length of the feature dimension
      # with padding and account for the parallel dimension, so the shape of the
      # bound is the same as `feature_shape` above.
      padded_bound = bound + (fill_value,) * (
          self.n_feature_dimensions_with_padding.continuous - len(bound)
      )
      bounds.append(np.stack([padded_bound] * parallel_dim, axis=0))

    constraints = sp.Constraint(bounds=(bounds[0], bounds[1]))

    new_features, _ = optimize(
        jax.vmap(setup)(jax.random.split(init_seed, self.random_restarts)),
        _opt_score_fn,
        optim_seed,
        constraints=constraints,
        best_n=count,
    )
    loss_val, aux = _opt_score_fn(new_features)
    new_rewards = -loss_val

    dimension_is_missing = (
        jnp.arange(self.n_feature_dimensions_with_padding.continuous)
        >= self.n_feature_dimensions.continuous
    )
    new_features_model_input = vb.VectorizedOptimizerInput(
        continuous=jnp.where(
            dimension_is_missing, jnp.zeros_like(new_features), new_features
        ),
        categorical=jnp.zeros(
            new_features.shape[:-1] + (0,), dtype=types.INT_DTYPE
        ),
    )
    return vb.VectorizedStrategyResults(
        new_features_model_input,
        new_rewards,
        aux,
    )


@attr.define
class LBFGSBOptimizerFactory:
  """LBFGSB strategy optimizer factory."""

  random_restarts: int = 25
  maxiter: int = 100

  def __call__(
      self,
      converter: converters.TrialToModelInputConverter,
  ) -> LBFGSBOptimizer:
    """Generates a new LBFGSBOptimizer object."""
    n_feature_dimensions = types.ContinuousAndCategorical(
        jnp.array(len(converter.output_specs.continuous)),
        jnp.array(len(converter.output_specs.categorical)),
    )
    empty_features = converter.to_features([])
    n_feature_dimensions_with_padding = types.ContinuousAndCategorical[int](
        empty_features.continuous.shape[-1],
        empty_features.categorical.shape[-1],
    )
    continous_features_bounds = [
        (float(spec.bounds[0]), float(spec.bounds[1]))
        for spec in converter.output_specs.continuous
    ]

    return LBFGSBOptimizer(
        n_feature_dimensions=n_feature_dimensions,
        n_feature_dimensions_with_padding=n_feature_dimensions_with_padding,
        continuous_features_bounds=continous_features_bounds,
        random_restarts=self.random_restarts,
        maxiter=self.maxiter,
    )


--- vizier/_src/algorithms/optimizers/lbfgsb_optimizer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for lbfgsb_optimizer."""

import jax.numpy as jnp
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import lbfgsb_optimizer as lo
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier.pyvizier import converters

from absl.testing import absltest
from absl.testing import parameterized


class LBFGSBOptimizer(parameterized.TestCase):

  def test_optimize_candidates_len(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_float_param('f2', 0.0, 10.0)
    problem.search_space.root.add_float_param('f3', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: jnp.sum(x.continuous.padded_array, axis=-1)
    optimizer = lo.LBFGSBOptimizerFactory(random_restarts=10, maxiter=20)(
        converter
    )
    res = optimizer(score_fn=score_fn)
    self.assertLen(res.rewards, 1)

  def test_singleton_constraints_are_respected(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_float_param('f2', 0.0, 10.0)
    problem.search_space.root.add_float_param('f3', 5.0, 5.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: jnp.sum(x.continuous.padded_array, axis=-1)
    optimizer = lo.LBFGSBOptimizerFactory(random_restarts=10, maxiter=20)(
        converter
    )
    res = optimizer(score_fn=score_fn)
    best_candidates = vb.best_candidates_to_trials(res, converter)
    best_score = score_fn(converter.to_features(best_candidates), None)
    # Evaluating the score function being optimized on the best candidate should
    # result in the same value as the output of the optimizer.
    self.assertSequenceAlmostEqual(best_score, res.rewards)

  def test_best_candidates_count_is_1(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    optimizer = lo.LBFGSBOptimizerFactory(random_restarts=10, maxiter=20)(
        converter
    )
    score_fn = lambda x, _: -jnp.sum(  # pylint: disable=g-long-lambda
        jnp.square(x.continuous.padded_array - 0.52), axis=-1
    )
    results = optimizer(score_fn=score_fn)
    candidates = vb.best_candidates_to_trials(results, converter)
    # check the best candidate
    self.assertLessEqual(
        np.abs(candidates[0].parameters['f1'].value - 0.52), 1e-6
    )
    self.assertLessEqual(
        np.abs(candidates[0].parameters['f2'].value - 0.52), 1e-6
    )
    self.assertIsNotNone(candidates[0].final_measurement)
    if candidates[0].final_measurement:
      self.assertLessEqual(
          np.abs(candidates[0].final_measurement.metrics['acquisition'].value),
          1e-6,
      )

  def test_batch_candidates(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    problem.search_space.root.add_float_param('f3', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    # Minimize sum over all features.
    score_fn = lambda x, _: -jnp.sum(  # pylint: disable=g-long-lambda
        jnp.square(x.continuous.padded_array - 0.52),
        axis=[-1, -2],
    )
    optimizer = lo.LBFGSBOptimizerFactory(random_restarts=10, maxiter=20)(
        converter
    )
    best_results = optimizer(score_fn=score_fn, count=1, n_parallel=3)
    best_candidates = vb.best_candidates_to_trials(best_results, converter)
    self.assertLen(best_candidates, 3)
    # check the best candidates
    for b in best_candidates:
      self.assertLessEqual(np.abs(b.parameters['f1'].value - 0.52), 1e-6)
      self.assertLessEqual(np.abs(b.parameters['f2'].value - 0.52), 1e-6)
      self.assertIsNotNone(b.final_measurement)
      if b.final_measurement:
        self.assertLessEqual(
            np.abs(b.final_measurement.metrics['acquisition'].value), 1e-6
        )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/optimizers/random_vectorized_optimizer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Random vectorized optimizer to be used in convergence test."""

from typing import Optional

import jax
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import types
from vizier.pyvizier import converters

tfd = tfp.distributions


class RandomVectorizedStrategy(vb.VectorizedStrategy[None]):
  """Random vectorized strategy."""

  def __init__(
      self,
      converter: converters.TrialToModelInputConverter,
      suggestion_batch_size: int,
  ):
    empty_features = converter.to_features([])
    n_feature_dimensions_with_padding = types.ContinuousAndCategorical(
        empty_features.continuous.shape[-1],
        empty_features.categorical.shape[-1],
    )

    categorical_sizes = []
    for spec in converter.output_specs.categorical:
      categorical_sizes.append(spec.bounds[1])

    self._suggestion_batch_size = suggestion_batch_size
    self.n_feature_dimensions_with_padding = n_feature_dimensions_with_padding
    self.n_feature_dimensions = n_feature_dimensions_with_padding
    self.dtype = types.ContinuousAndCategorical(jnp.float64, types.INT_DTYPE)

    self._categorical_logits = None
    if categorical_sizes:
      categorical_logits = np.zeros(
          [len(categorical_sizes), max(categorical_sizes)]
      )
      for i, s in enumerate(categorical_sizes):
        categorical_logits[i, s:] = -np.inf
      self._categorical_logits = categorical_logits

  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[vb.VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ) -> None:
    del seed
    return

  def suggest(
      self,
      seed: jax.Array,
      state: None,
      n_parallel: int = 1,
  ) -> vb.VectorizedOptimizerInput:
    del state
    cont_seed, cat_seed = jax.random.split(seed)
    cont_data = jax.random.uniform(
        cont_seed,
        shape=(
            self._suggestion_batch_size,
            n_parallel,
            self.n_feature_dimensions_with_padding.continuous,
        ),
    )
    if self._categorical_logits is None:
      cat_data = jnp.zeros(
          [self._suggestion_batch_size, n_parallel, 0], dtype=jnp.int32
      )
    else:
      cat_data = tfd.Categorical(logits=self._categorical_logits).sample(
          (self._suggestion_batch_size, n_parallel), seed=cat_seed
      )
    return vb.VectorizedOptimizerInput(cont_data, cat_data)

  def suggestion_batch_size(self) -> int:
    return self._suggestion_batch_size

  def update(
      self,
      seed: jax.Array,
      state: None,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> None:
    return


def random_strategy_factory(
    converter: converters.TrialToModelInputConverter,
    suggestion_batch_size: int,
) -> vb.VectorizedStrategy:
  """Creates a new vectorized strategy based on the Protocol."""
  return RandomVectorizedStrategy(
      converter=converter,
      suggestion_batch_size=suggestion_batch_size,
  )


def create_random_optimizer(
    converter: converters.TrialToModelInputConverter,
    max_evaluations: int,
    suggestion_batch_size: int,
) -> vb.VectorizedOptimizer:
  """Creates a random optimizer."""
  return vb.VectorizedOptimizerFactory(
      strategy_factory=random_strategy_factory,
      max_evaluations=max_evaluations,
      suggestion_batch_size=suggestion_batch_size,
  )(converter=converter)


def create_random_optimizer_factory(
    max_evaluations: int, suggestion_batch_size: int
) -> vb.VectorizedOptimizerFactory:
  """Creates a random optimizer factory."""
  return vb.VectorizedOptimizerFactory(
      strategy_factory=random_strategy_factory,
      max_evaluations=max_evaluations,
      suggestion_batch_size=suggestion_batch_size,
  )


--- vizier/_src/algorithms/optimizers/random_vectorized_optimizer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for random_vectorized_optimizer."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import random_vectorized_optimizer as rvo
from vizier.pyvizier import converters

from absl.testing import absltest


class RandomVectorizedOptimizerTest(absltest.TestCase):
  """Tests for random_vectorized_optimizer."""

  def test_random_optimizer(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_float_param('f2', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: np.sum(x.continuous.padded_array, axis=(-1, -2))
    n_parallel = 3
    random_optimizer = rvo.create_random_optimizer(
        converter=converter, max_evaluations=100, suggestion_batch_size=10
    )
    res = random_optimizer(score_fn=score_fn, count=5, n_parallel=n_parallel)
    self.assertLen(res.rewards, 5)
    self.assertSequenceEqual(res.features.continuous.shape, (5, n_parallel, 2))

  def test_random_optimizer_factory(self):
    random_optimizer_factory = rvo.create_random_optimizer_factory(
        max_evaluations=100, suggestion_batch_size=10
    )
    self.assertEqual(random_optimizer_factory.suggestion_batch_size, 10)
    self.assertEqual(random_optimizer_factory.max_evaluations, 100)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/optimizers/vectorized_base.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Base class for vectorized acquisition optimizers."""

import abc
import datetime
import json
from typing import Callable, Generic, Optional, Protocol, TypeVar, Union

from absl import logging
import attr
import equinox as eqx
from flax import struct
import jax
from jax import numpy as jnp
import numpy as np
from vizier import pyvizier as vz
from vizier._src.jax import types
from vizier.pyvizier import converters
from vizier.utils import json_utils

_S = TypeVar('_S')  # A container of optimizer state that works as a Pytree.

# `VectorizedOptimizerInput` holds features for acquisition function
# optimization and is intended to be private to the `optimizers` submodule.
# Each component has shape (batch_size, n_parallel, n_padded_features)
VectorizedOptimizerInput = types.ContinuousAndCategorical[types.Array]


ACQUISITION_OPTIMIZATION_WARNING_KEY = 'acquisition_optimization_warning'


class RandomSampler(abc.ABC):
  """Random sampler for vectorized optimizer."""

  @abc.abstractmethod
  def __call__(
      self,
      num_samples: int,
      n_parallel: int,
      seed: jax.Array,
  ) -> VectorizedOptimizerInput:
    """Samples random points for vectorized optimizer."""


class Projection(abc.ABC):
  """Projection operator for vectorized optimizer."""

  @abc.abstractmethod
  def __call__(
      self,
      features: VectorizedOptimizerInput,
  ) -> VectorizedOptimizerInput:
    """Projects features for vectorized optimizer."""


def optimizer_to_model_input_single_array(
    x: types.Array, n_features: jax.Array
) -> types.PaddedArray:
  mask = jnp.ones_like(x, dtype=bool)
  mask = jnp.logical_and(mask, jnp.arange(x.shape[-1]) < n_features)
  return types.PaddedArray(
      x,
      fill_value=jnp.zeros([], dtype=x.dtype),
      _original_shape=jnp.concatenate(
          [jnp.array(x.shape[:-1]), jnp.array([n_features])], axis=0
      ),
      _mask=mask,
      _nopadding_done=False,  # Fix
  )


def _optimizer_to_model_input(
    x: VectorizedOptimizerInput,
    n_features: types.ContinuousAndCategorical,
    squeeze_middle_dim: bool = False,
) -> types.ModelInput:
  if squeeze_middle_dim:
    x_cont = jnp.squeeze(x.continuous, axis=1)
    x_cat = jnp.squeeze(x.categorical, axis=1)
  else:
    x_cont = x.continuous
    x_cat = x.categorical
  return types.ModelInput(
      continuous=optimizer_to_model_input_single_array(
          x_cont, n_features.continuous
      ),
      categorical=optimizer_to_model_input_single_array(
          x_cat, n_features.categorical
      ),
  )


def _reshape_to_parallel_batches(
    x: types.PaddedArray, parallel_dim: int
) -> tuple[jax.Array, jax.Array]:
  """Docstring."""

  new_batch_dim = x.shape[0] // parallel_dim
  new_padded_array = jnp.reshape(
      x.padded_array[: new_batch_dim * parallel_dim],
      (new_batch_dim, parallel_dim, x.shape[-1]),
  )

  valid_batch_mask = (
      jnp.arange(new_batch_dim) < x._original_shape[0] // parallel_dim  # pylint: disable=protected-access
  )
  return new_padded_array, valid_batch_mask


class VectorizedStrategyResults(eqx.Module):
  """Container for a vectorized strategy result."""

  features: VectorizedOptimizerInput  # (batch_size, n_parallel, n_features)
  rewards: types.Array  # (batch_size,)
  aux: dict[str, jax.Array] = eqx.field(default_factory=dict)


class VectorizedStrategy(abc.ABC, Generic[_S]):
  """JIT-friendly optimizer that maintains an internal state of type `_S`.

  The strategy is responsible for generating suggestions that will maximize the
  reward. The order of calls is important. It's expected to be used in
  'suggest','update', 'suggest', 'update', etc.
  """

  @abc.abstractmethod
  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ) -> _S:
    """Initialize the state.

    Arguments:
      seed: Random seed for state initialization.
      n_parallel: Number of points that the acquisition function maps to a
        single value. This arg may be greater than 1 if a parallel acquisition
        function (qEI, qUCB) is used; otherwise it should be 1.
      prior_features: (n_prior_features, n_parallel, features_count)
      prior_rewards: (n_prior_features, )

    Returns:
      initial_state:
    """

  @abc.abstractmethod
  def suggest(
      self,
      seed: jax.Array,
      state: _S,
      n_parallel: int = 1,
  ) -> VectorizedOptimizerInput:
    """Generate new suggestions.

    Arguments:
      seed: Random seed.
      state: Optimizer state.
      n_parallel: Number of points that the acquisition function maps to a
        single value. This arg may be greater than 1 if a parallel acquisition
        function (qEI, qUCB) is used; otherwise it should be 1.

    Returns:
      suggested features: (batch_size, features_count)
    """

  @property
  @abc.abstractmethod
  def suggestion_batch_size(self) -> int:
    """The number of suggestions returned at every suggest call."""

  @abc.abstractmethod
  def update(
      self,
      seed: jax.Array,
      state: _S,
      batch_features: VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> _S:
    """Update the strategy state with the results of the last suggestions.

    Arguments:
      seed: Random seed.
      state: Optimizer state.
      batch_features: (batch_size, n_parallel, features_count)
      batch_rewards: (batch_size, )
    """


class VectorizedStrategyFactory(Protocol):
  """Factory class to generate vectorized strategy.

  It's used in VectorizedOptimizer to create a new strategy every 'optimize'
  call.
  """

  def __call__(
      self,
      converter: converters.TrialToModelInputConverter,
      *,
      suggestion_batch_size: int,
  ) -> VectorizedStrategy:
    """Create a new vectorized strategy.

    Arguments:
      converter: The trial to array converter.
      suggestion_batch_size: The number of trials to be evaluated at once.
    """
    ...


class ArrayScoreFunction(Protocol):
  """Protocol for scoring array of trials.

  This protocol is suitable for optimizing batch of candidates (each one with
  its own separate score).
  """

  def __call__(
      self,
      batched_array_trials: types.ModelInput,
      seed: jax.Array,
  ) -> types.Array:
    """Evaluates the array of batched trials.

    Arguments:
      batched_array_trials: Array of shape (batch_size, n_feature_dimensions).
      seed: Random seed.

    Returns:
      Array of shape (batch_size,).
    """


# TODO: Decide on consistent terminology for acquisition functions
# that evaluate sets of points.
class ParallelArrayScoreFunction(Protocol):
  """Protocol for scoring array of parallel trials.

  This protocol is suitable for optimizing in parallel multiple candidates
  (e.g. qUCB).
  """

  def __call__(
      self,
      parallel_array_trials: types.ModelInput,
      seed: jax.Array,
  ) -> types.Array:
    """Evaluates the array of batched trials.

    Arguments:
      parallel_array_trials: Array of shape (batch_size, n_parallel,
        n_feature_dimensions).
      seed: Random seed.

    Returns:
      Array of shape (batch_size).
    """


@struct.dataclass
class VectorizedOptimizer(Generic[_S]):
  """Vectorized strategy optimizer.

  The optimizer is stateless and will create a new vectorized strategy at the
  beginning of every 'optimize' call.

  The optimizer is responsible for running the iterative optimization process
  using the vectorized strategy, which consists of:
  1. Ask the strategy for suggestions.
  2. Evaluate the suggestions to get rewards.
  3. Tell the strategy about the rewards of its suggestion, so the strategy can
  update its internal state.

  The optimization process will terminate when the time limit or the total
  objective function evaluations limit has exceeded.

  Attributes:
    strategy: A factory for generating new strategy.
    n_feature_dimensions_with_padding: Number of feature dimensions including
      padding.
    n_feature_dimensions: Number of feature dimensions (less than or equal to
      `n_feature_dimensions_with_padding`).
    suggestion_batch_size: Number of suggested points returned at each call.
    max_evaluations: The maximum number of objective function evaluations.
    dtype: Dtype of input data.
    use_fori: Whether to use JAX's fori_loop in the suggest-evalute-update loop.
  """

  strategy: VectorizedStrategy[_S]
  n_feature_dimensions: types.ContinuousAndCategorical[jax.Array]
  n_feature_dimensions_with_padding: types.ContinuousAndCategorical[int] = (
      struct.field(pytree_node=False)
  )
  suggestion_batch_size: int = struct.field(pytree_node=False, default=25)
  max_evaluations: int = struct.field(pytree_node=False, default=75_000)
  dtype: types.ContinuousAndCategorical[jnp.dtype] = struct.field(
      pytree_node=False,
      default=types.ContinuousAndCategorical[jnp.dtype](  # pytype: disable=wrong-arg-types  # jnp-type
          jnp.float64, types.INT_DTYPE
      ),
  )
  use_fori: bool = struct.field(pytree_node=False, default=True)

  # TODO: Remove score_fn argument.
  # pylint: disable=g-bare-generic
  def __call__(
      self,
      score_fn: Union[ArrayScoreFunction, ParallelArrayScoreFunction],
      *,
      score_with_aux_fn: Optional[Callable] = None,
      count: int = 1,
      prior_features: Optional[types.ModelInput] = None,
      n_parallel: Optional[int] = None,
      seed: Optional[int] = None,
  ) -> VectorizedStrategyResults:
    """Optimize the objective function.

    The ask-evaluate-tell optimization procedure that runs until the allocated
    time or evaluations count exceeds.

    The number of suggestions is determined by the strategy, which is the
    `suggestion_count` property.

    The converter should be the same one used to convert trials to arrays in the
    format that the objective function expects to receive. It's used to convert
    backward the strategy's best array candidates to trial candidates.
    In addition, the converter is passed to the strategy so it could be aware
    of the type associated with each array index, and which indices are part of
    the same CATEGORICAL parameter one-hot encoding.

    The optimization stops when either of 'max_evaluations' or 'max_duration' is
    reached.

    Arguments:
      score_fn: A callback that expects 2D Array with dimensions (batch_size,
        features_count) or a 3D array with dimensions (batch_size, n_parallel,
        features_count), and a random seed, and returns a 1D Array
        (batch_size,).
      score_with_aux_fn: A callback similar to score_fn but additionally returns
        an array tree.
      count: The number of suggestions to generate.
      prior_features: Completed trials to be used for knowledge transfer. When
        the optimizer is used to optimize a designer's acquisition function, the
        prior trials are the previous designer suggestions provided in the
        ordered they were suggested.
      n_parallel: This arg should be specified if a parallel acquisition
        function (e.g. qEI, qUCB) is used, which evaluates a set of points of
        this size to a single value. Pass `None` when optimizing acquisition
        functions that evaluate a single point. (Note that `num_parallel=1` and
        `num_parallel=None` will each return a single suggestion, though they
        are different in that `num_parallel=1` indicates that the acquisition
        function consumes the two rightmost dimensions of the input while
        `num_parallel=None` indicates that only the rightmost dimension is
        consumed.)
      seed: The seed to use in the random generator.

    Returns:
      An array containing the best trials found in the optimization of shape
      (count, n_parallel or 1, n_feature_dimensions).
    """
    jax.monitoring.record_event('/vizier/jax/vectorized_optimizer/call/traced')
    start_time = datetime.datetime.now()
    seed = jax.random.PRNGKey(0) if seed is None else seed
    seed, acq_fn_seed = jax.random.split(seed)

    dimension_is_missing = jax.tree_util.tree_map(
        lambda pad_dim, dim: jnp.arange(pad_dim) >= dim,
        self.n_feature_dimensions_with_padding,
        self.n_feature_dimensions,
    )

    if n_parallel is None:
      # Squeeze out the singleton dimension of `features` before passing to a
      # non-parallel acquisition function to avoid batch shape collisions.
      # Use the same acquisition function seed at every call.
      eval_score_fn = lambda x: score_fn(  # pylint: disable=g-long-lambda
          _optimizer_to_model_input(
              x, self.n_feature_dimensions, squeeze_middle_dim=True
          ),
          acq_fn_seed,
      )
    else:
      eval_score_fn = lambda x: score_fn(  # pylint: disable=g-long-lambda
          _optimizer_to_model_input(x, self.n_feature_dimensions),
          acq_fn_seed,
      )

    # TODO: We should pass RNGKey to score_fn.
    prior_rewards = None
    parallel_dim = n_parallel or 1
    if prior_features is not None:
      continuous_prior, continuous_mask = _reshape_to_parallel_batches(
          prior_features.continuous, parallel_dim
      )
      categorical_prior, categorical_mask = _reshape_to_parallel_batches(
          prior_features.categorical, parallel_dim
      )
      prior_features = VectorizedOptimizerInput(
          continuous=continuous_prior, categorical=categorical_prior
      )
      prior_features = jax.tree_util.tree_map(
          lambda dim, feat: jnp.where(dim, jnp.zeros_like(feat), feat),
          dimension_is_missing,
          prior_features,
      )
      prior_rewards = eval_score_fn(prior_features)
      prior_rewards = jnp.where(
          jnp.logical_and(continuous_mask, categorical_mask),
          prior_rewards,
          -jnp.inf * jnp.ones_like(prior_rewards),
      )

    def _optimization_one_step(_, args):
      jax.monitoring.record_event(
          '/vizier/jax/vectorized_optimizer/call/one_step/traced'
      )
      state, best_results, seed = args
      suggest_seed, update_seed, new_seed = jax.random.split(seed, num=3)
      new_features = self.strategy.suggest(
          suggest_seed, state=state, n_parallel=parallel_dim
      )
      # Ensure masking out padded dimensions in new features.
      new_features = jax.tree_util.tree_map(
          lambda dim, feat: jnp.where(dim, jnp.zeros_like(feat), feat),
          dimension_is_missing,
          new_features,
      )

      new_rewards = eval_score_fn(new_features)
      new_state = self.strategy.update(
          update_seed, state, new_features, new_rewards
      )
      new_best_results = self._update_best_results(
          best_results, count, new_features, new_rewards
      )
      return new_state, new_best_results, new_seed

    init_seed, loop_seed = jax.random.split(seed)
    # TODO: Consider initializing with prior features/rewards.
    init_best_results = VectorizedStrategyResults(
        rewards=-jnp.inf * jnp.ones([count]),
        features=VectorizedOptimizerInput(
            continuous=jnp.zeros(
                [
                    count,
                    parallel_dim,
                    self.n_feature_dimensions_with_padding.continuous,
                ],
                dtype=self.dtype.continuous,
            ),
            categorical=jnp.zeros(
                [
                    count,
                    parallel_dim,
                    self.n_feature_dimensions_with_padding.categorical,
                ],
                dtype=self.dtype.categorical,
            ),
        ),
    )
    init_args = (
        self.strategy.init_state(
            init_seed,
            n_parallel=parallel_dim,
            prior_features=prior_features,
            prior_rewards=prior_rewards,
        ),
        init_best_results,
        loop_seed,
    )
    if self.use_fori:
      _, best_results, _ = jax.lax.fori_loop(
          0,
          (self.max_evaluations - 1) // self.suggestion_batch_size + 1,
          _optimization_one_step,
          init_args,
      )
    else:
      args = init_args
      for i in range(
          (self.max_evaluations - 1) // self.suggestion_batch_size + 1
      ):
        args = _optimization_one_step(i, args)
      _, best_results, _ = args

    if score_with_aux_fn:
      if n_parallel is None:
        aux = score_with_aux_fn(
            _optimizer_to_model_input(
                best_results.features,
                self.n_feature_dimensions,
                squeeze_middle_dim=True,
            ),
            seed=acq_fn_seed,
        )[1]
      else:
        aux = score_with_aux_fn(
            _optimizer_to_model_input(
                best_results.features, self.n_feature_dimensions
            ),
            seed=acq_fn_seed,
        )[1]

      best_results = VectorizedStrategyResults(
          best_results.features,
          best_results.rewards,
          aux,
      )

    logging.info(
        (
            'Optimization completed. Duration: %s. Evaluations: %s. Best'
            ' Results: %s'
        ),
        datetime.datetime.now() - start_time,
        (
            self.max_evaluations
            // self.suggestion_batch_size
            * self.suggestion_batch_size
        ),
        best_results,
    )

    return best_results

  def _update_best_results(
      self,
      best_results: VectorizedStrategyResults,
      count: int,
      batch_features: VectorizedOptimizerInput,
      batch_rewards: jax.Array,
  ) -> VectorizedStrategyResults:
    """Update the best results the optimizer seen thus far.

    The best results are kept in a heap to efficiently maintain the top 'count'
    results throughout the optimizer run.

    Arguments:
      best_results: A heap storing the best results seen thus far. Implemented
        as a list of maximum size of 'count'.
      count: The number of best results to store.
      batch_features: The current suggested features batch array with a
        dimension of (batch_size, feature_dim) or (batch_size, n_parallel,
        feature_dim).
      batch_rewards: The current reward batch array with a dimension of
        (batch_size,).

    Returns:
      trials:
    """
    all_rewards = jnp.concatenate([batch_rewards, best_results.rewards], axis=0)
    all_features = VectorizedOptimizerInput(
        continuous=jnp.concatenate(
            [batch_features.continuous, best_results.features.continuous],
            axis=0,
        ),
        categorical=jnp.concatenate(
            [batch_features.categorical, best_results.features.categorical],
            axis=0,
        ),
    )
    top_indices = jnp.argpartition(-all_rewards, count - 1)[:count]
    return VectorizedStrategyResults(
        rewards=all_rewards[top_indices],
        features=VectorizedOptimizerInput(
            continuous=all_features.continuous[top_indices],
            categorical=all_features.categorical[top_indices],
        ),
    )


# TODO: Should return suggestions not trials.
def best_candidates_to_trials(
    best_results: VectorizedStrategyResults,
    converter: converters.TrialToModelInputConverter,
) -> list[vz.Trial]:
  """Returns the best candidate trials in the original search space."""
  best_features = best_results.features
  trials = []
  sorted_ind = jnp.argsort(-best_results.rewards)
  for i in range(len(best_results.rewards)):
    # Create trials and convert the strategy features back to parameters.
    ind = sorted_ind[i]
    suggested_features = VectorizedOptimizerInput(
        best_features.continuous[ind], best_features.categorical[ind]
    )
    reward = best_results.rewards[ind]

    # Loop over the number of candidates per batch (which will be one, unless a
    # parallel acquisition function is used).
    for j in range(suggested_features.continuous.shape[0]):
      features = VectorizedOptimizerInput(
          continuous=jnp.expand_dims(suggested_features.continuous[j], axis=0),
          categorical=jnp.expand_dims(
              suggested_features.categorical[j], axis=0
          ),
      )
      trial = vz.Trial(
          parameters=converter.to_parameters(
              _optimizer_to_model_input(
                  features,
                  n_features=types.ContinuousAndCategorical(
                      len(converter.output_specs.continuous),
                      len(converter.output_specs.categorical),
                  ),
              )
          )[0]
      )
      metadata = trial.metadata.ns('devinfo')
      aux_tree = jax.tree.map(
          lambda x, ind=ind: np.asarray(x[ind]), best_results.aux
      )
      metadata['acquisition_optimization'] = json.dumps(
          {'acquisition': best_results.rewards[ind]} | aux_tree,
          cls=json_utils.NumpyEncoder,
      )
      leaves, _ = jax.tree.flatten(
          {
              'acquisition': best_results.rewards[ind],
              'aux': aux_tree,
          },
      )
      any_nan = any([jnp.isnan(leaf).any() for leaf in leaves])
      if any_nan:
        metadata[ACQUISITION_OPTIMIZATION_WARNING_KEY] = (
            'NaNs encountered in acquisition optimization. See the'
            ' "acquisition_optimization" field in the metadata for more'
            ' details.'
        )

      trial.complete(vz.Measurement({'acquisition': reward}))
      trials.append(trial)
  return trials


# TODO: This function should return jax types.
def trials_to_sorted_array(
    prior_trials: list[vz.Trial],
    converter: converters.TrialToModelInputConverter,
) -> Optional[types.ModelInput]:
  """Sorts trials by the order they were created and converts to array."""
  if prior_trials:
    prior_trials = sorted(prior_trials, key=lambda x: x.creation_time)
    prior_features = converter.to_features(prior_trials)
  else:
    prior_features = None
  return prior_features


@attr.define
class VectorizedOptimizerFactory:
  """Vectorized strategy optimizer factory."""

  strategy_factory: VectorizedStrategyFactory
  max_evaluations: int = 75_000
  suggestion_batch_size: int = 25
  use_fori: bool = True

  def __call__(
      self,
      converter: converters.TrialToModelInputConverter,
  ) -> VectorizedOptimizer:
    """Generates a new VectorizedOptimizer object."""
    strategy = self.strategy_factory(
        converter, suggestion_batch_size=self.suggestion_batch_size
    )
    n_feature_dimensions = getattr(
        strategy,
        'n_feature_dimensions',
        types.ContinuousAndCategorical(
            len(converter.output_specs.continuous),
            len(converter.output_specs.categorical),
        ),
    )
    empty_features = converter.to_features([])
    n_feature_dimensions_with_padding = getattr(
        strategy,
        'n_feature_dimensions_with_padding',
        types.ContinuousAndCategorical[int](
            empty_features.continuous.shape[-1],
            empty_features.categorical.shape[-1],
        ),
    )
    return VectorizedOptimizer(
        strategy=strategy,
        n_feature_dimensions=n_feature_dimensions,
        n_feature_dimensions_with_padding=n_feature_dimensions_with_padding,
        suggestion_batch_size=self.suggestion_batch_size,
        max_evaluations=self.max_evaluations,
        use_fori=self.use_fori,
        dtype=converter._impl.dtype,
    )


--- vizier/_src/algorithms/optimizers/vectorized_base_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vectorized_base."""

from typing import Optional

import chex
import jax
from jax import numpy as jnp
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.jax import types
from vizier.pyvizier import converters

from absl.testing import absltest
from absl.testing import parameterized

# pylint: disable=g-long-lambda


@chex.dataclass(frozen=True)
class FakeIncrementVectorizedStrategyState:
  """State for FakeIncrementVectorizedStrategy."""

  iterations: int


class FakeIncrementVectorizedStrategy(
    vb.VectorizedStrategy[FakeIncrementVectorizedStrategyState]
):
  """Fake vectorized strategy with incrementing suggestions."""

  def __init__(self, *args, **kwargs):
    pass

  def suggest(
      self,
      seed: jax.Array,
      state: FakeIncrementVectorizedStrategyState,
      n_parallel: int = 1,
  ) -> vb.VectorizedOptimizerInput:
    # The following structure allows to test the top K results.
    i = state.iterations
    suggestions = (
        jnp.array([
            [i % 10, i % 10],
            [(i + 1) % 10, (i + 1) % 10],
            [(i + 2) % 10, (i + 2) % 10],
            [(i + 3) % 10, (i + 3) % 10],
            [(i + 4) % 10, (i + 4) % 10],
        ])
        / 10
    )
    return vb.VectorizedOptimizerInput(
        jnp.repeat(suggestions[:, jnp.newaxis, :], n_parallel, axis=1),
        jnp.zeros([5, n_parallel, 0], dtype=types.INT_DTYPE),
    )

  @property
  def suggestion_batch_size(self) -> int:
    return 5

  def update(
      self,
      seed: jax.Array,
      state: FakeIncrementVectorizedStrategyState,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> FakeIncrementVectorizedStrategyState:
    return FakeIncrementVectorizedStrategyState(iterations=state.iterations + 5)

  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[vb.VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ) -> FakeIncrementVectorizedStrategyState:
    del seed
    return FakeIncrementVectorizedStrategyState(iterations=0)


# pylint: disable=unused-argument
def fake_increment_strategy_factory(
    converter: converters.TrialToModelInputConverter,
    suggestion_batch_size: int,
) -> vb.VectorizedStrategy:
  return FakeIncrementVectorizedStrategy()


@chex.dataclass(frozen=True)
class FakePriorTrialsStrategyState:
  """State for FakeIncrementVectorizedStrategy."""

  features: vb.VectorizedOptimizerInput
  rewards: types.Array


class FakePriorTrialsVectorizedStrategy(
    vb.VectorizedStrategy[FakePriorTrialsStrategyState]
):
  """Fake vectorized strategy to test prior trials."""

  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[vb.VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ):
    if prior_rewards is not None and len(prior_rewards.shape) != 1:
      raise ValueError('Expected seed labels to have 1D dimension!')
    return FakePriorTrialsStrategyState(
        features=prior_features, rewards=prior_rewards
    )

  def suggest(
      self,
      seed: jax.Array,
      state: FakePriorTrialsStrategyState,
      n_parallel: int = 1,
  ) -> vb.VectorizedOptimizerInput:
    return vb.VectorizedOptimizerInput(
        continuous=state.features.continuous[
            jnp.argmax(state.rewards, axis=-1)
        ][jnp.newaxis, :],
        categorical=jnp.zeros([1, n_parallel, 0], dtype=types.INT_DTYPE),
    )

  @property
  def suggestion_batch_size(self) -> int:
    return 1

  def update(
      self,
      seed: jax.Array,
      state: FakePriorTrialsStrategyState,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> FakePriorTrialsStrategyState:
    return state


# pylint: disable=unused-argument
def fake_prior_trials_strategy_factory(
    converter: converters.TrialToModelInputConverter,
    suggestion_batch_size: int,
) -> vb.VectorizedStrategy:
  return FakePriorTrialsVectorizedStrategy()


class VectorizedBaseTest(parameterized.TestCase):

  @parameterized.parameters(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
  def test_optimize_candidates_len(self, count):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_float_param('f2', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: jnp.sum(x.continuous.padded_array, axis=-1)
    optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_increment_strategy_factory,
        max_evaluations=100,
    )(converter=converter)
    res_array = optimizer(score_fn=score_fn, count=count)
    res = vb.best_candidates_to_trials(res_array, converter=converter)
    self.assertLen(res, count)

  @parameterized.parameters(
      (1, 3),
      (2, 4),
  )
  def test_optimize_parallel_candidates_len(self, count, n_parallel):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_float_param('f2', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: jnp.sum(x.continuous.padded_array, axis=(-1, -2))
    optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_increment_strategy_factory,
        max_evaluations=100,
    )(converter=converter)
    res_array = optimizer(score_fn=score_fn, count=count, n_parallel=n_parallel)
    res = vb.best_candidates_to_trials(res_array, converter=converter)
    self.assertLen(res, count * n_parallel)

  @parameterized.parameters(True, False)
  def test_best_candidates_count_is_1(self, use_fori):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: -jnp.max(
        jnp.square(x.continuous.padded_array - 0.52), axis=-1
    )
    strategy_factory = FakeIncrementVectorizedStrategy
    optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=10,
        use_fori=use_fori,
    )(converter=converter)
    best_candidates_array = optimizer(score_fn=score_fn, count=1)
    best_candidates = vb.best_candidates_to_trials(
        best_candidates_array, converter=converter
    )
    # check the best candidate
    self.assertEqual(best_candidates[0].parameters['f1'].value, 0.5)
    self.assertEqual(best_candidates[0].parameters['f2'].value, 0.5)
    self.assertAlmostEqual(
        best_candidates[0]
        .final_measurement_or_die.metrics['acquisition']
        .value,
        -((0.5 - 0.52) ** 2),
    )

  @parameterized.parameters(True, False)
  def test_best_candidates_count_is_3(self, use_fori):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_fn = lambda x, _: -jnp.max(
        jnp.square(x.continuous.padded_array - 0.52), axis=-1
    )
    optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_increment_strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=10,
        use_fori=use_fori,
    )(converter=converter)
    best_candidates_array = optimizer(score_fn=score_fn, count=3)
    best_candidates = vb.best_candidates_to_trials(
        best_candidates_array, converter=converter
    )
    # check 1st best candidate
    self.assertAlmostEqual(best_candidates[0].parameters['f1'].value, 0.5)
    self.assertAlmostEqual(best_candidates[0].parameters['f2'].value, 0.5)
    self.assertAlmostEqual(
        best_candidates[0]
        .final_measurement_or_die.metrics['acquisition']
        .value,
        -((0.5 - 0.52) ** 2),
    )
    # check 2nd best candidate
    self.assertAlmostEqual(best_candidates[1].parameters['f1'].value, 0.6)
    self.assertAlmostEqual(best_candidates[1].parameters['f2'].value, 0.6)
    self.assertAlmostEqual(
        best_candidates[1]
        .final_measurement_or_die.metrics['acquisition']
        .value,
        -((0.6 - 0.52) ** 2),
    )
    # check 3rd best candidate
    self.assertAlmostEqual(best_candidates[2].parameters['f1'].value, 0.4)
    self.assertAlmostEqual(best_candidates[2].parameters['f2'].value, 0.4)
    self.assertAlmostEqual(
        best_candidates[2]
        .final_measurement_or_die.metrics['acquisition']
        .value,
        -((0.4 - 0.52) ** 2),
    )

  @parameterized.named_parameters(
      ('score_nan_aux_ok', jnp.nan, 1.0), ('score_ok_aux_nan', -2.0, jnp.nan)
  )
  def test_best_candidates_to_trials_warnings_on_nans(
      self, score_value: float, aux1_value: float
  ):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    score_with_aux_fn = lambda x, seed: (
        score_value * jnp.ones(x.continuous.padded_array.shape[0]),
        {
            'aux1': aux1_value * jnp.ones(x.continuous.padded_array.shape[0]),
            'aux2': 5.0 * jnp.ones(x.continuous.padded_array.shape[0]),
        },
    )
    optimizer = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_increment_strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=10,
    )(converter=converter)
    best_candidates_array = optimizer(
        score_fn=lambda x, seed: score_with_aux_fn(x, seed)[0],
        score_with_aux_fn=score_with_aux_fn,
        count=1,
    )
    best_candidates = vb.best_candidates_to_trials(
        best_candidates_array, converter=converter
    )
    self.assertLen(best_candidates, 1)
    self.assertIn(
        vb.ACQUISITION_OPTIMIZATION_WARNING_KEY,
        best_candidates[0].metadata.ns('devinfo'),
    )
    self.assertIn(
        'NaN',
        best_candidates[0].metadata.ns('devinfo')[
            vb.ACQUISITION_OPTIMIZATION_WARNING_KEY
        ],
    )

  def test_vectorized_optimizer_factory(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    converter = converters.TrialToModelInputConverter.from_problem(problem)
    optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_increment_strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=1000,
    )
    optimizer = optimizer_factory(converter)
    self.assertEqual(optimizer.max_evaluations, 1000)
    self.assertEqual(optimizer.suggestion_batch_size, 5)

  @parameterized.parameters(True, False)
  def test_prior_trials(self, use_fori):
    """Test that the optimizer can correctly parsae and pass seed trials."""
    optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_prior_trials_strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=100,
        use_fori=use_fori,
    )

    study_config = vz.ProblemStatement(
        metric_information=[
            vz.MetricInformation(
                name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = study_config.search_space.root
    root.add_float_param('x1', 0.0, 10.0)
    root.add_float_param('x2', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(study_config)
    optimizer = optimizer_factory(converter)

    trial1 = vz.Trial(parameters={'x1': 1, 'x2': 1})
    measurement1 = vz.Measurement(metrics={'obj': vz.Metric(value=-10.33)})
    trial1.complete(measurement1, inplace=True)

    trial2 = vz.Trial(parameters={'x1': 2, 'x2': 2})
    measurement2 = vz.Measurement(metrics={'obj': vz.Metric(value=5.0)})
    trial2.complete(measurement2, inplace=True)

    prior_features = vb.trials_to_sorted_array(
        [trial1, trial2, trial1], converter=converter
    )
    best_trial_array = optimizer(
        lambda x, _: -jnp.max(
            jnp.square(x.continuous.padded_array - 0.52), axis=-1
        ),
        count=1,
        prior_features=prior_features,
    )
    best_trial = vb.best_candidates_to_trials(
        best_trial_array, converter=converter
    )
    self.assertEqual(best_trial[0].parameters['x1'].value, 2)
    self.assertEqual(best_trial[0].parameters['x2'].value, 2)

    best_trial_array = optimizer(
        lambda x, _: -jnp.max(
            jnp.square(x.continuous.padded_array - 0.52), axis=-1
        ),
        count=1,
        prior_features=vb.trials_to_sorted_array([trial1], converter=converter),
    )
    best_trial = vb.best_candidates_to_trials(
        best_trial_array, converter=converter
    )
    self.assertEqual(best_trial[0].parameters['x1'].value, 1)
    self.assertEqual(best_trial[0].parameters['x2'].value, 1)

  @parameterized.parameters(True, False)
  def test_prior_trials_parallel(self, use_fori):
    optimizer_factory = vb.VectorizedOptimizerFactory(
        strategy_factory=fake_prior_trials_strategy_factory,
        suggestion_batch_size=5,
        max_evaluations=100,
    )

    study_config = vz.ProblemStatement(
        metric_information=[
            vz.MetricInformation(
                name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = study_config.search_space.root
    root.add_float_param('x1', 0.0, 10.0)
    root.add_float_param('x2', 0.0, 10.0)
    root.add_float_param('x3', 0.0, 10.0)
    converter = converters.TrialToModelInputConverter.from_problem(study_config)
    optimizer = optimizer_factory(converter)
    prior_features = types.ModelInput(
        continuous=types.PaddedArray.as_padded(
            jax.random.uniform(jax.random.PRNGKey(0), (14, 3))
        ),
        categorical=types.PaddedArray.as_padded(
            jnp.zeros([14, 0], dtype=types.INT_DTYPE)
        ),
    )
    suggestions = optimizer(
        lambda x, _: -jnp.max(
            jnp.square(x.continuous.padded_array - 0.52), axis=(-1, -2)
        ),
        prior_features=prior_features,
        n_parallel=2,
    )
    self.assertSequenceEqual(suggestions.features.continuous.shape, (1, 2, 3))
    self.assertSequenceEqual(suggestions.rewards.shape, (1,))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/policies/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations




--- vizier/_src/algorithms/policies/designer_policy.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Wrappers for Designer into Policy.

This file should contain the core logic to run Designers
efficiently for testing, benchmarking, or production purposes. Typical
usage is via PolicySuggester.from_designer_factory.
"""

import abc
from typing import Generic, Optional, Type, TypeVar

from absl import logging
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.core import abstractions as vza
from vizier._src.algorithms.policies import trial_caches
from vizier.interfaces import serializable


_T = TypeVar('_T')

_INCOPORATED_COMPLETED_TRIALS_IDS = 'incorporated_completed_trials_ids'


class DesignerPolicy(pythia.Policy):
  """Wraps a Designer into a pythia Policy.

  > IMPORTANT: If your Designer class is (partially) serializable, use
  > (Partially)SerializableDesignerPolicy instead.

  When a Designer cannot be (partially) serialized, we create a new Designer
  instance at the start of each `suggest()` call. The most recently used
  Designer instance can be saved in self._designer. However, it is for
  interactive analysis/debugging purposes only and never used in
  future `suggest()` calls.
  """

  def __init__(
      self,
      supporter: pythia.PolicySupporter,
      designer_factory: vza.DesignerFactory[vza.Designer],
      *,
      policy_name: str = 'DesignerPolicy',
      use_seeding: bool = True,
  ):
    """Init.

    Args:
      supporter:
      designer_factory:
      policy_name: A user visible name.
      use_seeding:
    """
    self._supporter = supporter
    self._designer_factory = designer_factory
    self._policy_name = policy_name
    self._use_seeding = use_seeding

    if self._use_seeding:
      self.suggest = pythia.seed_with_default(self.suggest)

  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:
    logging.info(
        'Processing request for study guid %s, with max trial id %s',
        request.study_guid,
        request.max_trial_id,
    )
    designer = self._designer_factory(request.study_config)
    logging.info(
        'Study guid %s: Designer created. Getting trials from Vizier service.',
        request.study_guid,
    )
    completed = self._supporter.GetTrials(
        status_matches=vz.TrialStatus.COMPLETED
    )
    logging.info(
        'Study guid %s: got %s COMPLETED trials',
        request.study_guid,
        len(completed),
    )
    active = self._supporter.GetTrials(status_matches=vz.TrialStatus.ACTIVE)
    logging.info(
        'Study guid %s: got %s ACTIVE trials', request.study_guid, len(active)
    )
    logging.info(
        'Study guid %s: Updating designer with trials.', request.study_guid
    )
    designer.update(vza.CompletedTrials(completed), vza.ActiveTrials(active))
    self._designer = designer  # saved for debugging purposes only.
    logging.info(
        'Study guid %s: Asking designer for %s suggestions.',
        request.study_guid,
        request.count,
    )
    return pythia.SuggestDecision(
        designer.suggest(request.count), metadata=vz.MetadataDelta()
    )

  def early_stop(
      self, request: pythia.EarlyStopRequest
  ) -> pythia.EarlyStopDecisions:
    raise NotImplementedError(
        'DesignerPolicy does not support the early_stop() method.'
    )

  @property
  def name(self) -> str:
    return self._policy_name


class _SerializableDesignerPolicyBase(
    pythia.Policy, serializable.PartiallySerializable, Generic[_T], abc.ABC
):
  """Partially implemented class for wrapping a (Partially)SerializableDesigner.

  Inherited by (Partially)SerializableDesignerPolicy which is fully implemented.

  (Partially)SerializableDesignerPolicy maintains a synchronized state between
  the set of trial ids that were passed to Designer via `update()` call, and
  the Designer instance that was used for during last `suggest()` call.

  The policy's `dump()` contains the trial ids passed to update() and the
  result of `dump()` called on the wrapped Designer.

  Unlike in the basic DesignerPolicy class, this class tries to minimize
  the computation by following these steps in order:
    * Re-use the saved Designer instance from the last `suggest()` call.
    * `load()` the Designer state from the study-level metadata.
    * If either of the first two steps succeeds, then we update the Designer
    with **newly** completed and **all** active (pending) trials.
    * Otherwise, we update the Designer with all trials.

  > NOTE: This Policy itself is PartiallySerializable, i.e. it implements its
  own 'load' and 'dump' which wrap the designer_factory designer's 'load' and
  'dump' methods.
  """

  _ns_designer = 'designer'
  _ns_cache = 'cache'

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      supporter: pythia.PolicySupporter,
      designer_factory: vza.DesignerFactory[_T],
      *,
      ns_root: str = 'designer_policy_v0',
      verbose: int = 0,
      seed: Optional[int] = None,
      policy_name: str = '_SerializableDesignerPolicyBase',
  ):
    """Init.

    Args:
      problem_statement:
      supporter:
      designer_factory: Factory function to create.
        (Partially)SerializableDesigner designers.
      ns_root: Root of the namespace where policy state is stored.
      verbose: Logging verbosity.
      seed: Random seed to be passed to the designer factory.
      policy_name: A user visible name.
    """
    self._supporter = supporter
    self._designer_factory = designer_factory
    self._ns_root = ns_root
    self._cache: trial_caches.IdDeduplicatingTrialLoader = (
        trial_caches.IdDeduplicatingTrialLoader(
            supporter, include_intermediate_measurements=False
        )
    )
    self._problem_statement = problem_statement
    self._verbose = verbose
    self._designer = None
    self._seed = seed
    self._policy_name = policy_name

  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:
    """Perform a suggest operation.

    The order of operations is:
    1. Initialize the designer and load its state from metadata.
    2. Update the designer with newly completed trials.
    3. Generate suggestions from the designer.
    4. Dump the state of the designer and store it in metadata.

    Arguments:
      request: Pythia suggestion request objects.

    Returns:
      The suggestions from the designer.
    """
    # Note that we can avoid O(Num trials) dependency in the standard scenario,
    # by storing only the last element in a consecutive sequence, e.g.,
    # instead of storing [1,2,3,4,11,12,13,21], store: [4,13,21], but
    # we keep things simple in this pseudocode.
    self._initialize_designer(request.study_config)
    new_completed_trials = self._cache.get_newly_completed_trials(
        request.max_trial_id
    )
    active_trials = self._cache.get_active_trials()
    self.designer.update(
        completed=vza.CompletedTrials(new_completed_trials),
        all_active=vza.ActiveTrials(active_trials),
    )
    logging.info(
        (
            'Updated with %s new completed trials and %s new active trials. '
            'Trial Id cache state: %s'
        ),
        len(new_completed_trials),
        len(active_trials),
        str(self._cache),
    )
    metadata_delta = vz.MetadataDelta()
    # During the 'suggest' call the designer's state could be changed, therefore
    # the state is dumped and stored only after 'suggest' was called.
    suggestions = pythia.SuggestDecision(
        self.designer.suggest(request.count), metadata=metadata_delta
    )
    metadata_delta.on_study.ns(self._ns_root).attach(self.dump())
    return suggestions

  def early_stop(
      self, request: pythia.EarlyStopRequest
  ) -> pythia.EarlyStopDecisions:
    raise NotImplementedError(
        'PartiallySerializableDesignerPolicy does not implement early_stop().'
    )

  @property
  def designer(self) -> _T:
    if self._designer is None:
      raise ValueError(
          '`self._designer` has not been initialized!'
          'Use self._restore_designer(..) to initialize it.'
      )
    return self._designer

  @abc.abstractmethod
  def _restore_designer(self, designer_metadata: vz.Metadata) -> _T:
    """Creates a new Designer by restoring the state from `designer_metadata`.

    Args:
      designer_metadata:

    Returns:
      New Designer object.

    Raises:
      DecodeError: `designer_metadata` does not contain valid information
        to restore a Designer state.
    """

  def _initialize_designer(
      self, problem_statement: vz.ProblemStatement
  ) -> None:
    """Guarantees that `self._designer` is populated.

    This method guarantees that after a successful call, `self.designer` does
    not raise an Exception.
    This method catches all DecodeErrors.

    Args:
      problem_statement:

    Raises:
      ValueError: If problem_statement is different from the initially
        received problem_statement.
    """
    if self._designer is not None:
      # When the same policy object is maintained in RAM, prefer keeping
      # the designer object over restoring the state from metadata.
      # TOCONSIDER: Adding a boolean knob to turn off this behavior.
      logging.log_if(
          logging.INFO,
          (
              'Policy already has a designer. '
              'It will not attempt to load from metadata.'
          ),
          self._verbose >= 2,
      )
      return
    elif self._problem_statement != problem_statement:
      raise ValueError(
          f'{type(self)} cannot be re-used for different study configs!'
          f'Policy: {self}, previous study: {self._problem_statement} '
          f'new study: {problem_statement}'
      )

    metadata = problem_statement.metadata.ns(self._ns_root)
    try:
      self.load(metadata)
      logging.log_if(
          logging.INFO, 'Successfully decoded all states!', self._verbose >= 1
      )
    except serializable.DecodeError as e:
      logging.log_if(
          logging.INFO, 'Failed to decode state. %s', self._verbose >= 1, e
      )
      # Restart the state of the policy.
      self._designer = self._designer_factory(
          problem_statement, seed=self._seed
      )
      self._cache.clear()

  def dump(self) -> vz.Metadata:
    """Dump state.

    Returns:
      Metadata has the following namespace hierarchy:
        Namespace([self._ns_root]): contains the policy's state.
        Namespace([self._ns_root, self._ns_designer]: contains the designer's
          state.
    """
    md = vz.Metadata()
    md.ns(self._ns_designer).attach(self.designer.dump())
    md.ns(self._ns_cache).attach(self._cache.dump())
    return md

  def load(self, md: vz.Metadata) -> None:
    self._cache.load(md.ns(self._ns_cache))
    self._designer = self._restore_designer(md.ns(self._ns_designer))

  @property
  def name(self) -> str:
    return self._policy_name


# TODO: Rewrite base to separate incorporated trial logic from
# serialization logic. For now, it simply disables the serialization.
class InRamDesignerPolicy(_SerializableDesignerPolicyBase[vza.Designer]):
  """Wraps a Designer in RAM for efficient updates()."""

  # Override restore() to simply return the designer.
  def _restore_designer(self, designer_metadata: vz.Metadata) -> vza.Designer:
    if self._designer is None:
      raise ValueError(
          '`self._designer` has not been initialized!'
          ' Call self._initialize_designer(..) first.'
      )
    return self._designer

  # Override dump() since this is in ram.
  def dump(self) -> vz.Metadata:
    return vz.Metadata()


class PartiallySerializableDesignerPolicy(
    _SerializableDesignerPolicyBase[vza.PartiallySerializableDesigner]
):
  """Wraps a PartiallySerializableDesigner."""

  def _restore_designer(
      self, designer_metadata: vz.Metadata
  ) -> vza.PartiallySerializableDesigner:
    designer = self._designer_factory(self._problem_statement)
    designer.load(designer_metadata)
    return designer


class SerializableDesignerPolicy(
    _SerializableDesignerPolicyBase[vza.SerializableDesigner]
):
  """Wraps a SerializableDesigner."""

  def __init__(
      self,
      problem_statement: vz.ProblemStatement,
      supporter: pythia.PolicySupporter,
      designer_factory: vza.DesignerFactory[vza.SerializableDesigner],
      designer_cls: Type[vza.SerializableDesigner],
      *,
      ns_root: str = 'designer_policy_v0',
      verbose: int = 0,
  ):
    """Init.

    Args:
      problem_statement:
      supporter:
      designer_factory: Used when designer state cannot be restored.
      designer_cls: Type name of the designer. Its load() classmethod is called
        to restore the designer state.
      ns_root: Root of the namespace where policy state is stored.
      verbose: Logging verbosity.
    """
    super().__init__(
        problem_statement,
        supporter,
        designer_factory,
        ns_root=ns_root,
        verbose=verbose,
    )
    self._designer_cls = designer_cls

  def _restore_designer(
      self, designer_metadata: vz.Metadata
  ) -> vza.SerializableDesigner:
    return self._designer_cls.recover(designer_metadata)


--- vizier/_src/algorithms/policies/designer_policy_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for designer_policy."""

from typing import Optional, Sequence
from vizier import algorithms as vza
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies import designer_policy as dp
from vizier.interfaces import serializable
from absl.testing import absltest


class _FakeDesigner(vza.Designer):

  def __init__(self):
    self.num_incorporated_completed_trials = 0
    self._num_incorporated_active_trials = 0
    self.last_completed = []
    self.last_active = []

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    return [vz.TrialSuggestion(vz.ParameterDict())] * count

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    self.last_completed = completed.trials
    self.last_active = all_active.trials
    self.num_incorporated_completed_trials += len(completed.trials)
    self._num_incorporated_active_trials += len(all_active.trials)


class _FakeSerializableDesigner(vza.PartiallySerializableDesigner,
                                vza.SerializableDesigner):

  def __init__(self):
    self.num_incorporated_completed_trials = 0
    self.num_incorporated_active_trials = 0
    self.last_completed = []
    self.last_active = []

    # For debugging and testing. Not stored in the metadata.
    self.last_completed = None

  def suggest(self,
              count: Optional[int] = None) -> Sequence[vz.TrialSuggestion]:
    return [vz.TrialSuggestion(vz.ParameterDict())] * count

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ):
    self.last_completed = completed.trials
    self.last_active = all_active.trials
    self.num_incorporated_completed_trials += len(completed.trials)
    self.num_incorporated_active_trials += len(all_active.trials)

  def dump(self) -> vz.Metadata:
    md = vz.Metadata()
    md.ns('ns1')['foo1'] = 'bar1'
    md.ns('ns1').ns('ns11')['foo11'] = 'bar11'
    md['num_incorporated_completed_trials'] = str(
        self.num_incorporated_completed_trials
    )
    return md

  @classmethod
  def recover(cls, md: vz.Metadata) -> '_FakeSerializableDesigner':
    try:
      designer = cls()
      designer.num_incorporated_completed_trials = int(
          md['num_incorporated_completed_trials']
      )
      return designer
    except KeyError as e:
      raise KeyError(f'Cannot find in {md}') from e

  def load(self, md: vz.Metadata):
    try:
      self.num_incorporated_completed_trials = int(
          md['num_incorporated_completed_trials']
      )
    except KeyError as e:
      raise serializable.DecodeError(f'Cannot find in {md}') from e


_NUM_INITIAL_COMPLETED_TRIALS = 10
_NUM_INITIAL_ACTIVE_TRIALS = 2


def _create_runner() -> pythia.InRamPolicySupporter:
  """Creates the default runner with completed and active trials."""
  runner = pythia.InRamPolicySupporter(vz.ProblemStatement())
  runner.AddTrials(
      [
          vz.Trial().complete(vz.Measurement())
          for _ in range(_NUM_INITIAL_COMPLETED_TRIALS)
      ]
  )
  # The default status is ACTIVE.
  runner.AddTrials([vz.Trial() for _ in range(_NUM_INITIAL_ACTIVE_TRIALS)])
  return runner


class DesignerPolicyNormalOperationTest(absltest.TestCase):
  """Tests Designer policies under error-free conditions."""

  def setUp(self):
    super().setUp()
    self.maxDiff = None

  def test_restore_fully_serializable_designer(self):
    runner = _create_runner()
    policy = dp.SerializableDesignerPolicy(
        problem_statement=vz.ProblemStatement(),
        supporter=runner,
        designer_factory=lambda _, **kwargs: _FakeSerializableDesigner(),
        designer_cls=_FakeSerializableDesigner,
        ns_root='test',
        verbose=2,
    )
    runner.SuggestTrials(policy, 5)
    # Simulate restoring the designer from metadata.
    metadata = policy.dump()
    restored_policy = dp.SerializableDesignerPolicy(
        problem_statement=vz.ProblemStatement(metadata=metadata),
        supporter=runner,
        designer_factory=lambda _, **kwargs: _FakeSerializableDesigner(),
        designer_cls=_FakeSerializableDesigner,
        ns_root='test',
        verbose=2,
    )
    restored_policy.load(metadata)
    designer = restored_policy.designer
    self.assertEqual(
        getattr(designer, 'num_incorporated_completed_trials'),
        _NUM_INITIAL_COMPLETED_TRIALS,
    )

  def test_restore_partially_serializable_designer(self):
    runner = _create_runner()
    policy = dp.PartiallySerializableDesignerPolicy(
        vz.ProblemStatement(),
        runner,
        lambda _, **kwargs: _FakeSerializableDesigner(),
        ns_root='test',
        verbose=2,
    )
    runner.SuggestTrials(policy, 5)
    metadata = policy.dump()
    # Simluate restoring the policy from the metadata.
    restored_policy = dp.PartiallySerializableDesignerPolicy(
        vz.ProblemStatement(),
        runner,
        lambda _, **kwargs: _FakeSerializableDesigner(),
        ns_root='test',
        verbose=2,
    )
    restored_policy.load(metadata)
    self.assertLen(
        restored_policy._cache._incorporated_completed_trial_ids,
        _NUM_INITIAL_COMPLETED_TRIALS,
    )
    self.assertEqual(
        getattr(restored_policy.designer, 'num_incorporated_completed_trials'),
        _NUM_INITIAL_COMPLETED_TRIALS,
    )

  def test_update_stateless_designer(self):
    runner = _create_runner()
    designer = _FakeDesigner()
    policy = dp.DesignerPolicy(runner, lambda _, **kwargs: designer)
    runner.SuggestTrials(policy, 5)
    self.assertLen(
        designer.last_completed,
        _NUM_INITIAL_COMPLETED_TRIALS,
    )
    self.assertLen(
        designer.last_active,
        _NUM_INITIAL_ACTIVE_TRIALS,
    )
    # Add newly completed trials
    runner.AddTrials([vz.Trial().complete(vz.Measurement()) for _ in range(33)])
    runner.SuggestTrials(policy, 15)
    # Check that all completed trials are passed again.
    self.assertLen(
        designer.last_completed,
        _NUM_INITIAL_COMPLETED_TRIALS + 33,
    )
    # Check that all active trials are passed again, including the new ones.
    self.assertLen(
        designer.last_active,
        _NUM_INITIAL_ACTIVE_TRIALS + 5,
    )

  def test_update_stateful_designer(self):
    runner = _create_runner()
    designer = _FakeSerializableDesigner()
    policy = dp.PartiallySerializableDesignerPolicy(
        vz.ProblemStatement(),
        runner,
        lambda _, **kwargs: designer,
        ns_root='test',
        verbose=2,
    )
    runner.SuggestTrials(policy, 11)
    self.assertLen(
        designer.last_completed,
        _NUM_INITIAL_COMPLETED_TRIALS,
    )
    self.assertLen(
        designer.last_active,
        _NUM_INITIAL_ACTIVE_TRIALS,
    )
    # Add newly completed trials
    runner.AddTrials([vz.Trial().complete(vz.Measurement()) for _ in range(33)])
    runner.SuggestTrials(policy, 5)
    # Check that completed trials are not passed again.
    self.assertLen(designer.last_completed, 33)
    # Check that the newly active trials are passed.
    self.assertLen(designer.last_active, _NUM_INITIAL_ACTIVE_TRIALS + 11)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/policies/random_policy.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Random Pythia Policy which produces uniform sampling of Trial parameter values.

Since this is a RandomPolicy (i.e. stateless), we don't use the PolicySupporter
API when suggesting trials, but we do for the early stopping in order to
showcase how the policy supporter should be used.
"""
import random
from vizier import pythia
from vizier import pyvizier
from vizier._src.algorithms.designers import random as random_designer_lib


class RandomPolicy(pythia.Policy):
  """A policy that picks random hyper-parameter values."""

  def __init__(self, policy_supporter: pythia.PolicySupporter):
    self._policy_supporter = policy_supporter

  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:
    """Gets number of Trials to propose, and produces random Trials."""
    suggestions = random_designer_lib.RandomDesigner(
        request.study_config.search_space
    ).suggest(request.count)
    return pythia.SuggestDecision(suggestions)

  def early_stop(
      self, request: pythia.EarlyStopRequest
  ) -> pythia.EarlyStopDecisions:
    """Selects a random ACTIVE/PENDING trial to stop from datastore."""
    decisions = []

    all_active_trials = self._policy_supporter.GetTrials(
        study_guid=request.study_guid,
        status_matches=pyvizier.TrialStatus.ACTIVE,
    )
    trial_to_stop_id = None
    if all_active_trials:
      trial_to_stop_id = random.choice(all_active_trials).id
      decisions.append(
          pythia.EarlyStopDecision(
              id=trial_to_stop_id, reason='Random early stopping.'
          )
      )

    for trial_id in list(request.trial_ids):
      if trial_id != trial_to_stop_id:
        decisions.append(
            pythia.EarlyStopDecision(
                id=trial_id, reason='Trial should not stop.', should_stop=False
            )
        )

    return pythia.EarlyStopDecisions(decisions, pyvizier.MetadataDelta())


--- vizier/_src/algorithms/policies/random_policy_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pythia.policies.random_policy."""
from vizier import pythia
from vizier import pyvizier
from vizier._src.algorithms.policies import random_policy
from vizier.testing import test_studies

from absl.testing import absltest


class RandomPolicyTest(absltest.TestCase):
  # TODO: Add conditional test case.

  def setUp(self):
    """Setups up search space and policy."""
    self.study_config = pyvizier.ProblemStatement(
        test_studies.flat_space_with_all_types()
    )
    self.policy_supporter = pythia.InRamPolicySupporter(self.study_config)
    self.policy = random_policy.RandomPolicy(
        policy_supporter=self.policy_supporter
    )
    super().setUp()

  def test_make_suggestions(self):
    """Tests random parameter generation wrapped around Policy."""
    num_suggestions = 5
    num_params = len(self.study_config.search_space.parameters)

    suggest_request = pythia.SuggestRequest(
        study_descriptor=self.policy_supporter.study_descriptor(),
        count=num_suggestions,
    )
    decisions = self.policy.suggest(suggest_request)
    self.assertLen(decisions.suggestions, num_suggestions)
    self.assertFalse(decisions.metadata)
    for suggestion in decisions.suggestions:
      self.assertLen(suggestion.parameters, num_params)

  def test_make_early_stopping_decisions(self):
    """Checks if all ACTIVE/PENDING trials become completed in random order."""
    count = 10
    _ = self.policy_supporter.SuggestTrials(self.policy, count=count)

    request_trial_ids = [1, 2]
    trial_ids_stopped = set()
    for _ in range(count):
      request = pythia.EarlyStopRequest(
          study_descriptor=self.policy_supporter.study_descriptor(),
          trial_ids=request_trial_ids,
      )
      early_stop_decisions = self.policy.early_stop(request)
      self.assertContainsSubset(
          request_trial_ids,
          [decision.id for decision in early_stop_decisions.decisions],
      )

      for decision in early_stop_decisions.decisions:
        if decision.should_stop:
          # Stop trials that need to be stopped.
          self.policy_supporter.trials[decision.id - 1].complete(
              pyvizier.Measurement()
          )
          trial_ids_stopped.add(decision.id)
    self.assertEqual(trial_ids_stopped, set(range(1, count + 1)))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/policies/trial_caches.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Cache the list of trials so that we can avoid reloading them."""

import json
from typing import Sequence

from absl import logging
import attrs
from vizier import pythia
from vizier import pyvizier as vz
from vizier.interfaces import serializable


_INCOPORATED_COMPLETED_TRIALS_IDS = 'incorporated_completed_trials_ids'


@attrs.define
class IdDeduplicatingTrialLoader(serializable.PartiallySerializable):
  """Wrapper around PolicySupporter to avoid reloading completed trials.

  get_newly_completed_trials() only return the trials completed since the
  last call.

  get_active_trials() is just a convienence wrapper.
  """

  # TODO: Storing every id is inefficient and max_trial id
  # can be out of sync. Optimize this class.

  _supporter: pythia.PolicySupporter = attrs.field(
      repr=str,  # Use the concise representation.
  )
  _incorporated_completed_trial_ids: set[int] = attrs.field(
      kw_only=True,
      factory=set,
      repr=lambda x: repr(x) if len(x) < 100 else f'{len(x)} elements.',
  )
  _include_intermediate_measurements: bool = attrs.field(
      kw_only=True, default=False
  )

  def __attrs_post_init__(self):
    logging.info('Initialized %s', self)

  def num_incorporated_trials(self) -> int:
    return len(self._incorporated_completed_trial_ids)

  def clear(self) -> None:
    """Make the next call to get_newly_completed_trials() return all trials."""
    self._incorporated_completed_trial_ids = set()

  def get_active_trials(self) -> Sequence[vz.Trial]:
    """Returns all active trials."""
    trials = self._supporter.GetTrials(
        status_matches=vz.TrialStatus.ACTIVE,
        include_intermediate_measurements=self._include_intermediate_measurements,
    )

    logging.info(
        'Loaded %s active trials.',
        len(trials),
    )
    return trials

  def get_newly_completed_trials(self, max_trial_id: int) -> Sequence[vz.Trial]:
    """Returns trials completed between the last call and max_trial_id."""
    if len(self._incorporated_completed_trial_ids) == max_trial_id:
      # no trials need to be loaded.
      return []
    all_trial_ids = set(range(1, max_trial_id + 1))
    # Exclude completed trials that were already passed to the designer.
    trial_ids_to_load = all_trial_ids - self._incorporated_completed_trial_ids
    new_trials = self._supporter.GetTrials(
        trial_ids=trial_ids_to_load,
        status_matches=vz.TrialStatus.COMPLETED,
        include_intermediate_measurements=self._include_intermediate_measurements,
    )
    # Add new completed trials to not report on them again in the future.
    self._incorporated_completed_trial_ids |= set(t.id for t in new_trials)

    logging.info(
        'Loaded %s completed trials. Max trial id is %s.',
        len(new_trials),
        max_trial_id,
    )
    return new_trials

  def dump(self) -> vz.Metadata:
    """Dump state.

    Returns:
      Metadata has the following namespace hierarchy:
        Namespace([self._ns_root]): contains the policy's state.
        Namespace([self._ns_root, self._ns_designer]: contains the designer's
          state.
    """
    md = vz.Metadata()
    # TODO: Storing every id is inefficient. Optimize this.
    # We don't store/load ACTIVE trials as we always pass all of them.
    md[_INCOPORATED_COMPLETED_TRIALS_IDS] = json.dumps(
        list(self._incorporated_completed_trial_ids)
    )
    return md

  def load(self, md: vz.Metadata) -> None:
    if _INCOPORATED_COMPLETED_TRIALS_IDS in md:
      try:
        # We don't store/load ACTIVE trials as we always pass all of them.
        self._incorporated_completed_trial_ids = set(
            json.loads(md[_INCOPORATED_COMPLETED_TRIALS_IDS])
        )
      except json.JSONDecodeError as e:
        raise serializable.HarmlessDecodeError from e
    else:
      raise serializable.HarmlessDecodeError('Missing expected metadata keys.')

    logging.info(
        'Successfully recovered the cache, which has %s completed trials.',
        len(self._incorporated_completed_trial_ids),
    )


--- vizier/_src/algorithms/policies/trial_caches_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies import trial_caches
from absl.testing import absltest


def new_completed() -> vz.CompletedTrial:
  return vz.Trial(final_measurement=vz.Measurement())


def new_active() -> vz.Trial:
  return vz.Trial()


class CompletedTrialIdsCacheTest(absltest.TestCase):

  def test_all(self):
    problem = vz.ProblemStatement()
    supporter = pythia.InRamPolicySupporter(problem)
    cache = trial_caches.IdDeduplicatingTrialLoader(supporter)

    ################## PHASE 1 ################
    active_trial = new_active()
    supporter.AddTrials([new_completed(), active_trial, new_completed()])

    self.assertLen(cache.get_newly_completed_trials(3), 2)
    self.assertEqual(cache.num_incorporated_trials(), 2)

    # dump and load should restore the state.
    dump = cache.dump()
    cache = trial_caches.IdDeduplicatingTrialLoader(supporter)
    cache.load(dump)
    self.assertEmpty(cache.get_newly_completed_trials(3))
    self.assertEqual(cache.num_incorporated_trials(), 2)

    ################## PHASE 2 ################

    # Trial 2 completed
    active_trial.complete(vz.Measurement())
    # Trial 4 added as pending. 5 added as completed
    supporter.AddTrials([new_active(), new_completed()])
    self.assertLen(cache.get_newly_completed_trials(5), 2)
    self.assertLen(cache.get_active_trials(), 1)
    self.assertEqual(cache.num_incorporated_trials(), 4)

    # dump and load should restore the state.
    dump = cache.dump()
    cache = trial_caches.IdDeduplicatingTrialLoader(supporter)
    cache.load(dump)
    self.assertEmpty(cache.get_newly_completed_trials(5))
    self.assertEqual(cache.num_incorporated_trials(), 4)

    # clear should reset.
    cache.clear()
    self.assertEmpty(cache._incorporated_completed_trial_ids)
    self.assertLen(cache.get_newly_completed_trials(5), 4)


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/random/random_sample.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Randomization related utils."""

import logging
from typing import Dict, List, TypeVar

import numpy as np
from vizier import pyvizier as vz

_T = TypeVar('_T')


def sample_uniform(rng: np.random.Generator, min_value=0, max_value=1) -> float:
  """Samples unifrom value and udpate key."""
  return float(rng.uniform(low=min_value, high=max_value))


def sample_bernoulli(
    rng: np.random.Generator,
    prob1: float,
    value1: _T = 0,
    value2: _T = 1,
) -> _T:
  """Samples value1 with probability prob1."""
  return value1 if rng.binomial(1, p=prob1) else value2


def sample_integer(
    rng: np.random.Generator,
    min_value: float,
    max_value: float,
) -> int:
  """Samples a random integer."""
  val = sample_uniform(rng, min_value, max_value)
  return round(val)


def sample_categorical(rng: np.random.Generator, categories: List[str]) -> str:
  """Samples a random categorical value."""
  return rng.choice(categories)


def sample_discrete(rng: np.random.Generator,
                    feasible_points: List[float]) -> float:
  """Samples random discrete value.

  To sample a discrete value we sample uniformly a decimal value between the
  minimum and maximum feasible points and returns the closest feasible point.

  Args:
    rng:
    feasible_points:

  Returns:
    The sampled feasible point and a new key.
  """
  min_value = min(feasible_points)
  max_value = max(feasible_points)
  value = sample_uniform(rng, min_value, max_value)
  closest_element = get_closest_element(feasible_points, value)
  return closest_element


def get_closest_element(array: List[float], value: float) -> float:
  """Finds closest element in array to value."""
  gaps = [abs(x - value) for x in array]
  closest_idx = min(enumerate(gaps), key=lambda x: x[1])[0]
  return array[closest_idx]


def _sample_value(
    rng: np.random.Generator,
    param_config: vz.ParameterConfig,
) -> vz.ParameterValueTypes:
  """Samples random value based on the parameter type."""
  if param_config.type == vz.ParameterType.CATEGORICAL:
    return sample_categorical(rng, param_config.feasible_values)
  elif param_config.type == vz.ParameterType.DISCRETE:
    return sample_discrete(rng, param_config.feasible_values)
  else:
    min_value, max_value = param_config.bounds
    if param_config.type == vz.ParameterType.INTEGER:
      return sample_integer(rng, min_value, max_value)
    elif param_config.type == vz.ParameterType.DOUBLE:
      return sample_uniform(rng, min_value, max_value)
    else:
      logging.error('Invalid parameter config type: %s; deafults to DOUBLE.',
                    param_config.type)
      return sample_uniform(rng, min_value, max_value)


def sample_parameters(rng: np.random.Generator,
                      search_space: vz.SearchSpace) -> vz.ParameterDict:
  """Randomly samples parameter values from the search space."""
  sampled_parameters: Dict[str, vz.ParameterValue] = {}
  parameter_configs: List[vz.ParameterConfig] = search_space.parameters

  for param_config in parameter_configs:
    sample_param_value = _sample_value(rng, param_config)
    sampled_parameters[param_config.name] = vz.ParameterValue(
        sample_param_value)

  return vz.ParameterDict(sampled_parameters)


def shuffle_list(rng: np.random.Generator, items: List[_T]) -> List[_T]:
  """Shuffled a list of items (inplace)."""
  rng.shuffle(items)
  return items


--- vizier/_src/algorithms/random/random_sample_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for random_sample."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.random import random_sample

from absl.testing import absltest
from absl.testing import parameterized


class RandomSampleTest(parameterized.TestCase):

  def setUp(self):
    super(RandomSampleTest, self).setUp()
    self.rng = np.random.default_rng(0)

  @parameterized.named_parameters(
      dict(testcase_name='prob=0', value=0.2, target=0.0),
      dict(testcase_name='prob=1.0', value=-0.5, target=0.0),
      dict(testcase_name='prob=1.5', value=0.6, target=1.0),
      dict(testcase_name='prob=-0.1', value=3.0, target=2.0),
  )
  def test_get_closest_element(self, value, target):
    items = [0.0, 1.0, 2.0]
    self.assertEqual(random_sample.get_closest_element(items, value), target)

  def test_shuffle_list(self):
    items = ['a', 'b', 'c', 'd']
    shuffled_items = random_sample.shuffle_list(self.rng, items)
    # Check that all items appear once
    items_set = set(items)
    for item in shuffled_items:
      self.assertIn(item, items_set)
      items_set.remove(item)

  def test_sample_uniform(self):
    value1 = random_sample.sample_uniform(self.rng)
    self.assertTrue(0.0 <= value1 <= 1.0)
    value2 = random_sample.sample_uniform(self.rng)
    self.assertNotEqual(value1, value2)
    value3 = random_sample.sample_uniform(self.rng, 10.0, 20.0)
    self.assertTrue(10.0 <= value3 <= 20.0)

  def test_sample_bernoulli(self):
    value1 = random_sample.sample_bernoulli(self.rng, 0.5)
    self.assertIn(value1, [0, 1])
    value2 = random_sample.sample_bernoulli(self.rng, 0.0)
    self.assertEqual(value2, 1)
    value3 = random_sample.sample_bernoulli(self.rng, 1.0)
    self.assertEqual(value3, 0)
    value4 = random_sample.sample_bernoulli(self.rng, 0.5, 'A', 'B')
    self.assertIn(value4, ['A', 'B'])

  def test_sample_integer(self):
    value1 = random_sample.sample_integer(self.rng, 5, 5)
    self.assertEqual(value1, 5)
    value2 = random_sample.sample_integer(self.rng, 0, 10)
    self.assertIn(value2, list(range(0, 11)))

  def test_sample_categorical(self):
    value1 = random_sample.sample_categorical(self.rng, ['A'])
    self.assertEqual(value1, 'A')
    value2 = random_sample.sample_categorical(self.rng, ['A', 'B', 'C'])
    self.assertIn(value2, ['A', 'B', 'C'])

  def test_sample_discrete(self):
    value1 = random_sample.sample_discrete(self.rng, [2.5])
    self.assertEqual(value1, 2.5)
    value2 = random_sample.sample_discrete(self.rng, [0.0, 1.0, 10.0])
    self.assertIn(value2, [0.0, 1.0, 10.0])

  def test_sample_value_integer(self):
    int_param = vz.ParameterConfig.factory('int', bounds=(0, 10))
    value = random_sample._sample_value(self.rng, int_param)
    self.assertIn(value, list(range(0, 11)))

  def test_sample_value_discrete(self):
    discrete_param = vz.ParameterConfig.factory(
        'discrete', feasible_values=[1.0, 2.0, 10.0])
    value = random_sample._sample_value(self.rng, discrete_param)
    self.assertIn(value, [1.0, 2.0, 10.0])

  def test_sample_value_float(self):
    float_param = vz.ParameterConfig.factory('float', bounds=(0.0, 1.0))
    value = random_sample._sample_value(self.rng, float_param)
    self.assertTrue(0.0 <= value <= 1.0)

  def test_sample_value_categorical(self):
    categorical_param = vz.ParameterConfig.factory(
        'categorical', feasible_values=['a', 'b', 'c'])
    value = random_sample._sample_value(self.rng, categorical_param)
    self.assertIn(value, ['a', 'b', 'c'])

  def test_sample_input_parameters(self):
    space = vz.SearchSpace()
    root = space.root
    root.add_bool_param('b1')
    root.add_discrete_param('d1', [1.0, 2.0, 10.0])
    root.add_float_param('f1', 0.0, 15.0, scale_type=vz.ScaleType.LINEAR)
    root.add_float_param('f2', 100.0, 200.0, scale_type=vz.ScaleType.LINEAR)
    root.add_int_param('i1', 0, 10, scale_type=vz.ScaleType.LINEAR)
    root.add_categorical_param('c1', ['a', 'b', 'c'])
    parameter_dict = random_sample.sample_parameters(self.rng, space)
    self.assertIn(parameter_dict['b1'].value, ['True', 'False'])
    self.assertIn(parameter_dict['d1'].value, [1.0, 2.0, 10.0])
    self.assertTrue(0.0 <= parameter_dict['f1'].value <= 15.0)
    self.assertTrue(100.0 <= parameter_dict['f2'].value <= 200.0)
    self.assertIn(parameter_dict['i1'].value, list(range(0, 11)))
    self.assertIn(parameter_dict['c1'].value, ['a', 'b', 'c'])


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/regression/trial_regression_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""The file has utilities to regress on trial intermediate measurements.

This contains utilities to fit regression models that predict the objective
at a particular future step of ACTIVE trials. We notably support LightGBM
models and necessary datastructures.
"""

import copy
from typing import (Any, Callable, Dict, Optional, Tuple, Union)

from absl import logging
import attrs
import lightgbm.sklearn as lightgbm
import numpy as np
from scipy.interpolate.fitpack2 import InterpolatedUnivariateSpline
import six
from six.moves import range
from sklearn import model_selection
from vizier import algorithms as vza
from vizier import pyvizier
from vizier.pyvizier import converters


@attrs.define
class TrialData:
  """Light weight trial data class to be used for training regression models."""

  id: int
  learning_rate: float
  final_objective: float
  steps: list[int]
  objective_values: list[float]

  @classmethod
  def from_trial(
      cls,
      trial: pyvizier.Trial,
      learning_rate_param_name: str,
      metric_name: str,
      converter: converters.TimedLabelsExtractor,
  ):
    """Preprocess the pyvizier trial into an instance of the class.

    Args:
      trial: pyvizier.Trial containing trial to process.
      learning_rate_param_name: name of learning rate param
      metric_name: name of optimization metric
      converter: vizier tool to convert trials to times sequences

    Returns:
      returned_trial: the trial in TrialData format
    """

    learning_rate = trial.parameters.get(
        learning_rate_param_name, pyvizier.ParameterValue(0.0)
    ).value

    timedlabels = converter.convert([trial])[0]
    steps, values = (
        np.asarray(timedlabels.times, np.int32).reshape(-1).tolist(),
        timedlabels.labels[metric_name].reshape(-1).tolist(),
    )

    if trial.final_measurement and (
        metric_name in trial.final_measurement.metrics
    ):
      final_value = converter.metric_converters[0].convert(
          [trial.final_measurement]
      )[0]
    else:
      final_value = values[-1] if values else 0.0

    return cls(
        id=trial.id,
        learning_rate=learning_rate,
        final_objective=final_value,
        steps=steps,
        objective_values=values,
    )

  def extrapolate_trial_objective_value(self, max_num_steps: int):
    """Extend the measurements of self to max_num_steps.

    Args:
      max_num_steps: target steps to extend the measurement.
    """
    last_step = self.steps[-1]
    if last_step >= max_num_steps:
      return

    last_objective = self.objective_values[-1]
    self.steps.append(max_num_steps)
    self.objective_values.append(last_objective)


def _generate_interpolation_fn_from_trial(
    steps: list[int], values: list[float]
) -> Callable[[int], float]:
  """Generates an interpolation function from a trial's measurement data.

  Since different trials have evaluations at different step numbers,
  we need to be able to interpolate the objective value between steps
  in order to compare trials and regress against trial data. This function
  converts a trial into a function suitable for this use.

  Args:
    steps:  list of integers indicating the x-axis of the input data points.
    values: list of floats indicating the y-axis of the input data points. steps
      and values list contains the same number of elements.

  Returns:
    interpolation function that takes input a number t and returns
    interpolated value of objective function for this trial at t steps.
  """
  return InterpolatedUnivariateSpline(steps, values, k=1)


def _sort_dedupe_measurements(
    steps: list[Union[int, float]], values: list[float]
) -> Tuple[list[Union[int, float]], list[float]]:
  """Sort and remove duplicates in the trial's measurements.

  Args:
    steps: a list of integer measurement steps for a given trial.
    values: a list of objective values corresponding to the steps for a given
      trial.

  Returns:
    steps: a list of integer measurement steps after dedupe.
    values: a list of objective values corresponding to the steps after dedupe.
  """
  if isinstance(steps[0], float):
    # Dedupe is skipped when steps are not integers.
    return steps, values
  step_obj_dict = {}
  updated_steps = []
  updated_values = []
  for index in range(len(steps)):
    step_obj_dict[steps[index]] = values[index]
  last_step = None
  for step, value in sorted(six.iteritems(step_obj_dict)):
    if last_step is None or step > last_step:
      updated_steps.append(step)
      updated_values.append(value)
      last_step = step
  return updated_steps, updated_values


class GBMAutoRegressor(object):
  """Train and predict trial measurements using auto-regressive GBM model."""

  def __init__(
      self,
      target_step: Union[int, float],
      min_points: int,
      learning_rate_param_name: str,
      metric_name: str,
      converter: converters.TimedLabelsExtractor,
      gbdt_param_grid: Optional[Dict[str, Any]] = None,
      cv: int = 2,
      random_state: Optional[int] = None,
  ):
    """Initialize model params.

    Args:
      target_step: the step to compute the prediction according to existing data
        points.
      min_points: number of lag points in the auto-regressive model
      learning_rate_param_name: name of learning rate param
      metric_name: name of optimization metric
      converter: vizier tool to convert trials to times sequences
      gbdt_param_grid: parameter grid for CV grid search for lightGBM model
      cv: k in k-fold cross validation
      random_state: random state for GBDT model
    """
    self._target_step = target_step
    self._min_points = min_points
    self._converter = converter
    self._learning_rate_param_name = learning_rate_param_name
    self._metric_name = metric_name
    self._gbdt_param_grid = gbdt_param_grid or {
        "max_depth": [2, 3, 5],
        "n_estimators": [50, 100],
    }
    self._cv = cv
    self._random_state = random_state
    self._model: lightgbm.LGBMRegressor = None  # place holder for trained model
    self._best_params: Dict[str, Any] = None  # place holder for best parameters

  @property
  def is_trained(self) -> bool:
    return self._model is not None

  def train(self, completed: vza.CompletedTrials) -> None:
    """Trains a GBDT combined models for auto-regression given completed trials.

    Args:
      completed: Sequence of completed trials.

    Returns:
      Nothing. Updated `_model` and `_best_params` members of the class.
    """
    completed_trials = []
    for trial in completed.trials:
      completed_trials.append(
          TrialData.from_trial(
              trial,
              learning_rate_param_name=self._learning_rate_param_name,
              metric_name=self._metric_name,
              converter=self._converter,
          )
      )
    if len(completed_trials) < self._min_points + 1:
      logging.info(
          "Not enough completed trials (only %d) to train GBDT model.",
          len(completed_trials),
      )
      return
    feature_matrix = []
    targets = []
    for trialc in completed_trials:
      # only consider trials with at least min_points + 1 steps as otherwise
      # the features constructed are mostly interpolated
      if len(trialc.steps) < self._min_points + 1:
        continue
      trialc.extrapolate_trial_objective_value(self._target_step)
      tc_steps, tc_values = _sort_dedupe_measurements(
          trialc.steps, trialc.objective_values
      )
      trial_inter_func = _generate_interpolation_fn_from_trial(
          tc_steps, tc_values
      )
      for i, step in enumerate(trialc.steps):
        if i < self._min_points - 1 or step >= self._target_step:
          continue
        features = self._create_features_from_trial(trialc, i)
        feature_matrix.append(features)
        targets.append(trial_inter_func(self._target_step))
    feature_matrix = np.array(feature_matrix)
    logging.info("Feature matrix shape: %s", feature_matrix.shape)
    if feature_matrix.shape[0] <= (self._min_points + 1) / (
        1.0 - 1.0 / self._cv
    ):
      logging.info(
          "Not enough rows in feature matrix. "
          "This can happen when there are not enough measurements in "
          "the completed trials."
      )
      return
    targets = np.array(targets)
    gbdt_cv = model_selection.GridSearchCV(
        lightgbm.LGBMRegressor(random_state=self._random_state),
        self._gbdt_param_grid,
        cv=self._cv,
    )
    gbdt_cv = gbdt_cv.fit(feature_matrix, targets)
    self._best_params = gbdt_cv.best_params_
    gbm = lightgbm.LGBMRegressor(
        **self._best_params, random_state=self._random_state
    )
    self._model = gbm.fit(feature_matrix, targets)

  def predict(self, trial: pyvizier.Trial) -> Union[float, None]:
    """Estimate objective values at target_steps using autoregression algorithm.

    Args:
      trial: current pyvizier trial.

    Returns:
      prediction: the predicted objective value at target_steps.
      It returns None when the trial has less than min_points steps.
    """
    trial_data = TrialData.from_trial(
        trial,
        learning_rate_param_name=self._learning_rate_param_name,
        metric_name=self._metric_name,
        converter=self._converter,
    )
    if not self.is_trained:
      raise ValueError("Prediction cannot be performed before model training.")
    # Not enough features for prediction
    if len(trial_data.steps) < self._min_points:
      return None
    features = self._create_features_from_trial(
        trial_data, len(trial_data.steps) - 1
    )
    features = np.asarray(features).reshape(1, -1)
    return self._model.predict(features)[0]

  def _create_features_from_trial(
      self,
      trial: TrialData,
      end_index: int,
  ) -> list[float]:
    """Create feature vector for auto-regressive model from a trial.

    Args:
      trial: sequence of measurements as a trial
      end_index: last index in steps to be included in the feature set

    Returns:
      list of features as 1D list of size `min_points`.
    """
    if self._min_points > end_index + 1:
      raise ValueError("Not enough data before end_index for creating features")
    if end_index >= len(trial.steps):
      raise ValueError("Not enough indices in trials.steps")
    features = [trial.learning_rate]
    for j in range(self._min_points):
      # Difference from target_step and value at the step into `features`.
      features.append(self._target_step - trial.steps[end_index - j])
      features.append(trial.objective_values[end_index - j])
    return features


@attrs.define
class HallucinationOptions(object):
  """Options for hallucinated suggestions policy.

  Attributes:
    autoregressive_order: order of the auto regressive predictor.
    use_steps: if true we use `steps` for prediction, otherwise we use
      `elapsed_seconds`.
    learning_rate_param_name: name of learning rate parameter.
    gbdt_param_grid: param grid for tuning the gbdt regressor.
    min_completed_trials: only start hallucinations after this many completed
      trials.
    min_steps: only hallucinate a stopped trial which has greater than equal to
      these many steps.
    max_steps: maximum number of steps in a trial.
    random_state: random seed.
    elapsed_seconds_gap: When using steps, the hallucinated measurements
      `elapsed_seconds` is set to be the current elapsed_seconds` + this
      constant.
  """

  autoregressive_order: int = attrs.field(default=5)
  learning_rate_param_name: str = attrs.field(default="learning_rate")
  use_steps: bool = attrs.field(default=True)
  gbdt_param_grid: Dict[str, Any] = attrs.field(
      default={"max_depth": [2, 3, 5], "n_estimators": [50, 100]}
  )
  min_completed_trials: int = attrs.field(default=5)
  min_steps: int = attrs.field(default=5)
  max_steps: Optional[int] = attrs.field(default=None)
  random_state: Optional[int] = attrs.field(default=None)
  elapsed_seconds_gap: Optional[int] = attrs.field(default=0)


class GBMTrialHallucinator:
  """Regression based early stopping hallucinations."""

  def __init__(
      self,
      study_config: pyvizier.ProblemStatement,
      options: HallucinationOptions,
      verbose: int = 0,
  ):
    """Initialization.

    Args:
      study_config: pyvizier study config.
      options: hallucination options
      verbose: verbosity level. If set to > 0, then we log trial predictions.
    """
    self._options = options
    self._study_config = study_config
    if not self._study_config.metric_information.is_single_objective:
      raise ValueError("This policy only supports one objective.")
    self._metric = self._study_config.metric_information.item()
    self._converter = converters.TimedLabelsExtractor(
        [
            converters.DefaultModelOutputConverter(
                self._metric, flip_sign_for_minimization_metrics=False
            ),
        ],
        timestamp="steps" if self._options.use_steps else "elapsed_secs",
        value_extraction="raw",
    )
    self._max_steps = self._options.max_steps
    self._min_steps = self._options.min_steps
    # Place holder for model.
    self._model: Optional[GBMAutoRegressor] = None
    self._verbose = verbose

  def train(self, train_trials: vza.CompletedTrials):
    """Returns the selected trials to be tried out next.

    Args:
      train_trials: a collection of completed trials to be used for training.
    """

    # Decide whether to update stopped trials or not based on
    # `update_all_stopped_trials` flag and if there are enough train trials.
    # Finally, training an autoregressive model if the conditions are met.
    has_enough_data = len(train_trials.trials) >= max(
        self._options.autoregressive_order + 1,
        self._options.min_completed_trials,
    )
    if not has_enough_data:
      logging.info("Not enough train trials.")
      return

    self._max_steps = self._max_steps or int(
        np.percentile([len(t.measurements) for t in train_trials.trials], 95)
    )

    # If min_steps was not set, take 1/10th of the max_num_steps as its
    # value.
    self._min_steps = self._min_steps or int(self._max_steps / 10)

    if not self._max_steps:
      logging.info("Not updating stopped trials as max steps could not be set.")
      return

    global_autoregressive_model = GBMAutoRegressor(
        target_step=self._max_steps,
        min_points=self._options.autoregressive_order,
        learning_rate_param_name=self._options.learning_rate_param_name,
        metric_name=self._metric.name,
        converter=self._converter,
        gbdt_param_grid=self._options.gbdt_param_grid,
        random_state=self._options.random_state,
    )
    global_autoregressive_model.train(train_trials)
    logging.info("Finished Training global Auto-regressive GBDT model.")

    # If `global_autoregressive_model` could not be trained, then stopped trials
    # are not updated before making suggestions.
    if not global_autoregressive_model.is_trained:
      logging.info(
          "Not updating stopped trials as global GBDT model is not trained."
      )
      return

    self._model = global_autoregressive_model

  def update_stopped_trials(
      self, stopped_trials: list[pyvizier.Trial]
  ) -> list[pyvizier.Trial]:
    """Add hallucinated final measurements to stopped trials.

    Args:
      stopped_trials: stopped trials.

    Returns:
      updated stopped trials with final measurement potentially added.
    """
    if not self._model:
      logging.info("Not updating stopped trials as model was not trained.")
      return stopped_trials
    updated_trials = []  # Collect all stopped trials that need to be updated.
    for pytrial in stopped_trials:
      if pytrial.infeasible:
        updated_trials.append(pytrial)
        continue
      auto_prediction = self._model.predict(pytrial)
      if not auto_prediction:
        updated_trials.append(pytrial)
        continue
      logging.log_if(
          logging.INFO,
          "Trial generated prediction %f",
          auto_prediction,
          self._verbose >= 1,
      )
      self._create_final_measurement(
          pytrial,
          auto_prediction=auto_prediction,
      )
      logging.info("Updated stopped trial : %s, using GBDT model.", pytrial.id)
      updated_trials.append(pytrial)
    return updated_trials

  def _create_final_measurement(
      self, pytrial: pyvizier.Trial, auto_prediction: float
  ):
    """Creates a final measurement for stopped trials."""
    if pytrial.final_measurement:
      logging.info(
          "A pending trial somehow has a final measurement and will"
          "remain unchanged"
          "(trial_id=%s)",
          pytrial.id,
      )
      return
    final_measurement = copy.deepcopy(pytrial.measurements[-1])
    final_measurement.metrics[self._metric.name] = pyvizier.Metric(
        value=auto_prediction
    )
    if self._options.use_steps:
      final_measurement.steps = self._max_steps
      # Increase the elapsed_secs for the final_measurement to
      # make sure new measurements won't have newer timestamps.
      final_measurement.elapsed_secs = (
          pytrial.measurements[-1].elapsed_secs
          + self._options.elapsed_seconds_gap
      )
    else:
      final_measurement.elapsed_secs = self._max_steps
      # Increase the steps for the final_measurement to ensure it's the last
      # step. Note that the gap does not effect vizier's suggestion policy
      # and therefore is a safe operation.
      if len(pytrial.measurements) > 1:
        checkpoint_gap = (
            pytrial.measurements[-1].steps - pytrial.measurements[-2].steps
        )
      else:
        checkpoint_gap = pytrial.measurements[-1].steps
      final_measurement.steps = (
          pytrial.measurements[-1].steps + checkpoint_gap * 2
      )
    pytrial.complete(final_measurement)


--- vizier/_src/algorithms/regression/trial_regression_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for trial_regression_utils."""

import copy
from typing import Union

import lightgbm.sklearn as lightgbm
import numpy as np
from sklearn.model_selection import GridSearchCV
from vizier import algorithms as vza
from vizier import pyvizier
from vizier._src.algorithms.regression import trial_regression_utils
from vizier.pyvizier import converters

from absl.testing import absltest

_METRIC_NAME = 'objective_value'


def _create_trial_for_testing(
    learning_rate: float,
    steps: list[int],
    seconds: list[Union[int, float]],
    values: list[float],
    stop_reason: Union[None, str],
    trial_id: int,
    metric_name: str = _METRIC_NAME,
):
  measurements = []
  for i in range(len(steps)):
    measurements.append(
        pyvizier.Measurement(
            steps=steps[i],
            elapsed_secs=seconds[i],
            metrics={metric_name: values[i]},
        )
    )
  trial = pyvizier.Trial(
      id=trial_id,
      measurements=measurements,
      stopping_reason=stop_reason,
      parameters=pyvizier.ParameterDict({'learning_rate': learning_rate}),
      final_measurement=measurements[-1],
  )
  return trial


class TrialRegressionUtilsTest(absltest.TestCase):

  def test_preprocess_trial(self):
    steps = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    seconds = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
    values = [0.1, 0.3, 0.4, 0.45, 0.48, 0.49, 0.5, 0.49, 0.51, 0.5]

    metric = pyvizier.MetricInformation(
        name=_METRIC_NAME, goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
    )
    conv1 = converters.TimedLabelsExtractor(
        [
            converters.DefaultModelOutputConverter(
                metric, flip_sign_for_minimization_metrics=True
            ),
        ],
        timestamp='steps',
        value_extraction='raw',
    )
    trial = _create_trial_for_testing(
        0.3, steps, seconds, values, None, 1, _METRIC_NAME
    )
    conv2 = converters.TimedLabelsExtractor(
        [
            converters.DefaultModelOutputConverter(
                metric, flip_sign_for_minimization_metrics=True
            ),
        ],
        timestamp='elapsed_secs',
        value_extraction='raw',
    )

    conv_trial1 = trial_regression_utils.TrialData.from_trial(
        trial=trial,
        learning_rate_param_name='learning_rate',
        metric_name=_METRIC_NAME,
        converter=conv1,
    )
    conv_trial2 = trial_regression_utils.TrialData.from_trial(
        trial=trial,
        learning_rate_param_name='learning_rate',
        metric_name=_METRIC_NAME,
        converter=conv2,
    )
    self.assertListEqual(steps, conv_trial1.steps)
    self.assertSequenceAlmostEqual(values, conv_trial1.objective_values)
    self.assertListEqual(seconds, conv_trial2.steps)
    self.assertSequenceAlmostEqual(values, conv_trial2.objective_values)

  def test_sort_dedupe_measurements(self):
    # Case 1: no duplicates.
    steps = [10, 20]
    values = [0.1, 0.2]
    actual_steps, actual_values = (
        trial_regression_utils._sort_dedupe_measurements(steps, values)
    )
    self.assertListEqual(steps, actual_steps)
    self.assertListEqual(values, actual_values)

    # Case 2: with duplicates, and measurement order needs correction.
    steps = [10, 20, 10]
    values = [0.1, 0.2, 0.2]
    expected_steps = [10, 20]
    expected_values = [0.2, 0.2]
    actual_steps, actual_values = (
        trial_regression_utils._sort_dedupe_measurements(steps, values)
    )
    self.assertListEqual(expected_steps, actual_steps)
    self.assertListEqual(expected_values, actual_values)

  def test_generate_interpolation_fn_from_trial(self):
    steps = [10, 20]
    values = [0.1, 0.5]
    fn = trial_regression_utils._generate_interpolation_fn_from_trial(
        steps, values
    )
    self.assertEqual(fn(steps[0]), values[0])
    self.assertEqual(fn(steps[1]), values[1])
    self.assertGreater(fn(15), values[0])
    self.assertLess(fn(15), values[1])

  def test_extrapolate_trial(self):
    steps = [10, 20, 30]
    values = [0.1, 0.3, 0.5]

    expected_steps = [10, 20, 30, 100]
    expected_values = [0.1, 0.3, 0.5, 0.5]

    trial = trial_regression_utils.TrialData(
        id=1,
        learning_rate=0.1,
        final_objective=values[-1],
        steps=steps,
        objective_values=values,
    )

    expected_trial = copy.deepcopy(trial)

    # Case 1: enough steps, no extension needed.
    trial.extrapolate_trial_objective_value(30)
    self.assertEqual(trial, expected_trial)

    # Case 2: fewer steps, extension needed.
    expected_trial.extrapolate_trial_objective_value(100)
    self.assertEqual(expected_trial.id, trial.id)
    self.assertEqual(expected_trial.learning_rate, trial.learning_rate)
    self.assertEqual(expected_trial.final_objective, trial.final_objective)
    self.assertListEqual(expected_steps, expected_trial.steps)
    self.assertListEqual(expected_values, expected_trial.objective_values)


class GBMAutoRegressorTest(absltest.TestCase):

  def test_create_features_from_trial(self):
    steps = [10, 20, 30]
    values = [0.1, 0.3, 0.5]

    trial = trial_regression_utils.TrialData(
        id=1,
        learning_rate=0.1,
        final_objective=values[-1],
        steps=steps,
        objective_values=values,
    )
    metric = pyvizier.MetricInformation(
        name=_METRIC_NAME, goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
    )
    conv = converters.TimedLabelsExtractor(
        [
            converters.DefaultModelOutputConverter(
                metric, flip_sign_for_minimization_metrics=True
            ),
        ],
        timestamp='steps',
        value_extraction='raw',
    )

    gbm = trial_regression_utils.GBMAutoRegressor(
        min_points=3,
        target_step=40,
        learning_rate_param_name='learning_rate',
        metric_name=_METRIC_NAME,
        converter=conv,
    )
    features = gbm._create_features_from_trial(trial, end_index=2)
    expected_features = [0.1, 10, 0.5, 20, 0.3, 30, 0.1]
    self.assertListEqual(features, expected_features)

  def test_gbmautoregressor_train_predict(self):
    metric = pyvizier.MetricInformation(
        name=_METRIC_NAME, goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
    )
    conv = converters.TimedLabelsExtractor(
        [
            converters.DefaultModelOutputConverter(
                metric, flip_sign_for_minimization_metrics=True
            ),
        ],
        timestamp='steps',
        value_extraction='raw',
    )
    gbm = trial_regression_utils.GBMAutoRegressor(
        target_step=50,
        min_points=2,
        learning_rate_param_name='learning_rate',
        metric_name=_METRIC_NAME,
        converter=conv,
        random_state=111,
    )
    with self.subTest('TestModelCreation'):
      steps = [10, 20, 30, 40, 50]
      values1 = [0.1, 0.3, 0.5, 0.6, 0.66]
      values2 = [0.1, 0.25, 0.4, 0.5, 0.55]
      values3 = [0.1, 0.2, 0.35, 0.48, 0.6]

      trial1 = trial_regression_utils.TrialData(
          id=1,
          learning_rate=0.1,
          final_objective=values1[-1],
          steps=steps,
          objective_values=values1,
      )
      pytrial1 = _create_trial_for_testing(
          learning_rate=0.1,
          steps=steps,
          seconds=steps,
          values=values1,
          trial_id=1,
          stop_reason=None,
          metric_name=_METRIC_NAME,
      )
      trial2 = trial_regression_utils.TrialData(
          id=2,
          learning_rate=0.3,
          final_objective=values2[-1],
          steps=steps,
          objective_values=values2,
      )
      pytrial2 = _create_trial_for_testing(
          learning_rate=0.3,
          steps=steps,
          seconds=steps,
          values=values2,
          trial_id=2,
          stop_reason=None,
          metric_name=_METRIC_NAME,
      )
      trial3 = trial_regression_utils.TrialData(
          id=3,
          learning_rate=0.2,
          final_objective=values3[-1],
          steps=steps,
          objective_values=values3,
      )
      pytrial3 = _create_trial_for_testing(
          learning_rate=0.2,
          steps=steps,
          seconds=steps,
          values=values3,
          trial_id=3,
          stop_reason=None,
          metric_name=_METRIC_NAME,
      )
      feat_mat = []
      targets = []
      for trial in [trial1, trial2, trial3]:
        for i, step in enumerate(trial.steps):
          if i < 1 or step >= 50:
            continue
          features = gbm._create_features_from_trial(trial, 2)
          feat_mat.append(features)
          targets.append(trial.objective_values[-1])
      feat_mat = np.array(feat_mat)
      targets = np.array(targets)
      gbdt_param_grid = {'max_depth': [2, 3], 'n_estimators': [50, 100]}
      gbdt_cv = GridSearchCV(
          lightgbm.LGBMRegressor(random_state=111), gbdt_param_grid, cv=2
      )
      gbdt_cv = gbdt_cv.fit(feat_mat, targets)
      best_params = gbdt_cv.best_params_
      ideal_model = lightgbm.LGBMRegressor(**best_params, random_state=111)
      ideal_model = ideal_model.fit(feat_mat, targets)
      gbm.train(vza.CompletedTrials(trials=[pytrial1, pytrial2, pytrial3]))
      pred = gbm._model.predict(feat_mat[[1], :])
      pred_expected = ideal_model.predict(feat_mat[[1], :])
      self.assertAlmostEqual(pred, pred_expected)

    with self.subTest('TestPrediction'):
      steps = [10, 20, 30, 40]
      values = [0.1, 0.3, 0.5, 0.6]
      trial_pred = trial_regression_utils.TrialData(
          id=2,
          learning_rate=0.1,
          final_objective=values[-1],
          steps=steps,
          objective_values=values,
      )
      pytrial_pred = _create_trial_for_testing(
          learning_rate=0.1,
          steps=steps,
          seconds=steps,
          values=values,
          trial_id=2,
          stop_reason=None,
          metric_name=_METRIC_NAME,
      )
      pred = gbm.predict(pytrial_pred)
      features = gbm._create_features_from_trial(trial_pred, end_index=2)
      features = np.array(features).reshape(1, -1)
      pred_expected = ideal_model.predict(features)[0]
      self.assertAlmostEqual(pred, pred_expected)

  def test_converter_in_trial_hallucinator(self):
    steps = [10, 20, 30, 40, 50]
    values = [0.1, 0.3, 0.5, 0.6, 0.66]
    pytrial = _create_trial_for_testing(
        learning_rate=0.1,
        steps=steps,
        seconds=steps,
        values=values,
        trial_id=1,
        stop_reason=None,
        metric_name=_METRIC_NAME,
    )
    options = trial_regression_utils.HallucinationOptions(max_steps=50)
    problem = pyvizier.ProblemStatement()
    problem.search_space.root.add_float_param(
        'learning_rate', min_value=0.0, max_value=1.0, default_value=0.5
    )
    problem.search_space.root.add_int_param(
        'num_layers', min_value=1, max_value=5
    )
    problem.metric_information = [
        pyvizier.MetricInformation(
            name=_METRIC_NAME, goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        )
    ]
    hallucinator = trial_regression_utils.GBMTrialHallucinator(
        study_config=problem, options=options
    )
    trial = trial_regression_utils.TrialData.from_trial(
        trial=pytrial,
        learning_rate_param_name='learning_rate',
        metric_name=_METRIC_NAME,
        converter=hallucinator._converter,
    )
    self.assertSequenceAlmostEqual(
        trial.objective_values, [0.1, 0.3, 0.5, 0.6, 0.66]
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/testing/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Init."""


--- vizier/_src/algorithms/testing/comparator_runner.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Comparison test for algorithms using score analysis.

Ex: Typical Efficiency Convergence Test Example
-----------------------------------------------
baseline_factory = benchmarks.BenchmarkStateFactory(...)
candidate_factory = benchmarks.BenchmarkStateFactory(...)

# Run each algorithm for 100 Trials with 5 repeats each.
comparator = comparator_runner.EfficiencyComparisonTester(
        num_trials=100, num_repeats=5)
comparator.assert_better_efficiency(candidate_factory, baseline_factory)

NOTE: assert_converges_faster is a generic method name that conveys the general
use of the class.
"""

from absl import logging
import attr
from jax import random
import numpy as np
from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.benchmarks.analyzers import simple_regret_score
from vizier.benchmarks import analyzers
from vizier.pyvizier import converters


class FailedComparisonTestError(Exception):
  """Exception raised for comparison test fails."""


class FailedSimpleRegretConvergenceTestError(Exception):
  """Exception raised for simple-regret convergence test fails."""


@attr.define
class EfficiencyComparisonTester:
  """Comparison test between algorithms using analysis scores."""
  num_trials: int = attr.field(
      default=1, validator=attr.validators.instance_of(int))
  num_repeats: int = attr.field(
      default=1, validator=attr.validators.instance_of(int))

  def assert_better_efficiency(
      self,
      candidate_state_factory: benchmarks.BenchmarkStateFactory,
      baseline_state_factory: benchmarks.BenchmarkStateFactory,
      score_threshold: float = 0.0) -> None:
    """Asserts that candidate is better than baseline via log_eff_score."""
    # TODO: Consider making this more flexible with more runners
    # And enable multimetric.
    runner = benchmarks.BenchmarkRunner(
        benchmark_subroutines=[benchmarks.GenerateAndEvaluate()],
        num_repeats=self.num_trials)

    baseline_curves = []
    candidate_curves = []
    for _ in range(self.num_repeats):
      baseline_state = baseline_state_factory()
      candidate_state = candidate_state_factory()

      baseline_statement = baseline_state.experimenter.problem_statement()
      if len(baseline_statement.metric_information) > 1:
        raise ValueError('Support for multimetric is not yet')
      if baseline_statement != (
          candidate_statement :=
          candidate_state.experimenter.problem_statement()):
        raise ValueError('Comparison tests done for different statements: '
                         f'{baseline_statement} vs {candidate_statement}')

      runner.run(baseline_state)
      runner.run(candidate_state)
      baseline_curves.append(
          analyzers.ConvergenceCurveConverter(
              baseline_statement.metric_information.item()
          ).convert(baseline_state.algorithm.supporter.GetTrials())
      )
      candidate_curves.append(
          analyzers.ConvergenceCurveConverter(
              baseline_statement.metric_information.item()
          ).convert(candidate_state.algorithm.supporter.GetTrials())
      )

    baseline_curve = analyzers.ConvergenceCurve.align_xs(
        baseline_curves, interpolate_repeats=True
    )[0]
    candidate_curve = analyzers.ConvergenceCurve.align_xs(
        candidate_curves, interpolate_repeats=True
    )[0]
    comparator = analyzers.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=baseline_curve, compared_curve=candidate_curve
    )

    if (log_eff_score := comparator.score()) < score_threshold:
      raise FailedComparisonTestError(
          f'Log efficiency score {log_eff_score} is less than {score_threshold}'
          f' when comparing algorithms: {candidate_state_factory} '
          f'vs baseline of {baseline_state_factory} for {self.num_trials} '
          f' Trials with {self.num_repeats} repeats')


@attr.define(kw_only=True)
class SimpleRegretComparisonTester:
  """Compare two algorithms by their simple regrets.

  The test runs the baseline algorithm 'baseline_num_repeats' times each with
  'baseline_num_trials' trials and computes the simple regret in each trial,
  and similarly for the candidate algorithm.

  A one-sided T-test is performed to compute the p-value of observing the
  difference in the simple regret sample means. The T-test score (p-value) is
  compared against the significance level (alpha) to determine if the test
  passed.
  """
  baseline_num_trials: int
  candidate_num_trials: int
  baseline_suggestion_batch_size: int
  candidate_suggestion_batch_size: int
  baseline_num_repeats: int
  candidate_num_repeats: int
  alpha: float = attr.field(
      validator=attr.validators.and_(
          attr.validators.ge(0), attr.validators.le(0.1)),
      default=0.05)
  goal: vz.ObjectiveMetricGoal

  def assert_optimizer_better_simple_regret(
      self,
      converter: converters.TrialToModelInputConverter,
      score_fn: vb.ArrayScoreFunction,
      baseline_strategy_factory: vb.VectorizedStrategyFactory,
      candidate_strategy_factory: vb.VectorizedStrategyFactory,
  ) -> None:
    """Assert if candidate optimizer has better simple regret than the baseline.
    """
    baseline_obj_values = []
    candidate_obj_values = []

    baseline_optimizer_factory = vb.VectorizedOptimizerFactory(
        baseline_strategy_factory,
        suggestion_batch_size=self.baseline_suggestion_batch_size,
        max_evaluations=self.baseline_num_trials,
    )
    candidate_optimizer_factory = vb.VectorizedOptimizerFactory(
        candidate_strategy_factory,
        suggestion_batch_size=self.candidate_suggestion_batch_size,
        max_evaluations=self.candidate_num_trials,
    )
    baseline_optimizer = baseline_optimizer_factory(converter)
    candidate_optimizer = candidate_optimizer_factory(converter)

    for i in range(self.baseline_num_repeats):
      res = baseline_optimizer(score_fn, count=1, seed=random.PRNGKey(i))  # pytype: disable=wrong-arg-types
      trial = vb.best_candidates_to_trials(res, converter)
      baseline_obj_values.append(
          trial[0].final_measurement_or_die.metrics['acquisition'].value
      )

    for i in range(self.candidate_num_repeats):
      res = candidate_optimizer(score_fn, count=1, seed=random.PRNGKey(i))  # pytype: disable=wrong-arg-types
      trial = vb.best_candidates_to_trials(res, converter)
      candidate_obj_values.append(
          trial[0].final_measurement_or_die.metrics['acquisition'].value
      )

    self._conclude_test(baseline_obj_values, candidate_obj_values)

  def assert_benchmark_state_better_simple_regret(
      self,
      baseline_benchmark_state_factory: benchmarks.BenchmarkStateFactory,
      candidate_benchmark_state_factory: benchmarks.BenchmarkStateFactory,
  ) -> None:
    """Runs simple-regret convergence test for benchmark state."""

    def _run_one(benchmark_state_factory: benchmarks.BenchmarkStateFactory,
                 num_trials: int, batch_size: int, seed: int) -> float:
      """Run one benchmark run and returns simple regret."""
      benchmark_state = benchmark_state_factory(seed=seed)
      baseline_runner = benchmarks.BenchmarkRunner(
          benchmark_subroutines=[benchmarks.GenerateAndEvaluate(batch_size)],
          num_repeats=num_trials // batch_size)
      baseline_runner.run(benchmark_state)
      # Extract best metric
      best_trial = benchmark_state.algorithm.supporter.GetBestTrials(count=1)[0]
      metric_name = benchmark_state.experimenter.problem_statement(
      ).single_objective_metric_name
      return best_trial.final_measurement_or_die.metrics[metric_name].value

    baseline_obj_values = []
    candidate_obj_values = []

    for idx in range(self.baseline_num_repeats):
      baseline_obj_values.append(
          _run_one(
              benchmark_state_factory=baseline_benchmark_state_factory,
              num_trials=self.baseline_num_trials,
              batch_size=self.baseline_suggestion_batch_size,
              seed=idx))

    for idx in range(self.candidate_num_repeats):
      candidate_obj_values.append(
          _run_one(
              benchmark_state_factory=candidate_benchmark_state_factory,
              num_trials=self.candidate_num_trials,
              batch_size=self.candidate_suggestion_batch_size,
              seed=idx))
    self._conclude_test(baseline_obj_values, candidate_obj_values)

  def _conclude_test(self, baseline_obj_values: list[float],
                     candidate_obj_values: list[float]) -> None:
    """Concludes test based on baseline and candidate objective func values."""

    p_value = simple_regret_score.t_test_mean_score(baseline_obj_values,
                                                    candidate_obj_values,
                                                    self.goal)
    msg = self._generate_summary(baseline_obj_values, candidate_obj_values,
                                 p_value)
    if p_value <= self.alpha:
      logging.info('Convergence test PASSED:\n %s', msg)
    else:
      raise FailedSimpleRegretConvergenceTestError(msg)

  def _generate_summary(
      self,
      baseline_obj_values: list[float],
      candidate_obj_values: list[float],
      p_value: float,
  ) -> str:
    """Generate summary message."""
    baseline_mean = np.mean(baseline_obj_values)
    baseline_std = np.std(baseline_obj_values)
    candidate_mean = np.mean(candidate_obj_values)
    candidate_std = np.std(candidate_obj_values)
    return (f'\nObjective goal={self.goal.name}'
            f'\nP-value={p_value}'
            f'\nAlpha={self.alpha}'
            f'\nBaseline Objective Mean: {baseline_mean}'
            f'\nBaseline Objective Std: {baseline_std}'
            f'\nCandidate Objective Mean: {candidate_mean}'
            f'\nCandidate Objective Std: {candidate_std}'
            f'\nBaseline Objective Scores: {baseline_obj_values}'
            f'\nCandidate Objective Scores: {candidate_obj_values}')


--- vizier/_src/algorithms/testing/comparator_runner_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for comparator_runner."""

from typing import Optional, Sequence

import jax
from jax import numpy as jnp
import numpy as np
from vizier import algorithms as vza
from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.algorithms.optimizers import vectorized_base as vb
from vizier._src.algorithms.testing import comparator_runner
from vizier._src.jax import types
from vizier.benchmarks import experimenters
from vizier.pyvizier import converters

from absl.testing import absltest
from absl.testing import parameterized


class FakeVectorizedStrategy(vb.VectorizedStrategy):
  """Dummy vectorized strategy to control convergence."""

  def __init__(
      self,
      converter: converters.TrialToModelInputConverter,
      good_value: float = 1.0,
      bad_value: float = 0.0,
      num_trial_to_converge: int = 0,
  ):
    self.converter = converter
    self.good_value = good_value
    self.bad_value = bad_value
    self.num_trial_to_converge = num_trial_to_converge
    self.num_trials_so_far = 0

  def init_state(
      self,
      seed: jax.Array,
      n_parallel: int = 1,
      *,
      prior_features: Optional[vb.VectorizedOptimizerInput] = None,
      prior_rewards: Optional[types.Array] = None,
  ) -> None:
    return

  def suggest(
      self,
      seed: jax.Array,
      state: None,
      n_parallel: int = 1,
  ) -> vb.VectorizedOptimizerInput:
    continuous_output_len = sum(
        [spec.num_dimensions for spec in self.converter.output_specs.continuous]
    )
    categorical_output_len = len(self.converter.output_specs.categorical)
    shape = (1, n_parallel, continuous_output_len)
    if self.num_trials_so_far < self.num_trial_to_converge:
      continuous = jnp.ones(shape) * self.bad_value
    else:
      continuous = jnp.ones(shape) * self.good_value
    return vb.VectorizedOptimizerInput(
        continuous=continuous,
        categorical=jnp.zeros(
            (1, n_parallel, categorical_output_len), dtype=types.INT_DTYPE
        ),
    )

  @property
  def suggestion_batch_size(self) -> int:
    return 1

  def update(
      self,
      seed: jax.Array,
      state: None,
      batch_features: vb.VectorizedOptimizerInput,
      batch_rewards: types.Array,
  ) -> None:
    pass


class FakeDesigner(vza.Designer):
  """Dummy designer to control convergence."""

  def __init__(
      self,
      search_space: vz.SearchSpace,
      *,
      good_value: float = 1.0,
      bad_value: float = 0.0,
      noise: float = 0.1,
      num_trial_to_converge: int = 0,
      seed: Optional[int] = None,
  ):
    self.search_space = search_space
    self.good_value = good_value
    self.bad_value = bad_value
    self.noise = noise
    self.num_trial_to_converge = num_trial_to_converge
    self.num_trials_so_far = 0

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    del all_active
    self.num_trials_so_far += len(completed.trials)

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    if self.num_trials_so_far < self.num_trial_to_converge:
      parameters = {
          param.name: self.bad_value + self.noise * np.random.uniform()
          for param in self.search_space.parameters
      }
    else:
      parameters = {
          param.name: self.good_value + self.noise * np.random.uniform()
          for param in self.search_space.parameters
      }
    return [vz.TrialSuggestion(parameters)]


class EfficiencyConvergenceTest(absltest.TestCase):

  def test_comparison(self):
    experimenter = experimenters.BBOBExperimenterFactory('Sphere', 3)()

    num_trials = 20

    def _baseline_designer(
        problem: vz.ProblemStatement, seed: Optional[int] = None
    ) -> vza.Designer:
      return FakeDesigner(
          problem.search_space,
          num_trial_to_converge=num_trials,
          good_value=0.0,
          bad_value=1.0,
          seed=seed,
      )

    def _good_designer(
        problem: vz.ProblemStatement, seed: Optional[int] = None
    ) -> vza.Designer:
      return FakeDesigner(
          problem.search_space,
          good_value=0.0,
          bad_value=1.0,
          num_trial_to_converge=int(num_trials / 4),
          seed=seed,
      )

    comparator = comparator_runner.EfficiencyComparisonTester(
        num_trials=num_trials, num_repeats=5
    )
    comparator.assert_better_efficiency(
        benchmarks.DesignerBenchmarkStateFactory(
            experimenter=experimenter, designer_factory=_good_designer
        ),
        benchmarks.DesignerBenchmarkStateFactory(
            experimenter=experimenter, designer_factory=_baseline_designer
        ),
        score_threshold=0.3,
    )

    # Test that our baseline is worse.
    with self.assertRaises(comparator_runner.FailedComparisonTestError):  # pylint: disable=g-error-prone-assert-raises
      comparator.assert_better_efficiency(
          benchmarks.DesignerBenchmarkStateFactory(
              experimenter=experimenter, designer_factory=_baseline_designer
          ),
          benchmarks.DesignerBenchmarkStateFactory(
              experimenter=experimenter, designer_factory=_good_designer
          ),
          score_threshold=-0.1,
      )


class SimpleRegretConvergenceRunnerTest(parameterized.TestCase):
  """Test suite for convergence runner."""

  def setUp(self):
    super(SimpleRegretConvergenceRunnerTest, self).setUp()
    self.experimenter = experimenters.BBOBExperimenterFactory('Sphere', 3)()
    self.converter = converters.TrialToModelInputConverter.from_problem(
        self.experimenter.problem_statement()
    )

  @parameterized.parameters(
      {
          'candidate_num_trials': 1,
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': True,
      },
      {
          'candidate_num_trials': 100,
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': True,
      },
      {
          'candidate_num_trials': 1,
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': False,
      },
      {
          'candidate_num_trials': 100,
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': False,
      },
      {
          'candidate_num_trials': 1,
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': False,
      },
      {
          'candidate_num_trials': 100,
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': False,
      },
      {
          'candidate_num_trials': 1,
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': True,
      },
      {
          'candidate_num_trials': 100,
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': True,
      },
  )
  def test_designer_convergence(
      self, candidate_num_trials, candidate_x_value, goal, should_pass
  ):
    def _better_designer_factory(problem, seed):
      return FakeDesigner(
          search_space=problem.search_space,
          good_value=candidate_x_value,
          bad_value=0.0,
          noise=0.0,
          seed=seed,
      )

    def _baseline_designer_factory(problem, seed):
      return FakeDesigner(
          search_space=problem.search_space,
          good_value=1.0,
          bad_value=1.0,
          noise=0.0,
          seed=seed,
      )

    baseline_benchmark_state_factory = benchmarks.DesignerBenchmarkStateFactory(
        experimenter=self.experimenter,
        designer_factory=_baseline_designer_factory,
    )

    candidate_benchmark_state_factory = (
        benchmarks.DesignerBenchmarkStateFactory(
            experimenter=self.experimenter,
            designer_factory=_better_designer_factory,
        )
    )

    simple_regret_test = comparator_runner.SimpleRegretComparisonTester(
        baseline_num_trials=1000,
        candidate_num_trials=candidate_num_trials,
        baseline_suggestion_batch_size=1,
        candidate_suggestion_batch_size=1,
        baseline_num_repeats=5,
        candidate_num_repeats=5,
        alpha=0.05,
        goal=goal,
    )

    if should_pass:
      simple_regret_test.assert_benchmark_state_better_simple_regret(
          baseline_benchmark_state_factory,
          candidate_benchmark_state_factory,
      )
    else:
      with self.assertRaises(  # pylint: disable=g-error-prone-assert-raises
          comparator_runner.FailedSimpleRegretConvergenceTestError
      ):
        simple_regret_test.assert_benchmark_state_better_simple_regret(
            baseline_benchmark_state_factory,
            candidate_benchmark_state_factory,
        )

  @parameterized.parameters(
      {
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': True,
      },
      {
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': False,
      },
      {
          'candidate_x_value': 5.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': False,
      },
      {
          'candidate_x_value': 0.0,
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': True,
      },
  )
  def test_optimizer_convergence(self, candidate_x_value, goal, should_pass):
    score_fn = lambda x, _: jnp.sum(x.continuous.padded_array, axis=-1)
    simple_regret_test = comparator_runner.SimpleRegretComparisonTester(
        baseline_num_trials=100,
        candidate_num_trials=100,
        baseline_suggestion_batch_size=1,
        candidate_suggestion_batch_size=1,
        baseline_num_repeats=5,
        candidate_num_repeats=5,
        alpha=0.05,
        goal=goal,
    )

    # pylint: disable=unused-argument
    def _baseline_strategy_factory(converter, suggestion_batch_size):
      return FakeVectorizedStrategy(
          converter=converter,
          good_value=1.0,
          bad_value=1.0,
          num_trial_to_converge=0,
      )

      # pylint: disable=unused-argument

    def _candidate_strategy_factory(converter, suggestion_batch_size):
      return FakeVectorizedStrategy(
          converter=converter,
          good_value=candidate_x_value,
          bad_value=0.0,
          num_trial_to_converge=0,
      )

    if should_pass:
      simple_regret_test.assert_optimizer_better_simple_regret(
          self.converter,
          score_fn,
          _baseline_strategy_factory,
          _candidate_strategy_factory,
      )
    else:
      with self.assertRaises(  # pylint: disable=g-error-prone-assert-raises
          comparator_runner.FailedSimpleRegretConvergenceTestError
      ):
        simple_regret_test.assert_optimizer_better_simple_regret(
            self.converter,
            score_fn,
            _baseline_strategy_factory,
            _candidate_strategy_factory,
        )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/algorithms/testing/failing.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Failing designers used for testing."""

from typing import Optional, Sequence
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random


class FailedSuggestError(Exception):
  """Exception to raise during failing suggest call."""


class FailingDesigner(vza.Designer):
  """Failing Designer.

  The designer raises exception at every suggest call.
  """

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    pass

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    raise FailedSuggestError()


class AlternateFailingDesigner(vza.Designer):
  """Alternate Failing Designer.

  The designer raises exception at every second suggest call.

  Note: the designer doesn't persist a state across different instantiations and
  therefore should only be used during in memory run.
  """

  def __init__(self, search_space: vz.SearchSpace):
    self._suggest_count = 0
    self._random_designer = random.RandomDesigner(search_space)

  def update(
      self, completed: vza.CompletedTrials, all_active: vza.ActiveTrials
  ) -> None:
    pass

  def suggest(
      self, count: Optional[int] = None
  ) -> Sequence[vz.TrialSuggestion]:
    count = count or 1
    self._suggest_count += count
    if self._suggest_count % 2 == 0 or count > 1:
      raise FailedSuggestError()
    else:
      return self._random_designer.suggest(1)


--- vizier/_src/algorithms/testing/failing_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for failing."""

from vizier import pyvizier as vz
from vizier._src.algorithms.testing import failing
from absl.testing import absltest


class FailingTest(absltest.TestCase):

  def test_failing_designer(self):
    failing_designer = failing.FailingDesigner()
    with self.assertRaises(failing.FailedSuggestError):
      failing_designer.suggest(1)

  def test_alternate_failing_designer(self):
    search_space = vz.SearchSpace()
    search_space.root.add_float_param("x", 0.0, 1.0)
    alt_failing_designer = failing.AlternateFailingDesigner(search_space)
    for _ in range(5):
      alt_failing_designer.suggest(1)
      with self.assertRaises(failing.FailedSuggestError):
        alt_failing_designer.suggest(1)


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/algorithms/testing/optimizer_test_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for gradient-free optimizers."""

from absl import logging
import numpy as np
from vizier import algorithms as vza
from vizier import pyvizier as vz


def assert_passes_on_random_single_metric_function(
    self, search_space: vz.SearchSpace, optimizer: vza.GradientFreeOptimizer, *,
    np_random_seed: int):
  """Smoke test on random score."""
  rng = np.random.default_rng(np_random_seed)

  logging.info('search space: %s', search_space)

  problem = vz.ProblemStatement(
      search_space=search_space,
      metric_information=[
          vz.MetricInformation(
              'acquisition', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
      ])

  def mock_score(trials):
    return {'acquisition': rng.uniform(size=[len(trials), 1])}

  suggestions = optimizer.optimize(mock_score, problem, count=5)
  self.assertNotEmpty(suggestions)

  logging.info('suggestions: %s', suggestions)
  for suggestion in suggestions:
    problem.search_space.assert_contains(suggestion.parameters)


def assert_passes_on_random_multi_metric_function(
    self,
    search_space: vz.SearchSpace,
    optimizer: vza.GradientFreeOptimizer,
    *,
    np_random_seed: int
):
  """Bi-objective test on random score."""
  rng = np.random.default_rng(np_random_seed)

  logging.info('search space: %s', search_space)

  problem = vz.ProblemStatement(
      search_space=search_space,
      metric_information=[
          vz.MetricInformation(
              'acquisition_1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
          ),
          vz.MetricInformation(
              'acquisition_2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
          ),
      ],
  )

  def mock_score(trials):
    return {
        'acquisition_1': rng.uniform(size=[len(trials), 1]),
        'acquisition_2': rng.uniform(size=[len(trials), 1]),
    }

  suggestions = optimizer.optimize(mock_score, problem, count=5)
  self.assertNotEmpty(suggestions)

  logging.info('suggestions: %s', suggestions)
  for suggestion in suggestions:
    problem.search_space.assert_contains(suggestion.parameters)


--- vizier/_src/algorithms/testing/simplekd_runner.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""SimpleKD convergence tests."""

from absl import logging
import attrs
from vizier import algorithms as vza
from vizier._src.benchmarks.experimenters.synthetic import simplekd
from vizier._src.benchmarks.runners import benchmark_runner
from vizier._src.benchmarks.runners import benchmark_state


class FailedSimpleKDConvergenceTestError(Exception):
  """Exception raised for simplekd convergence test fails."""


@attrs.define(kw_only=True)
class SimpleKDConvergenceTester:
  """The class tests designers convergence on SimpleKD problem."""

  # The SimpleKD problem type.
  best_category: simplekd.SimpleKDCategory

  # A designer factory to generate different instances of the designer.
  designer_factory: vza.DesignerFactory[vza.Designer]

  # The number of the suggested trials in each run.
  num_trials: int

  # The maximum objective value relative error to determine convergence.
  max_relative_error: float

  # The number of repeated runs used to obtain reliable results.
  num_repeats: int

  # The target number of converged runs.
  target_num_convergence: int

  # The number of designer trials suggested in each repeat.
  batch_size: int = 1

  # The number of float parameters.
  num_float_param: int = 1

  # The number of discrete parameters.
  num_discrete_param: int = 1

  # The number of integer parameters.
  num_int_param: int = 1

  # Whether to use random seeds when instantiating the designer.
  is_deterministic: bool = True

  def assert_convergence(self) -> None:
    """Run the convergence test."""
    exptr = simplekd.SimpleKDExperimenter(
        self.best_category,
        num_float_param=self.num_float_param,
        num_discrete_param=self.num_discrete_param,
        num_int_param=self.num_int_param,
        output_relative_error=True,
    )
    runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[
            benchmark_runner.GenerateAndEvaluate(batch_size=self.batch_size),
        ],
        num_repeats=self.num_trials // self.batch_size,
    )
    num_success = 0
    for seed in range(self.num_repeats):
      state = benchmark_state.BenchmarkState(
          exptr,
          benchmark_state.PolicySuggester.from_designer_factory(
              problem=exptr.problem_statement(),
              designer_factory=self.designer_factory,
              seed=seed if self.is_deterministic else None,
          ),
      )
      runner.run(state)
      best_trial = state.algorithm.supporter.GetBestTrials(count=1)[0]
      metric_name = exptr.problem_statement().metric_information.item().name
      best_err = best_trial.final_measurement_or_die.metrics[metric_name].value
      if best_err <= self.max_relative_error:
        num_success += 1
    if num_success < self.target_num_convergence:
      raise FailedSimpleKDConvergenceTestError(
          'The simplekd convergence test failed. The number of converged runs'
          f' is {num_success}, though expected at least'
          f' {self.target_num_convergence} converged runs.'
      )
    else:
      logging.info(
          'The simplekd convergence test passed. The number of converged runs'
          ' is %s, and the expected number of converged runs is %s.',
          num_success,
          self.target_num_convergence,
      )


--- vizier/_src/algorithms/testing/test_runners.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Test runners for algorithms."""
from typing import Any, Callable, List, Optional, Sequence


from absl import logging
import attr
import numpy as np
from vizier import algorithms as vza
from vizier import benchmarks
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies.designer_policy import InRamDesignerPolicy


@attr.define
class RandomMetricsRunner:
  """Generates completed `Trial`s with random metrics.

  EXAMPLE: This method can be used for smoke testing a `Designer`.
  ```
  from vizier._src.algorithms.testing import test_runners
  from vizier.testing import test_studies
  from vizier import pyvizier as vz

  problem = vz.ProblemStatement(
      test_studies.flat_space_with_all_types(),
      [vz.MetricInformation(
          'objective',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE)])
  test_runners.RandomMetricsRunner(
      problem, validate_parameters=True).run_designer(my_designer)
  ```

  EXAMPLE: This method can be used for generating a large number of trials
  to be used as a test dataset.
  ```
  # (Continued from the above code block)
  from vizier._src.algorithms.designers import random
  trials = test_runners.RandomMetricsRunner(problem).run_designer(
      random.RandomDesigner(problem.search_space),
      validate_parameters=False)
  ```

  run_* methods run a suggest-update loop, completing suggestions with
  random metric values and then return all generated trials.
  The random values are sampled from uniform distribution over the metric's
  specified range. If the minimum or maximum is not specified, it defaults to
  -10 and +10 respectively.

  Attributes:
    problem: Problem definition.
    iters: Number of suggest-update iterations.
    batch_size: Number of suggestions to ask in each suggest() call.
    seed: Random seed for generating metrics.
    verbose: Increase the verbosity to see more logs.
    validate_parameters: If True, check if the suggested trials are valid in the
      search space.
  """

  problem: vz.ProblemStatement = attr.field()
  iters: int = attr.field(default=5)
  batch_size: Optional[int] = attr.field(default=1, kw_only=True)
  seed: Any = attr.field(default=None, kw_only=True)
  verbose: int = attr.field(default=0, kw_only=True)
  validate_parameters: bool = attr.field(default=False, kw_only=True)

  def _run(self, algorithm: benchmarks.PolicySuggester) -> List[vz.Trial]:
    """Implementation of run methods."""
    rng = np.random.RandomState(self.seed)
    for it in range(self.iters):
      suggestions = algorithm.suggest(self.batch_size)
      if not suggestions:
        logging.info(
            (
                'Preemptively finished at iteration %s'
                'because designer returned nothing.'
            ),
            it,
        )
        break

      for suggestion in suggestions:
        if self.validate_parameters:
          self.problem.search_space.assert_contains(suggestion.parameters)
        measurement = vz.Measurement()
        for mi in self.problem.metric_information:
          measurement.metrics[mi.name] = rng.uniform(
              mi.min_value_or(lambda: -10.0), mi.max_value_or(lambda: 10.0)
          )
        # PolicySupporter owns PolicySuggester, which owns reference to the
        # suggestions. Mutating these trials here will affect future
        # suggestions from the algorithm.
        suggestion.complete(measurement)
      if self.verbose:
        logging.info(
            'At iteration %s, trials suggested and evaluated:\n%s',
            it,
            suggestions,
        )
    return algorithm.supporter.GetTrials()

  def run_policy(
      self, policy_factory: Callable[[pythia.PolicySupporter], pythia.Policy]
  ) -> List[vz.Trial]:
    """Run the policy generated by policy_factory."""
    supporter = pythia.InRamPolicySupporter(self.problem)
    policy = policy_factory(supporter)
    return self._run(benchmarks.PolicySuggester(policy, supporter))

  def run_designer(self, designer: vza.Designer) -> List[vz.Trial]:
    """Run the specified $designer."""
    supporter = pythia.InRamPolicySupporter(self.problem)
    policy = InRamDesignerPolicy(
        self.problem,
        supporter=supporter,
        designer_factory=lambda _, **kwargs: designer,
    )
    return self._run(benchmarks.PolicySuggester(policy, supporter))


def run_with_random_metrics(
    designer: vza.Designer,
    problem: vz.ProblemStatement,
   
    iters: int = 5,
    *,
    batch_size: Optional[int] = 1,
    seed: Any = None,
    verbose: int = 0,
    validate_parameters: bool = False,
) -> Sequence[vz.Trial]:
  """DO NOT USE. DEPRECATED. Use RandomMetricsRunner.run_designer()."""
  return RandomMetricsRunner(
      problem,
      iters,
      batch_size=batch_size,
      seed=seed,
      verbose=verbose,
      validate_parameters=validate_parameters,
  ).run_designer(designer)


--- vizier/_src/benchmarks/analyzers/convergence_curve.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Converters and comparators for convergence curves."""

import abc
import bisect
import copy
import enum
import logging
from typing import Callable, List, Literal, Optional, Protocol, Sequence, Union

import attr
import numpy as np
from vizier import pyvizier
from vizier.pyvizier import converters
from vizier.pyvizier import multimetric
from vizier.pyvizier.multimetric import xla_pareto


@attr.s(auto_attribs=True)
class ConvergenceCurve:
  """Represents multiple convergence curves on the same task."""

  xs: np.ndarray  # [T] array. All curves share the x axis.
  ys: np.ndarray  # [N x T] array where N is the number of curves.
  ylabel: str = ''  # Optional for plotting.
  xlabel: str = ''  # Optional for plotting.
  # Indicates ys should be increasing in t or decreasing.

  @enum.unique
  class YTrend(enum.Enum):
    """Trend of ys across t."""

    UNKNOWN = 'unknown'
    INCREASING = 'increasing'
    DECREASING = 'decreasing'

  trend: YTrend = YTrend.UNKNOWN

  def __attrs_post_init__(self):
    if len(self.ys.shape) != 2:
      raise ValueError(
          'The shape of ys should be (num_curves, n_steps); but ys shape is'
          f' {self.ys.shape}'
      )
    if len(self.xs) != self.ys.shape[1]:
      raise ValueError(
          f'Shape mismatch for time dim: {len(self.xs)} vs {self.ys.shape}'
      )
    # Allow for small numerical imprecisions.
    if self.trend == ConvergenceCurve.YTrend.INCREASING:
      if not np.all(np.nan_to_num(np.diff(self.ys, axis=-1)) >= -1e-8):
        raise ValueError(f'Increasing trend not found: {self.ys}')

    if self.trend == ConvergenceCurve.YTrend.DECREASING:
      if not np.all(np.nan_to_num(np.diff(self.ys, axis=-1)) <= 1e-8):
        raise ValueError(f'Decreasing trend not found: {self.ys}')

  @property
  def num_curves(self) -> int:
    return self.ys.shape[0]

  @classmethod
  def _interpolate_curves(
      cls,
      curves: Sequence['ConvergenceCurve'],
      *,
      resolution: Optional[int] = None,
      interpolate_repeats: bool = False,
  ) -> tuple[np.ndarray, list[np.ndarray]]:
    """Interpolate curves to have the same xs using linear interpolation.

    If xs are greater than xp[-1], then default is np.nan.

    Args:
      curves:
      resolution: Number of points to interpolate to. Leave it None to use the
        maximal resolution.
      interpolate_repeats: Interpolate repeated values (xs and ys) in the curve
        via linear interpolation (except for the last repeated segment).

    Returns:
      A tuple of the interpolated xs array, and a list of the interpolated ys
      arrays.
    """
    if not curves:
      raise ValueError('Empty sequence of curves.')
    if len(set([c.trend for c in curves])) > 1:
      raise ValueError('All curves must be increasing or decreasing.')
    minx = np.min([np.min(c.xs) for c in curves])
    maxx = np.max([np.max(c.xs) for c in curves])
    resolution = resolution or np.max([np.size(c.xs) for c in curves])
    xs = np.linspace(minx, maxx, resolution)

    all_ys = []
    for curve in curves:
      curve_ys = []
      for ys in curve.ys:
        if interpolate_repeats:
          _, indices = np.unique(ys, return_index=True)
          # Sorting indices from increasing order due to sign differences.
          indices = sorted(indices)
          if len(ys) - 1 not in indices:
            indices.append(len(ys) - 1)
        else:
          # Use the whole curve.
          indices = range(len(ys))
        curve_ys.append(
            np.interp(
                xs,
                np.array([curve.xs[i] for i in indices]),
                np.array([ys[i] for i in indices]),
                right=np.nan,
            )
        )
      all_ys.append(np.stack(curve_ys, axis=0))
    return xs, all_ys

  @classmethod
  def _align_xs_combine_ys(
      cls,
      curves: Sequence['ConvergenceCurve'],
      *,
      resolution: Optional[int] = None,
      interpolate_repeats: bool = False,
  ) -> list['ConvergenceCurve']:
    """Align curves (same xs) using linear interpolation and combine all ys."""
    xs, all_ys = cls._interpolate_curves(
        curves, resolution=resolution, interpolate_repeats=interpolate_repeats
    )
    # Take all non-empty ylabels.
    ylabels = list(set([c.ylabel for c in curves if c.ylabel]))
    if not ylabels:
      ylabel = ''
    elif len(ylabels) > 1:
      logging.info(
          'Curves have different ylabels: %s. None of them is selected.',
          ylabels,
      )
      ylabel = ''
    else:
      ylabel = ylabels[0]
    return [
        cls(xs=xs, ys=np.vstack(all_ys), ylabel=ylabel, trend=curves[0].trend)
    ]

  @classmethod
  def _align_xs_keep_ys(
      cls,
      curves: Sequence['ConvergenceCurve'],
      *,
      resolution: Optional[int] = None,
      interpolate_repeats: bool = False,
  ) -> list['ConvergenceCurve']:
    """Align curves (same xs) using linear interpolation and keep ys."""
    xs, all_ys = cls._interpolate_curves(
        curves, resolution=resolution, interpolate_repeats=interpolate_repeats
    )
    return [
        cls(xs=xs, ys=ys, ylabel=curves[i].ylabel, trend=curves[0].trend)
        for i, ys in enumerate(all_ys)
    ]

  @classmethod
  def align_xs(
      cls,
      curves: Sequence['ConvergenceCurve'],
      *,
      resolution: Optional[int] = None,
      interpolate_repeats: bool = False,
      keep_curves_separate: bool = False,
  ) -> list['ConvergenceCurve']:
    """Align curves (same xs) using linear interpolation."""
    if keep_curves_separate:
      return cls._align_xs_keep_ys(
          curves, resolution=resolution, interpolate_repeats=interpolate_repeats
      )
    else:
      return cls._align_xs_combine_ys(
          curves, resolution=resolution, interpolate_repeats=interpolate_repeats
      )

  @classmethod
  def extrapolate_ys(
      cls, curve: 'ConvergenceCurve', steps: int = 0
  ) -> 'ConvergenceCurve':
    """Extrapolates the future ys using a variant of linear extrapolation.

    Args:
      curve:
      steps: Number of steps to perform extrapolation.

    Returns:
      ConvergenceCurve whose xs, ys are extrapolated. Batch size remains the
      same but xs, ys now have length T + steps.
    """
    if curve.trend not in (cls.YTrend.INCREASING, cls.YTrend.DECREASING):
      raise ValueError('Curve must be increasing or decreasing.')

    all_extra_ys = []
    for ys in curve.ys:
      # Use average slope in the last half of the curve as slope extrapolate.
      ys_later_half = ys[int(len(ys) / 2) :]
      slope = np.mean(
          [
              ys_later_half[idx] - ys_later_half[idx - 1]
              for idx in range(1, len(ys_later_half))
          ]
      )

      extra_ys = np.append(
          ys, [ys[-1] + slope * (1 + step) for step in range(steps)]
      )
      all_extra_ys.append(extra_ys)

    return cls(
        xs=np.append(
            curve.xs, [curve.xs[-1] + 1 + step for step in range(steps)]
        ),
        ys=np.stack(all_extra_ys),
        ylabel=curve.ylabel,
        trend=curve.trend,
    )


@attr.define
class StatefulCurveConverter(abc.ABC):
  """Converter that updates its state upon each curve conversion.

  This is to ensure that distributive property holds:
  converter.convert(trials1 + trials2) = converter.convert(trials1) +
  converter.convert(trials2)
  """

  @abc.abstractmethod
  def convert(self, trials: Sequence[pyvizier.Trial]) -> ConvergenceCurve:
    """Returns a ConvergenceCurve corresponding to trials and updates state."""


@attr.define
class ConvergenceCurveConverter(StatefulCurveConverter):
  """Converter for Trial sequence to ConvergenceCurve.

  Attributes:
      metric_information: Information of relevant metric.
      flip_signs_for_min: If True, flips the signs of metric values to always
        maximize. Useful when desiring all increasing curves.
      cost_fn: Cost of each Trial (to determine xs in ConvergenceCurve).
      measurements_type: ['final', 'intermediate', 'all']
      batch_size: Number of trials in each batch. In each batch, the order of
        trials is ignored and the best trial is used.
      _best_yval: Best y-value seen. Updated in convert.
      _cumulative_cost: Cumulative cost of trials. Updated in convert.
  """

  metric_information: pyvizier.MetricInformation
  flip_signs_for_min: bool = attr.field(default=False, kw_only=True)
  cost_fn: Callable[[pyvizier.Trial], float] = attr.field(
      default=lambda _: 1, kw_only=True
  )
  measurements_type: str = attr.field(default='final', kw_only=True)
  batch_size: int = attr.field(default=1, kw_only=True)

  # Private attributes for stateful converts.
  _best_yval: float = attr.field(default=np.nan, kw_only=True)
  _cumulative_cost: float = attr.field(default=0.0, kw_only=True)

  def convert(self, trials: Sequence[pyvizier.Trial]) -> ConvergenceCurve:
    """Returns ConvergenceCurve with a single curve."""
    if not trials:
      raise ValueError(f'No trials provided {trials}')

    yvals = [self._best_yval]
    xvals = [self._cumulative_cost]
    candidates = []

    for i in range(0, len(trials), self.batch_size):
      batch = trials[i : i + self.batch_size]
      for trial in batch:
        if self.measurements_type in ('final', 'all'):
          if trial.final_measurement and (
              self.metric_information.name in trial.final_measurement.metrics
          ):
            candidates.append(
                trial.final_measurement.metrics[
                    self.metric_information.name
                ].value
            )
        if self.measurements_type in ('intermediate', 'all'):
          for measurement in trial.measurements:
            if self.metric_information.name in measurement.metrics:
              candidates.append(
                  measurement.metrics[self.metric_information.name].value
              )
        xvals.append(xvals[-1] + self.cost_fn(trial))

      new_yval = self.comparator([self.comparator(candidates), yvals[-1]])
      yvals.extend([new_yval] * len(batch))
      candidates = []

    yvals = np.asarray(yvals[1:])
    self._best_yval = self.comparator(yvals)
    self._cumulative_cost = xvals[-1]
    if self.metric_information.goal == pyvizier.ObjectiveMetricGoal.MAXIMIZE:
      trend = ConvergenceCurve.YTrend.INCREASING
      flipped = False
    elif self.flip_signs_for_min:
      trend = ConvergenceCurve.YTrend.INCREASING
      flipped = True
    else:
      trend = ConvergenceCurve.YTrend.DECREASING
      flipped = False
    return ConvergenceCurve(
        xs=np.asarray(xvals[1:]),
        ys=np.asarray(yvals).reshape([1, -1]) * (-1 if flipped else 1),
        trend=trend,
        ylabel=self.metric_information.name,
    )

  @property
  def comparator(self):
    """Comparator used for creating the convergence curve."""
    return np.nanmax if (
        self.metric_information.goal
        == pyvizier.ObjectiveMetricGoal.MAXIMIZE) else np.nanmin


class HypervolumeCurveConverter(StatefulCurveConverter):
  """Converts Trials to cumulative hypervolume curve for multiobjective."""

  def __init__(
      self,
      metric_informations: Sequence[pyvizier.MetricInformation],
      *,
      reference_value: Optional[np.ndarray] = None,
      num_vectors: int = 10000,
      infer_origin_factor: float = 0.0,
  ):
    """Init.

    Args:
      metric_informations:
      reference_value: Reference point value from which hypervolume is computed,
        with shape that is broadcastable with (dim,). Note that the sign is
        flipped for minimization metrics. If None, this computes the minimum of
        each objective as the reference point.
      num_vectors: Number of vectors from which hypervolume is computed.
      infer_origin_factor: When inferring the reference point, set origin to be
        minimum value - factor * (range).
    """
    if len(metric_informations) < 2:
      raise ValueError(
          'Should not use hypervolume curve with less than'
          f'two metrics {metric_informations}'
      )
    self._metric_informations = metric_informations
    self._num_vectors = num_vectors

    def create_metric_converter(mc):
      return converters.DefaultModelOutputConverter(
          mc,
          flip_sign_for_minimization_metrics=True,
          raise_errors_for_missing_metrics=False,
      )

    self._converter = converters.DefaultTrialConverter(
        parameter_converters=[],
        metric_converters=[
            create_metric_converter(mc) for mc in metric_informations
        ],
    )
    self._origin_value = reference_value
    # TODO: Speed this up with hypervolume vector tracking.
    self._min_trial_idx = 1
    self._pareto_frontier = np.empty(shape=(0, len(metric_informations)))
    self._infer_origin_factor = infer_origin_factor

  def convert(self, trials: Sequence[pyvizier.Trial]) -> ConvergenceCurve:
    """Returns ConvergenceCurve with a curve of shape 1 x len(trials)."""
    # Returns a len(trials) x number of metrics np.ndarray.
    if not trials:
      raise ValueError(f'No trials provided {trials}')

    metrics = self._converter.to_labels_array(trials)
    if self._origin_value is None:
      # Set origin to the minimum of finite values.
      origin = np.zeros(shape=(metrics.shape[1],))
      for metric_idx, metric_is_finite in enumerate(
          np.any(np.isfinite(metrics), axis=0)
      ):
        # If all metrics are infinite, leave origin unchanged at 0.
        if metric_is_finite:
          metric_arr = metrics[:, metric_idx]
          all_finite = metric_arr[np.isfinite(metric_arr)]
          min_finite = np.min(all_finite)
          max_finite = np.max(all_finite)
          origin[metric_idx] = min_finite - self._infer_origin_factor * (
              max_finite - min_finite
          )
      self._origin_value = origin
      logging.info(
          'Inferring origin_value as %s with metrics %s',
          self._origin_value,
          metrics,
      )
    else:
      if len(self._origin_value) == 1:
        origin = np.broadcast_to(self._origin_value, (metrics.shape[1],))
      else:
        if self._origin_value.shape != (metrics.shape[1],):
          raise ValueError(
              f'Metric shapes {self._origin_value.shape} do not '
              f'match: {(metrics.shape[1],)}'
          )
        origin = self._origin_value

    # Calculate cumulative hypervolume with the Pareto frontier.
    all_metrics = np.vstack(
        [self._pareto_frontier, metrics]
    )  # shape is [num_pareto_points + num_points, feature dimension]
    front = multimetric.ParetoFrontier(
        points=all_metrics,
        origin=origin,
        num_vectors=self._num_vectors,
        cum_hypervolume_base=xla_pareto.jax_cum_hypervolume_origin,
    )
    all_hv_curve = front.hypervolume(is_cumulative=True)

    # Remove the Pareto frontier add-in and update state.
    hv_curve = all_hv_curve[len(self._pareto_frontier) :]
    xs = np.asarray(
        range(self._min_trial_idx, len(hv_curve) + self._min_trial_idx)
    )
    self._min_trial_idx += len(hv_curve)
    algo = multimetric.FastParetoOptimalAlgorithm(
        xla_pareto.JaxParetoOptimalAlgorithm()
    )
    pareto_points = algo.is_pareto_optimal(points=all_metrics)
    self._pareto_frontier = all_metrics[pareto_points]

    return ConvergenceCurve(
        xs=xs,
        ys=np.asarray(hv_curve).reshape([1, -1]),
        trend=ConvergenceCurve.YTrend.INCREASING,
        ylabel='hypervolume',
    )


@attr.define(init=True)
class MultiMetricCurveConverter(StatefulCurveConverter):
  """Converts Trials to cumulative convergence curve for all multimetric studies.

  Attributes:
      metrics_config: Metrics config of the whole study.
      converter: Wrapped curve converter.
  """

  metrics_config: pyvizier.MetricsConfig = attr.field(
      validator=attr.validators.instance_of(pyvizier.MetricsConfig)
  )
  converter: Union[ConvergenceCurveConverter, HypervolumeCurveConverter] = (
      attr.field()
  )

  @classmethod
  def from_metrics_config(
      cls, metrics_config: pyvizier.MetricsConfig, **kwargs
  ) -> 'MultiMetricCurveConverter':
    """Builds MultiMetricConverter from config.

    Args:
      metrics_config:
      **kwargs: Kwargs forwarded to Converter.

    Returns:
    """
    if metrics_config.is_single_objective:
      single_metric_info = metrics_config.of_type(
          pyvizier.MetricType.OBJECTIVE
      ).item()
      converter = ConvergenceCurveConverter(single_metric_info, **kwargs)
      return cls(metrics_config, converter)
    else:
      converter = HypervolumeCurveConverter(
          list(metrics_config.of_type(pyvizier.MetricType.OBJECTIVE)), **kwargs
      )
    return cls(metrics_config, converter)

  def convert(self, trials: Sequence[pyvizier.Trial]) -> ConvergenceCurve:
    """Returns ConvergenceCurve with a curve of shape 1 x len(trials)."""
    if not trials:
      raise ValueError(f'No trials provided {trials}')

    # Add safety understanding by setting unsafe Trials to inf/-inf.
    safety_checker = multimetric.SafetyChecker(self.metrics_config)
    warped_trials = safety_checker.warp_unsafe_trials(copy.deepcopy(trials))

    return self.converter.convert(warped_trials)


@attr.define(init=True)
class RestartingCurveConverter(StatefulCurveConverter):
  """StatefulConverter that restarts the underlying stateful converters."""

  converter_factory: Callable[[], StatefulCurveConverter] = attr.field()
  # The minimum number of Trials needed before restarts occur.
  restart_min_trials: int = attr.field(
      default=10,
      validator=[attr.validators.instance_of(int), attr.validators.ge(0)],
      kw_only=True,
  )
  # The exponential rate at which restarts occur.
  restart_rate: float = attr.field(
      default=2,
      validator=attr.validators.ge(1),
      kw_only=True,
  )
  _all_trials: List[pyvizier.Trial] = attr.field(factory=list)
  _converter: Optional[StatefulCurveConverter] = attr.field(default=None)

  def convert(self, trials: Sequence[pyvizier.Trial]) -> ConvergenceCurve:
    if self._converter is None:
      self._converter = self.converter_factory()
      if self._all_trials:
        self._converter.convert(self._all_trials)

    curve = self._converter.convert(trials)
    self._all_trials.extend(trials)
    if len(self._all_trials) < self.restart_min_trials:
      return curve

    # Reset converter when refresh rate is crossed.
    log_previous_num_trials = np.log(
        1 + len(self._all_trials) - len(trials)
    ) / np.log(self.restart_rate)
    log_num_trials = np.log(1 + len(self._all_trials)) / np.log(
        self.restart_rate
    )
    if int(log_num_trials) > int(log_previous_num_trials):
      self._converter = None

    return curve


@attr.define
class ConvergenceComparator(abc.ABC):
  """Base class for convergence curve compartors.

  Attributes:
    baseline_curve: The baseline ConvergenceCurve.
    compared_curve: The compared ConvergenceCurve.
    baseline_quantile: Quantile in [0, 1] of the batched baseline curve to use
      for efficiency comparison. The higher the quantile, the better the quality
      of the baseline batch.
    compared_quantile: Quantile in [0, 1] of the batched compared curve to use
      for efficiency comparison. The higher the quantile, the better the quality
      of the baseline batch.
    name: Name of comparator.
    steps_cutoff: The number of initial steps (i.e., trials) to exclude from the
      convergence curve. This allows any-time algorithms to disregard initial
      trial performance, for example, when the study is guaranteed to run beyond
      that number of steps.
  """

  _baseline_curve: ConvergenceCurve = attr.field()
  _compared_curve: ConvergenceCurve = attr.field()
  _baseline_quantile: float = attr.field(
      default=0.5,
      validator=[attr.validators.le(1), attr.validators.ge(0)],
      kw_only=True,
  )
  _compared_quantile: float = attr.field(
      default=0.5,
      validator=[attr.validators.le(1), attr.validators.ge(0)],
      kw_only=True,
  )
  _name: str = attr.field(
      default='score', validator=attr.validators.instance_of(str), kw_only=True
  )
  _sign: float = attr.field(init=False)
  _steps_cutoff: Optional[float] = None

  def __attrs_post_init__(self):
    """Validates the curves and determines the sign.

    Raises:
      ValueError: If baseline curve is not INCREASING or DECREASING.
      ValueError: If the trends mismatch.
      ValueError: If baseline_quantile or compared_quantile are not in [0, 1].
    """
    if self._baseline_curve.trend not in (
        ConvergenceCurve.YTrend.INCREASING,
        ConvergenceCurve.YTrend.DECREASING,
    ):
      raise ValueError(
          f'Curve trend {self._baseline_curve.trend} must be either'
          'increasing or decreasing.'
      )
    if self._baseline_curve.trend != self._compared_curve.trend:
      raise ValueError(
          f'Baseline curve trend {self._baseline_curve.trend}'
          f' must match compared curve trend {self._compared_curve.trend}'
      )
    self._sign = (
        1.0
        if (self._baseline_curve.trend == ConvergenceCurve.YTrend.INCREASING)
        else -1.0
    )

  @abc.abstractmethod
  def score(self) -> float:
    """Returns a summary score for the comparison between base and compared.

    Usually, higher positive numbers mean the compared curve is better than the
    baseline and vice versa.
    """
    pass

  @abc.abstractmethod
  def curve(self) -> ConvergenceCurve:
    """Returns a score curve for each xs."""
    pass

  @property
  def name(self) -> str:
    return self._name

  def standardize_curves(
      self,
      apply_quantiles: bool = True,
  ) -> tuple[np.ndarray, np.ndarray]:
    """Standardize convergence curves.

    1. Align xs and keep each ys.
    2. Convert curves to INCREASING.
    3. Apply quantiles and impute NaN (optional).
    4. Remove convergence curve points where xs < steps_cutoff.

    Args:
      apply_quantiles: Whether to compute quantiles on the batches.

    Returns:
      The standardize baseline and compared curves. If apply_quantiles=True, the
      shape is (num_steps,), otherwise the shape is (batch_size, num_steps).
    """
    # Align the curves while keeping each ys.
    align_baseline_curve, align_compared_curve = ConvergenceCurve.align_xs(
        [self._baseline_curve, self._compared_curve],
        interpolate_repeats=False,
        keep_curves_separate=True,
    )
    # Adjust sign to increasing.
    baseline_ys = self._sign * align_baseline_curve.ys
    compared_ys = self._sign * align_compared_curve.ys

    if apply_quantiles:
      # Apply batch quantiels (notice the dimension reduction).
      baseline_ys = np.nanquantile(baseline_ys, self._baseline_quantile, axis=0)
      compared_ys = np.nanquantile(compared_ys, self._compared_quantile, axis=0)

    # Impute NaN values as -inf. This happens due to `align_xs` assigning
    # np.nan for xs that are outside the original convergence curve.
    baseline_ys = np.nan_to_num(baseline_ys, nan=-np.inf)
    compared_ys = np.nan_to_num(compared_ys, nan=-np.inf)

    # Remove convergence curve points where xs < steps_cutoff.
    if self._steps_cutoff is not None:
      baseline_cutoff_ind = np.where(
          align_baseline_curve.xs >= self._steps_cutoff
      )[0]
      compared_cutoff_ind = np.where(
          align_compared_curve.xs >= self._steps_cutoff
      )[0]
      if np.size(baseline_cutoff_ind) == 0 or np.size(compared_cutoff_ind) == 0:
        raise ValueError(
            f'fThe given steps_cutoff {self._steps_cutoff} is too high'
        )
      else:
        baseline_ys = baseline_ys[baseline_cutoff_ind[0] :]
        compared_ys = compared_ys[compared_cutoff_ind[0] :]

    return baseline_ys, compared_ys


class ConvergenceComparatorFactory(Protocol):
  """ConvergenceComparator factory interface."""

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      **kwargs,
  ) -> ConvergenceComparator:
    ...


@attr.define
class LogEfficiencyConvergenceCurveComparator(ConvergenceComparator):
  """Comparator methods for ConvergenceCurves.

  Methods in this class generally return comparison metrics for a compared curve
  against a baseline curve. Only works for curves with INCREASING or DECREASING
  trend.

  Example usage:
    baseline_curve = ConvergenceCurve(...)
    comparator = LogEfficiencyConvergenceCurveComparator(baseline_curve)
    comparator.curve(compared_curve)
  """

  max_score: float = attr.field(
      default=1.0, validator=[attr.validators.ge(0)], kw_only=True
  )
  summary_function: Callable[[np.ndarray], float] = attr.field(
      default=np.median
  )

  def curve(self) -> ConvergenceCurve:
    """Builds the log sample efficiency curve.

    The compared curve should approximately use exp(-relative efficiency)% less
    Trials than the baseline curve. Note that a positive relative effiency
    demonstrates that the compared curve is better than the baseline. Also,
    the relative efficiency CURVES are not fully symmetric due to differences
    in drops in objective values.

    Returns:
      ConvergenceCurves with ys (batch size 1) as the relative efficiency curve.

      The xs of curve is equal to the xs of baseline curve but there
      may be indices equal to positive 'inf' which indicate that no point
      in the compared curve outperforms the baseline at that index.

    Raises:
      ValueError: If the trends do mismatch.
      ValueError: If baseline_quantile or compared_quantile are not in [0, 1].
    """

    baseline_quantile = np.nanquantile(
        self._sign * self._baseline_curve.ys, self._baseline_quantile, axis=0
    )
    # This may not be [1,2,...] due to repeats.
    baseline_index_curve = build_convergence_curve(
        baseline_quantile, baseline_quantile
    )

    other_index_curve = build_convergence_curve(
        baseline_quantile,
        np.nanquantile(
            self._sign * self._compared_curve.ys,
            self._compared_quantile,
            axis=0,
        ),
    )

    ys = np.clip(
        np.log(1 + np.asarray(baseline_index_curve))
        - np.log(1 + np.asarray(other_index_curve)),
        a_min=-self.max_score,
        a_max=self.max_score,
    )
    return ConvergenceCurve(
        xs=self._baseline_curve.xs, ys=ys.reshape(1, len(ys))
    )

  def score(self) -> float:
    """Gets a finalized log efficiency score.

    The compared curve should approximately use exp(-score)% Trials compared to
    the baseline curve. Note that a high positive score demonstrates that the
    compared curve uses less Trials and is better than the baseline.

    Returns:
      Sample efficiency score. This score is symmetric and always finite when
      baseline_quantile <= compare_quantile (recommended setting).
    """
    baseline_curve = ConvergenceCurve.align_xs(
        [self._baseline_curve], interpolate_repeats=True
    )[0]
    compared_curve = ConvergenceCurve.align_xs(
        [self._compared_curve], interpolate_repeats=True
    )[0]
    # Combined curve (as the baseline) becomes the y-values at which
    # Trial efficiency is evaluated.
    combined_curve = ConvergenceCurve.align_xs(
        [baseline_curve, compared_curve]
    )[0]
    combined_curve.ys = np.nanmedian(combined_curve.ys, axis=0, keepdims=True)

    # Look ahead for exp(max_score)*T steps, as score is in the log space.
    extend_steps = int(np.exp(self.max_score) * len(self._baseline_curve.xs))
    extended_baseline = ConvergenceCurve.extrapolate_ys(
        baseline_curve, extend_steps
    )
    extended_compared = ConvergenceCurve.extrapolate_ys(
        compared_curve, extend_steps
    )
    baseline_comparator = LogEfficiencyConvergenceCurveComparator(
        baseline_curve=combined_curve,
        compared_curve=extended_baseline,
        compared_quantile=self._baseline_quantile,
    )
    efficiency_baseline = baseline_comparator.curve()
    compared_comparator = LogEfficiencyConvergenceCurveComparator(
        baseline_curve=combined_curve,
        compared_curve=extended_compared,
        compared_quantile=self._compared_quantile,
    )
    efficiency_compared = compared_comparator.curve()

    # Clip log efficiency and return median log efficiency in last half.
    diff = np.clip(
        efficiency_compared.ys, a_min=-self.max_score, a_max=self.max_score
    ) - np.clip(
        efficiency_baseline.ys, a_min=-self.max_score, a_max=self.max_score
    )
    return self.summary_function(diff)


@attr.define
class PercentageBetterConvergenceCurveComparator(ConvergenceComparator):
  """Comparator method based on percentage better.

  PercentageBetter is the average percentage of steps that one curve is better
  than the other.

  For example, assuming a study with 100 trials, a score of 0.07 means that on
  average for each 'baseline' trial the 'compared' convergence curve has already
  reached that value 7 steps before.
  """

  def _compute_directional_score(
      self, baseline: np.ndarray, compared: np.ndarray
  ) -> float:
    """Compute the percentage better score of 'compared' vs. 'baseline'.

    Note that: sum_i sum_j 1{c_j > b_i} = sum_j sum_i {b_i < c_j}. Therefore, we
    can either iterate over 'compared' and count the number of steps that
    'baseline' is worse OR we can iterate over 'baseline' and count the number
    of steps that 'compared' is better (which is the current implementation).

    Implementation
    --------------
    1. For each index of `baseline`:
      - Finds the smallest index of 'compared' that is better.
      - Compute the percentage of `compared` steps that are better.
    2. Average the percentages across all 'baseline' indices.

    The more dominante 'compared' over 'baseline' the closer the score is to
    1.0.

    Args:
      baseline: The baseline convergence curve.
      compared: The compared convergence curve.

    Returns:
      The average number of steps that compared is better than baseline (score
      is in [0, 1]).
    """
    convergence_curve = build_convergence_curve(list(baseline), list(compared))
    pct_baseline_compared = [
        (len(compared) - i) / len(compared) if i != float('inf') else 0
        for i in convergence_curve
    ]
    return np.mean(pct_baseline_compared)

  def score(self) -> float:
    """Computes the percentage better score.

    The score has two components:
    (a) sub-score quantifying how much 'compared' domniates 'baseline'.
    (b) sub-score quantifying how much 'baseline' domniates 'compared'.

    The final score is (a) - (b).

    Returns:
      The normalized percentage better convergence score [-1.0, 1.0].

    Raises:
      ValueError: If curve trends are not INCREASING or DECREASING, or not
      equal.
    """
    baseline_ys, compared_ys = self.standardize_curves()
    baseline_compared_score = self._compute_directional_score(
        baseline_ys, compared_ys
    )
    compared_baseline_score = self._compute_directional_score(
        compared_ys, baseline_ys
    )
    return baseline_compared_score - compared_baseline_score

  def curve(self) -> ConvergenceCurve:
    raise NotImplementedError('Curve not yet implemented.')


@attr.define
class WinRateConvergenceCurveComparator(ConvergenceComparator):
  """Comparator method based on convergence curves simple win rate comparison.

  The comparator has two modes of comparing convergence curves:

  1. Pairwise - Compare all pairs of repeated convergence curves and then
    compute the mean win-rate over all the steps (i.e. trial) and pairs.

  2. Quantiles - First compute the quantiles convergence curve per step across
    the repeates, and then compute the mean win-rate over the steps.

  The score ranges within [-0.5, 0.5], such that a score of  0.5 indicates that
  the 'compared' curve is better than 'baseline' across all stpes.
  """

  comparison_mode: Literal['pairwise', 'quantiles'] = 'pairwise'

  def score(self) -> float:
    return np.nanmean(self.curve().ys)

  def curve(self) -> ConvergenceCurve:
    """Computes the curve that represents the average win rate."""
    if self.comparison_mode == 'pairwise':
      baseline_ys, compared_ys = self.standardize_curves(apply_quantiles=False)
      # Compares all pairs of compared to baseline curve.
      all_comparisons = np.apply_along_axis(
          lambda base: np.mean(compared_ys > base, axis=0)
          + 0.5 * np.mean(base == compared_ys, axis=0),
          axis=1,
          arr=baseline_ys,
      )
      curve_ys = np.mean(all_comparisons, axis=0, keepdims=True) - 0.5
    elif self.comparison_mode == 'quantiles':
      baseline_ys, compared_ys = self.standardize_curves(apply_quantiles=True)
      curve_ys = np.asarray(
          compared_ys > baseline_ys, dtype='float'
      ) + 0.5 * np.asarray(compared_ys == baseline_ys, dtype='float')
      # Note that 0.5 is the natural average, so subtracting it to make
      # positive/negative score imply better/worse comparison.
      curve_ys = curve_ys[np.newaxis, :] - 0.5
    else:
      raise ValueError(f'Unknown comparison mode: {self.comparison_mode}')

    return ConvergenceCurve(xs=self._baseline_curve.xs, ys=curve_ys)


@attr.define
class OptimalityGapWinRateComparator(ConvergenceComparator):
  """Comparator method based on win-rate of the optimality gap."""

  def score(self):
    """Computes the normalized simple regert score."""
    baseline_ys, compared_ys = self.standardize_curves()
    return float(compared_ys[-1] > baseline_ys[-1])

  def curve(self) -> ConvergenceCurve:
    """Returns a score curve for each xs."""
    raise NotImplementedError('Curve not yet implemented.')


class OptimalityGapGainComparator(ConvergenceComparator):
  """Comparator method based on optimality gap gain.

  The optimality gap gain ('compared' - 'baseline') is normalized by the
  'baseline' absolute optimality gap and then truncated.
  """

  min_value: float = -0.5
  max_value: float = 1.0
  eps: float = 0.0001

  def score(self):
    """Computes the normalized simple regert score."""
    baseline_ys, compared_ys = self.standardize_curves()
    d = (compared_ys[-1] - baseline_ys[-1]) / (abs(baseline_ys[-1]) + self.eps)
    return min(max(d, self.min_value), self.max_value)

  def curve(self) -> ConvergenceCurve:
    """Returns a score curve for each xs."""
    raise NotImplementedError('Curve not yet implemented.')


class OptimalityGapWinRateComparatorFactory(ConvergenceComparatorFactory):
  """Factory class for OptimalityGapWinRateComparator."""

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      steps_cutoff: Optional[int] = None,
  ) -> ConvergenceComparator:
    return OptimalityGapWinRateComparator(
        baseline_curve=baseline_curve,
        compared_curve=compared_curve,
        baseline_quantile=baseline_quantile,
        compared_quantile=compared_quantile,
        name='optimality_gap_win_rate',
        steps_cutoff=steps_cutoff,
    )


class OptimalityGapGainComparatorFactory(ConvergenceComparatorFactory):
  """Factory class for OptimalityGapGainComparator."""

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      steps_cutoff: Optional[int] = None,
  ) -> ConvergenceComparator:
    return OptimalityGapGainComparator(
        baseline_curve=baseline_curve,
        compared_curve=compared_curve,
        baseline_quantile=baseline_quantile,
        compared_quantile=compared_quantile,
        name='optimality_gap_gain',
        steps_cutoff=steps_cutoff,
    )


@attr.define
class WinRateConvergenceCurveComparatorFactory(ConvergenceComparatorFactory):
  """Factory class for WinRateConvergenceCurveComparatorFactory."""

  comparison_mode: Literal['pairwise', 'quantiles'] = 'pairwise'

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      steps_cutoff: Optional[int] = None,
  ) -> ConvergenceComparator:
    return WinRateConvergenceCurveComparator(
        baseline_curve=baseline_curve,
        compared_curve=compared_curve,
        baseline_quantile=baseline_quantile,
        compared_quantile=compared_quantile,
        name='convergence_curve_win_rate',
        comparison_mode=self.comparison_mode,
        steps_cutoff=steps_cutoff,
    )


class LogEfficiencyConvergenceCurveComparatorFactory(
    ConvergenceComparatorFactory
):
  """Factory class for LogEfficiencyConvergenceCurveComparator."""

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      steps_cutoff: Optional[int] = None,
  ) -> ConvergenceComparator:
    return LogEfficiencyConvergenceCurveComparator(
        baseline_curve=baseline_curve,
        compared_curve=compared_curve,
        baseline_quantile=baseline_quantile,
        compared_quantile=compared_quantile,
        name='log_eff',
        steps_cutoff=steps_cutoff,
    )


class PercentageBetterConvergenceCurveComparatorFactory(
    ConvergenceComparatorFactory
):
  """Factory class for PercentageBetterConvergenceCurveComparator."""

  def __call__(
      self,
      baseline_curve: ConvergenceCurve,
      compared_curve: ConvergenceCurve,
      baseline_quantile: float = 0.5,
      compared_quantile: float = 0.5,
      steps_cutoff: Optional[int] = None,
  ) -> ConvergenceComparator:
    return PercentageBetterConvergenceCurveComparator(
        baseline_curve=baseline_curve,
        compared_curve=compared_curve,
        baseline_quantile=baseline_quantile,
        compared_quantile=compared_quantile,
        name='pct_better',
        steps_cutoff=steps_cutoff,
    )


def build_convergence_curve(
    baseline_curve: Sequence[float], compared_curve: Sequence[float]
) -> List[float]:
  """Builds a relative convergence curve (see returns for definition).

  Finds the smallest index j for each element i in 'baseline_curve'
  such that baseline_curve[i] <= compared_curve[j]. The function uses the
  'bisect_left' function to efficiently perform binary search under the
  assumption that 'baseline_curve' and 'compared_curve' are sorted in
  non-decreasing order.

  Args:
    baseline_curve: Baseline maximization convergence curve.
    compared_curve: Compared maximization convergence curve.

  Returns:
    A list of numbers where i-th (zero-index) element is the smallest "j" such
    that baseline_curve[i] <= compared_curve[j]
  """
  convergence_curve = []
  for value in baseline_curve:
    j = bisect.bisect_left(compared_curve, value)
    convergence_curve.append(j if j != len(compared_curve) else float('inf'))
  return convergence_curve


--- vizier/_src/benchmarks/analyzers/convergence_curve_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for convergence_curve."""

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.analyzers import convergence_curve as convergence

from absl.testing import absltest
from absl.testing import parameterized


def _gen_trials(values):
  """Returns trials where trials[i] has empty metric name equal to values[i]."""
  trials = []
  for v in values:
    trial = pyvizier.Trial()
    trials.append(
        trial.complete(
            pyvizier.Measurement(metrics={'': pyvizier.Metric(value=v)})))
  return trials


class ConvergenceCurveTest(absltest.TestCase):

  def test_align_xs_merge_ys_on_different_lengths(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 2, 3]),
        ys=np.array([[2, 1, 1]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    c2 = convergence.ConvergenceCurve(
        xs=np.array([1]),
        ys=np.array([[3]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    aligned = convergence.ConvergenceCurve.align_xs([c1, c2])[0]

    np.testing.assert_array_equal(aligned.xs, [1, 2, 3])
    np.testing.assert_array_equal(
        aligned.ys, np.array([[2, 1, 1], [3, np.nan, np.nan]])
    )

  def test_align_xs_merge_ys_on_distinct_xvalues(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 3, 4]),
        ys=np.array([[2, 1, 1]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    c2 = convergence.ConvergenceCurve(
        xs=np.array([2]),
        ys=np.array([[3]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    aligned = convergence.ConvergenceCurve.align_xs([c1, c2])[0]

    np.testing.assert_array_equal(aligned.xs.shape, (3,))
    np.testing.assert_array_equal(
        aligned.ys, np.array([[2, 1.25, 1], [3, np.nan, np.nan]])
    )

  def test_align_xs_merge_ys_with_interpolation(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 2, 3, 4, 5]),
        ys=np.array([[2, 2, 1, 0.5, 0.5], [1, 1, 1, 1, 1]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    aligned = convergence.ConvergenceCurve.align_xs(
        [c1], interpolate_repeats=True
    )[0]

    np.testing.assert_array_equal(aligned.xs, np.array([1, 2, 3, 4, 5]))
    np.testing.assert_array_equal(
        aligned.ys, np.array([[2, 1.5, 1.0, 0.5, 0.5], [1, 1, 1, 1, 1]])
    )

  def test_extrapolate_ys_with_steps(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 2, 3, 4]),
        ys=np.array([[2, 1.5, 1, 0.5], [1, 1, 1, 1]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

    extra_c1 = convergence.ConvergenceCurve.extrapolate_ys(c1, steps=2)

    np.testing.assert_array_equal(extra_c1.xs.shape, (6,))
    np.testing.assert_array_equal(
        extra_c1.ys,
        np.array([[2, 1.5, 1.0, 0.5, 0.0, -0.5], [1, 1, 1, 1, 1, 1]]),
    )

  def test_align_xs_merge_ys_on_increasing_and_dicreasing_fails(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 3, 4]),
        ys=np.array([[2, 3, 3]]),
        trend=convergence.ConvergenceCurve.YTrend.INCREASING,
    )
    c2 = convergence.ConvergenceCurve(
        xs=np.array([2]),
        ys=np.array([[3]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    with self.assertRaisesRegex(ValueError, 'increasing'):
      # pylint: disable=[expression-not-assigned]
      convergence.ConvergenceCurve.align_xs([c1, c2])[0]

  def test_align_xs_keep_ys(self):
    c1 = convergence.ConvergenceCurve(
        xs=np.array([1, 3, 4]),
        ys=np.array([[2, 1, 1], [8, 3, 1]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    c2 = convergence.ConvergenceCurve(
        xs=np.array([2]),
        ys=np.array([[3]]),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

    aligned1, aligned2 = convergence.ConvergenceCurve.align_xs(
        [c1, c2], keep_curves_separate=True
    )

    np.testing.assert_array_equal(aligned1.xs, np.array([1.0, 2.5, 4.0]))
    np.testing.assert_array_equal(aligned2.xs, np.array([1.0, 2.5, 4.0]))

    np.testing.assert_array_equal(
        aligned1.ys, np.array([[2.0, 1.25, 1.0], [8.0, 4.25, 1.0]])
    )
    np.testing.assert_array_equal(
        aligned2.ys, np.array([[3.0, np.nan, np.nan]])
    )


class ConvergenceCurveConverterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('maximize', pyvizier.ObjectiveMetricGoal.MAXIMIZE, [[2, 2, 3]]),
      ('minimize', pyvizier.ObjectiveMetricGoal.MINIMIZE, [[2, 1, 1]]),
  )
  def test_convert_basic(self, goal, expected):
    trials = _gen_trials([2, 1, 3])
    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal)
    )
    curve = generator.convert(trials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_equal(curve.ys, expected)

  @parameterized.named_parameters(
      ('maximize', pyvizier.ObjectiveMetricGoal.MAXIMIZE, [[2, 2, 3]]),
      ('minimize', pyvizier.ObjectiveMetricGoal.MINIMIZE, [[-2, -1, -1]]),
  )
  def test_convert_flip_signs(self, goal, expected):
    trials = _gen_trials([2, 1, 3])
    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal), flip_signs_for_min=True
    )
    curve = generator.convert(trials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_equal(curve.ys, expected)

  @parameterized.parameters(
      (
          pyvizier.ObjectiveMetricGoal.MAXIMIZE,
          2,
          [2, 1, 4, 5],
          [[2, 2, 5, 5]],
      ),
      (
          pyvizier.ObjectiveMetricGoal.MAXIMIZE,
          2,
          [4, 5, 2, 1],
          [[5, 5, 5, 5]],
      ),
      (
          pyvizier.ObjectiveMetricGoal.MAXIMIZE,
          2,
          [2, 1, 4],
          [[2, 2, 4]],
      ),
      (
          pyvizier.ObjectiveMetricGoal.MAXIMIZE,
          3,
          [2, 1, 4, 7, 9, 8, 10],
          [[4, 4, 4, 9, 9, 9, 10]],
      ),
      (
          pyvizier.ObjectiveMetricGoal.MINIMIZE,
          3,
          [3, 7, 2, 3, 0],
          [[2, 2, 2, 0, 0]],
      ),
  )
  def test_convert_with_batch_size(self, goal, batch_size, values, expected):
    trials = _gen_trials(values)
    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal),
        flip_signs_for_min=False,
        batch_size=batch_size,
    )
    curve = generator.convert(trials)
    np.testing.assert_array_equal(curve.xs, list(range(1, len(trials) + 1)))
    np.testing.assert_array_equal(curve.ys, np.float64(expected))

  @parameterized.named_parameters(
      ('maximize', pyvizier.ObjectiveMetricGoal.MAXIMIZE, [[-np.inf, 2, 2]]),
      ('minimize', pyvizier.ObjectiveMetricGoal.MINIMIZE, [[-np.inf, -2, -1]]),
  )
  def test_convert_flip_signs_inf(self, goal, expected):
    sign = 1 if goal == pyvizier.ObjectiveMetricGoal.MINIMIZE else -1
    trials = _gen_trials([sign * np.inf, 2, 1])
    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal), flip_signs_for_min=True
    )
    curve = generator.convert(trials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_equal(curve.ys, expected)

  @parameterized.parameters(
      (pyvizier.ObjectiveMetricGoal.MAXIMIZE, [2, 1, 4, 5], 2),
      (pyvizier.ObjectiveMetricGoal.MAXIMIZE, [2, 1, 4], 1),
      (pyvizier.ObjectiveMetricGoal.MAXIMIZE, [2, 1, 4, 7, 9, 8, 10], 4),
      (pyvizier.ObjectiveMetricGoal.MINIMIZE, [3, 7, 2, 3, 4], 3),
  )
  def test_convert_with_state_updates(self, goal, values, split_idx):
    trials = _gen_trials(values)
    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal),
        flip_signs_for_min=False,
    )
    curve = generator.convert(trials)

    generator = convergence.ConvergenceCurveConverter(
        pyvizier.MetricInformation(name='', goal=goal),
        flip_signs_for_min=False,
    )
    first_trials = trials[:split_idx]
    second_trials = trials[split_idx:]
    first_curve = generator.convert(first_trials)
    second_curve = generator.convert(second_trials)

    np.testing.assert_array_equal(
        curve.xs, np.concatenate((first_curve.xs, second_curve.xs), axis=-1)
    )
    np.testing.assert_array_equal(
        curve.ys, np.concatenate((first_curve.ys, second_curve.ys), axis=-1)
    )


class HypervolumeCurveConverterTest(parameterized.TestCase):

  def test_convert_with_origin_reference(self):
    generator = convergence.HypervolumeCurveConverter(
        [
            pyvizier.MetricInformation(
                name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            ),
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
        ],
        reference_value=np.array([0.0]),
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': 2.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 3.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 3.0, 8.0]], decimal=0.5
    )

  def test_convert_with_reference(self):
    generator = convergence.HypervolumeCurveConverter(
        [
            pyvizier.MetricInformation(
                name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            ),
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
        ],
        reference_value=np.array([3.0, 0.0]),
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': 2.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 3.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 0.0, 2.0]], decimal=0.5
    )

  def test_convert_with_none_reference(self):
    generator = convergence.HypervolumeCurveConverter([
        pyvizier.MetricInformation(
            name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
        ),
        pyvizier.MetricInformation(
            name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
    ])
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -np.inf, 'min': np.inf})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 3.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 0.0, 1.0]], decimal=0.5
    )

  def test_convert_with_inf_none_reference(self):
    generator = convergence.HypervolumeCurveConverter([
        pyvizier.MetricInformation(
            name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
        ),
        pyvizier.MetricInformation(
            name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
    ])
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -np.inf, 'min': np.inf})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -np.inf, 'min': 2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2])
    np.testing.assert_array_equal(curve.ys, [[0.0, 0.0]])

  def test_convert_with_state(self):
    generator = convergence.HypervolumeCurveConverter(
        [
            pyvizier.MetricInformation(
                name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            ),
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
        ],
        reference_value=np.array([0.0]),
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': 2.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 5.0, 9.0]], decimal=0.5
    )

    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 2.0, 'min': -0.1})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [4, 5])
    np.testing.assert_array_almost_equal(curve.ys, [[9.0, 10.0]], decimal=0.5)

  def test_convert_factor_with_inf(self):
    generator = convergence.HypervolumeCurveConverter(
        [
            pyvizier.MetricInformation(
                name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            ),
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
        ],
        infer_origin_factor=1.0,
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -np.inf, 'min': np.inf})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 3.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 1.0, 4.0]], decimal=0.5
    )


class MultiMetricCurveConverterTest(parameterized.TestCase):

  def test_convert_single_objective(self):
    generator = convergence.MultiMetricCurveConverter.from_metrics_config(
        metrics_config=pyvizier.MetricsConfig([
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
            pyvizier.MetricInformation(
                name='safe',
                goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
                safety_threshold=0.1,
            ),
        ]),
        flip_signs_for_min=True,
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'min': 4.0, 'safe': -2.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'min': 3.0, 'safe': 1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'min': 2.0, 'safe': -1.0})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(curve.ys, [[-4.0, -4.0, -2.0]])

  def test_convert_multiobjective(self):
    generator = convergence.MultiMetricCurveConverter.from_metrics_config(
        metrics_config=pyvizier.MetricsConfig([
            pyvizier.MetricInformation(
                name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            ),
            pyvizier.MetricInformation(
                name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            ),
            pyvizier.MetricInformation(
                name='safe',
                goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE,
                safety_threshold=-0.2,
            ),
        ]),
        reference_value=np.array([0.0]),
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -1.0, 'safe': 1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(
                metrics={'max': 5.0, 'min': -1.0, 'safe': -1.0}
            )
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0, 'safe': 2.1})
        )
    )

    curve = generator.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[4.0, 4.0, 8.0]], decimal=0.5
    )


class RestartingCurveConverterTest(absltest.TestCase):

  def test_convert_with_restart(self):
    def converter_factory():
      return convergence.HypervolumeCurveConverter(
          [
              pyvizier.MetricInformation(
                  name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
              ),
              pyvizier.MetricInformation(
                  name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
              ),
          ],
      )

    restart_converter = convergence.RestartingCurveConverter(
        converter_factory, restart_min_trials=2, restart_rate=1.001
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 0.0, 'min': 0.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = restart_converter.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 5.0, 9.0]], decimal=0.5
    )

    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -1.0, 'min': 1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -2.0})
        )
    )

    restart_converter.convert([pytrials[0]])
    curve = restart_converter.convert([pytrials[1]])
    np.testing.assert_array_equal(curve.xs, [5])
    np.testing.assert_array_almost_equal(curve.ys, [[18.0]], decimal=0.5)

  def test_convert_no_restart(self):
    def converter_factory():
      return convergence.HypervolumeCurveConverter(
          [
              pyvizier.MetricInformation(
                  name='max', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
              ),
              pyvizier.MetricInformation(
                  name='min', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
              ),
          ],
      )

    converter = convergence.RestartingCurveConverter(
        converter_factory, restart_min_trials=10
    )
    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 0.0, 'min': 0.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 4.0, 'min': -2.0})
        )
    )

    curve = converter.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [1, 2, 3])
    np.testing.assert_array_almost_equal(
        curve.ys, [[0.0, 5.0, 9.0]], decimal=0.5
    )

    pytrials = []
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': -1.0, 'min': 1.0})
        )
    )
    pytrials.append(
        pyvizier.Trial().complete(
            pyvizier.Measurement(metrics={'max': 5.0, 'min': -2.0})
        )
    )

    curve = converter.convert(pytrials)
    np.testing.assert_array_equal(curve.xs, [4, 5])
    np.testing.assert_array_almost_equal(curve.ys, [[9.0, 10.0]], decimal=0.5)


class WinRateConvergenceCurveComparatorAllComparisonsModeTest(
    absltest.TestCase
):
  """Test the WinRateConvergenceCurveComparator with "all comparisons" mode."""

  def setUp(self):
    super(WinRateConvergenceCurveComparatorAllComparisonsModeTest, self).setUp()
    xs = np.array(range(0, 20))
    xs_t = xs.reshape(1, len(xs))
    self._baseline_curve = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(np.array([-0.9, -1.0, -1.1]).reshape(3, 1) * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

    self._worse_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(-0.5 * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    self._better_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(np.array([-1.5, -1.8, -2.0]).reshape(3, 1) * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

  def test_higher_quantile_curve(self):
    baseline_length = len(self._baseline_curve.xs)
    median_score = convergence.WinRateConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._better_curves
    ).curve()
    reverse_median_score = convergence.WinRateConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._worse_curves
    ).curve()
    higher_quantile_score = convergence.WinRateConvergenceCurveComparator(
        baseline_curve=self._baseline_curve,
        compared_curve=self._better_curves,
        compared_quantile=0.9,
    ).curve()

    self.assertEqual(median_score.ys.shape, (1, baseline_length))
    self.assertEqual(higher_quantile_score.ys.shape, (1, baseline_length))
    # Better curves should have positive efficiency.
    self.assertTrue((median_score.ys >= 0.0).all())
    self.assertTrue((reverse_median_score.ys <= 0.0).all())
    # Higher quantile means better efficiency which means more positive scores.
    self.assertTrue((higher_quantile_score.ys >= median_score.ys).all())

  def test_get_winrate_score(self):
    # Higher compared quantile should increase score. Higher baseline
    # quantile should decrease score.
    median_score = convergence.WinRateConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._better_curves
    ).score()
    self.assertGreaterEqual(
        convergence.WinRateConvergenceCurveComparator(
            baseline_curve=self._baseline_curve,
            compared_curve=self._better_curves,
            compared_quantile=0.9,
        ).score(),
        median_score,
    )
    self.assertLessEqual(
        convergence.WinRateConvergenceCurveComparator(
            baseline_curve=self._baseline_curve,
            compared_curve=self._better_curves,
            baseline_quantile=0.9,
        ).score(),
        median_score,
    )


class LogEfficiencyConvergenceComparatorTest(absltest.TestCase):

  def setUp(self):
    super(LogEfficiencyConvergenceComparatorTest, self).setUp()
    xs = np.array(range(0, 20))
    xs_t = xs.reshape(1, len(xs))
    self._baseline_curve = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(np.array([-0.9, -1.0, -1.1]).reshape(3, 1) * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

    self._worse_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(-0.5 * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    self._better_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(np.array([-1.5, -1.8, -2.0]).reshape(3, 1) * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )

  def test_get_relative_efficiency_curve(self):
    baseline_length = len(self._baseline_curve.xs)
    rel_effiency = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._better_curves
    ).curve()
    higher_quantile = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._baseline_curve,
        compared_curve=self._better_curves,
        compared_quantile=0.9,
    ).curve()

    self.assertEqual(rel_effiency.ys.shape, (1, baseline_length))
    self.assertEqual(higher_quantile.ys.shape, (1, baseline_length))
    # Better curves should have positive efficiency.
    self.assertTrue((rel_effiency.ys >= 0.0).all())
    # Higher quantile means better efficiency which means more positive scores.
    self.assertTrue((higher_quantile.ys >= rel_effiency.ys).all())

  def test_get_relative_efficiency_flat(self):
    flat_curve = convergence.ConvergenceCurve(
        xs=np.array(range(0, 20)),
        ys=np.array([4.0, 3.0, 2.0] + [1.5] * 17).reshape(1, 20),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    self_eff = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=flat_curve, compared_curve=flat_curve
    ).curve()
    # Relative efficiency of a curve on itself is close to 0.
    self.assertAlmostEqual(np.linalg.norm(self_eff.ys), 0.0, delta=0.1)

  def test_get_relative_efficiency_short_curve(self):
    baseline_length = len(self._baseline_curve.xs)
    short_length = round(baseline_length / 2)
    short_curve = convergence.ConvergenceCurve(
        xs=self._baseline_curve.xs[:short_length],
        ys=self._baseline_curve.ys[:, :short_length],
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    max_score = 10.3
    short_efficiency = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._baseline_curve,
        compared_curve=short_curve,
        max_score=max_score,
    ).curve()
    self.assertEqual(short_efficiency.ys.shape, (1, baseline_length))
    self.assertEqual(float(short_efficiency.ys[:, -1].item()), -max_score)

  def test_get_efficiency_score(self):
    # Higher compared quantile should increase score. Higher baseline
    # quantile should decrease score.
    median_score = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._better_curves
    ).score()
    self.assertGreater(
        convergence.LogEfficiencyConvergenceCurveComparator(
            baseline_curve=self._baseline_curve,
            compared_curve=self._better_curves,
            compared_quantile=0.9,
        ).score(),
        median_score,
    )
    self.assertLess(
        convergence.LogEfficiencyConvergenceCurveComparator(
            baseline_curve=self._baseline_curve,
            compared_curve=self._better_curves,
            baseline_quantile=0.9,
        ).score(),
        median_score,
    )

  def test_effiency_score_symmetry(self):
    base_score = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._baseline_curve, compared_curve=self._better_curves
    ).score()
    reversed_score = convergence.LogEfficiencyConvergenceCurveComparator(
        baseline_curve=self._better_curves, compared_curve=self._baseline_curve
    ).score()
    self.assertAlmostEqual(base_score, -reversed_score, delta=0.01)

  def test_efficiency_score_value(self):
    xs = self._baseline_curve.xs
    xs_t = xs.reshape(1, len(xs))

    worse_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(-0.5 * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    better_curves = convergence.ConvergenceCurve(
        xs=xs,
        ys=np.exp(np.array([-1.5, -1.8, -2.0]).reshape(3, 1) * xs_t),
        trend=convergence.ConvergenceCurve.YTrend.DECREASING,
    )
    # Efficiency score for exponential curves can be approximated.

    self.assertGreater(
        convergence.LogEfficiencyConvergenceCurveComparator(
            baseline_curve=self._baseline_curve, compared_curve=better_curves
        ).score(),
        0.4,
    )
    self.assertLess(
        convergence.LogEfficiencyConvergenceCurveComparator(
            baseline_curve=self._baseline_curve, compared_curve=worse_curves
        ).score(),
        -0.4,
    )

  def test_comparator_failure(self):
    unknown_curve = convergence.ConvergenceCurve(
        xs=self._baseline_curve.xs, ys=self._baseline_curve.ys
    )
    with self.assertRaisesRegex(ValueError, 'increasing or decreasing'):
      convergence.LogEfficiencyConvergenceCurveComparator(
          baseline_curve=unknown_curve, compared_curve=self._baseline_curve
      )


class WinRateConvergenceCurveComparatorQuantilesModeTest(
    parameterized.TestCase
):
  """Tests for WinRateConvergenceCurveComparator with quantiles mode."""

  @parameterized.parameters(
      {
          'ys1': np.array([[11, 12, 20]]),
          'ys2': np.array([[1, 2, 10]]),
          'res': -0.5,
      },
      {
          'ys1': np.array([[1, 2, 10]]),
          'ys2': np.array([[11, 12, 20]]),
          'res': 0.5,
      },
      {
          'ys1': np.array([[1, 4, 8, 10]]),
          'ys2': np.array([[2, 5, 6, 8]]),
          'res': 0.0,
      },
  )
  def test_score_one_curve_above_other(self, ys1, ys2, res):
    xs1 = np.arange(ys1.shape[1])
    xs2 = np.arange(ys2.shape[1])
    curve1 = convergence.ConvergenceCurve(
        xs=xs1, ys=ys1, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    curve2 = convergence.ConvergenceCurve(
        xs=xs2, ys=ys2, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    comparator = convergence.WinRateConvergenceCurveComparator(
        curve1, curve2, comparison_mode='quantiles'
    )
    self.assertEqual(comparator.score(), res)


class PercentageBetterConvergenceComparatorTest(parameterized.TestCase):

  @parameterized.parameters(
      {
          'ys1': np.array([[1, 2, 3, 4], [1, 2, 3, 4]]),
          'ys2': np.array([[-1, 20, 30, 70]]),
          'res': 0.5,
      },
      {
          'ys1': np.array([[1, 2, 3, 4]]),
          'ys2': np.array([[10, 20, 30, 70]]),
          'res': 1.0,
      },
      {
          'ys1': np.array([[10, 20, 30, 70]]),
          'ys2': np.array([[1, 2, 3, 4]]),
          'res': -1.0,
      },
  )
  def test_score(self, ys1, ys2, res):
    xs1 = np.arange(ys1.shape[1])
    xs2 = np.arange(ys2.shape[1])
    curve1 = convergence.ConvergenceCurve(
        xs=xs1, ys=ys1, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    curve2 = convergence.ConvergenceCurve(
        xs=xs2, ys=ys2, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    comparator = convergence.PercentageBetterConvergenceCurveComparator(
        curve1, curve2
    )
    self.assertEqual(comparator.score(), res)

  @parameterized.parameters(
      {
          'ys1': np.array([[1, 2, 3, 4]]),
          'ys2': np.array([[-2, -1, 5, 6]]),
          'steps_cutoff': 2,
          'res': 1.0,
      },
      {
          'ys1': np.array([[1, 2, 3, 4, 8, 8]]),
          'ys2': np.array([[0, 1, 5, 6, 6, 6]]),
          'steps_cutoff': 4,
          'res': -1.0,
      },
      {
          'ys1': np.array([[-7, -7, -7, -7, -7, 1, 2, 3, 4]]),
          'ys2': np.array([[-1, -1, -1, -1, -1, -1, 20, 30, 70]]),
          'steps_cutoff': 5,
          'res': 0.5,
      },
  )
  def test_score_with_steps_cutoff(self, ys1, ys2, steps_cutoff, res):
    xs1 = np.arange(ys1.shape[1])
    xs2 = np.arange(ys2.shape[1])
    curve1 = convergence.ConvergenceCurve(
        xs=xs1, ys=ys1, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    curve2 = convergence.ConvergenceCurve(
        xs=xs2, ys=ys2, trend=convergence.ConvergenceCurve.YTrend.INCREASING
    )
    comparator = convergence.PercentageBetterConvergenceCurveComparator(
        curve1, curve2, steps_cutoff
    )
    self.assertEqual(comparator.score(), res)


class OptimalityGapGainComparatorTest(parameterized.TestCase):

  @parameterized.parameters(
      {
          'ys1': np.array([[1, 4, 8, 10, 12]]),
          'ys2': np.array([[2, 5, 6, 8, 10]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': (10 - 12) / 12,
      },
      {
          'ys1': np.array([[1, 4, 8, 10, 12]]),
          'ys2': np.array([[2, 5, 6, 8, 1000]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': 1.0,
      },
      {
          'ys1': np.array([[1, 4, 8, 10, 1000]]),
          'ys2': np.array([[2, 5, 6, 8, 10]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': -0.5,
      },
      {
          'ys1': np.array([[11, 5, 3, 1]]),
          'ys2': np.array([[130, 4, 2, 0.5]]),
          'trend': convergence.ConvergenceCurve.YTrend.DECREASING,
          'res': (-0.5 - (-1)) / 1,
      },
  )
  def test_score(self, ys1, ys2, trend, res):
    xs1 = np.arange(ys1.shape[1])
    xs2 = np.arange(ys2.shape[1])
    curve1 = convergence.ConvergenceCurve(xs=xs1, ys=ys1, trend=trend)
    curve2 = convergence.ConvergenceCurve(xs=xs2, ys=ys2, trend=trend)
    comparator = convergence.OptimalityGapGainComparator(curve1, curve2)
    self.assertAlmostEqual(comparator.score(), res, delta=0.0001)


class OptimalityGapWinRateComparatorTest(parameterized.TestCase):

  @parameterized.parameters(
      {
          'ys1': np.array([[1, 4, 8, 10, 12]]),
          'ys2': np.array([[2, 5, 6, 8, 10]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': 0.0,
      },
      {
          'ys1': np.array([[1, 4, 8, 10, 12]]),
          'ys2': np.array([[2, 5, 6, 8, 1000]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': 1.0,
      },
      {
          'ys1': np.array([[1, 4, 8, 10, 1000]]),
          'ys2': np.array([[2, 5, 6, 8, 10]]),
          'trend': convergence.ConvergenceCurve.YTrend.INCREASING,
          'res': 0.0,
      },
      {
          'ys1': np.array([[11, 5, 3, 1]]),
          'ys2': np.array([[130, 4, 2, 0.5]]),
          'trend': convergence.ConvergenceCurve.YTrend.DECREASING,
          'res': 1.0,
      },
  )
  def test_score(self, ys1, ys2, trend, res):
    xs1 = np.arange(ys1.shape[1])
    xs2 = np.arange(ys2.shape[1])
    curve1 = convergence.ConvergenceCurve(xs=xs1, ys=ys1, trend=trend)
    curve2 = convergence.ConvergenceCurve(xs=xs2, ys=ys2, trend=trend)
    comparator = convergence.OptimalityGapWinRateComparator(curve1, curve2)
    self.assertEqual(comparator.score(), res)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/analyzers/exploration_score_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utilities for computing exploration scores."""

from typing import Iterable, Optional

import numpy as np
import scipy
from vizier import pyvizier as vz

BenchmarkResults = dict[str, dict[str, dict[int, vz.ProblemAndTrials]]]


def compute_parameter_entropy(
    parameter_config: vz.ParameterConfig,
    parameter_values: Iterable[Optional[vz.ParameterValue]],
) -> float:
  """Computes the entropy of parameter values.

  Args:
    parameter_config: The parameter config.
    parameter_values: Values of a parameter.

  WARNING: Entropy estimation accuracy depends on the sample size, so to compare
  the entropies of two `parameter_values`, make sure they have the same size.

  Returns:
    The entropy of parameter values.
  """
  values = [pv.value for pv in parameter_values if pv is not None]
  if not values:
    return 0.0
  if parameter_config.type in [
      vz.ParameterType.CATEGORICAL,
      vz.ParameterType.DISCRETE,
  ] and hasattr(parameter_config, 'feasible_values'):
    if any([value not in parameter_config.feasible_values for value in values]):
      raise ValueError(
          f'Parameter values: {parameter_values} contain out-of-bound values.'
          f' Feasible values: {parameter_config.feasible_values}'
      )
    _, counts = np.unique(values, return_counts=True)
  elif hasattr(parameter_config, 'bounds'):
    min_val = parameter_config.bounds[0]
    max_val = parameter_config.bounds[1]
    if any([value < min_val or value > max_val for value in values]):
      raise ValueError(
          f'Parameter values: {parameter_values} contain out-of-bound values.'
          f' Bound: [{min_val}, {max_val}]'
      )
    if parameter_config.type == vz.ParameterType.INTEGER:
      _, counts = np.unique(values, return_counts=True)
    else:
      # Sets the number of fixed-width bins as c * sample_size ** (1.0 / 3.0).
      # The cubic-root dependency on the sample size appears in several common
      # bin-size selection strategies, e.g.
      # https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width
      # The multiplier `c` is chosen such that for a small sample size, say 100,
      # we still get a reasonable number of bins, say 30.
      alpha = 1.0 / 3.0
      c = 30.0 / (100**alpha)
      n_sample = len(values)
      # We also ensure that the number of bins is at most the sample size.
      num_bins = np.min((int(c * n_sample**alpha), n_sample))
      counts, _ = np.histogram(
          values,
          bins=np.linspace(
              min_val,
              max_val,
              num=num_bins + 1,
              dtype=np.float32,
          ),
      )
  else:
    raise ValueError(
        'Invalid parameter config: either `feasible_values` or'
        '`bounds` is expected to be set, but both are unset. '
        f'Parameter config: {parameter_config}'
    )
  return float(scipy.stats.entropy(counts))


def compute_average_marginal_parameter_entropy(
    results: BenchmarkResults,
) -> float:
  """Computes the average marginal parameter entropy across results.

  Computes the marginal entropy of every parameter in every study, and then
  returns the average marginal entropy over all parameters and all studies.

  Args:
    results: Benchmark results.

  Returns:
    Average marginal parameter entropy.
  """
  marginal_param_entropies = []
  for _, spec_gen_results in results.items():
    for _, spec_results in spec_gen_results.items():
      for _, study in spec_results.items():
        for param_config in study.problem.search_space.parameters:
          param_values = [
              trial.parameters.get(param_config.name) for trial in study.trials
          ]
          marginal_param_entropies.append(
              compute_parameter_entropy(
                  parameter_config=param_config, parameter_values=param_values
              )
          )

  return np.mean(marginal_param_entropies)


--- vizier/_src/benchmarks/analyzers/exploration_score_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for exploration_score_utils."""

from typing import Tuple

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.analyzers import exploration_score_utils

from absl.testing import absltest
from absl.testing import parameterized


def _generate_min_and_max_ent_studies() -> (
    Tuple[vz.ProblemAndTrials, vz.ProblemAndTrials]
):
  """Generates two studies with zero and large parameter entropies."""
  space = vz.SearchSpace()
  root = space.root
  root.add_float_param('continuous', -5.0, 5.0)
  root.add_int_param('integer', -5, 5)
  root.add_categorical_param(
      'categorical', [str(v) for v in np.linspace(-5, 5, 11)]
  )
  root.add_discrete_param('discrete', list(np.linspace(-5, 5, 11)))
  problem = vz.ProblemStatement(
      search_space=space,
      metric_information=[
          vz.MetricInformation('x1', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
      ],
  )
  max_ent_trials = []
  min_ent_trials = []
  values = list(np.linspace(-5, 5, 11)) * 10
  for idx, value in enumerate(values):
    # Generates trials with large marginal parameter entropies by looping
    # through the feasible values of each parameter.
    max_ent_trials.append(
        vz.Trial(
            id=idx + 1,
            parameters={
                'continuous': vz.ParameterValue(value),
                'integer': vz.ParameterValue(int(value)),
                'categorical': vz.ParameterValue(str(value)),
                'discrete': vz.ParameterValue(value),
            },
        )
    )
    # Generates trials with zero marginal parameter entropies by setting every
    # parameter to the same value.
    min_ent_trials.append(
        vz.Trial(
            id=idx + 1,
            parameters={
                'continuous': vz.ParameterValue(values[55]),
                'integer': vz.ParameterValue(int(values[17])),
                'categorical': vz.ParameterValue(str(values[96])),
                'discrete': vz.ParameterValue(values[34]),
            },
        )
    )
  return vz.ProblemAndTrials(
      problem=problem, trials=min_ent_trials
  ), vz.ProblemAndTrials(problem=problem, trials=max_ent_trials)


class ExplorationScoreUtilsTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Continuous', vz.ParameterType.DOUBLE),
      ('Discrete', vz.ParameterType.DISCRETE),
      ('Integer', vz.ParameterType.INTEGER),
      ('Categorical', vz.ParameterType.CATEGORICAL),
  )
  def test_compute_parameter_entropy(self, parameter_type):
    def _cast(v):
      if parameter_type == vz.ParameterType.INTEGER:
        return int(v)
      elif parameter_type == vz.ParameterType.CATEGORICAL:
        return str(v)
      else:
        return v

    feasible_values = [_cast(v) for v in np.linspace(-5, 5, 11)]
    if parameter_type in [vz.ParameterType.DOUBLE, vz.ParameterType.INTEGER]:
      parameter_config = vz.ParameterConfig.factory(
          name='param',
          bounds=(-5, 5)
          if parameter_type == vz.ParameterType.INTEGER
          else (-5.0, 5.0),
      )
    else:
      parameter_config = vz.ParameterConfig.factory(
          name='param', feasible_values=feasible_values
      )
    parameter_values = [vz.ParameterValue(value=v) for v in feasible_values]
    max_ent_parameter_values = parameter_values * 10
    min_ent_parameter_values = [parameter_values[5]] * len(
        max_ent_parameter_values
    )

    max_ent = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=max_ent_parameter_values,
    )
    min_ent = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=min_ent_parameter_values,
    )
    self.assertAlmostEqual(min_ent, 0)
    if parameter_type != vz.ParameterType.DOUBLE:
      self.assertAlmostEqual(max_ent, -np.log(1 / 11))
    else:
      # For continuous parameters the entropy depends on the histogram, whose
      # number of bins is chosen based on data. There are 11 unique values with
      # equal counts, and we simply assert that the entropy is greater than
      # the entropy of two bins with equal counts.
      self.assertGreater(max_ent, -np.log(0.5))

  @parameterized.parameters(3, 5, 11, 101, 1001, 10001)
  def test_compute_parameter_entropy_uniform_full_range_gt_half_range(
      self, sample_size
  ):
    parameter_config = vz.ParameterConfig.factory(
        name='param',
        bounds=(-5.0, 5.0),
    )
    full_range_uniform_ent = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=[
            vz.ParameterValue(value=v)
            for v in np.linspace(-5.0, 5.0, sample_size)
        ],
    )
    half_range_uniform_ent = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=[
            vz.ParameterValue(value=v)
            for v in np.linspace(-5.0, 0.0, sample_size)
        ],
    )
    # The entropy of a uniform distribution is ln(range), so when the range
    # shrinks by half, the entropy is expected to decrease additively by
    # ln(2.0). Relaxes the threshold to account for estimation errors.
    self.assertGreater(
        full_range_uniform_ent - half_range_uniform_ent, 0.6 * np.log(2.0)
    )

  @parameterized.parameters(11, 101, 1001, 10001)
  def test_compute_parameter_entropy_continuous_uniform_gt_standard_normal(
      self, sample_size
  ):
    parameter_config = vz.ParameterConfig.factory(
        name='param',
        bounds=(-5.0, 5.0),
    )
    uniform_ent_est = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=[
            vz.ParameterValue(value=v)
            for v in np.linspace(-5.0, 5.0, sample_size)
        ],
    )
    standard_normal_ent_est = exploration_score_utils.compute_parameter_entropy(
        parameter_config=parameter_config,
        parameter_values=[
            vz.ParameterValue(value=v)
            for v in np.random.default_rng(seed=sample_size).normal(
                size=sample_size
            )
        ],
    )
    # The entropy is ln(range) for a uniform distribution, and
    # ln(sqrt(2 * PI * e)) for a standard Normal distribution. Relaxes the
    # expected gap to account for estimation errors.
    expected_ent_gap = 0.8 * (
        np.log(10.0) - np.log(np.sqrt(2.0 * np.pi * np.exp(1.0)))
    )
    self.assertGreater(
        uniform_ent_est - standard_normal_ent_est, expected_ent_gap
    )

  def test_compute_average_marginal_parameter_entropy(self):
    min_ent_study, max_ent_study = _generate_min_and_max_ent_studies()
    max_ent = (
        exploration_score_utils.compute_average_marginal_parameter_entropy(
            {'spec_gen': {'hash_': {0: max_ent_study}}}
        )
    )
    min_ent = (
        exploration_score_utils.compute_average_marginal_parameter_entropy(
            {'spec_gen': {'hash_': {0: min_ent_study}}}
        )
    )
    self.assertAlmostEqual(min_ent, 0)
    # The study has four parameters: continuous, integer, categorical and
    # discrete. The entropy for all parameters except the continuous are
    # exactly -np.log(1/11) since every parameter has 11 unique values with
    # equal counts. The entropy of the continuous parameter depends on the
    # histogram, whose number of bins is chosen based on data, and we simply
    # lower bound it by the entropy of two bins with equal counts.
    self.assertGreater(
        max_ent, 0.75 * (-np.log(1 / 11)) + 0.25 * (-np.log(0.5))
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/analyzers/plot_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tools for visualization with PlotElements."""


import json
from typing import Optional, Sequence, Tuple
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from vizier import pyvizier as vz
from vizier._src.benchmarks.analyzers import state_analyzer
from vizier.utils import json_utils


def plot_median_convergence(
    ax: mpl.axes.Axes,
    curves: 'np.ndarray',
    *,
    percentiles: Sequence[Tuple[int, int]] = ((40, 60),),
    alphas: Sequence[float] = (0.2,),
    xs: Optional['np.ndarray'] = None,
    **kwargs,
):
  """Aggregates multiple convergence curves into a plot with confidence bounds.

  Example usage:
    ```python
    fig, ax = plt.subplots(1, 1, figsize=(12,8))
    plot_median_convergence(ax,
                            [[1,1,2,3,4], [1,1,1,2,nan]],
                            percentiles=((40, 60), (30, 70)),
                            alphas=(0.4, 0.2),
                            xs=np.arange(1,6),
                            color='r')
    ```

  Args:
    ax: matplotlib axis to plot on.
    curves: Expected to have shape (Number of studies, points), where rows are
      convergence curves from repeated studies from the same algorithm and
      settings. May contain NaNs, which will be excluded from plotting.
    percentiles: Each pair defines (lower_percentile, upper_percentile).
    alphas: Must have the same length as percentiles. Defines the color strength
      for the confidence bounds. Make it decrease in the distance between lower
      and upper percentiles. (See example above).
    xs: x values for the plot. If not provided, uses np.arange(curves.shape[1]).
      Must have the shape (curves.shape[1], 0)
    **kwargs: Forwared to ax.plot().
  """
  if xs is None:
    xs = np.arange(curves.shape[1])

  line = ax.plot(xs, np.nanmedian(curves, axis=0), **kwargs)
  for (lower, upper), alpha in zip(percentiles, alphas):
    ax.fill_between(
        xs,
        np.nanpercentile(curves, lower, axis=0),
        np.nanpercentile(curves, upper, axis=0),
        alpha=alpha,
        color=line[0].get_color(),
    )


def plot_mean_convergence(
    ax: mpl.axes.Axes,
    curves: 'np.ndarray',
    *,
    alpha: float = 0.2,
    xs: Optional['np.ndarray'] = None,
    **kwargs,
):
  """Aggregates multiple convergence curves into a plot with standard error bounds.

  Example usage:
    ```python
    fig, ax = plt.subplots(1, 1, figsize=(12,8))
    plot_mean_convergence(ax,
                          [[1,1,2,3,4], [1,1,1,2,nan]],
                          alpha=0.3,
                          xs=np.arange(1,6),
                          color='r')
    ```

  Args:
    ax: matplotlib axis to plot on.
    curves: Expected to have shape (Number of studies, points), where rows are
      convergence curves from repeated studies from the same algorithm and
      settings. May contain NaNs, which will be excluded from plotting.
    alpha: Defines the color strength for the standard error bounds.
    xs: x values for the plot. If not provided, uses np.arange(curves.shape[1]).
      Must have the shape (curves.shape[1], 0)
    **kwargs: Forwared to ax.plot().
  """
  if xs is None:
    xs = np.arange(curves.shape[1])
  curves_mean = np.nanmean(curves, axis=0)
  curves_std_error = np.nanstd(curves, axis=0) / np.sqrt(curves.shape[0])

  line = ax.plot(xs, curves_mean, **kwargs)
  ax.fill_between(
      xs,
      curves_mean + 1.5 * curves_std_error,
      curves_mean - 1.5 * curves_std_error,
      alpha=alpha,
      color=line[0].get_color(),
  )


def plot_from_records(
    records: Sequence[state_analyzer.BenchmarkRecord],
    metrics: Optional[Sequence[str]] = None,
    *,
    fig_title: str = 'All Plot Elements',
    title_maxlen: int = 50,
    col_figsize: float = 6.0,
    row_figsize: float = 6.0,
    **kwargs,
) -> mpl.figure.Figure:
  """Generates a grid of plot elements.

  Generates one plot for each Experimenter x Metrics in records. Note that
  each row = Experimenter and each column = Metrics.

  Args:
    records: All BenchmarkRecords used for plotting.
    metrics: Keys in the plot_elements dict in BenchmarkRecord used for plot. If
      not supplied, all keys are plotted.
    fig_title: Title of the entire grid plot.
    title_maxlen: Maximum length of title of each Experimenter.
    col_figsize: Size of the column of each subfigure.
    row_figsize: Size of the row of each subfigure.
    **kwargs: Additional keyword args forwarded to pyplot.

  Raises:
    ValueError: When plot type is not supported.

  Returns:
    The figure object.
  """

  def _metadata_to_str(metadata: vz.Metadata) -> str:
    visual_dict = {}
    for _, key, value in metadata.all_items():
      try:
        loaded = json.loads(value, cls=json_utils.NumpyDecoder)
        assert isinstance(loaded, dict)
        visual_dict = visual_dict | {k: v for k, v in loaded.items() if v}
      except Exception as e:  # pylint: disable=broad-except
        del e
        visual_dict[key] = value
    return str(visual_dict)

  records_list = [
      (rec.algorithm, _metadata_to_str(rec.experimenter_metadata), rec)
      for rec in records
  ]
  df = pd.DataFrame(
      records_list, columns=['algorithm', 'experimenter', 'record']
  )

  algorithms = df.algorithm.unique()
  colors = {
      algorithm: plt.get_cmap('tab10')(i)
      for i, algorithm in enumerate(algorithms)
  }
  total_rows = len(df.groupby('experimenter'))
  if metrics is None:
    metrics = set()
    for record in df.record:
      metrics = metrics.union(set(record.plot_elements.keys()))
    print(f'All inferred metrics {metrics}')

  fig, axes = plt.subplots(
      total_rows,
      len(metrics),
      figsize=(col_figsize * len(metrics), row_figsize * total_rows),
      squeeze=False,
  )
  fig.suptitle(fig_title, fontsize=16)

  fig_idx = 0
  for experimenter_key, group_by_experimenter in df.groupby('experimenter'):
    for metric_idx, metric in enumerate(metrics):
      ax = axes[fig_idx, metric_idx]
      subplot_title = (
          str(experimenter_key)[:title_maxlen] if experimenter_key else metric
      )
      ax.set_title(subplot_title)
      ax.set_ylabel(metric)
      for algorithm_name, group in group_by_experimenter.groupby('algorithm'):
        if not group.size:
          continue
        if len(group) != 1:
          print(
              f'Found more records than expected in {algorithm_name} for'
              f' {group}'
          )
        elems = group.record.iloc[0].plot_elements
        if metric not in elems:
          print(f'metric {metric} not found in {group.record.iloc[0]}')
          continue

        elem_for_metric = elems[metric]
        plot_type = elem_for_metric.plot_type
        if plot_type == 'error-bar':
          plot_median_convergence(
              ax,
              elem_for_metric.curve.ys,
              xs=elem_for_metric.curve.xs,
              label=f'{algorithm_name}',
              color=colors[algorithm_name],
              percentiles=(elem_for_metric.percentile_error_bar,),
              **kwargs,
          )
        elif plot_type == 'scatter':
          plot = elem_for_metric.plot_array
          ax.scatter(
              plot[:, 0],
              plot[:, 1],
              label=f'{algorithm_name}',
              color=colors[algorithm_name],
              **kwargs,
          )
        elif plot_type == 'histogram':
          plot = elem_for_metric.plot_array
          linewidth = (
              len(algorithms)
              + 1
              - float(list(algorithms).index(algorithm_name))
          )
          ax.hist(
              plot,
              histtype='step',
              density=True,
              fill=False,
              linewidth=linewidth,
              label=f'{algorithm_name}',
              color=colors[algorithm_name],
              **kwargs,
          )
        else:
          raise ValueError(f'{plot_type} plot not yet supported!')
        ax.set_xlabel(elem_for_metric.xlabel)
        ax.set_yscale(elem_for_metric.yscale)
        ax.yaxis.set_major_locator(mpl.ticker.LinearLocator(20))
        ax.yaxis.set_minor_locator(mpl.ticker.LinearLocator(100))
        ax.yaxis.set_major_formatter(mpl.ticker.ScalarFormatter())
      ax.legend()
    fig_idx += 1
  return fig


--- vizier/_src/benchmarks/analyzers/plot_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import itertools
from matplotlib.pylab import plt
import numpy as np
from vizier import benchmarks as vzb
from vizier._src.algorithms.designers import grid
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.analyzers import plot_utils
from vizier._src.benchmarks.analyzers import state_analyzer
from vizier.benchmarks import experimenters
from absl.testing import absltest


class PlotUtilsTest(absltest.TestCase):

  def test_plot_median_convergence(self):
    _, ax = plt.subplots(1, 1, figsize=(12, 8))
    plot_utils.plot_median_convergence(
        ax,
        curves=np.asarray([[1, 1, 2, 3, 4], [1, 1, 1, 2, float('nan')]]),
        percentiles=((40, 60), (30, 70)),
        alphas=(0.2, 0.4),
        xs=np.arange(1, 6),
        color='r',
    )

  def test_plot_mean_convergence(self):
    _, ax = plt.subplots(1, 1, figsize=(12, 8))
    plot_utils.plot_mean_convergence(
        ax,
        curves=np.asarray([[1, 1, 2, 3, 4], [1, 1, 1, 2, float('nan')]]),
        xs=np.arange(1, 6),
        color='r',
    )

  def test_plot_median_convergence_omit_optional_args(self):
    _, ax = plt.subplots(1, 1, figsize=(12, 8))
    plot_utils.plot_median_convergence(
        ax, curves=np.asarray([[1, 1, 2, 3, 4], [1, 1, 1, 2, float('nan')]])
    )

  def test_plot_records(self):
    function_names = ['Sphere', 'Discus']
    dimensions = [4, 8]
    product_list = list(itertools.product(function_names, dimensions))

    experimenter_factories = []
    for product in product_list:
      experimenter_factory = experimenters.BBOBExperimenterFactory(
          name=product[0], dim=product[1]
      )
      experimenter_factories.append(experimenter_factory)

    num_repeats = 5
    num_iterations = 20
    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[
            vzb.GenerateSuggestions(),
            vzb.EvaluateActiveTrials(),
        ],
        num_repeats=num_iterations,
    )
    algorithms = {
        'grid': grid.GridSearchDesigner.from_problem,
        'random': random.RandomDesigner.from_problem,
    }

    records = []
    for experimenter_factory in experimenter_factories:
      for algo_name, algo_factory in algorithms.items():
        benchmark_state_factory = vzb.ExperimenterDesignerBenchmarkStateFactory(
            experimenter_factory=experimenter_factory,
            designer_factory=algo_factory,
        )
        states = []
        for _ in range(num_repeats):
          benchmark_state = benchmark_state_factory()
          runner.run(benchmark_state)
          states.append(benchmark_state)
        record = state_analyzer.BenchmarkStateAnalyzer.to_record(
            algorithm=algo_name,
            experimenter_factory=experimenter_factory,
            states=states,
        )
        records.append(record)
    plot_utils.plot_from_records(records)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/analyzers/simple_regret_score.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Simple regret score to compare algorithms."""

from typing import Union

import numpy as np
from scipy import stats
from vizier import pyvizier as vz


def t_test_mean_score(baseline_mean_values: Union[list[float], np.ndarray],
                      candidate_mean_values: Union[list[float], np.ndarray],
                      objective_goal: vz.ObjectiveMetricGoal) -> float:
  """Computes the one-sided T-test score.

  In case of a maximization (minimizatoin) problem, it scores the confidence
  that the mean of 'baseline_mean_values' is less (greater) than the mean of
  'candidate_mean_values'.

  The lower the score the higher the confidence that it's the case.

  One-sample
  ----------
  The test assumes t-distribution with mean given by 'candidate' and compute
  the probability of observing sample mean of 'baseline' or less.

  Two-sample
  ----------
  The test assumes 'baseline' and 'candidate' have the same mean and computes
  the probability that the 'baseline' sample mean is less than the 'candidate'
  sample mean.

  The lower the T-test p-value score the more confidence we have that
  'candidate' is indeed "better" than 'baseline'.

  Arguments:
    baseline_mean_values: List of baseline simple regret values.
    candidate_mean_values: List of candidate simple regret values.
    objective_goal: The optimization problem type (MAXIMIZE or MINIMIZE).

  Returns:
    The p-value score of the one-sided T test.
  """
  if objective_goal == vz.ObjectiveMetricGoal.MAXIMIZE:
    alternative = 'less'
  else:
    alternative = 'greater'

  if len(candidate_mean_values) == 1:
    return stats.ttest_1samp(
        a=baseline_mean_values,
        popmean=candidate_mean_values[0],
        alternative=alternative).pvalue
  else:
    # use Welch’s t-test
    return stats.ttest_ind(
        baseline_mean_values,
        candidate_mean_values,
        equal_var=False,
        alternative=alternative).pvalue


--- vizier/_src/benchmarks/analyzers/simple_regret_score_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for simple_regret_score."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.analyzers import simple_regret_score

from absl.testing import absltest
from absl.testing import parameterized


class SimpleRegretScoreTest(parameterized.TestCase):

  # @parameterized.product(
  #       create_problem=[
  #           create_continuous_problem,
  #           create_categorical_problem,
  #           create_mix_problem,
  #       ],
  #       n_features=list(range(10, 20)),
  #   )

  @parameterized.parameters(
      {
          'candidate_mean_values': [0.9],
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': False
      },
      {
          'candidate_mean_values': [1.2],
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': True
      },
      {
          'candidate_mean_values': 0.9 * np.ones(200),
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': False
      },
      {
          'candidate_mean_values': 1.2 * np.ones(200),
          'goal': vz.ObjectiveMetricGoal.MAXIMIZE,
          'should_pass': True
      },
      {
          'candidate_mean_values': [0.9],
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': True
      },
      {
          'candidate_mean_values': [1.2],
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': False
      },
      {
          'candidate_mean_values': 0.9 * np.ones(200),
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': True
      },
      {
          'candidate_mean_values': 1.2 * np.ones(200),
          'goal': vz.ObjectiveMetricGoal.MINIMIZE,
          'should_pass': False
      },
  )
  def test_mean_score(self, candidate_mean_values, goal, should_pass):
    baseline_mean_values = np.ones(500) + 0.1 * np.random.normal(size=(500,))
    p_value = simple_regret_score.t_test_mean_score(baseline_mean_values,
                                                    candidate_mean_values, goal)
    if should_pass:
      self.assertLess(p_value, 0.05)
    else:
      self.assertGreater(p_value, 0.95)

  def test_score_decreases_with_samples_two_sample(self):
    goal = vz.ObjectiveMetricGoal.MAXIMIZE
    baseline_simple_regrets = np.ones(500) + 0.1 * np.random.normal(size=(500,))
    candidate_simple_regrets = 1.2 * np.ones(200)
    p_value500 = simple_regret_score.t_test_mean_score(
        baseline_simple_regrets, candidate_simple_regrets, goal)

    baseline_simple_regrets = np.ones(20) + 0.1 * np.random.normal(size=(20,))
    candidate_simple_regrets = 1.2 * np.ones(200)
    p_value20 = simple_regret_score.t_test_mean_score(baseline_simple_regrets,
                                                      candidate_simple_regrets,
                                                      goal)
    # test that the p-value decreases with the number of samples
    self.assertLess(p_value500, p_value20)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/analyzers/state_analyzer.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Analyzers for BenchmarkStates for fast comparisons and statistics."""

import collections
import json
from typing import Callable, Dict, Optional, Sequence, Tuple

from absl import logging
import attrs
import numpy as np
import pandas as pd
from vizier import benchmarks
from vizier import pyvizier as vz
from vizier._src.benchmarks.analyzers import convergence_curve
from vizier.benchmarks import experimenters


RECORD_OBJECTIVE_KEY = 'objective'


@attrs.define(init=True, kw_only=True)
class PlotElement:
  """PlotElement with relevant information for a subplot."""

  curve: Optional[convergence_curve.ConvergenceCurve] = attrs.field(
      default=None,
      validator=attrs.validators.optional(
          attrs.validators.instance_of(convergence_curve.ConvergenceCurve)
      ),
  )
  plot_array: Optional[np.ndarray] = attrs.field(
      default=None,
      validator=attrs.validators.optional(
          attrs.validators.instance_of(np.ndarray)
      ),
  )
  # Error-bar uses curve, whereas histogram/scatter uses plot_array.
  plot_type: str = attrs.field(
      default='error-bar',
      validator=attrs.validators.in_(['error-bar', 'histogram', 'scatter']),
  )
  xlabel: str = attrs.field(
      default='Num Trials', validator=attrs.validators.instance_of(str)
  )
  yscale: str = attrs.field(
      default='linear',
      validator=attrs.validators.in_(['linear', 'symlog', 'logit']),
  )
  # Lower and upper percentiles to display for error bar.
  percentile_error_bar: Tuple[int, int] = attrs.field(
      default=(25, 75),
      validator=attrs.validators.deep_iterable(
          member_validator=attrs.validators.instance_of(int),
          iterable_validator=attrs.validators.instance_of(tuple),
      ),
  )


# Stores all relevant information and plots for a specific BenchmarkState.
@attrs.define(init=True, kw_only=True)
class BenchmarkRecord:
  algorithm: str = attrs.field(
      default='',
      validator=attrs.validators.instance_of(str),
  )
  experimenter_metadata: vz.Metadata = attrs.field(
      factory=vz.Metadata, validator=attrs.validators.instance_of(vz.Metadata)
  )
  plot_elements: Dict[str, PlotElement] = attrs.field(factory=dict)


class BenchmarkStateAnalyzer:
  """Analyzer for BenchmarkStates."""

  @classmethod
  def to_curve(
      cls,
      states: list[benchmarks.BenchmarkState],
      flip_signs_for_min: bool = False,
      reference_value: Optional[np.ndarray] = None,
  ) -> convergence_curve.ConvergenceCurve:
    """Generates a ConvergenceCurve from a batch of BenchmarkStates.

    Each state in batch should represent the same study (different repeat).

    Args:
      states: List of BenchmarkStates.
      flip_signs_for_min: If true, flip signs of curve when it is MINIMIZE
        metric.
      reference_value: Reference value for multiobjective hypervolume curve.

    Returns:
      Convergence curve with batch size equal to length of states.

    Raises:
      ValueError: When problem statements are not the same or is multiobjective.
    """
    if not states:
      raise ValueError('Empty States.')

    problem_statement = states[0].experimenter.problem_statement()

    curves = []
    for state in states:
      if problem_statement != state.experimenter.problem_statement():
        raise ValueError(
            f'States must have same problem {problem_statement}'
            f' and {state.experimenter.problem_statement()}'
        )
      state_trials = state.algorithm.supporter.GetTrials()

      if problem_statement.is_single_objective:
        kwargs = {'flip_signs_for_min': flip_signs_for_min}
      else:
        kwargs = {'reference_value': reference_value}

      converter = (
          convergence_curve.MultiMetricCurveConverter.from_metrics_config(
              problem_statement.metric_information,
              **kwargs,
          )
      )
      curve = converter.convert(state_trials)
      curves.append(curve)
    return convergence_curve.ConvergenceCurve.align_xs(curves)[0]

  @classmethod
  def to_record(
      cls,
      algorithm: str,
      experimenter_factory: experimenters.SerializableExperimenterFactory,
      states: list[benchmarks.BenchmarkState],
      flip_signs_for_min: bool = False,
  ) -> BenchmarkRecord:
    """Generates a BenchmarkRecord from a batch of BenchmarkStates.

    Each state in batch should represent the same study (different repeat).

    Args:
      algorithm: Algorithm name.
      experimenter_factory: Factory used for running BenchmarkState.
      states: List of BenchmarkStates.
      flip_signs_for_min: If true, flip signs of curve when it is MINIMIZE
        metric.

    Returns:
      BenchmarkRecord.
    """
    plot_elements = {}
    plot_elements[RECORD_OBJECTIVE_KEY] = PlotElement(
        curve=cls.to_curve(states, flip_signs_for_min=flip_signs_for_min),
        yscale='symlog',
    )
    return BenchmarkRecord(
        algorithm=algorithm,
        experimenter_metadata=experimenter_factory.dump(),
        plot_elements=plot_elements,
    )


def summarize_element(element: PlotElement) -> np.ndarray:
  if element.plot_type == 'error-bar':
    assert element.curve is not None
    return np.median(np.array(element.curve.ys[:, -1]))
  elif element.plot_type == 'histogram':
    return np.median(element.plot_array)
  else:
    raise NotImplementedError(f'Unsupported plot type {element.plot_type}')


def summarize_elements(
    plot_elements_list: Sequence[PlotElement],
) -> PlotElement:
  summary_vector = [
      summarize_element(element) for element in plot_elements_list
  ]
  return PlotElement(plot_array=np.array(summary_vector), plot_type='histogram')


class BenchmarkRecordAnalyzer:
  """Analyzer for a sequence of Benchmark Records."""

  @classmethod
  def add_comparison_metrics(
      cls,
      records: Sequence[BenchmarkRecord],
      baseline_algo: str,
      *,
      compare_metric: str = RECORD_OBJECTIVE_KEY,
      comparator_factory: convergence_curve.ConvergenceComparatorFactory = convergence_curve.LogEfficiencyConvergenceCurveComparator,
  ) -> list[BenchmarkRecord]:
    """Adds comparison scores as metrics via PlotElements to BenchmarkRecord.

    Comparisons are done for compare_metric with respect to the baseline_algo.

    Args:
      records: Sequence of BenchmarkRecords
      baseline_algo: Baseline algorithm to be compared against.
      compare_metric: Metric of comparison.
      comparator_factory: Comparator used for scoring.

    Returns:
      List of BenchmarkRecords with comparison scores added as metrics.

    Raises:
      ValueError: When baseline_algo cannot be found in records or the metric
      of comparison does not correspond to a curve.
    """
    records_list = [
        (rec.algorithm, json.dumps(dict(rec.experimenter_metadata)), rec)
        for rec in records
    ]
    df = pd.DataFrame(
        records_list, columns=['algorithm', 'experimenter', 'record']
    )

    analyzed_records = []
    for experimenter_key, experimenter_group in df.groupby('experimenter'):
      # Checks and stores the mapping from algorithm to plot_elements
      algo_to_elements_dict = {}
      for algorithm_name, group in experimenter_group.groupby('algorithm'):
        if not group.size:
          continue
        if len(group) != 1:
          output_str = (
              f'Condense {len(group)} records for {algorithm_name} with exptr '
              f' {experimenter_key} before applying comparisons for'
              f' {group}'
          )
          logging.error('%s', output_str)
          continue
        algo_to_elements_dict[algorithm_name] = group.record.iloc[
            0
        ].plot_elements

      # Finds the baseline algorithm and the comparison element.
      if compare_metric not in algo_to_elements_dict[baseline_algo]:
        raise ValueError(
            f'Compare metric {compare_metric} not in baseline {baseline_algo}'
        )
      baseline_element = algo_to_elements_dict[baseline_algo][compare_metric]
      if baseline_element.plot_type != 'error-bar':
        raise ValueError(
            f'No comparison can be done for {compare_metric} since'
            f' plot type is {baseline_element.plot_type}'
        )

      # Attempts to apply comparison and add comparison metrics.
      for algorithm_name, elems_dict in algo_to_elements_dict.items():
        compared_element = elems_dict[compare_metric]
        comparator = comparator_factory(
            baseline_curve=baseline_element.curve,
            compared_curve=compared_element.curve,
        )
        try:
          dict_key = (
              compare_metric + f':{comparator.name}_curve:' + baseline_algo
          )
          elems_dict[dict_key] = PlotElement(
              curve=comparator.curve(),
              plot_type='error-bar',
          )
        except Exception as e:  # pylint: disable=broad-exception-caught
          output_str = (
              f'Skip comparing curve for algo {algorithm_name} with'
              f' {compared_element} \n due to error {e}'
          )
          logging.error('%s', output_str)
        try:
          dict_key = compare_metric + f':{comparator.name}:' + baseline_algo
          elems_dict[dict_key] = PlotElement(
              plot_array=np.asarray([comparator.score()]),
              plot_type='histogram',
              xlabel='score frequency',
          )
        except Exception as e:  # pylint: disable=broad-exception-caught
          output_str = (
              f'Skip comparing score for algo {algorithm_name} with '
              f'  {compared_element} \n due to error {e}'
          )
          logging.error('%s', output_str)
        analyzed_records.append(
            BenchmarkRecord(
                algorithm=algorithm_name,
                experimenter_metadata=vz.Metadata(json.loads(experimenter_key)),  # pytype: disable=wrong-arg-types  # pandas-drop-duplicates-overloads
                plot_elements=elems_dict,
            )
        )

    return analyzed_records

  @classmethod
  def summarize(
      cls,
      records: Sequence[BenchmarkRecord],
      record_to_reduced_keys: Callable[[BenchmarkRecord], tuple[str, str]],
      *,
      summarize_elements_fn: Callable[
          [Sequence[PlotElement]], PlotElement
      ] = summarize_elements,
  ) -> Sequence[BenchmarkRecord]:
    """Summarizes PlotElements to BenchmarkRecord.

    Comparisons are done for compare_metric with respect to the baseline_algo.

    Args:
      records: Sequence of BenchmarkRecords
      record_to_reduced_keys: Takes a BenchmarkRecord and returns the removed
        key as string and the reduced metadata as a json string. For example, if
        metadata = {'experimenter': Sphere, 'dim': 4} and we want to reduce
        across experimenter, then the removed key would be Sphere and the
        reduced metadata would be '{"dim": 4}'.
      summarize_elements_fn: PlotElement summarization function.

    Returns:
      List of summarized BenchmarkRecords.
    """

    records_list = [
        (rec.algorithm, *record_to_reduced_keys(rec), rec) for rec in records
    ]
    df = pd.DataFrame(
        records_list,
        columns=['algorithm', 'removed_key', 'reduced_metadata', 'record'],
    )

    summarized_records = []
    for algorithm_name, group_by_algo in df.groupby('algorithm'):
      if not group_by_algo.size:
        continue
      for reduced_metadata, group_by_reduced_metadata in group_by_algo.groupby(
          'reduced_metadata'
      ):
        if not group_by_reduced_metadata.size:
          continue
        summarized_records.append(
            BenchmarkRecord(
                algorithm=str(algorithm_name),
                experimenter_metadata=vz.Metadata(json.loads(reduced_metadata)),
                plot_elements=cls._summarize_elements_df(
                    group_by_reduced_metadata,
                    summarize_elements_fn=summarize_elements_fn,
                ),
            )
        )
    return summarized_records

  @classmethod
  def _summarize_elements_df(
      cls,
      elements_df: pd.DataFrame,
      summarize_elements_fn: Callable[
          [Sequence[PlotElement]], PlotElement
      ] = summarize_elements,
  ) -> dict[str, PlotElement]:
    """Summarizes PlotElements via removed_key in a dataframe."""
    key_to_elements_list = collections.defaultdict(list)
    for removed_key, group in elements_df.groupby('removed_key'):
      if len(group) > 1:
        raise ValueError(f'More than 1 group after analysis for {removed_key}')
      for key, element in group.record.iloc[0].plot_elements.items():
        key_to_elements_list[key].append(element)

    # Summarize metadata and plot elements
    summarized_plot_elements = {}
    for key, elements_list in key_to_elements_list.items():
      summarized_plot_elements[key] = summarize_elements_fn(elements_list)
    return summarized_plot_elements


--- vizier/_src/benchmarks/analyzers/state_analyzer_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import itertools
import json
import numpy as np
from vizier import benchmarks as vzb
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import grid
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.analyzers import state_analyzer
from vizier.benchmarks import experimenters
from absl.testing import absltest


class StateAnalyzerTest(absltest.TestCase):

  def test_curve_conversion(self):
    dim = 10
    experimenter = experimenters.BBOBExperimenterFactory('Sphere', dim)()

    def _designer_factory(config: vz.ProblemStatement, seed: int):
      return random.RandomDesigner(config.search_space, seed=seed)

    benchmark_state_factory = vzb.DesignerBenchmarkStateFactory(
        designer_factory=_designer_factory, experimenter=experimenter
    )
    num_trials = 20
    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[vzb.GenerateAndEvaluate()],
        num_repeats=num_trials,
    )

    states = []
    num_repeats = 3
    for i in range(num_repeats):
      bench_state = benchmark_state_factory(seed=i)
      runner.run(bench_state)
      states.append(bench_state)

    curve = state_analyzer.BenchmarkStateAnalyzer.to_curve(states)
    self.assertEqual(curve.ys.shape, (num_repeats, num_trials))

  def test_empty_curve_error(self):
    with self.assertRaisesRegex(ValueError, 'Empty'):
      state_analyzer.BenchmarkStateAnalyzer.to_curve([])

  def test_multiobj_curve_conversion(self):
    dim = 10
    experimenter_factories = {
        'sphere': experimenters.BBOBExperimenterFactory('Sphere', dim),
        'discus': experimenters.BBOBExperimenterFactory('Discus', dim),
    }
    multi_experimenter = experimenters.CombinedExperimenterFactory(
        base_factories=experimenter_factories
    )()

    def _designer_factory(config: vz.ProblemStatement, seed: int):
      return random.RandomDesigner(config.search_space, seed=seed)

    benchmark_state_factory = vzb.DesignerBenchmarkStateFactory(
        designer_factory=_designer_factory, experimenter=multi_experimenter
    )
    num_trials = 20
    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[vzb.GenerateAndEvaluate()],
        num_repeats=num_trials,
    )

    states = []
    num_repeats = 3
    for i in range(num_repeats):
      bench_state = benchmark_state_factory(seed=i)
      runner.run(bench_state)
      states.append(bench_state)

    curve = state_analyzer.BenchmarkStateAnalyzer.to_curve(
        states, reference_value=np.asarray([-1])
    )
    self.assertEqual(curve.ys.shape, (num_repeats, num_trials))

  def test_different_curve_error(self):
    exp1 = experimenters.BBOBExperimenterFactory('Sphere', dim=2)()
    exp2 = experimenters.BBOBExperimenterFactory('Sphere', dim=3)()

    def _designer_factory(config: vz.ProblemStatement, seed: int):
      return random.RandomDesigner(config.search_space, seed=seed)

    state1_factory = vzb.DesignerBenchmarkStateFactory(
        designer_factory=_designer_factory, experimenter=exp1
    )
    state2_factory = vzb.DesignerBenchmarkStateFactory(
        designer_factory=_designer_factory, experimenter=exp2
    )

    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[vzb.GenerateAndEvaluate()],
        num_repeats=10,
    )

    state1 = state1_factory()
    state2 = state2_factory()
    runner.run(state1)
    runner.run(state2)

    with self.assertRaisesRegex(ValueError, 'must have same problem'):
      state_analyzer.BenchmarkStateAnalyzer.to_curve([state1, state2])

  def test_record_conversion(self):
    dim = 10
    factory = experimenters.BBOBExperimenterFactory('Sphere', dim)
    experimenter = factory()

    def _designer_factory(config: vz.ProblemStatement, seed: int):
      return random.RandomDesigner(config.search_space, seed=seed)

    benchmark_state_factory = vzb.DesignerBenchmarkStateFactory(
        designer_factory=_designer_factory, experimenter=experimenter
    )
    num_trials = 20
    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[vzb.GenerateAndEvaluate()],
        num_repeats=num_trials,
    )

    states = []
    num_repeats = 3
    for i in range(num_repeats):
      bench_state = benchmark_state_factory(seed=i)
      runner.run(bench_state)
      states.append(bench_state)

    record = state_analyzer.BenchmarkStateAnalyzer.to_record(
        algorithm='random', experimenter_factory=factory, states=states
    )
    objective_curve = record.plot_elements['objective'].curve
    self.assertIsNotNone(objective_curve)
    self.assertEqual(objective_curve.ys.shape, (num_repeats, num_trials))
    self.assertEqual(record.algorithm, 'random')
    self.assertIn('Sphere', str(record.experimenter_metadata))
    self.assertIn(f'{dim}', str(record.experimenter_metadata))

  def test_summarize_records(self):
    function_names = ['Sphere', 'Discus', 'AttractiveSector']
    dimensions = [4, 8]
    product_list = list(itertools.product(function_names, dimensions))

    experimenter_factories = []
    for product in product_list:
      experimenter_factory = experimenters.BBOBExperimenterFactory(
          name=product[0], dim=product[1]
      )
      experimenter_factories.append(experimenter_factory)

    num_repeats = 5
    num_iterations = 20
    runner = vzb.BenchmarkRunner(
        benchmark_subroutines=[
            vzb.GenerateSuggestions(),
            vzb.EvaluateActiveTrials(),
        ],
        num_repeats=num_iterations,
    )
    algorithms = {
        'grid': grid.GridSearchDesigner.from_problem,
        'random': random.RandomDesigner.from_problem,
    }

    records = []
    for experimenter_factory in experimenter_factories:
      for algo_name, algo_factory in algorithms.items():
        benchmark_state_factory = vzb.ExperimenterDesignerBenchmarkStateFactory(
            experimenter_factory=experimenter_factory,
            designer_factory=algo_factory,
        )
        states = []
        for _ in range(num_repeats):
          benchmark_state = benchmark_state_factory()
          runner.run(benchmark_state)
          states.append(benchmark_state)
        record = state_analyzer.BenchmarkStateAnalyzer.to_record(
            algorithm=algo_name,
            experimenter_factory=experimenter_factory,
            states=states,
        )
        records.append(record)

    def record_to_reduced_keys(record: state_analyzer.BenchmarkRecord):
      json_metadata = record.experimenter_metadata[
          experimenters.experimenter_factory.BBOB_FACTORY_KEY
      ]
      metadata_dict = json.loads(json_metadata)
      exptr_name = metadata_dict.pop('name')
      return (exptr_name, json.dumps(metadata_dict))

    summarized_records = state_analyzer.BenchmarkRecordAnalyzer.summarize(
        records, record_to_reduced_keys
    )
    self.assertLen(summarized_records, len(dimensions) * len(algorithms))
    for record in summarized_records:
      self.assertLen(record.plot_elements, 1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/atari100k_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Atari100k benchmarks from Dopamine.

Reference on benchmark noise and settings:

`Deep RL at the Edge of the Statistical Precipice.`
(https://arxiv.org/abs/2108.13264)

"""
# pylint:disable=dangerous-default-value
from typing import Dict, Optional, Sequence, Union

from absl import logging
from dopamine.discrete_domains import iteration_statistics
from dopamine.labs.atari_100k import atari_100k_rainbow_agent
from dopamine.labs.atari_100k import eval_run_experiment
import gin

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter


def create_agent_fn(
    sess,  # pylint: disable=unused-argument
    environment,
    seed: Optional[int] = None,
    summary_writer=None) -> atari_100k_rainbow_agent.Atari100kRainbowAgent:
  """Helper function for creating full rainbow-based Atari 100k agent."""
  return atari_100k_rainbow_agent.Atari100kRainbowAgent(
      num_actions=environment.action_space.n,
      seed=seed,
      summary_writer=summary_writer)


@gin.configurable
class WrappedRunner(eval_run_experiment.MaxEpisodeEvalRunner):
  """Wraps the original Dopamine Runner for Vizier's convenience."""

  def __init__(self, base_dir: str = '/tmp/'):
    super().__init__(base_dir=base_dir, create_agent_fn=create_agent_fn)

  def run_trial(self) -> iteration_statistics.IterationStatistics:
    """Replacement of `run_experiment()` to avoid TensorBoard summary logging."""

    statistics = iteration_statistics.IterationStatistics()

    if self._num_iterations <= self._start_iteration:
      logging.warning('num_iterations (%d) < start_iteration(%d)',
                      self._num_iterations, self._start_iteration)
      return statistics

    for _ in range(self._start_iteration, self._num_iterations):
      self._run_train_phase(statistics)
      self._run_eval_phase(statistics)

    return statistics


GinParameterType = Union[float, int, str]


def default_search_space() -> pyvizier.SearchSpace:
  """Produces a reasonable SearchSpace for tuning the Rainbow training process."""
  ss = pyvizier.SearchSpace()
  ss.root.add_float_param(
      'JaxDQNAgent.gamma',
      0.7,
      0.999999,
      scale_type=pyvizier.ScaleType.REVERSE_LOG)
  ss.root.add_int_param('JaxDQNAgent.update_horizon', 1, 20)
  ss.root.add_int_param('JaxDQNAgent.update_period', 1, 10)
  ss.root.add_int_param('JaxDQNAgent.target_update_period', 1, 10000)
  ss.root.add_int_param('JaxDQNAgent.min_replay_history', 100, 100000)
  ss.root.add_float_param(
      'JaxDQNAgent.epsilon_train',
      0.0000001,
      1.0,
      scale_type=pyvizier.ScaleType.LOG)
  ss.root.add_int_param('JaxDQNAgent.epsilon_decay_period', 1000, 10000)
  ss.root.add_bool_param('JaxFullRainbowAgent.noisy')
  ss.root.add_bool_param('JaxFullRainbowAgent.dueling')
  ss.root.add_bool_param('JaxFullRainbowAgent.double_dqn')
  ss.root.add_int_param('JaxFullRainbowAgent.num_atoms', 1, 100)
  ss.root.add_bool_param('Atari100kRainbowAgent.data_augmentation')
  ss.root.add_float_param(
      'create_optimizer.learning_rate',
      0.0000001,
      1.0,
      scale_type=pyvizier.ScaleType.LOG)
  ss.root.add_float_param(
      'create_optimizer.eps', 0.0000001, 1.0, scale_type=pyvizier.ScaleType.LOG)

  return ss


class Atari100kExperimenter(experimenter.Experimenter):
  """Atari100k Experimenter."""

  def __init__(self,
               game_name: str = 'Pong',
               agent_name: str = 'DER',
               initial_gin_bindings: Dict[str, GinParameterType] = {}):
    """Initializes the Atari100k Experimenter.

    Args:
      game_name: Atari game name. Can be one of 57 games.
      agent_name: Name of the base config file to use. Note that the
        corresponding gin files contain reported/default values. To run them,
        just send in a trivial study_config, which leads to a separate XM run
        containing baseline results.
      initial_gin_bindings: Initial gin bindings to use. Useful for making tests
        small. Will be overridden by the trial's gin bindings.
    """
    self._game_name = game_name
    assert agent_name in ['DER', 'DrQ', 'DrQ_eps', 'OTRainbow']
    self._gin_file = f'vizier/_src/benchmarks/experimenters/atari100k_configs/{agent_name}.gin'
    self._initial_gin_bindings = initial_gin_bindings

  def problem_statement(self) -> pyvizier.ProblemStatement:
    ss = default_search_space()
    problem_statement = pyvizier.ProblemStatement(search_space=ss)
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='eval_average_return',
            goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE))
    return problem_statement

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for trial in suggestions:
      with gin.unlock_config():
        # Lock in initial values.
        gin.parse_config_file(self._gin_file)
        gin.bind_parameter('atari_lib.create_atari_environment.game_name',
                           self._game_name)
        for parameter_name in self._initial_gin_bindings:
          gin.bind_parameter(parameter_name,
                             self._initial_gin_bindings[parameter_name])

        # Lock in trial parameters.
        for parameter_name in trial.parameters:
          gin.bind_parameter(parameter_name,
                             trial.parameters[parameter_name].value)

      # Run actual training + inference.
      runner = WrappedRunner()
      statistics = runner.run_trial()
      logging.info('Statistics: %s', statistics.data_lists)

      # Add Intermediate Measurements.
      num_intermediate_measurements = len(
          statistics.data_lists['eval_average_return'])
      for i in range(num_intermediate_measurements):
        measurement = pyvizier.Measurement()
        measurement.metrics['train_average_return'] = statistics.data_lists[
            'train_average_return'][i]
        measurement.metrics[
            'train_average_steps_per_second'] = statistics.data_lists[
                'train_average_steps_per_second'][i]
        measurement.metrics['eval_average_return'] = statistics.data_lists[
            'eval_average_return'][i]
        trial.measurements.append(measurement)

      # Final Measurement.
      trial.complete(trial.measurements[-1])


--- vizier/_src/benchmarks/experimenters/atari100k_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for atari100k."""
from absl import logging

from vizier import pyvizier
from vizier._src.algorithms.designers import random

from absl.testing import absltest
from absl.testing import parameterized


class Atari100KTest(parameterized.TestCase):

  @absltest.skip("ALE ROMS must be installed manually.")
  @parameterized.parameters('DER', 'DrQ', 'DrQ_eps', 'OTRainbow')
  @absltest.skip('Jax versioning not updated in Dopamine.')
  def test_e2e_evaluation(self, agent_name):
    from vizier._src.benchmarks.experimenters import atari100k_experimenter  # pylint: disable=g-import-not-at-top

    initial_gin_bindings = {
        'Runner.training_steps': 2,
        'MaxEpisodeEvalRunner.num_eval_episodes': 2,
        'Runner.num_iterations': 2,
        'Runner.max_steps_per_episode': 2,
        'JaxDQNAgent.min_replay_history': 2,
        'OutOfGraphPrioritizedReplayBuffer.replay_capacity': 1000,
    }
    experimenter = atari100k_experimenter.Atari100kExperimenter(
        game_name='Pong',
        agent_name=agent_name,
        initial_gin_bindings=initial_gin_bindings,
    )

    designer = random.RandomDesigner(
        experimenter.problem_statement().search_space, seed=None
    )

    suggestions = designer.suggest(2)
    trials = [suggestion.to_trial() for suggestion in suggestions]
    experimenter.evaluate(trials)
    # Trials should be completed in place.
    for trial in trials:
      self.assertEqual(trial.status, pyvizier.TrialStatus.COMPLETED)
      logging.info('Evaluated Trial: %s', trial)
      if trial.final_measurement:
        value = trial.final_measurement.metrics['eval_average_return'].value
        self.assertGreaterEqual(value, 0.0)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/combo/common.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Common utility functions for COMBO benchmarks."""
# pylint:disable = missing-function-docstring
import itertools
from typing import Tuple
import numpy as np


def spin_covariance(interaction: Tuple[np.ndarray, np.ndarray],
                    grid_shape: Tuple[int, int]) -> Tuple[np.ndarray, float]:
  horizontal_interaction, vertical_interaction = interaction
  n_vars = horizontal_interaction.shape[0] * vertical_interaction.shape[1]
  spin_cfgs = np.array(list(itertools.product(*([[-1, 1]] * n_vars))))
  density = np.zeros(spin_cfgs.shape[0])
  for i in range(spin_cfgs.shape[0]):
    spin_cfg = spin_cfgs[i].reshape(grid_shape)
    h_comp = spin_cfg[:, :-1] * horizontal_interaction * spin_cfg[:, 1:] * 2
    v_comp = spin_cfg[:-1] * vertical_interaction * spin_cfg[1:] * 2
    log_interaction_energy = np.sum(h_comp) + np.sum(v_comp)
    density[i] = np.exp(log_interaction_energy)
  interaction_partition = np.sum(density)
  density = density / interaction_partition

  covariance = spin_cfgs.T.dot(spin_cfgs * density.reshape((-1, 1)))
  return covariance, interaction_partition


def partition(interaction: Tuple[np.ndarray, np.ndarray],
              grid_shape: Tuple[int, int]) -> float:
  horizontal_interaction, vertical_interaction = interaction
  n_vars = horizontal_interaction.shape[0] * vertical_interaction.shape[1]
  spin_cfgs = np.array(list(itertools.product(*([[-1, 1]] * n_vars))))
  interaction_partition = 0.0
  for i in range(spin_cfgs.shape[0]):
    spin_cfg = spin_cfgs[i].reshape(grid_shape)
    h_comp = spin_cfg[:, :-1] * horizontal_interaction * spin_cfg[:, 1:] * 2
    v_comp = spin_cfg[:-1] * vertical_interaction * spin_cfg[1:] * 2
    log_interaction_energy = np.sum(h_comp) + np.sum(v_comp)
    interaction_partition += np.exp(log_interaction_energy)

  return interaction_partition


def log_partition(interaction: Tuple[np.ndarray, np.ndarray],
                  grid_shape: Tuple[int, int]) -> float:
  horizontal_interaction, vertical_interaction = interaction
  n_vars = horizontal_interaction.shape[0] * vertical_interaction.shape[1]
  spin_cfgs = np.array(list(itertools.product(*([[-1, 1]] * n_vars))))
  log_interaction_energy_list = []
  for i in range(spin_cfgs.shape[0]):
    spin_cfg = spin_cfgs[i].reshape(grid_shape)
    h_comp = spin_cfg[:, :-1] * horizontal_interaction * spin_cfg[:, 1:] * 2
    v_comp = spin_cfg[:-1] * vertical_interaction * spin_cfg[1:] * 2
    log_interaction_energy = np.sum(h_comp) + np.sum(v_comp)
    log_interaction_energy_list.append(log_interaction_energy)

  log_interaction_energy_list = np.array(log_interaction_energy_list)
  max_log_interaction_energy = np.max(log_interaction_energy_list)
  interaction_partition = np.sum(
      np.exp(log_interaction_energy_list - max_log_interaction_energy))

  return np.log(interaction_partition) + max_log_interaction_energy


def generate_ising_interaction(
    grid_h: int,
    grid_w: int,
    random_seed=None) -> Tuple[np.ndarray, np.ndarray]:
  np.random.seed(random_seed)
  horizontal_interaction = (
      (np.random.randint(0, 2,
                         (grid_h * (grid_w - 1),)) * 2 - 1).astype(float) *
      (np.random.rand(grid_h * (grid_w - 1)) * (5 - 0.05) + 0.05)).reshape(
          grid_h, grid_w - 1)
  vertical_interaction = ((np.random.randint(0, 2, (
      (grid_h - 1) * grid_w,)) * 2 - 1).astype(float) * (np.random.rand(
          (grid_h - 1) * grid_w) * (5 - 0.05) + 0.05)).reshape(
              grid_h - 1, grid_w)
  return horizontal_interaction, vertical_interaction


def ising_dense(ising_grid_h: int, interaction_original: Tuple[np.ndarray,
                                                               np.ndarray],
                interaction_sparsified: Tuple[np.ndarray, np.ndarray],
                covariance: np.ndarray, log_partition_original: float,
                log_partition_new: float) -> float:
  diff_horizontal = interaction_original[0] - interaction_sparsified[0]
  diff_vertical = interaction_original[1] - interaction_sparsified[1]

  kld = 0
  n_spin = covariance.shape[0]
  for i in range(n_spin):
    i_h, i_v = int(i / ising_grid_h), int(i % ising_grid_h)
    for j in range(i, n_spin):
      j_h, j_v = int(j / ising_grid_h), int(j % ising_grid_h)
      if i_h == j_h and abs(i_v - j_v) == 1:
        kld += diff_horizontal[i_h, min(i_v, j_v)] * covariance[i, j]
      elif abs(i_h - j_h) == 1 and i_v == j_v:
        kld += diff_vertical[min(i_h, j_h), i_v] * covariance[i, j]

  return kld * 2 + log_partition_new - log_partition_original


--- vizier/_src/benchmarks/experimenters/combo_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Categorical benchmarks from https://github.com/QUVA-Lab/COMBO.

Lines with 'ATTENTION' mean that code was modified from original Github, in
order to fix bugs with the original code.
"""

# pyformat: disable
from typing import List, Optional, Sequence, Tuple
import numpy as np

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter
from vizier._src.benchmarks.experimenters.combo import common

fileOpen = open


class IsingExperimenter(experimenter.Experimenter):
  """Ising Sparisification Problem."""

  def __init__(self,
               lamda: float = 1e-2,
               ising_grid_h: int = 4,
               ising_grid_w: int = 4,
               ising_n_edges: int = 24,
               random_seed: Optional[int] = None):
    self._lamda = lamda
    self._ising_grid_h = ising_grid_h
    self._ising_grid_w = ising_grid_w
    self._ising_n_edges = ising_n_edges
    self._interaction = common.generate_ising_interaction(
        self._ising_grid_h, self._ising_grid_w, random_seed)
    self._covariance, self._partition_original = common.spin_covariance(
        self._interaction, (self._ising_grid_h, self._ising_grid_w))

    self._problem_statement = self.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for suggestion in suggestions:
      # TODO: Switch to using StudyConfig.
      x = np.array([
          int(suggestion.parameters[f'x_{i}'].value == 'True')
          for i in range(self._ising_n_edges)
      ])
      x_h, x_v = self._bocs_consistency_mapping(x)
      interaction_sparsified = x_h * self._interaction[
          0], x_v * self._interaction[1]
      log_partition_sparsified = common.log_partition(
          interaction_sparsified, (self._ising_grid_h, self._ising_grid_w))
      evaluation = common.ising_dense(
          ising_grid_h=self._ising_grid_h,
          interaction_original=self._interaction,
          interaction_sparsified=interaction_sparsified,
          covariance=self._covariance,
          log_partition_original=np.log(self._partition_original),
          log_partition_new=log_partition_sparsified)
      evaluation += self._lamda * float(np.sum(x))

      suggestion.complete(
          pyvizier.Measurement(metrics={
              self._problem_statement.single_objective_metric_name: evaluation
          }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._ising_n_edges):
      root.add_bool_param(name=f'x_{i}')
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='main_objective', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
    return problem_statement

  def _bocs_consistency_mapping(self,
                                x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    horizontal_ind = [0, 2, 4, 7, 9, 11, 14, 16, 18, 21, 22, 23]
    vertical_ind = sorted(
        [elm for elm in range(24) if elm not in horizontal_ind])
    return x[horizontal_ind].reshape(
        (self._ising_grid_h, self._ising_grid_w - 1)), x[vertical_ind].reshape(
            (self._ising_grid_h - 1, self._ising_grid_w))


class ContaminationExperimenter(experimenter.Experimenter):
  """Contamination Control Problem."""

  def __init__(self,
               lamda: float = 1e-2,
               contamination_n_stages: int = 25,
               random_seed: Optional[int] = None):
    self._lamda = lamda
    self._contamination_n_stages = contamination_n_stages
    self._init_z, self._lambdas, self._gammas = self._generate_contamination_dynamics(
        random_seed)
    self._problem_statement = self.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for suggestion in suggestions:
      # TODO: Switch to using StudyConfig.
      x = np.array([
          int(suggestion.parameters[f'x_{i}'].value == 'True')
          for i in range(self._contamination_n_stages)
      ])
      evaluation = self._contamination(
          x=x,
          cost=np.ones(x.size),
          init_z=self._init_z,
          lambdas=self._lambdas,
          gammas=self._gammas,
          u=0.1,
          epsilon=0.05)
      evaluation += self._lamda * float(np.sum(x))
      suggestion.complete(
          pyvizier.Measurement(metrics={
              self._problem_statement.single_objective_metric_name: evaluation
          }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._contamination_n_stages):
      root.add_bool_param(name=f'x_{i}')
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='main_objective', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
    return problem_statement

  def _contamination(self, x: np.ndarray, cost: np.ndarray, init_z: np.ndarray,
                     lambdas: np.ndarray, gammas: np.ndarray, u: float,
                     epsilon: float) -> float:
    assert x.size == self._contamination_n_stages

    rho = 1.0
    n_simulations = 100

    z = np.zeros((x.size, n_simulations))
    z[0] = lambdas[0] * (1.0 - x[0]) * (1.0 - init_z) + (
        1.0 - gammas[0] * x[0]) * init_z
    for i in range(1, self._contamination_n_stages):
      z[i] = lambdas[i] * (1.0 - x[i]) * (1.0 - z[i - 1]) + (
          1.0 - gammas[i] * x[i]) * z[i - 1]

    below_threshold = z < u
    constraints = np.mean(below_threshold, axis=1) - (1.0 - epsilon)

    return np.sum(x * cost - rho * constraints)

  def _generate_contamination_dynamics(
      self, random_seed=None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    n_stages = self._contamination_n_stages
    n_simulations = 100

    init_alpha = 1.0
    init_beta = 30.0
    contam_alpha = 1.0
    contam_beta = 17.0 / 3.0
    restore_alpha = 1.0
    restore_beta = 3.0 / 7.0
    init_z = np.random.RandomState(random_seed).beta(
        init_alpha, init_beta, size=(n_simulations,))
    lambdas = np.random.RandomState(random_seed).beta(
        contam_alpha, contam_beta, size=(n_stages, n_simulations))
    gammas = np.random.RandomState(random_seed).beta(
        restore_alpha, restore_beta, size=(n_stages, n_simulations))

    return init_z, lambdas, gammas


class CentroidExperimenter(experimenter.Experimenter):
  """General parameter variant of the Ising Sparisification Problem."""

  def __init__(self,
               centroid_n_choice=3,
               centroid_grid=(4, 4),
               ising_grid_h: int = 4,
               random_seed: Optional[int] = None):
    self._centroid_n_choice = centroid_n_choice
    self._centroid_grid = centroid_grid
    self._centroid_n_edges = centroid_grid[0] * (centroid_grid[1] - 1) + (
        centroid_grid[0] - 1) * centroid_grid[1]
    self._ising_grid_h = ising_grid_h

    self._interaction_list = []
    self._covariance_list = []
    self._partition_original_list = []
    self._n_ising_models = 3
    ising_seeds = np.random.RandomState(random_seed).randint(
        0, 10000, (self._n_ising_models,))
    for i in range(self._n_ising_models):
      interaction = common.generate_ising_interaction(self._centroid_grid[0],
                                                      self._centroid_grid[1],
                                                      ising_seeds[i])
      covariance, partition_original = common.spin_covariance(
          interaction, self._centroid_grid)
      self._interaction_list.append(interaction)
      self._covariance_list.append(covariance)
      self._partition_original_list.append(partition_original)

    self._problem_statement = self.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for suggestion in suggestions:
      # TODO: Switch to using StudyConfig.
      x = np.array([
          int(suggestion.parameters[f'x_{i}'].value)
          for i in range(self._centroid_n_edges)
      ])
      interaction_mixed = self._edge_choice(x, self._interaction_list)
      log_partition_mixed = common.log_partition(
          interaction_mixed, self._centroid_grid)  # ATTENTION
      kld_sum = 0

      for i in range(self._n_ising_models):
        kld = common.ising_dense(
            ising_grid_h=self._centroid_grid[0],  # ATTENTION
            interaction_original=self._interaction_list[i],
            interaction_sparsified=interaction_mixed,
            covariance=self._covariance_list[i],
            log_partition_original=np.log(
                self._partition_original_list[i]),  # ATTENTION
            log_partition_new=log_partition_mixed)  # ATTENTION
        kld_sum += kld
      evaluation = float(kld_sum / float(self._n_ising_models))

      suggestion.complete(
          pyvizier.Measurement(metrics={
              self._problem_statement.single_objective_metric_name: evaluation
          }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._centroid_n_edges):
      root.add_categorical_param(
          name='x_{}'.format(i),
          feasible_values=[str(j) for j in range(self._centroid_n_choice)])
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='main_objective', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
    return problem_statement

  def _edge_choice(
      self, x: np.ndarray, interaction_list: List[Tuple[np.ndarray, np.ndarray]]
  ) -> Tuple[np.ndarray, np.ndarray]:
    edge_weight = np.zeros(x.shape)
    for i in range(len(interaction_list)):
      edge_weight[x == i] = np.hstack([
          interaction_list[i][0].reshape(-1), interaction_list[i][1].reshape(-1)
      ])[x == i]
    grid_h, grid_w = self._centroid_grid
    split_ind = grid_h * (grid_w - 1)
    return edge_weight[:split_ind].reshape(
        (grid_h, grid_w - 1)), edge_weight[split_ind:].reshape(
            (grid_h - 1, grid_w))


class PestControlExperimenter(experimenter.Experimenter):
  """Pest Control Problem."""

  def __init__(self,
               pest_control_n_choice: int = 5,
               pest_control_n_stages: int = 25,
               random_seed: Optional[int] = None):
    self._pest_control_n_choice = pest_control_n_choice
    self._pest_control_n_stages = pest_control_n_stages
    self._random_seed = random_seed
    self._problem_statement = self.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for suggestion in suggestions:
      # TODO: Switch to using StudyConfig.
      x = np.array([
          int(suggestion.parameters[f'x_{i}'].value)
          for i in range(self._pest_control_n_stages)
      ])
      evaluation = self._pest_control_score(x)
      suggestion.complete(
          pyvizier.Measurement(metrics={
              self._problem_statement.single_objective_metric_name: evaluation
          }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._pest_control_n_stages):
      root.add_categorical_param(
          name='x_{}'.format(i),
          feasible_values=[str(j) for j in range(self._pest_control_n_choice)])
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='main_objective', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
    return problem_statement

  def _pest_spread(self, curr_pest_frac: float, spread_rate: float,
                   control_rate: float, apply_control: bool):
    if apply_control:
      next_pest_frac = (1.0 - control_rate) * curr_pest_frac
    else:
      next_pest_frac = spread_rate * (1 - curr_pest_frac) + curr_pest_frac
    return next_pest_frac

  def _pest_control_score(self, x: np.ndarray) -> float:
    u = 0.1
    n_stages = x.size
    n_simulations = 100

    init_pest_frac_alpha = 1.0
    init_pest_frac_beta = 30.0
    spread_alpha = 1.0
    spread_beta = 17.0 / 3.0

    control_alpha = 1.0
    control_price_max_discount = {1: 0.2, 2: 0.3, 3: 0.3, 4: 0.0}
    tolerance_develop_rate = {
        1: 1.0 / 7.0,
        2: 2.5 / 7.0,
        3: 2.0 / 7.0,
        4: 0.5 / 7.0
    }
    control_price = {1: 1.0, 2: 0.8, 3: 0.7, 4: 0.5}
    # below two changes over stages according to x
    control_beta = {1: 2.0 / 7.0, 2: 3.0 / 7.0, 3: 3.0 / 7.0, 4: 5.0 / 7.0}

    payed_price_sum = 0
    above_threshold = 0

    init_pest_frac = np.random.RandomState(self._random_seed).beta(
        init_pest_frac_alpha, init_pest_frac_beta, size=(n_simulations,))
    curr_pest_frac = init_pest_frac
    for i in range(n_stages):
      spread_rate = np.random.RandomState(self._random_seed).beta(
          spread_alpha, spread_beta, size=(n_simulations,))
      do_control = x[i] > 0
      if do_control:
        control_rate = np.random.RandomState(self._random_seed).beta(
            control_alpha, control_beta[x[i]], size=(n_simulations,))
        next_pest_frac = self._pest_spread(curr_pest_frac, spread_rate,
                                           control_rate, True)
        # Tolerance has been developed for pesticide type 1.
        control_beta[x[i]] += tolerance_develop_rate[x[i]] / float(n_stages)
        # You will get a discount.
        payed_price = control_price[x[i]] * (
            1.0 - control_price_max_discount[x[i]] / float(n_stages) *
            float(np.sum(x == x[i])))
      else:
        next_pest_frac = self._pest_spread(curr_pest_frac, spread_rate, 0,
                                           False)
        payed_price = 0
      payed_price_sum += payed_price
      above_threshold += np.mean(curr_pest_frac > u)
      curr_pest_frac = next_pest_frac

    return payed_price_sum + above_threshold


# MAXSAT Files can be found in
# https://github.com/QUVA-Lab/COMBO/tree/master/COMBO/experiments/MaxSAT/maxsat2018_data.

MAXSAT28_FILE = 'maxsat2018_data/maxcut-johnson8-2-4.clq.wcnf'
MAXSAT43_FILE = 'maxsat2018_data/maxcut-hamming8-2.clq.wcnf'
MAXSAT60_FILE = 'maxsat2018_data/frb-frb10-6-4.wcnf'


class MAXSATExperimenter(experimenter.Experimenter):
  """MAXSAT Problem."""

  def __init__(self, data_filename: str):
    self._data_filename = data_filename
    f = fileOpen(self._data_filename, 'rt')
    line_str = f.readline()
    while line_str[:2] != 'p ':
      line_str = f.readline()
    self._n_variables = int(line_str.split(' ')[2])
    self._n_clauses = int(line_str.split(' ')[3])
    self._n_vertices = np.array([2] * self._n_variables)
    raw_clauses = [(float(clause_str.split(' ')[0]),
                    clause_str.split(' ')[1:-1])
                   for clause_str in f.readlines()]
    f.close()
    weights = np.array([elm[0] for elm in raw_clauses]).astype(np.float32)
    weight_mean = np.mean(weights)
    weight_std = np.std(weights)
    self._weights = (weights - weight_mean) / weight_std
    self._clauses = []
    for _, clause in raw_clauses:
      pair = ([abs(int(elm)) - 1 for elm in clause],
              [int(elm) > 0 for elm in clause])
      self._clauses.append(pair)

    self._problem_statement = self.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for suggestion in suggestions:
      # TODO: Switch to using StudyConfig.
      bools = [
          int(suggestion.parameters[f'x_{i}'].value == 'True')
          for i in range(self._n_variables)
      ]
      x = np.array(bools, dtype=bool)
      satisfied = np.array([
          (x[clause[0]] == clause[1]).any() for clause in self._clauses
      ])
      evaluation = -np.sum(self._weights * satisfied)
      suggestion.complete(
          pyvizier.Measurement(metrics={
              self._problem_statement.single_objective_metric_name: evaluation
          }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._n_variables):
      root.add_bool_param(name=f'x_{i}')
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='main_objective', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
    return problem_statement


--- vizier/_src/benchmarks/experimenters/combo_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for combo_experimenter."""
# pylint:disable=g-long-lambda
import functools
from absl import logging

from vizier import pyvizier
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.experimenters import combo_experimenter

from absl.testing import absltest
from absl.testing import parameterized


class ComboExperimenterTest(parameterized.TestCase):
  """Default lamda values found in README of https://github.com/QUVA-Lab/COMBO."""

  @parameterized.named_parameters(
      ('contamination',
       functools.partial(
           combo_experimenter.ContaminationExperimenter,
           lamda=0.01), 20.0, 30.0),
      ('ising',
       functools.partial(combo_experimenter.IsingExperimenter,
                         lamda=0.01), 0.0, 50.0),
      ('centroid', combo_experimenter.CentroidExperimenter, 0.0, 150.0),
      ('pest_control', combo_experimenter.PestControlExperimenter, 12.0, 25.0),
  )
  def test_experimenters(self, experimenter_class, objective_min,
                         objective_max):
    """Tests if entire pipeline works and objective values make sense."""
    experimenter = experimenter_class()

    problem_statement = experimenter.problem_statement()
    logging.info(problem_statement)
    designer = random.RandomDesigner(
        search_space=problem_statement.search_space, seed=None)

    suggestions = designer.suggest(5)
    trials = [suggestion.to_trial() for suggestion in suggestions]
    experimenter.evaluate(trials)
    for trial in trials:
      logging.info('Evaluated Trial: %s', trial)
      self.assertEqual(trial.status, pyvizier.TrialStatus.COMPLETED)
      metric_name = problem_statement.metric_information.item().name
      eval_objective = trial.final_measurement_or_die.metrics[metric_name].value
      self.assertLessEqual(eval_objective, objective_max)
      self.assertGreaterEqual(eval_objective, objective_min)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/discretizing_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter that discretizes the parameters of search space."""

import copy
from typing import Mapping, Sequence, Union

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


class DiscretizingExperimenter(experimenter.Experimenter):
  """DiscretizingExperimenter discretizes the parameters of search space."""

  def __init__(
      self,
      exptr: experimenter.Experimenter,
      discretization: Mapping[str, pyvizier.MonotypeParameterSequence],
      *,
      allow_oov: bool = False,
  ):
    """DiscretizingExperimenter discretizes continuous parameters.

    Currently only supports flat double search spaces. Note that the discretized
    parameters must fit within the bounds of the continuous parameters. This
    also supports CATEGORICAL parameters but feasible categories must be
    convertible to floats.

    Args:
      exptr: Underlying experimenter to be wrapped.
      discretization: Dict of parameter name to discrete/categorical values.
      allow_oov: Allows out of vocabulary values for Trial parameter in
        Evaluate. If True, evaluate the underlying experimenter at any given
        parameter values, whether feasible or not.

    Raises:
      ValueError: Non-double underlying parameters or discrete values OOB.
    """
    self._exptr = exptr
    self._discretization = discretization
    self._allow_oov = allow_oov
    exptr_problem_statement = exptr.problem_statement()

    if exptr_problem_statement.search_space.is_conditional:
      raise ValueError(
          'Search space should not have conditional'
          f' parameters  {exptr_problem_statement}'
      )

    search_params = exptr_problem_statement.search_space.parameters
    param_names = [param.name for param in search_params]
    for name in discretization.keys():
      if name not in param_names:
        raise ValueError(
            f'Parameter {name} not in search space'
            f' parameters for discretization: {search_params}'
        )

    self._problem_statement = copy.deepcopy(exptr_problem_statement)
    self._problem_statement.search_space = pyvizier.SearchSpace()
    for parameter in search_params:
      if parameter.name not in discretization:
        self._problem_statement.search_space.add(parameter)
        continue

      if parameter.type != pyvizier.ParameterType.DOUBLE:
        raise ValueError(
            f'Non-double parameters cannot be discretized {parameter}'
        )
      # Discretize the parameters.
      min_value, max_value = parameter.bounds
      for value in discretization[parameter.name]:
        float_value = float(value)
        if float_value > max_value or float_value < min_value:
          raise ValueError(f'Discretized values are not in bounds {parameter}')
      self._problem_statement.search_space.add(
          pyvizier.ParameterConfig.factory(
              name=parameter.name,
              feasible_values=discretization[parameter.name],
              scale_type=parameter.scale_type,
              external_type=parameter.external_type,
          )
      )

  def problem_statement(self) -> pyvizier.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]) -> None:
    """Evaluate the trials after conversion to double."""

    old_parameters = []
    for suggestion in suggestions:
      old_parameters.append(suggestion.parameters)
      new_parameter_dict = {}
      for name, param in suggestion.parameters.items():
        if name in self._discretization:
          if self._allow_oov:
            if param.value not in self._discretization[name]:
              raise ValueError(
                  f'Parameter {param} not in {self._discretization[name]}'
              )
          new_parameter_dict[name] = param.as_float
        else:
          new_parameter_dict[name] = param
      suggestion.parameters = pyvizier.ParameterDict(new_parameter_dict)
    self._exptr.evaluate(suggestions)
    for old_param, suggestion in zip(old_parameters, suggestions):
      suggestion.parameters = old_param

  def __repr__(self):
    return f'DiscretizingExperimenter({self._discretization}) on {self._exptr}'

  @classmethod
  def create_with_grid(
      cls,
      exptr: experimenter.Experimenter,
      grid_discretization_count: Mapping[str, int],
      convert_to_str: Union[bool, Mapping[str, bool]] = False,
  ) -> 'DiscretizingExperimenter':
    """Creates Experimenter with continuous parameters discretized via grid.

    Note that the grid is generated according to ModelInputConverter scaling.

    Args:
      exptr: Experimenter with continuous parameters to be discretized.
      grid_discretization_count: Mapping from parameter names to number of
        feasible values to generate on grid. Parameter names should be a subset
        of all parameter names in the search space of the exptr.
      convert_to_str: For each parameter, if true, convert values to
        categories/strings. Otherwise, the values are discrete numbers. Can also
        be pure boolean, for all parameters.

    Returns:
      DiscreteExperimenter.

    Raises:
      ValueError: Invalid keys, discretizing non-double/singleton parameters.
    """
    if isinstance(convert_to_str, bool):
      convert_to_str = {
          k: convert_to_str for k in grid_discretization_count.keys()
      }

    if set(grid_discretization_count.keys()) != set(convert_to_str.keys()):
      raise ValueError(
          f'Grid discretization keys {grid_discretization_count.keys()} must'
          f' match convert_to_str keys {convert_to_str.keys()}'
      )

    # Create grid for each parameter config.
    discretization = {}
    problem_statement = copy.deepcopy(exptr.problem_statement())
    if not set(grid_discretization_count.keys()).issubset(
        set(problem_statement.search_space.parameter_names)
    ):
      raise ValueError(
          f'Grid discretization keys {grid_discretization_count.keys()}'
          f' are not in search space {problem_statement.search_space}'
      )

    for param in problem_statement.search_space.parameters:
      if param.name in grid_discretization_count:
        if param.type != pyvizier.ParameterType.DOUBLE:
          raise ValueError(
              f'Non-double parameters cannot be grid-discretized {param}'
          )

        # Grid creation logic for param.
        num_feasible_points = grid_discretization_count[param.name]
        min_value, max_value = param.bounds

        if min_value == max_value:
          raise ValueError(f'Cannot discretize singleton parameter {param}')
        converter = converters.DefaultModelInputConverter(param, scale=True)
        grid_scalars = np.linspace(0.0, 1.0, num=num_feasible_points)
        grid_values = converter.to_parameter_values(grid_scalars)

        if convert_to_str[param.name]:
          discretization[param.name] = [
              point.as_str for point in grid_values if point is not None
          ]
        else:
          discretization[param.name] = [
              point.as_float for point in grid_values if point is not None
          ]
        if len(discretization[param.name]) < len(grid_values):
          raise ValueError(
              '%d None values in grid values'
              % (len(grid_values) - len(discretization))
          )

    return cls(exptr, discretization)


--- vizier/_src/benchmarks/experimenters/discretizing_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import discretizing_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class DiscretizingExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere), ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated), ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar), ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass), ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel), ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek), ('Gallagher101Me', bbob.Gallagher101Me))
  def testNumpyExperimenter(self, func):
    dim = 3
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))

    # Asserts parameters are the same.
    parameters = list(exptr.problem_statement().search_space.parameters)
    self.assertLen(parameters, dim)

    discretization = {
        parameters[0].name: ['-1', '0', '1'],
        parameters[1].name: [0, 1, 2]
    }

    dis_exptr = discretizing_experimenter.DiscretizingExperimenter(
        exptr, discretization)
    discretized_parameters = dis_exptr.problem_statement(
    ).search_space.parameters

    self.assertLen(discretized_parameters, dim)
    self.assertListEqual([p.type for p in discretized_parameters], [
        pyvizier.ParameterType.CATEGORICAL, pyvizier.ParameterType.DISCRETE,
        pyvizier.ParameterType.DOUBLE
    ])

    parameters = {
        parameters[0].name: '0',
        parameters[1].name: 1,
        parameters[2].name: 1.5
    }
    t = pyvizier.Trial(parameters=parameters)

    dis_exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name
    self.assertAlmostEqual(
        func(np.array([0.0, 1.0, 1.5])),
        t.final_measurement_or_die.metrics[metric_name].value,
    )
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)
    self.assertDictEqual(t.parameters.as_dict(), parameters)

  def testGridCreation(self):
    dim = 3
    func = bbob.Sphere
    problem_statement = bbob.DefaultBBOBProblemStatement(dim)
    # Mutate the last parameter.
    parameters = list(problem_statement.search_space.parameters)
    log_param = problem_statement.search_space.pop(parameters[-1].name)
    problem_statement.search_space.add(
        pyvizier.ParameterConfig.factory(
            log_param.name,
            scale_type=pyvizier.ScaleType.LOG,
            bounds=(0.01, 10.0),
        )
    )
    exptr = numpy_experimenter.NumpyExperimenter(func, problem_statement)

    parameters = list(exptr.problem_statement().search_space.parameters)
    self.assertLen(parameters, dim)

    discretization = {parameters[0].name: 3, parameters[-1].name: 4}

    dis_exptr = (
        discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
            exptr, discretization
        )
    )
    search_space = dis_exptr.problem_statement().search_space
    self.assertEqual(search_space.num_parameters(), dim)
    self.assertEqual(
        search_space.num_parameters(pyvizier.ParameterType.DISCRETE), 2
    )
    self.assertLen(search_space.parameters[0].feasible_values, 3)
    self.assertSequenceEqual(
        search_space.parameters[0].feasible_values, [-5, 0, 5]
    )
    self.assertLen(search_space.parameters[-1].feasible_values, 4)
    self.assertSequenceAlmostEqual(
        search_space.parameters[-1].feasible_values,
        [0.01, 0.1, 1.0, 10.0],
        places=5,
    )

  def testGridCreationError(self):
    dim = 3
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim)
    )
    discretization = {'not_found_error': 3}

    with self.assertRaisesRegex(ValueError, 'not in search space'):
      discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
          exptr, discretization
      )

  def testGridCreationErrorNonDouble(self):
    dim = 5
    func = bbob.Sphere
    problem_statement = bbob.DefaultBBOBProblemStatement(dim)
    exptr = numpy_experimenter.NumpyExperimenter(func, problem_statement)
    parameters = list(exptr.problem_statement().search_space.parameters)
    discretization = {parameters[0].name: 3, parameters[1].name: 4}

    dis_exptr = (
        discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
            exptr, discretization
        )
    )

    with self.assertRaisesRegex(ValueError, 'Non-double parameters'):
      discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
          dis_exptr, discretization
      )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter base class for problem statement and evaluation.

Experimenters represent black-box optimization problems and/or users.
Each experimenter defines a ProblemStatement, representing the search space and
the metrics it returns in Evaluate (via CompletedTrials).

Pseudo-code for using Experimenters with Vizier Designers:

exp = ExperimenterSubClass(...)  # Possibly configure the experimenter.
problem_statement = exp.problem_statement()
designer = Designer(problem_statement)  # Configure the search algorithm
for i in range(10):
  suggestions = designer.suggest(count=2)
  exp.evaluate(suggestions) # Evaluate in-place, for maximum flexibility.
  designer.update(suggestions)
"""

import abc
from typing import Sequence

from vizier import pyvizier


class Experimenter(metaclass=abc.ABCMeta):
  """Abstract base class for Experimenters."""

  @abc.abstractmethod
  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    """Evaluates and mutates the Trials in-place.

    NOTE: The Experimenter is expected to mutate and/or complete the Trials as
    they wish, as to simulate users to maximum flexibility.

    Args:
      suggestions: Sequence of Trials to be evaluated.
    """
    pass

  @abc.abstractmethod
  def problem_statement(self) -> pyvizier.ProblemStatement:
    """The search configuration generated by this experimenter.

    The output should always be passed by value and not by reference, and thus
    should be generated inside this function or a deep copy of an existing
    problem statement.
    """
    pass


--- vizier/_src/benchmarks/experimenters/experimenter_factory.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter factories."""

import abc
import functools
import json
from typing import Optional

import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import discretizing_experimenter
from vizier._src.benchmarks.experimenters import experimenter
from vizier._src.benchmarks.experimenters import multiobjective_experimenter
from vizier._src.benchmarks.experimenters import noisy_experimenter
from vizier._src.benchmarks.experimenters import normalizing_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters import permuting_experimenter
from vizier._src.benchmarks.experimenters import shifting_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob
from vizier.interfaces import serializable
from vizier.utils import json_utils

BBOB_FACTORY_KEY = 'bbob_factory'
SINGLE_OBJECTIVE_FACTORY_KEY = 'single_objective_factory'
MULTI_OBJECTIVE_FACTORY_KEY = 'multi_objective_factory'


class ExperimenterFactory(abc.ABC):
  """Abstraction for creating Experimenters."""

  @abc.abstractmethod
  def __call__(self) -> experimenter.Experimenter:
    """Creates the Experimenter.

    This method should behave deterministcally. That is, multiple invocations of
    __call__ should always return the identical experimenter that given the
    same sequence of trials, behaves exactly the same.

    This method should not be used for generating a random experimenter.

    Returns:
      Same exact experimenter created fresh.
    """


class SerializableExperimenterFactory(
    ExperimenterFactory, serializable.Serializable
):
  """Abstraction for experimenter factories with dump/recover methods.

  Subclasses of this interface can be serialized so that we can re-create
  the exact same experimenter object.
  """


@attr.define
class BBOBExperimenterFactory(SerializableExperimenterFactory):
  """Factory for a BBOB function."""

  # Should be a BBOB function name in bbob.py (name should match exactly).
  name: str = attr.field(default='', validator=attr.validators.instance_of(str))
  dim: int = attr.field(
      default=1,
      validator=[attr.validators.instance_of(int), attr.validators.gt(0)],
  )
  rotation_seed: int = attr.field(default=0)

  def __call__(self) -> numpy_experimenter.NumpyExperimenter:
    bbob_function = getattr(bbob, self.name, None)
    if bbob_function is None:
      raise ValueError(f'{self.name} is not a valid BBOB function in bbob.py')
    bbob_function = functools.partial(bbob_function, seed=self.rotation_seed)
    return numpy_experimenter.NumpyExperimenter(
        bbob_function, bbob.DefaultBBOBProblemStatement(self.dim)
    )

  def dump(self) -> vz.Metadata:
    metadata = vz.Metadata()
    metadata_dict = {
        'name': self.name,
        'dim': self.dim,
        'rotation_seed': self.rotation_seed,
    }
    metadata[BBOB_FACTORY_KEY] = json.dumps(metadata_dict)
    return metadata

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> 'BBOBExperimenterFactory':
    metadata_dict = json.loads(metadata[BBOB_FACTORY_KEY])
    return cls(**metadata_dict)


@attr.define
class SingleObjectiveExperimenterFactory(SerializableExperimenterFactory):
  """Factory for a single objective Experimenter.

  Attributes:
    base_factory:
    shift: An array of doubles that is broadcastable to dim of search space.
    should_restrict: Whether to restrict the parameter bounds of search space
      when shifting. Note that this will change the search space bounds.
    noise_type: Should be one of the noise types in noisy_experimenter.py
    noise_seed: Seed for the noise.
    num_normalization_samples: Number of normalization samples. If zero, no
      normalization is done.
    discrete_dict: Dictionary of parameter indices to discretize in a grid. Key
      = index of parameter to be discretize Value = Number of feasible points to
      discretize to. For example, {0: 3, 2 : 2} discretizes the first parameter
      to 3 feasible points and the third to 2 feasible points. Generally, this
      should be used only when base_factory generates only continuous
      parameters.
    categorical_dict: Dictionary of parameter indices to categorize in a grid.
      Similar to `discrete_dict`, except this converts a parameter to
      categorical instead of discrete.
    permute_categoricals: Whether to permute the categorical values.
  """

  base_factory: SerializableExperimenterFactory = attr.field()

  # Below are optional randomization + transformation options.
  shift: Optional[np.ndarray] = attr.field(default=None, kw_only=True)
  should_restrict: bool = attr.field(default=True, kw_only=True)
  noise_type: Optional[str] = attr.field(default=None, kw_only=True)
  noise_seed: Optional[int] = attr.field(default=None, kw_only=True)
  num_normalization_samples: int = attr.field(default=0, kw_only=True)
  discrete_dict: dict[int, int] = attr.field(factory=dict, kw_only=True)
  categorical_dict: dict[int, int] = attr.field(factory=dict, kw_only=True)
  permute_categoricals: bool = attr.field(default=False, kw_only=True)
  permute_seed: Optional[int] = attr.field(default=None, kw_only=True)
  # TODO: Add support for sparsification.

  def __call__(self) -> experimenter.Experimenter:
    """Creates the SingleObjective Experimenter."""
    exptr = self.base_factory()
    if self.shift is not None:
      exptr = shifting_experimenter.ShiftingExperimenter(
          exptr, shift=self.shift, should_restrict=self.should_restrict
      )
    if self.num_normalization_samples:
      exptr = normalizing_experimenter.NormalizingExperimenter(
          exptr, num_normalization_samples=self.num_normalization_samples
      )

    # Discretization and categorization.
    if self.discrete_dict.keys() & self.categorical_dict.keys():
      raise ValueError(
          f'{self.discrete_dict} discretizing indices overlap with '
          f'{self.categorical_dict} categorical indices'
      )

    pcs = list(exptr.problem_statement().search_space.parameters)
    if self.discrete_dict:
      discretization = {
          pcs[idx].name: points for idx, points in self.discrete_dict.items()
      }
      exptr = (
          discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
              exptr, discretization, convert_to_str=False
          )
      )

    if self.categorical_dict:
      categorization = {
          pcs[idx].name: points for idx, points in self.categorical_dict.items()
      }
      exptr = (
          discretizing_experimenter.DiscretizingExperimenter.create_with_grid(
              exptr, categorization, convert_to_str=True
          )
      )

    if self.permute_categoricals:
      search_space = exptr.problem_statement().search_space
      categorical_parameters = [
          p.name
          for p in search_space.parameters
          if p.type == vz.ParameterType.CATEGORICAL
      ]
      exptr = permuting_experimenter.PermutingExperimenter(
          exptr, categorical_parameters, seed=self.permute_seed
      )

    if self.noise_type is not None:
      exptr = noisy_experimenter.NoisyExperimenter.from_type(
          exptr, noise_type=self.noise_type.upper(), seed=self.noise_seed
      )

    return exptr

  def dump(self) -> vz.Metadata:
    # The resulting metadata stores base factory metadata
    # and metadata_dict with different keys.
    metadata = self.base_factory.dump()
    metadata_dict = {
        'shift': self.shift,
        'noise_type': self.noise_type,
        'num_normalization_samples': self.num_normalization_samples,
        'discrete_dict': self.discrete_dict,
        'categorical_dict': self.categorical_dict,
        'should_restrict': self.should_restrict,
        'permute_categoricals': self.permute_categoricals,
        'noise_seed': self.noise_seed,
    }
    metadata[SINGLE_OBJECTIVE_FACTORY_KEY] = json.dumps(
        metadata_dict, cls=json_utils.NumpyEncoder
    )
    return metadata

  @classmethod
  def recover(
      cls, metadata: vz.Metadata
  ) -> 'SingleObjectiveExperimenterFactory':
    if BBOB_FACTORY_KEY in metadata:
      base_factory = BBOBExperimenterFactory.recover(metadata)
    else:
      raise serializable.DecodeError(
          f'No valid base factory found in {metadata}'
      )

    metadata_dict = json.loads(
        metadata[SINGLE_OBJECTIVE_FACTORY_KEY], cls=json_utils.NumpyDecoder
    )

    # Turn string keys back to int for discrete/categorical dicts.
    int_discrete_dict = {
        int(k): v for k, v in metadata_dict['discrete_dict'].items()
    }
    metadata_dict['discrete_dict'] = int_discrete_dict
    int_categorical_dict = {
        int(k): v for k, v in metadata_dict['categorical_dict'].items()
    }
    metadata_dict['categorical_dict'] = int_categorical_dict

    return SingleObjectiveExperimenterFactory(
        base_factory=base_factory, **metadata_dict
    )


@attr.define
class CombinedExperimenterFactory(SerializableExperimenterFactory):
  """Factory for a multi-objective Experimenter that combines multiple single-objective experimenters.

  Attributes:
    base_factories:
  """

  base_factories: dict[str, SerializableExperimenterFactory] = attr.field()

  def __call__(self) -> experimenter.Experimenter:
    """Creates the MultiObjective Experimenter."""
    exptrs = {name: factory() for name, factory in self.base_factories.items()}
    return multiobjective_experimenter.MultiObjectiveExperimenter(exptrs)

  def dump(self) -> vz.Metadata:
    metadata = vz.Metadata()
    metadata_dict = {
        name: factory.dump() for name, factory in self.base_factories.items()
    }
    metadata[MULTI_OBJECTIVE_FACTORY_KEY] = json.dumps(
        metadata_dict, cls=json_utils.NumpyEncoder
    )
    return metadata

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> 'CombinedExperimenterFactory':
    # TODO: Use generics to make this work.
    metadata_dict = json.loads(
        metadata[MULTI_OBJECTIVE_FACTORY_KEY], cls=json_utils.NumpyDecoder
    )
    return CombinedExperimenterFactory(
        base_factories={
            name: SerializableExperimenterFactory.recover(factory_dump)
            for name, factory_dump in metadata_dict.items()
        }
    )


--- vizier/_src/benchmarks/experimenters/experimenter_factory_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for experimenter_factory."""

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter_factory

from absl.testing import absltest
from absl.testing import parameterized


class ExperimenterFactoryTest(parameterized.TestCase):

  @parameterized.parameters(
      {'bbob_name': 'Sphere'},
      {'bbob_name': 'LinearSlope'},
      {'bbob_name': 'RosenbrockRotated'},
      {'bbob_name': 'SchaffersF7IllConditioned'},
  )
  def testBBOBFactory(self, bbob_name):
    dim = 4
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name=bbob_name, dim=dim)
    exptr = bbob_factory()

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })
    exptr.evaluate([t])
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testBBOBFactoryError(self):

    with self.assertRaisesRegex(ValueError, 'not a valid BBOB'):
      experimenter_factory.BBOBExperimenterFactory(name='Error', dim=3)()

  def testSingleObjectiveFactory(self):
    dim = 5
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim)
    exptr_factory = experimenter_factory.SingleObjectiveExperimenterFactory(
        base_factory=bbob_factory,
        shift=np.asarray(1.9),
        noise_type='moderate_gaussian')
    exptr = exptr_factory()

    self.assertIn('Shifting', str(exptr))
    self.assertIn('Noisy', str(exptr))

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })
    exptr.evaluate([t])
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testCombinedFactory(self):
    dim = 5
    experimenter_factories = {
        'sphere': experimenter_factory.BBOBExperimenterFactory('Sphere', dim),
        'discus': experimenter_factory.BBOBExperimenterFactory('Discus', dim),
    }
    exptr = experimenter_factory.CombinedExperimenterFactory(
        base_factories=experimenter_factories
    )()

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(
        parameters={
            param.name: float(index) for index, param in enumerate(parameters)
        }
    )
    exptr.evaluate([t])
    self.assertIn('sphere', t.final_measurement_or_die.metrics)
    self.assertIn('discus', t.final_measurement_or_die.metrics)
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testSingleObjectiveFactoryDiscrete(self):
    dim = 5
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim
    )
    exptr_factory = experimenter_factory.SingleObjectiveExperimenterFactory(
        base_factory=bbob_factory,
        shift=np.asarray(1.9),
        noise_type='moderate_gaussian',
        num_normalization_samples=10,
        discrete_dict={0: 3, 1: 5},
        categorical_dict={4: 4},
    )
    exptr = exptr_factory()

    self.assertIn('Shifting', str(exptr))
    self.assertIn('Noisy', str(exptr))
    self.assertIn('Discretizing', str(exptr))
    self.assertIn('Normalizing', str(exptr))

    space = exptr.problem_statement().search_space
    self.assertEqual(space.num_parameters(pyvizier.ParameterType.DISCRETE), 2)
    self.assertEqual(
        space.num_parameters(pyvizier.ParameterType.CATEGORICAL), 1
    )

    parameters = {}
    for param in space.parameters:
      if param.type == pyvizier.ParameterType.DOUBLE:
        parameters[param.name] = param.bounds[0]
      else:
        parameters[param.name] = param.feasible_values[0]
    t = pyvizier.Trial(parameters=parameters)
    exptr.evaluate([t])
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testSingleObjectiveWithPermutation(self):
    dim = 5
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim
    )
    exptr_factory = experimenter_factory.SingleObjectiveExperimenterFactory(
        base_factory=bbob_factory,
        shift=np.asarray(1.9),
        categorical_dict={0: 3, 1: 5, 2: 4},
        permute_categoricals=True,
    )
    exptr = exptr_factory()

    self.assertIn('Shifting', str(exptr))
    self.assertIn('Discretizing', str(exptr))
    self.assertIn('Permuting', str(exptr))

    space = exptr.problem_statement().search_space
    self.assertEqual(
        space.num_parameters(pyvizier.ParameterType.CATEGORICAL), 3
    )

    parameters = {}
    for param in space.parameters:
      if param.type == pyvizier.ParameterType.DOUBLE:
        parameters[param.name] = param.bounds[0]
      else:
        parameters[param.name] = param.feasible_values[0]
    t = pyvizier.Trial(parameters=parameters)
    exptr.evaluate([t])
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testSingleObjectiveFactoryError(self):
    dim = 4
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim)

    with self.assertRaisesRegex(ValueError, 'not supported'):
      experimenter_factory.SingleObjectiveExperimenterFactory(
          base_factory=bbob_factory, shift=np.asarray(1.9),
          noise_type='ERROR')()

  def testDiscreteSingleObjectiveFactoryError(self):
    dim = 4
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim
    )

    with self.assertRaisesRegex(ValueError, 'overlap'):
      experimenter_factory.SingleObjectiveExperimenterFactory(
          base_factory=bbob_factory,
          shift=np.asarray(1.9),
          noise_type='moderate_gaussian',
          num_normalization_samples=10,
          discrete_dict={0: 3, 1: 5},
          categorical_dict={1: 4},
      )()

  def testSingleObjectiveSerialization(self):
    dim = 5
    bbob_factory = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=dim
    )
    bbob_metadata = bbob_factory.dump()
    recovered_bbob = experimenter_factory.BBOBExperimenterFactory.recover(
        bbob_metadata
    )
    self.assertEqual(recovered_bbob.dim, dim)
    self.assertEqual(recovered_bbob.name, 'Sphere')

    exptr_factory = experimenter_factory.SingleObjectiveExperimenterFactory(
        base_factory=bbob_factory,
        shift=np.asarray(1.9),
        noise_type='moderate_gaussian',
        num_normalization_samples=10,
        discrete_dict={0: 3, 1: 5},
    )
    factory_metadata = exptr_factory.dump()
    recovered_factory = (
        experimenter_factory.SingleObjectiveExperimenterFactory.recover(
            factory_metadata
        )
    )
    self.assertEqual(recovered_factory.noise_type, 'moderate_gaussian')
    self.assertEqual(recovered_factory.num_normalization_samples, 10)
    np.testing.assert_array_equal(recovered_factory.shift, np.asarray(1.9))
    self.assertDictEqual(recovered_factory.discrete_dict, {0: 3, 1: 5})


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/hpob/handler.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""HPOB Handler copied (with slightly modified filesystem logic) from

https://github.com/releaunifreiburg/HPO-B/blob/main/hpob_handler.py.
"""
# pylint:skip-file

import json
import os

import functools
import numpy as np
import xgboost as xgb

Open = open
Exists = os.path.exists
IsDir = os.path.isdir


class HPOBHandler:

  def __init__(self,
               root_dir="hpob-data/",
               mode="v3-test",
               surrogates_dir="saved-surrogates/"):
    """Constructor for the HPOBHandler.

    Inputs:
        * root_dir: path to directory with the benchmark data.
        * mode: mode name indicating how to load the data. Options:
            - v1: Loads HPO-B-v1
            - v2: Loads HPO-B-v2
            - v3: Loads HPO-B-v3
            - v3-test: Loads only the meta-test split from HPO-B-v3
            - v3-train-augmented: Loads all splits from HPO-B-v3, but
            augmenting the meta-train data with the less frequent
            search-spaces.
        * surrogates_dir: path to directory with surrogates models.
    """

    print("Loading HPO-B handler")
    self.mode = mode
    self.surrogates_dir = surrogates_dir
    self.seeds = ["test0", "test1", "test2", "test3", "test4"]

    if self.mode == "v3-test":
      self.load_data(root_dir, only_test=True)
    elif self.mode == "v3-train-augmented":
      self.load_data(root_dir, only_test=False, augmented_train=True)
    elif self.mode in ["v1", "v2", "v3"]:
      self.load_data(root_dir, version=self.mode, only_test=False)
    else:
      raise ValueError("Provide a valid mode")

    self.surrogates_stats = HPOBHandler._cached_surrogates_stats(surrogates_dir)

  @classmethod
  @functools.lru_cache(maxsize=128)
  def _cached_load_data(cls,
                        rootdir="",
                        version="v3",
                        only_test=True,
                        augmented_train=False):

    meta_train_data = {}
    meta_validation_data = {}
    meta_test_data = {}

    print("Loading data...")
    meta_train_augmented_path = os.path.join(
        rootdir, "meta-train-dataset-augmented.json")
    meta_train_path = os.path.join(rootdir, "meta-train-dataset.json")
    meta_test_path = os.path.join(rootdir, "meta-test-dataset.json")
    meta_validation_path = os.path.join(rootdir, "meta-validation-dataset.json")
    bo_initializations_path = os.path.join(rootdir, "bo-initializations.json")

    with Open(meta_test_path, "rb") as f:
      meta_test_data = json.load(f)

    with Open(bo_initializations_path, "rb") as f:
      bo_initializations = json.load(f)

    if not only_test:
      if augmented_train or version == "v1":
        with Open(meta_train_augmented_path, "rb") as f:
          meta_train_data = json.load(f)
      else:
        with Open(meta_train_path, "rb") as f:
          meta_train_data = json.load(f)
      with Open(meta_validation_path, "rb") as f:
        meta_validation_data = json.load(f)

    if version != "v3":
      temp_data = {}
      for search_space in meta_train_data.keys():
        temp_data[search_space] = {}

        for dataset in meta_train_data[search_space].keys():
          temp_data[search_space][dataset] = meta_train_data[search_space][
              dataset]

        if search_space in meta_test_data.keys():
          for dataset in meta_test_data[search_space].keys():
            temp_data[search_space][dataset] = meta_test_data[search_space][
                dataset]

          for dataset in meta_validation_data[search_space].keys():
            temp_data[search_space][dataset] = meta_validation_data[
                search_space][dataset]
      meta_test_data = temp_data

    return meta_train_data, meta_validation_data, meta_test_data, bo_initializations

  @classmethod
  @functools.lru_cache(maxsize=128)
  def _cached_surrogates_stats(cls, surrogates_dir: str):
    surrogates_file = surrogates_dir + "summary-stats.json"
    if (Exists(surrogates_file) and not IsDir(surrogates_file)):
      with Open(surrogates_file, "rt") as f:
        surrogates_stats = json.load(f)
    else:
      surrogates_stats = None
    return surrogates_stats

  def load_data(self,
                rootdir="",
                version="v3",
                only_test=True,
                augmented_train=False):
    """
        Loads data with some specifications.
        Inputs:
            * root_dir: path to directory with the benchmark data.
            * version: name indicating what HPOB version to use. Options: v1,
            v2, v3).
            * Only test: Whether to load only testing data (valid only for
            version v3).  Options: True/False
            * augmented_train: Whether to load the augmented train data (valid
            only for version v3). Options: True/False
    """

    self.meta_train_data, self.meta_validation_data, self.meta_test_data, self.bo_initializations = HPOBHandler._cached_load_data(
        rootdir=rootdir,
        version=version,
        only_test=only_test,
        augmented_train=augmented_train)

  def normalize(self, y, y_min=None, y_max=None):

    if y_min is None:
      return (y - np.min(y)) / (np.max(y) - np.min(y))
    else:
      return (y - y_min) / (y_max - y_min)

  def evaluate(self,
               bo_method=None,
               search_space_id=None,
               dataset_id=None,
               seed=None,
               n_trials=10):
    """Evaluates a method on the benchmark with discretized search-spaces.

    Inputs:
        * bo_method: object to evaluate. It should have a function (class
        method) named 'observe_and_suggest'.
        * search_space_id: Identifier of the search spaces for the
        evaluation. Option: see original paper.
        * dataset_id: Identifier of the dataset for the evaluation. Options:
        see original paper.
        * seed: Identifier of the seed for the evaluation. Options: test0,
        test1, test2, test3, test4.
        * trails: Number of trials (iterations on the opoitmization).
    Ooutput:
        * a list with the maximumu performance (incumbent) for every trial.
    """

    assert bo_method != None, "Provide a valid method object for evaluation."
    assert hasattr(bo_method, "observe_and_suggest"), (
        "The provided  object does not have a method called "
        "´observe_and_suggest´")
    assert search_space_id != None, ("Provide a valid search space id. See "
                                     "documentatio for valid obptions.")
    assert dataset_id != None, ("Provide a valid dataset_id. See documentation "
                                "for valid options.")
    assert seed != None, ("Provide a valid initialization. Valid options are: "
                          "test0, test1, test2, test3, test4.")

    n_initial_evaluations = 5
    X = np.array(self.meta_test_data[search_space_id][dataset_id]["X"])
    y = np.array(self.meta_test_data[search_space_id][dataset_id]["y"])
    y = self.normalize(y)
    data_size = len(X)

    pending_evaluations = list(range(data_size))
    current_evaluations = []

    init_ids = self.bo_initializations[search_space_id][dataset_id][seed]

    for i in range(n_initial_evaluations):
      idx = init_ids[i]
      pending_evaluations.remove(idx)
      current_evaluations.append(idx)

    max_accuracy_history = [np.max(y[current_evaluations])]
    for i in range(n_trials):

      idx = bo_method.observe_and_suggest(X[current_evaluations],
                                          y[current_evaluations],
                                          X[pending_evaluations])
      idx = pending_evaluations[idx]
      pending_evaluations.remove(idx)
      current_evaluations.append(idx)
      max_accuracy_history.append(np.max(y[current_evaluations]))

    return max_accuracy_history

  def evaluate_continuous(self,
                          bo_method=None,
                          search_space_id=None,
                          dataset_id=None,
                          seed=None,
                          n_trials=10):
    """
        Evaluates a method on the benchmark with continuous search-spaces.
        Inputs:
            * bo_method: object to evaluate. It should have a function (class
            method) named 'observe_and_suggest'.
            * search_space_id: Identifier of the search spaces for the
            evaluation. Option: see original paper.
            * dataset_id: Identifier of the dataset for the evaluation. Options:
            see original paper.
            * seed: Identifier of the seed for the evaluation. Options: test0,
            test1, test2, test3, test4.
            * trails: Number of trials (iterations on the opoitmization).
        Ooutput:
            * a list with the maximumu performance (incumbent) for every trial.
        """

    assert bo_method != None, "Provide a valid method object for evaluation."
    assert hasattr(bo_method, "observe_and_suggest"), (
        "The provided  object does not have a method called "
        "´observe_and_suggest´")
    assert search_space_id != None, ("Provide a valid search space id. See "
                                     "documentatio for valid obptions.")
    assert dataset_id != None, ("Provide a valid dataset_id. See documentation "
                                "for valid options.")
    assert seed != None, ("Provide a valid initialization. Valid options are: "
                          "test0, test1, test2, test3, test4.")

    surrogate_name = "surrogate-" + search_space_id + "-" + dataset_id
    bst_surrogate = xgb.Booster()
    bst_surrogate.load_model(self.surrogates_dir + surrogate_name + ".json")

    n_initial_evaluations = 5
    X = np.array(self.meta_test_data[search_space_id][dataset_id]["X"])
    y = np.array(self.meta_test_data[search_space_id][dataset_id]["y"])
    y_min = self.surrogates_stats[surrogate_name]["y_min"]
    y_max = self.surrogates_stats[surrogate_name]["y_max"]
    dim = X.shape[1]
    current_evaluations = []
    init_ids = self.bo_initializations[search_space_id][dataset_id][seed]

    for i in range(n_initial_evaluations):
      idx = init_ids[i]
      current_evaluations.append(idx)

    x_observed = X[current_evaluations]
    y_observed = y[current_evaluations]

    max_accuracy_history = []

    for i in range(n_trials):
      y_tf_observed = self.normalize(y_observed, y_min, y_max)
      y_tf_observed = np.clip(y_tf_observed, 0, 1)
      best_f = np.max(y_tf_observed)
      max_accuracy_history.append(best_f)

      new_x = bo_method.observe_and_suggest(x_observed, y_tf_observed)
      x_q = xgb.DMatrix(new_x.reshape(-1, dim))
      new_y = bst_surrogate.predict(x_q)

      y_observed = np.append(y_observed, new_y).reshape(-1, 1)
      x_observed = np.append(x_observed, new_x).reshape(-1, x_observed.shape[1])

    y_tf_observed = self.normalize(y_observed, y_min, y_max)
    y_tf_observed = np.clip(y_tf_observed, 0, 1)
    max_accuracy_history.append(best_f)

    return max_accuracy_history

  def get_seeds(self):
    return self.seeds


--- vizier/_src/benchmarks/experimenters/infeasible_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenters which induce infeasibility behaviors."""

import copy
import json
import random
from typing import Sequence
import attrs
import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


@attrs.define
class HashingInfeasibleExperimenter(experimenter.Experimenter):
  """Simulates randomly selected (deterministic) infeasibility function."""

  exptr: experimenter.Experimenter = attrs.field()
  infeasible_prob: float = attrs.field(default=0.2, kw_only=True)
  seed: int = attrs.field(default=0, kw_only=True)

  def __attrs_post_init__(self):
    self._problem = copy.deepcopy(self.exptr.problem_statement())

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    metric_configs = self._problem.metric_information
    for suggestion in suggestions:
      if self._is_infeasible(suggestion.parameters):
        suggestion.complete(
            vz.Measurement(metrics={mc.name: np.nan for mc in metric_configs}),
            infeasibility_reason='HashingInfeasibleExperimenter',
        )
      else:
        self.exptr.evaluate([suggestion])

  def _is_infeasible(self, parameters: vz.ParameterDict) -> bool:
    hash_str = json.dumps(parameters.as_dict(), sort_keys=True) + str(self.seed)
    if random.Random(hash_str).random() < self.infeasible_prob:
      return True
    return False

  def problem_statement(self) -> vz.ProblemStatement:
    return self._problem


@attrs.define
class ParamRegionInfeasibleExperimenter(experimenter.Experimenter):
  """Selects a parameter and splits its values into feasible/infeasible."""

  exptr: experimenter.Experimenter = attrs.field()
  parameter_name: str = attrs.field()
  infeasible_interval: tuple[float, float] = attrs.field(
      default=(0.0, 0.2), kw_only=True
  )

  def __attrs_post_init__(self):
    self._problem = copy.deepcopy(self.exptr.problem_statement())

    param_config = self._problem.search_space.get(self.parameter_name)
    if param_config.type == vz.ParameterType.CATEGORICAL:
      raise ValueError('Categorical param type unsupported currently.')
    self._converter = converters.DefaultModelInputConverter(
        param_config, max_discrete_indices=0, scale=True
    )

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    param_features = self._converter.convert(suggestions)

    metric_configs = self._problem.metric_information
    for sugg_index, param_feature in enumerate(param_features):
      p = param_feature.item()
      suggestion = suggestions[sugg_index]
      if self.infeasible_interval[0] <= p <= self.infeasible_interval[1]:
        suggestion.complete(
            vz.Measurement(metrics={mc.name: np.nan for mc in metric_configs}),
            infeasibility_reason='ParameterRegionInfeasibleExperimenter',
        )
      else:
        self.exptr.evaluate([suggestion])

  def problem_statement(self) -> vz.ProblemStatement:
    return self._problem


--- vizier/_src/benchmarks/experimenters/infeasible_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import infeasible_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob
from absl.testing import absltest


class HashingInfeasibleExperimenterTest(absltest.TestCase):

  def test_consistency(self):
    exptr = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(2)
    )
    exptr = infeasible_experimenter.HashingInfeasibleExperimenter(
        exptr, infeasible_prob=0.5, seed=0
    )

    for i in range(10):
      trials = [vz.Trial(parameters={'x0': i, 'x1': -i}) for _ in range(10)]
      trials += [vz.Trial(parameters={'x1': -i, 'x0': i}) for _ in range(10)]
      exptr.evaluate(trials)

      for t in trials:
        self.assertEqual(t.infeasible, trials[0].infeasible)


class ParamRegionInfeasibleExperimenterTest(absltest.TestCase):

  def test_e2e(self):
    exptr = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(2)
    )
    exptr = infeasible_experimenter.ParamRegionInfeasibleExperimenter(
        exptr, parameter_name='x0', infeasible_interval=(0.0, 0.5)
    )

    infeasible_trial = vz.Trial(parameters={'x0': -3.5, 'x1': 0})
    feasible_trial = vz.Trial(parameters={'x0': 3.5, 'x1': 0})
    exptr.evaluate([infeasible_trial, feasible_trial])

    self.assertTrue(infeasible_trial.infeasible)
    self.assertFalse(feasible_trial.infeasible)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/l1_categorical_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""L1 categorical experimenter.

The experimenter's evaluation function counts the number of different parameters
values between the optimal point ('optimum') and the suggestion trial.
"""

import copy
import logging
from typing import Optional, Sequence

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


class L1CategorialExperimenter(experimenter.Experimenter):
  """Categorical experimenter associated minimum optimization problem."""

  def __init__(
      self,
      *,
      num_categories: Sequence[int],
      optimum: Optional[Sequence[int]] = None,
      seed: Optional[int] = None,
      verbose: bool = False,
  ):
    """Constructor.

    If optimum is not provided, a randomly generated optimum point will be
    created.

    Arguments:
      num_categories: The number of categories in each parameter.
      optimum: Optional list of indices indicating the optimum point. If not
        set, randomly created from seed.
      seed: Optional random generator seed.
      verbose: Whether to show logs.
    """
    rng = np.random.default_rng(seed=seed)
    self._problem = vz.ProblemStatement()
    self._optimum = {}
    for i, num_category in enumerate(num_categories):
      categories = [str(x) for x in range(num_category)]
      param_name = f'c{i}'
      self._problem.search_space.root.add_categorical_param(
          param_name, categories)
      if optimum is None:
        # Define the optimum point by randomally selecting categorical values.
        self._optimum[param_name] = str(rng.integers(low=0, high=num_category))
      elif optimum[i] >= num_category:
        raise ValueError("Optimum point doesn't match category dimensions!")
      else:
        self._optimum[param_name] = str(optimum[i])

    self._problem.metric_information.append(
        vz.MetricInformation(
            name='objective', goal=vz.ObjectiveMetricGoal.MINIMIZE))
    if verbose:
      logging.info('L1CategoricalExperimenter optimum point: %s', self._optimum)

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    for suggestion in suggestions:
      loss = 0.0
      for param_config in self._problem.search_space.parameters:
        if suggestion.parameters[param_config.name].value != self._optimum[
            param_config.name]:
          loss += 1.0
      suggestion.complete(vz.Measurement(metrics={'objective': loss}))

  @property
  def optimal_trial(self) -> vz.Trial:
    """Evaluates and returns the optimal trial."""
    optimal_trial = vz.Trial(parameters=self._optimum)
    self.evaluate([optimal_trial])
    return optimal_trial

  def problem_statement(self):
    """See superclass."""
    return copy.deepcopy(self._problem)


--- vizier/_src/benchmarks/experimenters/l1_categorical_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for categorical_experimenter."""

from absl.testing import parameterized
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import l1_categorical_experimenter

from absl.testing import absltest


class L1CategoricalExperimenterTest(parameterized.TestCase):
  """Tests for CategoricalExperimenter."""

  def test_evaluate_optimum(self):
    optimum = [0, 1]
    exptr = l1_categorical_experimenter.L1CategorialExperimenter(
        num_categories=[2, 2], optimum=optimum)
    suggestion = vz.Trial(parameters={'c0': '0', 'c1': '1'})
    exptr.evaluate([suggestion])
    self.assertEqual(
        suggestion.final_measurement_or_die.metrics['objective'].value, 0
    )

  def test_evaluate_non_optimum(self):
    optimum = [0, 1]
    exptr = l1_categorical_experimenter.L1CategorialExperimenter(
        num_categories=[2, 2], optimum=optimum)
    suggestion = vz.Trial(parameters={'c0': '1', 'c1': '0'})
    exptr.evaluate([suggestion])
    self.assertEqual(
        suggestion.final_measurement_or_die.metrics['objective'].value, 2
    )

  @parameterized.parameters({'num_categories': [10, 3, 2]},
                            {'num_categories': [10, 2, 10]},
                            {'num_categories': [10, 10, 12, 15, 2]},
                            {'num_categories': [10]})
  def test_generate_random_optimum(self, num_categories):
    exptr = l1_categorical_experimenter.L1CategorialExperimenter(
        num_categories=num_categories)
    self.assertLen(exptr._optimum, len(num_categories))
    for i, value in enumerate(exptr._optimum.values()):
      self.assertIsInstance(value, str)
      self.assertTrue(0 <= int(value) < num_categories[i])

  def test_optimal_trial(self):
    optimum = [9, 2, 1]
    exptr = l1_categorical_experimenter.L1CategorialExperimenter(
        num_categories=[10, 3, 2], optimum=optimum)
    self.assertEqual(exptr.optimal_trial.parameters['c0'].value, '9')
    self.assertEqual(exptr.optimal_trial.parameters['c1'].value, '2')
    self.assertEqual(exptr.optimal_trial.parameters['c2'].value, '1')

  def test_optimal_trial_validation(self):
    optimum = [9, 3, 1]
    with self.assertRaises(ValueError):
      l1_categorical_experimenter.L1CategorialExperimenter(
          num_categories=[10, 3, 2], optimum=optimum)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/multiobjective_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Multiobjective Experimenters."""

import copy
from typing import Dict, Sequence


from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter


class MultiObjectiveExperimenter(experimenter.Experimenter):
  """MultiObjective Experimenter from a Dict of Experimenters."""

  def __init__(
      self,
      exptrs: Dict[str, experimenter.Experimenter],
  ):
    """Creates a MultiObjectiveExperimenter from a Dict of Experimenters.

    Specifically, the Experimenter takes a Dict of metric names and
    single-objective experimenters with the same search space. Each metric
    is evaluated individually by exptr and combined into a final measurement
    whose name is updated to metric name in exptrs. The problem statement
    of this experimenter is multi-objective with objective names given by keys
    in exptrs.

    Args:
      exptrs: Dict of metric name to Experimenters

    Raises:
      ValueError: Mismatching search space or exptr is not single-objective.
    """
    self._exptrs = exptrs

    # Copy and check that problem statements have same search space.
    self._problem_statement = list(exptrs.values())[0].problem_statement()
    first_search_space = self._problem_statement.search_space
    for exptr in exptrs.values():
      if exptr.problem_statement().search_space != first_search_space:
        raise ValueError(
            'Search space must match for all objectives: \n'
            f'{first_search_space} does not match '
            f'{exptr.problem_statement().search_space}'
        )

    metric_infos = []
    # Keeps track of the underlying metric information name of each extpr.
    self._exptr_to_metric = {}
    for name, exptr in exptrs.items():
      metric_info = exptr.problem_statement().metric_information.item()
      self._exptr_to_metric[name] = metric_info.name
      metric_info.name = name
      metric_infos.append(metric_info)

    self._problem_statement.metric_information = pyvizier.MetricsConfig(
        metric_infos
    )

  def problem_statement(self) -> pyvizier.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    suggestions_copy = copy.deepcopy(suggestions)
    measurements = [pyvizier.Measurement() for _ in suggestions]
    for name, exptr in self._exptrs.items():
      exptr.evaluate(suggestions_copy)
      exptr_metric_name = self._exptr_to_metric[name]
      for idx, copied in enumerate(suggestions_copy):
        measurement = measurements[idx]
        assert copied.final_measurement is not None
        measurement.metrics[name] = copied.final_measurement.metrics[
            exptr_metric_name
        ]

    for suggestion, measurement in zip(suggestions, measurements):
      suggestion.complete(measurement)

    return suggestions

  def __repr__(self):
    return (
        f'MultiObjectiveExperimenter with {len(self._exptrs)} exptrs:'
        f' {self._exptrs}'
    )


--- vizier/_src/benchmarks/experimenters/multiobjective_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import multiobjective_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest


class MultiobjectiveExperimenterTest(absltest.TestCase):

  def test_mulitobjective_numpy(self):
    dim = 2
    func1 = bbob.Sphere
    func2 = bbob.Rastrigin
    exptr1 = numpy_experimenter.NumpyExperimenter(
        func1, bbob.DefaultBBOBProblemStatement(dim)
    )
    exptr2 = numpy_experimenter.NumpyExperimenter(
        func2, bbob.DefaultBBOBProblemStatement(dim)
    )
    exptr = multiobjective_experimenter.MultiObjectiveExperimenter(
        {'m1': exptr1, 'm2': exptr2}
    )
    parameters = exptr1.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(
        parameters={
            param.name: float(index) for index, param in enumerate(parameters)
        }
    )

    exptr.evaluate([t])
    self.assertAlmostEqual(
        func1(np.array([0.0, 1.0])),
        t.final_measurement_or_die.metrics['m1'].value,
    )
    self.assertAlmostEqual(
        func2(np.array([0.0, 1.0])),
        t.final_measurement_or_die.metrics['m2'].value,
    )
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def test_dimension_mismatch(self):
    exptr1 = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(dimension=3)
    )
    exptr2 = numpy_experimenter.NumpyExperimenter(
        bbob.Rastrigin, bbob.DefaultBBOBProblemStatement(dimension=6)
    )
    with self.assertRaisesRegex(ValueError, 'space must match'):
      multiobjective_experimenter.MultiObjectiveExperimenter(
          {'m1': exptr1, 'm2': exptr2}
      )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/nasbench101_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""NAS-Bench-101 Experimenter.

NOTE: Since our installation uses tensorflow>=2.0.0, you will need to change the
following in the nasbench package code (which assumes tensorflow-1 API):

```
tf.train.SessionRunHook -> tf.estimator.SessionRunHook
tf.train.CheckpointSaverListener -> tf.estimator.CheckpointSaverListener
tf.train.NanLossDuringTrainingError -> tf.estimator.NanLossDuringTrainingError
tf.python_io.tf_record_iterator -> tf.compat.v1.io.tf_record_iterator
```

See https://github.com/google-research/nasbench/issues/27.
"""
from typing import Sequence

from absl import logging
# from nasbench import api
import numpy as np

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter

logging.warning(
    'NASBENCH-101 assumes tensorflow<2.0. Please see above in this file for what to replace in nasbench code to make the benchmark work.'
)


class NASBench101Experimenter(experimenter.Experimenter):
  """This suggests model specs in the form of a matrix (for DAG topology) and ops (for convolutions).

  Search space is a union of binary strings and categoricals.
  """

  def __init__(self, nasbench):
    self._nasbench = nasbench
    self._num_vertices = 7
    self._allowed_ops = ['conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3']
    self._op_spots = self._num_vertices - 2
    self._input_op = 'input'
    self._output_op = 'output'

    self._metric_names = [
        'trainable_parameters', 'training_time', 'train_accuracy',
        'validation_accuracy', 'test_accuracy'
    ]

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    """Evaluates and completes the Trial using NASBench-101 querying."""

    for trial in suggestions:
      spec = self._trial_to_model_spec(trial)
      if self._nasbench.is_valid(spec):
        results = self._nasbench.query(spec)
        trial.complete(
            pyvizier.Measurement(
                metrics={k: results[k] for k in self._metric_names}))
      else:
        trial.complete(
            pyvizier.Measurement(), infeasibility_reason='Not in search space.')

  def _trial_to_model_spec(self, trial: pyvizier.Trial):
    matrix = np.zeros((self._num_vertices, self._num_vertices), dtype=int)
    for y in range(self._num_vertices):
      for x in range(self._num_vertices):
        if y > x:
          matrix[x][y] = int(
              trial.parameters['{}_{}'.format(x, y)].value == 'True')

    base_ops = []
    for i in range(self._op_spots):
      base_ops.append(trial.parameters['ops_{}'.format(i)].value)
    ops = [self._input_op] + base_ops + [self._output_op]

    return api.ModelSpec(matrix=matrix, ops=ops)

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root

    for y in range(self._num_vertices):
      for x in range(self._num_vertices):
        if y > x:
          root.add_bool_param(name='{}_{}'.format(x, y))

    for i in range(self._op_spots):
      root.add_categorical_param(
          name='ops_{}'.format(i), feasible_values=self._allowed_ops)

    # 'test_accuracy' also can be used for objective value.
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='validation_accuracy',
            goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE))
    return problem_statement


--- vizier/_src/benchmarks/experimenters/nasbench101_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for nasbench101_experimenter."""
from absl import logging
# from nasbench import api

from vizier import pyvizier
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.experimenters import nasbench101_experimenter
from absl.testing import absltest


class Nasbench101ExperimenterTest(absltest.TestCase):

  @absltest.skip("Files must be installed manually.")
  def test_experimenter(self):
    nasbench = api.NASBench(DEFAULT_NAS_BENCH_108_EPOCHS_FILE_EXTERNAL)
    experimenter = nasbench101_experimenter.NASBench101Experimenter(nasbench)
    problem_statement = experimenter.problem_statement()
    designer = random.RandomDesigner(
        search_space=problem_statement.search_space, seed=None)

    suggestions = designer.suggest(5)
    trials = [suggestion.to_trial() for suggestion in suggestions]
    experimenter.evaluate(trials)
    for trial in trials:
      logging.info('Evaluated Trial: %s', trial)
      self.assertEqual(trial.status, pyvizier.TrialStatus.COMPLETED)
      if not trial.infeasible:
        metric_name = problem_statement.metric_information.item().name
        eval_objective = trial.final_measurement_or_die.metrics[
            metric_name
        ].value
        self.assertGreaterEqual(eval_objective, 0.0)
        self.assertLessEqual(eval_objective, 100.0)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/nasbench201_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""NAS-Bench-201 Experimenter.

Note that 'NATS' is an alias of 201.

Currently we only consider the topology search space (TSS) as it is much more
frequently used, as opposed to the size search space (SSS) which checks
performance by varying depths.
"""

from typing import Sequence
import nats_bench

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter


def _model_tss_spc(ops: Sequence[str], num_nodes: int) -> str:
  """Converts ops and nodes to a string format recognized by NASBENCH-201."""
  nodes, k = [], 0
  for i in range(1, num_nodes):
    xstrs = []
    for j in range(i):
      xstrs.append('{:}~{:}'.format(ops[k], j))
      k += 1
    nodes.append('|' + '|'.join(xstrs) + '|')
  return '+'.join(nodes)


class NASBench201Experimenter(experimenter.Experimenter):
  """NASBENCH-201."""

  def __init__(self,
               nasbench: nats_bench.NATStopology,
               datastr_str: str = 'cifar10',
               validation_set_reporting_epoch: int = 12):
    self._nasbench = nasbench
    self._dataset_str = datastr_str
    tss_raw_config = nats_bench.search_space_info('nats-bench', 'tss')

    self._allowed_ops = tss_raw_config['op_names']
    self._num_nodes = tss_raw_config['num_nodes']
    self._op_spots = 6

  @property
  def nasbench(self) -> nats_bench.NATStopology:
    return self._nasbench

  def _trial_to_model_spec(self, trial: pyvizier.Trial):
    ops = [
        trial.parameters['op_{}'.format(i)].value for i in range(self._op_spots)
    ]
    return _model_tss_spc(ops, self._num_nodes)

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    for trial in suggestions:
      spec = self._trial_to_model_spec(trial)
      valid_acc, latency, time_cost, total_time = self._nasbench.simulate_train_eval(
          arch=spec, dataset=self._dataset_str)
      trial.complete(
          pyvizier.Measurement(
              metrics={
                  'valid_acc': valid_acc,
                  'latency': latency,
                  'time_cost': time_cost,
                  'total_time': total_time
              }))

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem_statement = pyvizier.ProblemStatement()
    root = problem_statement.search_space.root
    for i in range(self._op_spots):
      root.add_categorical_param(
          name='op_{}'.format(i), feasible_values=self._allowed_ops)
    problem_statement.metric_information.append(
        pyvizier.MetricInformation(
            name='valid_acc', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE))
    return problem_statement


--- vizier/_src/benchmarks/experimenters/nasbench201_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for nasbench201_experimenter."""
from absl import logging
import nats_bench
from vizier import pyvizier
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.experimenters import nasbench201_experimenter
from absl.testing import absltest


class Nasbench201ExperimenterTest(absltest.TestCase):

  @absltest.skip("Files must be installed manually.")
  def test_experimenter(self):
    nasbench = nats_bench.NATStopology(
        file_path_or_dict=DEFAULT_NATS_TSS_DIR, fast_mode=True, verbose=False)
    experimenter = nasbench201_experimenter.NASBench201Experimenter(nasbench)
    problem_statement = experimenter.problem_statement()
    designer = random.RandomDesigner(
        search_space=problem_statement.search_space, seed=None)

    suggestions = designer.suggest(5)
    trials = [suggestion.to_trial() for suggestion in suggestions]
    experimenter.evaluate(trials)
    for trial in trials:
      logging.info('Evaluated Trial: %s', trial)
      self.assertEqual(trial.status, pyvizier.TrialStatus.COMPLETED)
      metric_name = problem_statement.metric_information.item().name
      eval_objective = trial.final_measurement_or_die.metrics[metric_name].value
      self.assertGreaterEqual(eval_objective, 0.0)
      self.assertLessEqual(eval_objective, 100.0)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/noisy_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Applies the noise function to each metric in final measurement."""

import functools
from typing import Callable, Optional, Sequence

import attr
import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter


@attr.define(auto_attribs=True)
class NoisyExperimenter(experimenter.Experimenter):
  """NoisyExperimenter applies noise to all metric(s) in final measurement.

  It stores the unnoised metric as metric_name + '_before_noise'. The noise
  function is a callable that is applied to metrics.
  """

  exptr: experimenter.Experimenter = attr.field()
  noise_fn: Callable[[float], float] = attr.field()

  @classmethod
  def from_type(
      cls,
      exptr: experimenter.Experimenter,
      noise_type: str,
      seed: Optional[int] = None,
  ) -> 'NoisyExperimenter':
    """Initializes noise_fn with noise given by noise_type."""
    dim = len(exptr.problem_statement().search_space.parameters)
    noise_fn = _create_noise_fn(
        noise_type,
        dimension=dim,
        seed=seed,
    )
    return cls(exptr, noise_fn)

  def problem_statement(self) -> pyvizier.ProblemStatement:
    return self.exptr.problem_statement()

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    self.exptr.evaluate(suggestions)
    for suggestion in suggestions:
      if suggestion.final_measurement is None:
        continue
      metric_dict_with_noise = {}
      for name, metric in suggestion.final_measurement.metrics.items():
        metric_dict_with_noise[name] = pyvizier.Metric(
            value=self.noise_fn(metric.value))
        metric_dict_with_noise[name + '_before_noise'] = metric
      suggestion.final_measurement.metrics = metric_dict_with_noise

  def __repr__(self):
    return f'NoisyExperimenter({self.noise_fn}) on {str(self.exptr)}'


def _create_noise_fn(
    noise: str,
    dimension: int,
    target_value: float = 1e-8,
    seed: Optional[int] = None,
) -> Callable[[float], float]:
  """Creates a noise function via NumPy.

  See https://bee22.com/resources/bbob%20noisy%20functions.pdf

  Args:
    noise: Noise specification
    dimension: Dimensionality of bbob function that the noise is applied to.
    target_value: The noise does not apply to values less than this.
    seed:

  Returns:
    Callable that returns the noisy version of the input.

  Raises:
    ValueError: if noise is not supported.
  """
  rng = np.random.default_rng(seed or 0)
  if noise == 'NO_NOISE':
    noise_fn = lambda v: v
  elif noise == 'MODERATE_GAUSSIAN':
    noise_fn = lambda v: v * rng.lognormal(0, 0.01)
  elif noise == 'SEVERE_GAUSSIAN':
    noise_fn = lambda v: v * rng.lognormal(0, 0.1)
  elif noise == 'MODERATE_UNIFORM':
    noise_fn = functools.partial(
        _uniform_noise,
        rng=rng,
        amplifying_exponent=0.01 * (0.49 + 1.0 / dimension),
        shrinking_exponent=0.01,
    )
  elif noise == 'SEVERE_UNIFORM':
    noise_fn = functools.partial(
        _uniform_noise,
        rng=rng,
        amplifying_exponent=0.1 * (0.49 + 1.0 / dimension),
        shrinking_exponent=0.1,
    )
  elif noise == 'MODERATE_SELDOM_CAUCHY':
    noise_fn = functools.partial(
        _cauchy_noise, rng=rng, noise_strength=0.01, noise_frequency=0.05
    )
  elif noise == 'SEVERE_SELDOM_CAUCHY':
    noise_fn = functools.partial(
        _cauchy_noise, rng=rng, noise_strength=0.1, noise_frequency=0.25
    )
  elif noise == 'LIGHT_ADDITIVE_GAUSSIAN':
    return functools.partial(_additive_normal_noise, rng=rng, stddev=0.01)
  elif noise == 'MODERATE_ADDITIVE_GAUSSIAN':
    return functools.partial(_additive_normal_noise, rng=rng, stddev=0.1)
  elif noise == 'SEVERE_ADDITIVE_GAUSSIAN':
    return functools.partial(_additive_normal_noise, rng=rng, stddev=1.0)
  else:
    raise ValueError('Noise was not supported: {}'.format(noise))
  return lambda v: _stabilized_noise(v, noise_fn, target_value)


def _uniform_noise(
    value: float,
    amplifying_exponent: float,
    shrinking_exponent: float,
    rng: np.random.Generator,
    epsilon: float = 1e-99,
) -> float:
  """Uniform noise model for bbob-noisy benchmark.

  The noise strength increases when value is small.

  Args:
    value: Function value to apply noise to.
    amplifying_exponent: "alpha" in the paper. The higher this number is, the
      more likely it is for the noisy value to be greater than the input value.
      0 or less means the noise never amplifies the function value.
    shrinking_exponent: "beta" in the paper. The higher this number is, the more
      likely it is for the noisy value to be less than the input value. 0 or
      less means the noise never shrinks the function value.
    rng: Rng.
    epsilon: "epsilon" in the paper. Prevents division by zero.

  Returns:
    Noisy version of value.
  """
  f1 = np.power(rng.uniform(), np.max([0.0, shrinking_exponent]))
  f2 = np.power(1e9 / (value + epsilon), amplifying_exponent * rng.uniform())
  return value * f1 * np.max([1.0, f2])


def _additive_normal_noise(
    value: float, stddev: float, rng: np.random.Generator
) -> float:
  """Additive normal noise."""
  return value + rng.normal(0.0, stddev)


def _cauchy_noise(
    value: float,
    noise_strength: float,
    noise_frequency: float,
    rng: np.random.Generator,
) -> float:
  """Cauchy noise model for bbob-noisy benchmark.

  The noise is infrequent and difficult to analyze due to large outliers.

  Args:
    value: Function value to apply noise to.
    noise_strength: "alpha" in the paper. Its absolute value determines the
      noise strength. The recommended setup as in the paper is to use a positive
      number.
    noise_frequency: "p" in the paper. Determines the probability of the noisy
      evaluation. Clipped (not explicitly but effectively) to [0, 1] range.
    rng:

  Returns:
    Noisy version of value.
  """
  noise = (rng.uniform() < noise_frequency) * rng.standard_cauchy()
  return value + noise_strength * np.max([0.0, 1000.0 + noise])


def _stabilized_noise(value: float,
                      noisy_fn: Callable[[float], float],
                      target_value: float = 1e-8) -> float:
  """Post processing of noise for bbob-noisy benchmark.

  We do not apply noise if the value is close to the global optima. This keeps
  the optimal value intact.


  Args:
    value: Function value to apply noise to.
    noisy_fn: "f_XX" in the paper. It applies noise to the input.
    target_value: If value is less than this number, then we do not apply the
      noise.

  Returns:
    value, if it is less than target_value. Otherwise, noisy version of
    value.
  """

  if value >= target_value:
    return noisy_fn(value) + 1.01 * target_value
  else:
    return value


--- vizier/_src/benchmarks/experimenters/noisy_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for noisy_experimenter."""

import numpy as np

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import noisy_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class NoisyExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere), ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated), ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar), ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass), ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel), ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek), ('Gallagher101Me', bbob.Gallagher101Me))
  def testDeterministicNoiseApply(self, func):
    dim = 2
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))
    noisy_exptr = noisy_experimenter.NoisyExperimenter(
        exptr=exptr, noise_fn=lambda v: v - 1)

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })

    exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name
    unnoised_value = t.final_measurement_or_die.metrics[metric_name].value

    noisy_exptr.evaluate([t])
    noised_value = t.final_measurement_or_die.metrics[metric_name].value
    self.assertEqual(unnoised_value - 1, noised_value)
    self.assertEqual(
        unnoised_value,
        t.final_measurement_or_die.metrics[metric_name + '_before_noise'].value,
    )

  @parameterized.named_parameters(
      ('NO_NOISE', 'NO_NOISE', 1e-5),
      ('SEVERE_ADDITIVE_GAUSSIAN', 'SEVERE_ADDITIVE_GAUSSIAN', 3),
      ('MODERATE_ADDITIVE_GAUSSIAN', 'MODERATE_ADDITIVE_GAUSSIAN', 0.3),
      ('LIGHT_ADDITIVE_GAUSSIAN', 'LIGHT_ADDITIVE_GAUSSIAN', 0.03),
      ('MODERATE_GAUSSIAN', 'MODERATE_GAUSSIAN', 0.05),
      ('SEVERE_GAUSSIAN', 'SEVERE_GAUSSIAN', 0.5),
      ('MODERATE_UNIFORM', 'MODERATE_UNIFORM', 0.2),
      ('SEVERE_UNIFORM', 'SEVERE_UNIFORM', 3.5),
      ('MODERATE_SELDOM_CAUCHY', 'MODERATE_SELDOM_CAUCHY', 10.3),
      ('SEVERE_SELDOM_CAUCHY', 'SEVERE_SELDOM_CAUCHY', 100.3),
  )
  def testGaussianNoiseApply(self, noise: str, delta: float):
    dim = 2
    exptr = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(dim))
    noisy_exptr = noisy_experimenter.NoisyExperimenter.from_type(
        exptr=exptr, noise_type=noise
    )

    parameters = exptr.problem_statement().search_space.parameters
    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })

    exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name
    unnoised_value = t.final_measurement_or_die.metrics[metric_name].value

    noisy_exptr.evaluate([t])
    noised_value1 = t.final_measurement_or_die.metrics[metric_name].value

    noisy_exptr.evaluate([t])
    noised_value2 = t.final_measurement_or_die.metrics[metric_name].value

    # Seldom noise is only injected sporadically.
    if 'SELDOM' not in noise and noise != 'NO_NOISE':
      self.assertNotEqual(noised_value1, noised_value2)
    self.assertAlmostEqual(noised_value1, unnoised_value, delta=delta)
    self.assertAlmostEqual(noised_value2, unnoised_value, delta=delta)

  def testSeedDeterminism(self):
    dim = 2
    seed = 7
    exptr = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(dim)
    )
    noisy_exptr = noisy_experimenter.NoisyExperimenter.from_type(
        exptr=exptr, noise_type='SEVERE_UNIFORM', seed=seed
    )

    parameters = exptr.problem_statement().search_space.parameters
    t = pyvizier.Trial(
        parameters={
            param.name: float(index) for index, param in enumerate(parameters)
        }
    )
    metric_name = exptr.problem_statement().metric_information.item().name

    noise_value_sequence = []
    for _ in range(10):
      noisy_exptr.evaluate([t])
      noise_value_sequence.append(
          t.final_measurement_or_die.metrics[metric_name].value
      )

    # Global NP seed should not affect randomness.
    np.random.seed(0)

    noisy_exptr = noisy_experimenter.NoisyExperimenter.from_type(
        exptr=exptr, noise_type='SEVERE_UNIFORM', seed=seed
    )
    noise_value_sequence_after = []
    for _ in range(10):
      noisy_exptr.evaluate([t])
      noise_value_sequence_after.append(
          t.final_measurement_or_die.metrics[metric_name].value
      )
    self.assertSequenceAlmostEqual(
        noise_value_sequence, noise_value_sequence_after
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/normalizing_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter that normalizes the range of each metric."""

import collections
import copy
from typing import Dict, Sequence

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


# TODO: Allow more sampling methods.
class NormalizingExperimenter(experimenter.Experimenter):
  """Normalizes an Experimenter output via (y - empirical_mean) / empirical_std."""

  def __init__(
      self,
      exptr: experimenter.Experimenter,
      num_normalization_samples: int = 100,
      noise_seed: int = 42,
  ):
    """Normalizing experimenter uses a grid to estimate a normalization constant.

    Args:
      exptr: Experimenter to be normalized.
      num_normalization_samples: Number of samples to determine normalization.
      noise_seed: Random seed for tiebreaking categorical samples.
    """
    self._exptr = exptr
    self._problem_statement = exptr.problem_statement()

    # Convert search space into hypercube, then sample.
    converter = converters.TrialToArrayConverter.from_study_config(
        study_config=self._problem_statement, scale=True, pad_oovs=False
    )
    feature_dim = sum([o.num_dimensions for o in converter.output_specs])

    normalized_samples = np.linspace(
        np.zeros(feature_dim), np.ones(feature_dim), num_normalization_samples
    )

    # Categoricals use argmax and break ties by always choosing the first
    # feasible choice. Let's randomize the tiebreaking.
    noise = np.random.RandomState(noise_seed).normal(
        scale=1e-6, size=(num_normalization_samples, feature_dim)
    )
    normalized_samples += noise

    sampled_params = converter.to_parameters(normalized_samples)
    metrics = collections.defaultdict(list)
    for parameters in sampled_params:
      trial = vz.Trial(parameters=parameters)
      exptr.evaluate([trial])
      measurement = trial.final_measurement
      for name, metric in (measurement.metrics if measurement else {}).items():
        metrics[name].append(metric.value)

    self._norm_means: Dict[str, float] = {}
    self._norm_stds: Dict[str, float] = {}
    for name, grid_values in metrics.items():
      self._norm_means[name] = np.mean(grid_values)
      self._norm_stds[name] = np.std(grid_values) + 1e-7

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    self._exptr.evaluate(suggestions)
    for suggestion in suggestions:
      if suggestion.final_measurement is None:
        continue
      normalized_metrics: Dict[str, vz.Metric] = {}
      for name, metric in suggestion.final_measurement.metrics.items():
        norm_val = metric.value - self._norm_means[name]
        norm_val /= self._norm_stds[name]
        normalized_metrics[name] = vz.Metric(norm_val)
      suggestion.final_measurement.metrics = normalized_metrics

  def __repr__(self):
    return (
        'NormalizingExperimenter with normalization means'
        f' {self._norm_means} and stds {self._norm_stds} on {self._exptr}'
    )


class HyperCubeExperimenter(experimenter.Experimenter):
  """Normalizes the search space into unit hypercube using converter."""

  def __init__(self, exptr: experimenter.Experimenter):
    self._exptr = exptr
    original_problem = exptr.problem_statement()

    converter = converters.TrialToArrayConverter.from_study_config(
        study_config=original_problem, scale=True, pad_oovs=False
    )
    self._converter = converter
    feature_dim = sum([o.num_dimensions for o in converter.output_specs])

    new_space = vz.SearchSpace()  # Setup hypercube search space.
    for i in range(feature_dim):
      new_space.add(vz.ParameterConfig.factory(f'h{i}', bounds=(0.0, 1.0)))
    self._problem_statement = copy.deepcopy(original_problem)
    self._problem_statement.search_space = new_space

    # For simply converting new hypercube parameters to numpy arrays.
    self._eval_converter = converters.TrialToArrayConverter.from_study_config(
        study_config=self._problem_statement, scale=False, pad_oovs=False
    )

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    hypercube_features = self._eval_converter.to_features(suggestions)
    original_params = self._converter.to_parameters(hypercube_features)

    orig_suggestions = copy.deepcopy(suggestions)
    for param_dict, orig_suggestion in zip(original_params, orig_suggestions):
      orig_suggestion.parameters = param_dict

    self._exptr.evaluate(orig_suggestions)

    for suggestion, orig_suggestion in zip(suggestions, orig_suggestions):
      suggestion.final_measurement = orig_suggestion.final_measurement


--- vizier/_src/benchmarks/experimenters/normalizing_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import pyvizier
from vizier._src.benchmarks.experimenters import combo_experimenter
from vizier._src.benchmarks.experimenters import normalizing_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob
from vizier._src.benchmarks.experimenters.synthetic import multiarm

from absl.testing import absltest
from absl.testing import parameterized


class NormalizingExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere),
      ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated),
      ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar),
      ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass),
      ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel),
      ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek),
      ('Gallagher101Me', bbob.Gallagher101Me),
  )
  def testNormalizationApply(self, func):
    dim = 5
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim)
    )
    normalizing_exptr = normalizing_experimenter.NormalizingExperimenter(
        exptr=exptr
    )

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(
        parameters={
            param.name: float(index) for index, param in enumerate(parameters)
        }
    )

    exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name

    normalizing_exptr.evaluate([t])
    normalized_value = t.final_measurement_or_die.metrics[metric_name].value
    self.assertBetween(normalized_value, -10, 10)

  def test_NormalizingCategoricals(self):
    mab_exptr = multiarm.FixedMultiArmExperimenter(
        arms_to_rewards={'0': -1e6, '1': 0.0, '2': 1e6}
    )
    norm_exptr = normalizing_experimenter.NormalizingExperimenter(mab_exptr)
    metric_name = norm_exptr.problem_statement().metric_information.item().name

    for arm in range(3):
      t = pyvizier.Trial(parameters={'arm': str(arm)})
      norm_exptr.evaluate([t])
      normalized_value = t.final_measurement_or_die.metrics[metric_name].value
      self.assertBetween(normalized_value, -10, 10)


class HyperCubeExperimenterTest(parameterized.TestCase):

  def testE2E(self):
    num_boolean_params = 3

    original_exptr = combo_experimenter.ContaminationExperimenter(
        contamination_n_stages=num_boolean_params
    )
    hypercube_exptr = normalizing_experimenter.HyperCubeExperimenter(
        original_exptr
    )

    problem = hypercube_exptr.problem_statement()
    metric_name = problem.metric_information.item().name

    # 3 Booleans -> 6 total coordinates after one-hots.
    self.assertLen(problem.search_space.parameters, 2 * num_boolean_params)

    t = pyvizier.Trial(
        parameters={
            'h0': 0.0,
            'h1': 1.0,
            'h2': 1.0,
            'h3': 0.0,
            'h4': 0.3,
            'h5': 0.6,
        }
    )
    hypercube_exptr.evaluate([t])
    hypercube_value = t.final_measurement_or_die.metrics[metric_name].value

    # Argmax over hypercube one-hots.
    t = pyvizier.Trial(
        parameters={'x_0': 'True', 'x_1': 'False', 'x_2': 'True'}
    )
    original_exptr.evaluate([t])
    original_value = t.final_measurement_or_die.metrics[metric_name].value

    self.assertEqual(hypercube_value, original_value)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/numpy_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Numpy experimenter for wrapping deterministic functions on ndarrays."""

import copy
import logging
import math
from typing import Callable, Sequence

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


def _get_name(f):
  """Gets the name of underlying objects."""
  if hasattr(f, '__name__'):
    return f.__name__
  # Next clause handles functools.partial objects.
  if hasattr(f, 'func') and hasattr(f.func, '__name__'):
    return f.func.__name__
  return repr(f)


class NumpyExperimenter(experimenter.Experimenter):
  """NumpyExperimenters take a deterministic function on ndarrays."""

  def __init__(
      self,
      impl: Callable[[np.ndarray], float],
      problem_statement: vz.ProblemStatement,
  ):
    """NumpyExperimenter with analytic function impl for one metric.

    NumpyExperimenter only supports single objectives, and flat numeric search
    spaces.

    Args:
      impl: Function that scalarizes np.ndarray of shape (dimension,).
      problem_statement: Problem statement.

    Raises:
      ValueError: Non-positive dimension or invalid problem statement.
    """
    dimension = len(problem_statement.search_space.parameters)
    self._impl_name = _get_name(impl)
    logging.info(
        'Initializing NumpyExperimenter with impl=%s, dimension=%s',
        self._impl_name,
        dimension,
    )
    if dimension <= 0:
      raise ValueError(f'Invalid dimension: {dimension}')
    self._dimension = dimension
    self.impl = impl

    if not problem_statement.metric_information.is_single_objective:
      raise ValueError(
          f'Statement should be single objective {problem_statement}'
      )
    if problem_statement.search_space.is_conditional:
      raise ValueError(f'Statement should be flat {problem_statement}')
    for parameter in problem_statement.search_space.parameters:
      if not parameter.type.is_numeric():
        raise ValueError(f'Non-numeric parameters {parameter}')

    objective_metrics = problem_statement.metric_information.of_type(
        vz.MetricType.OBJECTIVE
    )
    self._metric_name = objective_metrics.item().name

    self._problem_statement = copy.deepcopy(problem_statement)
    self._converter = converters.TrialToArrayConverter.from_study_config(
        study_config=self._problem_statement,
        scale=False,
        flip_sign_for_minimization_metrics=False,
    )

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    # Features has shape (num_trials, num_features).
    features = self._converter.to_features(suggestions)
    for idx, suggestion in enumerate(suggestions):
      val = self.impl(features[idx])
      if math.isfinite(val):
        suggestion.complete(vz.Measurement(metrics={self._metric_name: val}))
      else:
        self.problem_statement().search_space.assert_contains(
            suggestions[idx].parameters
        )
        suggestion.complete(
            vz.Measurement(),
            infeasibility_reason='Objective value is not finite: %f' % val,
        )

  def __repr__(self) -> str:
    return f'NumpyExperimenter {{name: {self._impl_name}}}'


class MultiObjectiveNumpyExperimenter(experimenter.Experimenter):
  """Multiobjective variant with variable number of objectives and dimensions.

  We assume that `impl` returns a list of values, one per objective, ordered by
  the metric information in the problem statement.
  """

  def __init__(
      self,
      impl: Callable[[np.ndarray], Sequence[float]],
      problem_statement: vz.ProblemStatement,
  ):
    self._impl = impl
    self._problem_statement = copy.deepcopy(problem_statement)
    self._converter = converters.TrialToArrayConverter.from_study_config(
        study_config=self.problem_statement(),
        scale=False,
        flip_sign_for_minimization_metrics=False,
    )

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    metric_info = self._problem_statement.metric_information
    features = self._converter.to_features(suggestions)
    for i, suggestion in enumerate(suggestions):
      feat = features[i]
      values = self._impl(feat)
      metrics = {mc.name: value for mc, value in zip(metric_info, values)}
      suggestion.complete(vz.Measurement(metrics))

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)


--- vizier/_src/benchmarks/experimenters/numpy_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for numpy_experimenter."""

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class NumpyExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere), ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated), ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar), ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass), ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel), ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek), ('Gallagher101Me', bbob.Gallagher101Me))
  def testNumpyExperimenter(self, func):
    dim = 2
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })

    exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name
    self.assertAlmostEqual(
        func(np.array([0.0, 1.0])),
        t.final_measurement_or_die.metrics[metric_name].value,
    )
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)

  def testNonFinite(self):
    dim = 2
    exptr = numpy_experimenter.NumpyExperimenter(
        impl=lambda x: np.inf,
        problem_statement=bbob.DefaultBBOBProblemStatement(dim))

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t1 = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })
    t2 = pyvizier.Trial(parameters={
        param.name: -float(index) for index, param in enumerate(parameters)
    })

    trials = [t1, t2]
    exptr.evaluate(trials)
    for trial in trials:
      self.assertEmpty(trial.final_measurement_or_die.metrics)
      self.assertTrue(trial.infeasible)

  def testNotInSearchSpace(self):
    exptr = numpy_experimenter.NumpyExperimenter(
        impl=lambda x: x,
        problem_statement=bbob.DefaultBBOBProblemStatement(1),
    )

    t1 = pyvizier.Trial(parameters={'yyyy': 0.0})
    with self.assertRaises(ValueError):
      exptr.evaluate([t1])


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/permuting_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter that permutes the parameters before evaluation."""

import copy
import logging
from typing import Optional, Sequence

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter


class PermutingExperimenter(experimenter.Experimenter):
  """PermutingExperimenter permutes discrete/categorical parameters."""

  def __init__(
      self,
      exptr: experimenter.Experimenter,
      parameters_to_permute: Sequence[str],
      seed: Optional[int] = None,
  ):
    """PermutingExperiment permutes discrete parameter values before passing to exptr.

    Args:
      exptr: Underlying experimenter to be wrapped.
      parameters_to_permute: Parameter names that are to be permuted.
      seed:

    Raises:
      ValueError: Condtional search space.
    """
    self._exptr = exptr
    exptr_problem_statement = exptr.problem_statement()
    self._problem_statement = exptr_problem_statement

    self._rng = np.random.default_rng(seed)
    if exptr_problem_statement.search_space.is_conditional:
      raise ValueError(
          'Search space should not have conditional'
          f' parameters  {exptr_problem_statement}'
      )

    self._parameter_permutation_dict = {}
    for parameter_name in parameters_to_permute:
      parameter = exptr_problem_statement.search_space.get(parameter_name)
      if not np.isfinite(parameter.num_feasible_values):
        raise ValueError(
            f'Parameter to permute {parameter} is continuous.'
            ' Permuting continuous parameters is not supported.'
        )

      permutation_list = self._rng.permuted(parameter.feasible_values)
      permutation_dict = {
          a: b for a, b in zip(parameter.feasible_values, permutation_list)
      }
      self._parameter_permutation_dict[parameter.name] = permutation_dict

  def problem_statement(self) -> pyvizier.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]) -> None:
    """Evaluate the trials after permuting the parameters."""
    previous_parameters = [suggestion.parameters for suggestion in suggestions]
    self._permute(suggestions)
    self._exptr.evaluate(suggestions)
    # Replace stored previous parameters.
    for parameters, suggestion in zip(previous_parameters, suggestions):
      suggestion.parameters = parameters

  def _permute(self, suggestions: Sequence[pyvizier.Trial]) -> None:
    """Permutes parameter values in place."""
    for suggestion in suggestions:
      new_parameters = {}
      for name, parameter in suggestion.parameters.items():
        if name in self._parameter_permutation_dict:
          permutation_dict = self._parameter_permutation_dict[name]
          logging.info('Permuting %s ', permutation_dict)
          new_parameters[name] = permutation_dict[parameter.value]
        else:
          new_parameters[name] = parameter
      suggestion.parameters = new_parameters

  def __repr__(self):
    return f'PermutingExperimenter on {str(self._exptr)}'


--- vizier/_src/benchmarks/experimenters/permuting_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import discretizing_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters import permuting_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class PermutingExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere),
      ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated),
      ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar),
      ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass),
      ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel),
      ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek),
      ('Gallagher101Me', bbob.Gallagher101Me),
  )
  def testNumpyExperimenter(self, func):
    dim = 3
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim)
    )

    # Asserts parameters are the same.
    parameters = list(exptr.problem_statement().search_space.parameters)
    self.assertLen(parameters, dim)

    discretization = {
        parameters[0].name: ['0.1', '0.5', '-0.1'],
        parameters[1].name: [0.2, 1.2, 2.2],
    }

    dis_exptr = discretizing_experimenter.DiscretizingExperimenter(
        exptr, discretization
    )
    discretized_parameters = [
        p.name
        for p in dis_exptr.problem_statement().search_space.parameters
        if np.isfinite(p.num_feasible_values)
    ]
    self.assertLen(discretized_parameters, 2)
    permuted_exptr = permuting_experimenter.PermutingExperimenter(
        dis_exptr, discretized_parameters, seed=0
    )

    self.assertEqual(
        permuted_exptr.problem_statement(), dis_exptr.problem_statement()
    )

    parameters = {
        parameters[0].name: '-0.1',
        parameters[1].name: 1.2,
        parameters[2].name: 1.5,
    }
    t = pyvizier.Trial(parameters=parameters)

    permuted_exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name
    self.assertNotAlmostEqual(
        func(np.array([-0.1, 1.2, 1.5])),
        t.final_measurement_or_die.metrics[metric_name].value,
    )
    self.assertEqual(t.status, pyvizier.TrialStatus.COMPLETED)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/shifting_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter that shifts the values of the parameters in Evaluation."""

import copy
from typing import Sequence

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


class ShiftingExperimenter(experimenter.Experimenter):
  """ShiftingExperimenter shifts the parameters of suggestions in Evaluate."""

  def __init__(
      self,
      exptr: experimenter.Experimenter,
      shift: np.ndarray,
      should_restrict=True,
  ):
    """ShiftingExperiment shifts parameter values before passing to exptr.

    Currently only supports flat double search spaces. Note that when
    should_restrict is True, the parameter bounds of the search space are
    RESTRICTED to never cause a parameter to
    exceed the underlying experimenter's parameter bounds, so the problem
    statement will change.

    Args:
      exptr: Underlying experimenter to be wrapped.
      shift: Shift that broadcasts to array of shape (dimension,).
      should_restrict: Whether to restrict the parameter bounds of search space.

    Raises:
      ValueError: Non-positive dimension or non-broadcastable/large shift.
    """
    self._exptr = exptr
    exptr_problem_statement = exptr.problem_statement()

    if exptr_problem_statement.search_space.is_conditional:
      raise ValueError(
          'Search space should not have conditional'
          f' parameters  {exptr_problem_statement}'
      )
    dimension = len(exptr_problem_statement.search_space.parameters)
    if dimension <= 0:
      raise ValueError(f'Invalid dimension: {dimension}')
    try:
      # Attempts a broadcast to check broadcasting.
      self._shift = np.broadcast_to(shift, (dimension,))
    except ValueError as broadcast_err:
      raise ValueError(
          f'Shift {shift} is not broadcastable for dim: {dimension}.\n'
      ) from broadcast_err

    # Converter should be in the underlying extpr space.
    self._converter = converters.TrialToArrayConverter.from_study_config(
        study_config=exptr_problem_statement,
        scale=False,
        should_clip=should_restrict,
    )

    self._problem_statement = copy.deepcopy(exptr_problem_statement)
    if should_restrict:
      self._problem_statement.search_space = pyvizier.SearchSpace()

      for parameter, shift in zip(
          exptr_problem_statement.search_space.parameters, self._shift
      ):
        if parameter.type != pyvizier.ParameterType.DOUBLE:
          raise ValueError(f'Non-double parameters {parameter}')
        if (bounds := parameter.bounds) is None:
          raise ValueError(f'Parameter {parameter} has no bounds')

        if abs(shift) >= bounds[1] - bounds[0]:
          raise ValueError(
              f'Bounds {bounds} may need to be extended'
              f'as shift {shift} is too large '
          )
        # Shift the bounds to maintain valid bounds.
        if shift >= 0:
          new_bounds = (bounds[0] + shift, bounds[1])
        else:
          new_bounds = (bounds[0], bounds[1] + shift)
        self._problem_statement.search_space.add(
            pyvizier.ParameterConfig.factory(
                name=parameter.name,
                bounds=new_bounds,
                scale_type=parameter.scale_type,
                default_value=parameter.default_value,
                external_type=parameter.external_type,
            ),
        )

  def problem_statement(self) -> pyvizier.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]) -> None:
    """Evaluate the trials after shifting their parameters by -shift."""
    previous_parameters = [suggestion.parameters for suggestion in suggestions]
    self._offset(suggestions, self._shift)
    self._exptr.evaluate(suggestions)
    # Must replace stored previous parameters since offsetting may clip.
    for parameters, suggestion in zip(previous_parameters, suggestions):
      suggestion.parameters = parameters

  def _offset(
      self, suggestions: Sequence[pyvizier.Trial], shift: np.ndarray
  ) -> None:
    """Offsets parameter values (OOB values are clipped)."""
    for suggestion in suggestions:
      features = self._converter.to_features([suggestion])
      new_parameters = self._converter.to_parameters(features - shift)[0]
      suggestion.parameters = new_parameters

  def __repr__(self):
    return f'ShiftingExperimenter({self._shift}) on {str(self._exptr)}'


--- vizier/_src/benchmarks/experimenters/shifting_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for shifting_experimenter."""

import numpy as np
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters import shifting_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class ShiftingExperimenterTest(parameterized.TestCase):

  @parameterized.named_parameters(
      ('Sphere', bbob.Sphere), ('Rastrigin', bbob.Rastrigin),
      ('BuecheRastrigin', bbob.BuecheRastrigin),
      ('LinearSlope', bbob.LinearSlope),
      ('AttractiveSector', bbob.AttractiveSector),
      ('StepEllipsoidal', bbob.StepEllipsoidal),
      ('RosenbrockRotated', bbob.RosenbrockRotated), ('Discus', bbob.Discus),
      ('BentCigar', bbob.BentCigar), ('SharpRidge', bbob.SharpRidge),
      ('DifferentPowers', bbob.DifferentPowers),
      ('Weierstrass', bbob.Weierstrass), ('SchaffersF7', bbob.SchaffersF7),
      ('SchaffersF7IllConditioned', bbob.SchaffersF7IllConditioned),
      ('GriewankRosenbrock', bbob.GriewankRosenbrock),
      ('Schwefel', bbob.Schwefel), ('Katsuura', bbob.Katsuura),
      ('Lunacek', bbob.Lunacek), ('Gallagher101Me', bbob.Gallagher101Me))
  def test_numpy_experimenter(self, func):
    dim = 2
    shift = 1.2
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))
    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift))

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })
    t_shifted = pyvizier.Trial(
        parameters={
            param.name: float(index) - shift
            for index, param in enumerate(parameters)
        }
    )

    exptr.evaluate([t_shifted])
    shifted_exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name

    self.assertAlmostEqual(
        t_shifted.final_measurement_or_die.metrics[metric_name].value,
        t.final_measurement_or_die.metrics[metric_name].value,
    )
    self.assertEqual(t.status, t_shifted.status)

    # Check parameter bounds are shifted.
    shifted_parameters = shifted_exptr.problem_statement(
    ).search_space.parameters
    for param, shifted_param in zip(parameters, shifted_parameters):
      self.assertEqual(param.bounds[0] + shift, shifted_param.bounds[0])
      self.assertEqual(param.bounds[1], shifted_param.bounds[1])

  def test_evaluate_shift(self):
    dim = 2
    shift = np.array([1.2, -2.3])
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))
    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift))

    parameters = exptr.problem_statement().search_space.parameters
    self.assertLen(parameters, dim)

    t = pyvizier.Trial(parameters={
        param.name: float(index) for index, param in enumerate(parameters)
    })
    t_shifted = pyvizier.Trial(
        parameters={
            param.name: float(index) - shift[index]
            for index, param in enumerate(parameters)
        }
    )

    exptr.evaluate([t_shifted])
    shifted_exptr.evaluate([t])
    metric_name = exptr.problem_statement().metric_information.item().name

    self.assertAlmostEqual(
        t_shifted.final_measurement_or_die.metrics[metric_name].value,
        t.final_measurement_or_die.metrics[metric_name].value,
    )
    self.assertEqual(t.status, t_shifted.status)
    self.assertNotEqual(t.parameters, t_shifted.parameters)

  def test_shift_backward(self):
    dim = 2
    shift = np.array([1.2, -2.3])
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))
    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift))
    # Test shift within bounds
    trial = pyvizier.Trial(parameters={'x0': 3.0, 'x1': 1.0})
    shifted_exptr._offset([trial], shift=-shift)
    self.assertEqual(
        trial.parameters.as_dict(), {'x0': 3.0 + 1.2, 'x1': 1.0 - 2.3}
    )
    # Test shift in out of bounds
    trial = pyvizier.Trial(parameters={'x0': -5.0, 'x1': 5.0})
    shifted_exptr._offset([trial], shift=shift)
    self.assertEqual(trial.parameters.as_dict(), {'x0': -5.0, 'x1': 5.0})

  def test_shift_forward(self):
    dim = 2
    shift = np.array([1.2, -2.3])
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))
    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift))
    # Test shift within bounds
    trial = pyvizier.Trial(parameters={'x0': 3.0, 'x1': 1.0})
    shifted_exptr._offset([trial], shift=shift)
    self.assertEqual(
        trial.parameters.as_dict(), {'x0': 3.0 - 1.2, 'x1': 1.0 + 2.3}
    )
    # Test shift in out of bounds
    trial = pyvizier.Trial(parameters={'x0': 5.0, 'x1': -5.0})
    shifted_exptr._offset([trial], shift=-shift)
    self.assertEqual(trial.parameters.as_dict(), {'x0': 5.0, 'x1': -5.0})

  @parameterized.parameters((True,), (False,))
  def test_shift_forward_oob(self, should_restrict):
    dim = 2
    shift = np.array([-2.2, 2.3])
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim)
    )
    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift), should_restrict=should_restrict
    )
    # Test OOB shifts does not change parameter values.
    trial = pyvizier.Trial(parameters={'x0': 3.0, 'x1': 1.0})
    shifted_exptr.evaluate([trial])
    self.assertEqual(trial.parameters.as_dict(), {'x0': 3.0, 'x1': 1.0})

    # Test OOB shifts stay within bounds.
    shifted_exptr._offset([trial], shift=shift)
    # x0 is shifted OOB, so clip at 5.0 if should_restrict=True.
    if should_restrict:
      self.assertEqual(trial.parameters.as_dict(), {'x0': 5.0, 'x1': 1.0 - 2.3})
    else:
      self.assertEqual(trial.parameters.as_dict(), {'x0': 5.2, 'x1': 1.0 - 2.3})

  def test_large_shift(self):
    dim = 2
    shift = np.array([10.2, 20.3])
    func = bbob.Sphere
    exptr = numpy_experimenter.NumpyExperimenter(
        func, bbob.DefaultBBOBProblemStatement(dim))

    with self.assertRaisesRegex(ValueError, 'is too large'):
      shifting_experimenter.ShiftingExperimenter(
          exptr=exptr, shift=np.asarray(shift))

    shifted_exptr = shifting_experimenter.ShiftingExperimenter(
        exptr=exptr, shift=np.asarray(shift), should_restrict=False
    )
    parameters = exptr.problem_statement().search_space.parameters
    self.assertEqual(
        parameters, shifted_exptr.problem_statement().search_space.parameters
    )
    t = pyvizier.Trial(
        parameters={
            param.name: float(index) for index, param in enumerate(parameters)
        }
    )
    shifted_exptr.evaluate([t])

if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/sign_flip_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Flips metric goal and value signs."""
from typing import Sequence
from vizier import pyvizier
from vizier._src.benchmarks.experimenters import experimenter as experimenter_lib


class SignFlipExperimenter(experimenter_lib.Experimenter):
  """Flips the goal and objective values of the wrapped Experimenter."""

  def __init__(
      self,
      experimenter: experimenter_lib.Experimenter,
      flip_objectives_only: bool = True,
  ):
    """Init.

    Args:
      experimenter: Original experimenter to flip.
      flip_objectives_only: Whether to only flip objective metrics in trials
        corresponding to problem statement objectives. If True, auxiliary
        metrics will not be flipped.
    """
    self._experimenter = experimenter
    self._flip_objectives_only = flip_objectives_only
    self._original_objectives = {
        m_config.name: m_config
        for m_config in self._experimenter.problem_statement().metric_information
    }

  def problem_statement(self) -> pyvizier.ProblemStatement:
    problem = self._experimenter.problem_statement()
    for metric_config in problem.metric_information:
      if metric_config.goal.is_maximize:
        metric_config.goal = pyvizier.ObjectiveMetricGoal.MINIMIZE
      elif metric_config.goal.is_minimize:
        metric_config.goal = pyvizier.ObjectiveMetricGoal.MAXIMIZE
    return problem

  def evaluate(self, suggestions: Sequence[pyvizier.Trial]):
    self._experimenter.evaluate(suggestions)
    for suggestion in suggestions:
      if suggestion.final_measurement is None:
        continue

      metric_dict = {}
      for name, metric in suggestion.final_measurement.metrics.items():
        if self._flip_objectives_only and name in self._original_objectives:
          metric_dict[name] = pyvizier.Metric(value=-1.0 * metric.value)
        elif not self._flip_objectives_only:
          metric_dict[name] = pyvizier.Metric(value=-1.0 * metric.value)
        else:
          metric_dict[name] = metric
      suggestion.final_measurement.metrics = metric_dict


--- vizier/_src/benchmarks/experimenters/sign_flip_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import copy

from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import noisy_experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters import sign_flip_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


# TODO: Add multimetric test once we have a multi-objective
# experimenter.
class SignFlipExperimenterTest(parameterized.TestCase):

  @parameterized.parameters((True,), (False))
  def test_flipped_bbob(self, flip_objectives_only: bool):
    exptr = numpy_experimenter.NumpyExperimenter(
        bbob.Sphere, bbob.DefaultBBOBProblemStatement(2)
    )
    exptr = noisy_experimenter.NoisyExperimenter(
        exptr, noise_fn=lambda v: v - 1
    )
    flipped_exptr = sign_flip_experimenter.SignFlipExperimenter(
        exptr, flip_objectives_only
    )

    metric_name = exptr.problem_statement().single_objective_metric_name
    self.assertEqual(
        flipped_exptr.problem_statement().metric_information.item().goal,
        vz.ObjectiveMetricGoal.MAXIMIZE,
    )

    suggestion_for_original = vz.Trial(parameters={'x0': 0.2, 'x1': -3.2})
    suggestion_for_flipped = copy.deepcopy(suggestion_for_original)

    exptr.evaluate([suggestion_for_original])
    flipped_exptr.evaluate([suggestion_for_flipped])

    self.assertEqual(
        suggestion_for_original.final_measurement_or_die.metrics[
            metric_name
        ].value,
        -1.0
        * suggestion_for_flipped.final_measurement_or_die.metrics[
            metric_name
        ].value,
    )

    aux_metric_name = metric_name + '_before_noise'
    if flip_objectives_only:
      self.assertEqual(
          suggestion_for_original.final_measurement_or_die.metrics[
              aux_metric_name
          ].value,
          suggestion_for_flipped.final_measurement_or_die.metrics[
              aux_metric_name
          ].value,
      )
    else:
      self.assertEqual(
          suggestion_for_original.final_measurement_or_die.metrics[
              aux_metric_name
          ].value,
          -1.0
          * suggestion_for_flipped.final_measurement_or_die.metrics[
              aux_metric_name
          ].value,
      )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/sparse_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Sparse experimenter.

The SparseExperimenter class wraps another experimenter and expands its search
space with placeholder parameters that don't impact the evaluation.

This experimenter allows testing that a designer/policy is able to optimize an
objective function when only a subset of its parameters affect the function
values.
"""

import copy
from typing import Optional, Sequence

import attr
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


# These default values are used for defining the placeholder parameters.
_DEFAULT_FLOAT_BOUNDS = (-5.0, 5.0)
_DEFAULT_INT_BOUNDS = (-5, 5)

_DEFAULT_DISCRETE_FEASIBLE_VALUES = (0, 1, 2, 3, 4)
_DEFAULT_CATEGORICAL_FEASIBLE_VALUES = ('a', 'b', 'c', 'd', 'e', 'f')


class SparseExperimenter(experimenter.Experimenter):
  """Sparse experimenter.

  The sparse experimenter allows expanding an experimenter's search space with
  additional placeholder parameters that don't have affect during evaluation.
  """

  def __init__(
      self,
      experiment: experimenter.Experimenter,
      search_space: vz.SearchSpace,
      prefix: str = '_SPARSE',
  ):
    """Initializes a sparse experimenter.

    The sparse experimenter is constructed by adding a copy of parameters from
    $search_space to the experimenter search space. Newly added parameter names
    are prefixed with $prefix.

    Arguments:
      experiment: An experimenter to add sparse parameters to.
      search_space: A search space to use for the sparse parameters.
      prefix: The sparse parameter name prefix (shouldn't already exist in the
        search space of 'experiment').
    """
    super().__init__()
    self._sparse_param_prefix = prefix
    self._experimenter = experiment
    self._search_space = copy.deepcopy(search_space)
    self._prefix = prefix

    problem = experiment.problem_statement()
    for pc in search_space.parameters:
      # Add a copy of the parameter config with a modified name.
      problem.search_space.add(attr.evolve(pc, name=prefix + '_' + pc.name))
    self._problem_statement = problem

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    """Evaluates and completes trials on sparse subset parameters."""
    # Evaluates trials on non-sparse parameters only determined by the prefix.
    original_params = []
    for trial in suggestions:
      original_params.append(trial.parameters)
      trial.parameters = {
          param_name: param_value
          for param_name, param_value in trial.parameters.items()
          if not param_name.startswith(self._sparse_param_prefix)
      }
    self._experimenter.evaluate(suggestions)
    # Restores the original parameters with the sparse parameters.
    for trial, params in zip(suggestions, original_params):
      trial.parameters = params

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  @classmethod
  def create(
      cls,
      experiment: experimenter.Experimenter,
      float_count: int,
      int_count: int,
      discrete_count: int,
      categorical_count: int,
      float_min_value: Optional[float] = None,
      float_max_value: Optional[float] = None,
      int_min_value: Optional[int] = None,
      int_max_value: Optional[int] = None,
      discrete_feasible_values: Optional[list[int]] = None,
      categorical_feasible_values: Optional[list[str]] = None,
  ) -> 'SparseExperimenter':
    """Creates a sparse experimenter with different parameter types.

    Arguments:
      experiment: The experimenter to add sparsity to.
      float_count: The number of FLOAT sparse parameters to add.
      int_count: The number of INT sparse parameters to add.
      discrete_count: The number of DISCRETE sparse parameters to add.
      categorical_count: The number of CATEGORICAL sparse parameters to add.
      float_min_value: The FLOAT sparse parameter min value.
      float_max_value: The INTEGER sparse parameter max value.
      int_min_value: The FLOAT sparse parameter min value.
      int_max_value: The INTEGER sparse parameter max value.
      discrete_feasible_values: The DISCRETE sparse parameter feasible values.
      categorical_feasible_values: The CTEGORICAL sparse parameter feasible
        values.

    Returns:
      The sparse experimenter.
    """
    sparse_search_space = vz.SearchSpace()
    for idx in range(float_count):
      sparse_search_space.root.add_float_param(
          name='FLOAT' + str(idx),
          min_value=float_min_value or _DEFAULT_FLOAT_BOUNDS[0],
          max_value=float_max_value or _DEFAULT_FLOAT_BOUNDS[1],
      )
    for idx in range(int_count):
      sparse_search_space.root.add_int_param(
          name='INT' + str(idx),
          min_value=float_min_value or _DEFAULT_INT_BOUNDS[0],
          max_value=float_max_value or _DEFAULT_INT_BOUNDS[1],
      )
    for idx in range(discrete_count):
      sparse_search_space.root.add_discrete_param(
          name='DISCRETE' + str(idx),
          feasible_values=categorical_feasible_values
          or _DEFAULT_DISCRETE_FEASIBLE_VALUES,
      )
    for idx in range(categorical_count):
      sparse_search_space.root.add_categorical_param(
          name='CATEGORICAL' + str(idx),
          feasible_values=categorical_feasible_values
          or _DEFAULT_CATEGORICAL_FEASIBLE_VALUES,
      )
    return cls(experiment, sparse_search_space)


--- vizier/_src/benchmarks/experimenters/sparse_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Test suite for SparseExperimenter."""

import attr
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter_factory
from vizier._src.benchmarks.experimenters import sparse_experimenter

from absl.testing import absltest
from absl.testing import parameterized


class SparseExperimenterTest(parameterized.TestCase):

  @parameterized.parameters(
      vz.ParameterConfig.factory('_', bounds=(0.0, 5.0)),
      vz.ParameterConfig.factory('_', bounds=(0, 5)),
      vz.ParameterConfig.factory('_', feasible_values=[1, 2, 3]),
      vz.ParameterConfig.factory('_', feasible_values=['a', 'b', 'c']),
  )
  def test_sparse_problem_statement_and_evaluate(self, sparse_param_config):
    sphere_experimenter = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=2
    )()
    search_space = vz.SearchSpace()
    for idx in range(8):
      search_space.add(attr.evolve(sparse_param_config, name=str(idx)))

    experimenter = sparse_experimenter.SparseExperimenter(
        experiment=sphere_experimenter,
        search_space=search_space,
        prefix='__SPARSE',
    )
    # Test that sparse parameters were added.
    self.assertLen(experimenter.problem_statement().search_space.parameters, 10)
    trial = vz.Trial()
    for pc in experimenter.problem_statement().search_space.parameters:
      trial.parameters[pc.name] = 2.0
    # Test that the evaluation uses only the non-sparse parameters.
    experimenter.evaluate([trial])
    self.assertEqual(
        trial.final_measurement_or_die.metrics['bbob_eval'].value, 8.0
    )
    # Test that the evaluated trial parameters remained the same.
    self.assertLen(trial.parameters, 10)
    for trial_param_name, exptr_param_config in zip(
        trial.parameters,
        sphere_experimenter.problem_statement().search_space.parameters,
    ):
      self.assertEqual(trial_param_name, exptr_param_config.name)

  @parameterized.parameters(
      {
          'float_count': 1,
          'int_count': 2,
          'discrete_count': 4,
          'cat_count': 3,
      },
      {'float_count': 10, 'int_count': 0, 'discrete_count': 0, 'cat_count': 0},
      {'float_count': 0, 'int_count': 0, 'discrete_count': 0, 'cat_count': 0},
      {'float_count': 0, 'int_count': 0, 'discrete_count': 3, 'cat_count': 5},
  )
  def test_sparse_experimenter_create(
      self, float_count, int_count, discrete_count, cat_count
  ):
    sphere_experimenter = experimenter_factory.BBOBExperimenterFactory(
        name='Sphere', dim=2
    )()
    experimenter = sparse_experimenter.SparseExperimenter.create(
        experiment=sphere_experimenter,
        float_count=float_count,
        int_count=int_count,
        discrete_count=discrete_count,
        categorical_count=cat_count,
    )

    # Test that sparse parameters were added corretly to the search space.
    def _count_param_by_type(problem):
      """Count the number of parameters by parameter type."""
      counts = {type: 0 for type in vz.ParameterType}
      for param in problem.search_space.parameters:
        counts[param.type] += 1
      return counts

    counts = _count_param_by_type(experimenter.problem_statement())
    self.assertEqual(counts[vz.ParameterType.DOUBLE], float_count + 2)
    self.assertEqual(counts[vz.ParameterType.INTEGER], int_count)
    self.assertEqual(counts[vz.ParameterType.CATEGORICAL], cat_count)
    self.assertEqual(counts[vz.ParameterType.DISCRETE], discrete_count)

    trial = vz.Trial()
    for pc in experimenter.problem_statement().search_space.parameters:
      trial.parameters[pc.name] = 2.0
    # Test that the evaluation uses only the non-sparse parameters.
    experimenter.evaluate([trial])
    self.assertEqual(
        trial.final_measurement_or_die.metrics['bbob_eval'].value, 8.0
    )
    # Test that the evaluated trial parameters remained the same.
    total_param_count = 2 + int_count + cat_count + discrete_count + float_count
    self.assertLen(trial.parameters, total_param_count)
    for trial_param_name, exptr_param_config in zip(
        trial.parameters,
        sphere_experimenter.problem_statement().search_space.parameters,
    ):
      self.assertEqual(trial_param_name, exptr_param_config.name)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/surrogate_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Uses a predictor as a surrogate model for evaluations."""
import copy
from typing import Sequence

import jax
from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


class PredictorExperimenter(experimenter.Experimenter):
  """Uses predictor as a surrogate for objective value."""

  def __init__(
      self,
      predictor: vza.Predictor,
      problem_statement: vz.ProblemStatement,
      seed: int = 0,
  ):
    """Init.

    Args:
      predictor: Surrogate model for extrapolating unseen trials.
      problem_statement: Original problem statement for suggestions.
      seed: RNG seed for predictor.
    """
    self._predictor = predictor
    self._problem_statement = problem_statement
    self._rng = jax.random.PRNGKey(seed)
    self._objective_name = self._problem_statement.single_objective_metric_name

  def evaluate(self, suggestions: Sequence[vz.Trial]):
    prediction = self._predictor.predict(suggestions, self._rng)
    for i, suggestion in enumerate(suggestions):
      evaluation = prediction.mean[i]
      suggestion.complete(
          vz.Measurement(metrics={self._objective_name: evaluation})
      )

  def problem_statement(self) -> vz.ProblemStatement:
    return copy.deepcopy(self._problem_statement)

  def __repr__(self) -> str:
    return (
        f'PredictorExperimenter on problem {self._problem_statement} with'
        f' {self._predictor}'
    )


--- vizier/_src/benchmarks/experimenters/surrogate_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for surrogate_experimenter."""
from typing import Sequence, Optional
import jax

from vizier import algorithms as vza
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import quasi_random
from vizier._src.benchmarks.experimenters import surrogate_experimenter
from vizier.testing import test_studies

from absl.testing import absltest


class DummyPredictor(vza.Predictor):

  def predict(
      self,
      trials: Sequence[vz.TrialSuggestion],
      rng: Optional[jax.Array] = None,
      num_samples: Optional[int] = None,
  ) -> vza.Prediction:
    num_trials = len(trials)
    mean = jax.random.normal(key=rng, shape=(num_trials,))
    stddev = jax.random.normal(key=rng, shape=(num_trials,))
    return vza.Prediction(mean=mean, stddev=stddev)


class PredictorExperimenterTest(absltest.TestCase):

  def test_e2e(self):
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    predictor = DummyPredictor()
    experimenter = surrogate_experimenter.PredictorExperimenter(
        predictor=predictor, problem_statement=problem
    )

    quasi_random_sampler = quasi_random.QuasiRandomDesigner(
        problem.search_space
    )
    suggestions = quasi_random_sampler.suggest(count=7)
    trials = [suggestion.to_trial() for suggestion in suggestions]
    experimenter.evaluate(trials)

    for trial in trials:
      self.assertEqual(trial.status, vz.TrialStatus.COMPLETED)
      self.assertContainsSubset(
          trial.final_measurement_or_die.metrics.keys(), ['metric']
      )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/switch_experimenter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Applies the noise function to each metric in final measurement."""

import copy
from typing import Sequence
import attrs
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


@attrs.define
class SwitchExperimenter(experimenter.Experimenter):
  """Creates conditional search space from multiple experimenters via a switch."""

  experimenters: Sequence[experimenter.Experimenter] = attrs.field()

  _switch_param_name: str = attrs.field(default='switch')
  _metric_name: str = attrs.field(default='switch_metric')

  # Created in __attrs_post_init__.
  _exptr_problems: Sequence[vz.ProblemStatement] = attrs.field(init=False)
  _exptr_objective_names: Sequence[str] = attrs.field(init=False)

  def __attrs_post_init__(self):
    self._exptr_problems = [
        exp.problem_statement() for exp in self.experimenters
    ]
    self._exptr_objective_names = [
        ps.metric_information.item().name for ps in self._exptr_problems
    ]

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    for trial in suggestions:
      exptr_index = trial.parameters[self._switch_param_name].value

      trial_copy = copy.deepcopy(trial)
      self.experimenters[exptr_index].evaluate([trial_copy])

      if trial_copy.final_measurement is None:
        continue

      val = trial_copy.final_measurement.metrics[
          self._exptr_objective_names[exptr_index]
      ]
      trial.complete(vz.Measurement(metrics={self._metric_name: val}))

  def problem_statement(self) -> vz.ProblemStatement:
    problem_statement = vz.ProblemStatement()

    children = []
    for i, problem in enumerate(self._exptr_problems):
      for pc in problem.search_space.parameters:
        children.append(([i], pc))

    switching_param = vz.ParameterConfig.factory(
        self._switch_param_name,
        feasible_values=range(len(self.experimenters)),
        children=children,
    )
    problem_statement.search_space.add(switching_param)
    problem_statement.metric_information.append(
        vz.MetricInformation(
            name=self._metric_name, goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    return problem_statement


--- vizier/_src/benchmarks/experimenters/switch_experimenter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import numpy_experimenter
from vizier._src.benchmarks.experimenters import switch_experimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob
from absl.testing import absltest


class SwitchExperimenterTest(absltest.TestCase):

  def test_switch_experimenter(self):
    exptr0 = numpy_experimenter.NumpyExperimenter(
        lambda x: 0.0 * x, bbob.DefaultBBOBProblemStatement(1, metric_name='0')
    )
    exptr1 = numpy_experimenter.NumpyExperimenter(
        lambda x: 1.0 * x, bbob.DefaultBBOBProblemStatement(1, metric_name='1')
    )
    switch_exptr = switch_experimenter.SwitchExperimenter([exptr0, exptr1])

    t0 = vz.Trial(parameters={'switch': 0, 'x0': 100.0})
    t1 = vz.Trial(parameters={'switch': 1, 'x0': 100.0})

    switch_exptr.evaluate([t0, t1])

    self.assertEqual(t0.final_measurement.metrics['switch_metric'].value, 0.0)  # pytype:disable=attribute-error
    self.assertEqual(t1.final_measurement.metrics['switch_metric'].value, 100.0)  # pytype:disable=attribute-error


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/bbob.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter function implementations for BBOB functions."""

import hashlib
import math
from typing import Any, Callable, Sequence
import numpy as np
from scipy import stats
from vizier import pyvizier


def DefaultBBOBProblemStatement(
    dimension: int,
    *,
    metric_name="bbob_eval",
    min_value: float = -5.0,
    max_value: float = 5.0,
    scale_type=None,
) -> pyvizier.ProblemStatement:
  """Returns default BBOB ProblemStatement for given dimension."""
  problem_statement = pyvizier.ProblemStatement()
  space = problem_statement.search_space
  for dim in range(dimension):
    space.root.add_float_param(
        name=f"x{dim}",
        min_value=min_value,
        max_value=max_value,
        scale_type=scale_type,
    )
  problem_statement.metric_information.append(
      pyvizier.MetricInformation(
          name=metric_name, goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
  return problem_statement


## Utility Functions for BBOB.
def LambdaAlpha(alpha: float, dim: int) -> np.ndarray:
  """The BBOB LambdaAlpha matrix creation function.

  Args:
    alpha: Function parameter.
    dim: Dimension of matrix created.

  Returns:
    Diagonal matrix of dimension dim with values determined by alpha.
  """
  lambda_alpha = np.zeros([dim, dim])
  for i in range(dim):
    exp = (0.5 * (float(i) / (dim - 1))) if dim > 1 else 0.5
    lambda_alpha[i, i] = alpha**exp
  return lambda_alpha


def ArrayMap(vector: np.ndarray, fn: Callable[[float], float]) -> np.ndarray:
  """Create a new array by mapping fn() to each element of the original array.

  Args:
    vector: ndarray to be mapped.
    fn: scalar function for mapping.

  Returns:
    New ndarray be values mapped by fn.
  """
  results = np.zeros(vector.shape)
  for i, v in enumerate(vector.flat):
    results.flat[i] = fn(v)
  return results


def Tosz(element: float) -> float:
  """The BBOB T_osz function.

  Args:
    element: float input.

  Returns:
    Tosz(input).
  """
  x_carat = 0.0 if element == 0 else math.log(abs(element))
  c1 = 10.0 if element > 0 else 5.5
  c2 = 7.9 if element > 0 else 3.1
  return np.sign(element) * math.exp(
      x_carat + 0.049 * (math.sin(c1 * x_carat) + math.sin(c2 * x_carat)))


def Tasy(vector: np.ndarray, beta: float) -> np.ndarray:
  """The BBOB Tasy function.

  Args:
    vector: ndarray
    beta: Function parameter

  Returns:
    ndarray with values determined by beta.
  """
  dim = len(vector)
  result = np.zeros([dim, 1])
  for i, val in enumerate(vector.flat):
    if val > 0:
      t = i / (dim - 1.0) if dim > 1 else 1
      exp = 1 + beta * t * (val**0.5)
    else:
      exp = 1
    result[i] = val**exp
  return result


def SIndex(dim: int, to_sz) -> float:
  """Calculate the BBOB s_i.

  Assumes i is 0-index based.

  Args:
    dim: dimension
    to_sz: values

  Returns:
    float representing SIndex(i, d, to_sz).
  """
  s = np.zeros([
      dim,
  ])
  for i in range(dim):
    if dim > 1:
      s[i] = 10**(0.5 * (i / (dim - 1.0)))
    else:
      s[i] = 10**0.5
    if i % 2 == 0 and to_sz[i] > 0:
      s[i] *= 10
  return s


def Fpen(vector: np.ndarray) -> float:
  """The BBOB Fpen function.

  Args:
    vector: ndarray.

  Returns:
    float representing Fpen(vector).
  """
  return sum([max(0.0, (abs(x) - 5.0))**2 for x in vector.flat])


def _IntSeeds(any_seeds: Sequence[Any], *, byte_length: int = 4) -> list[int]:
  """Array of integers that can be used as random state seed."""
  int_seeds = []
  for s in any_seeds:
    # Encode into 4 byte_length worth of a hexadecimal string.
    hashed = hashlib.shake_128(str(s).encode("utf-8")).hexdigest(byte_length)
    int_seeds.append(int(hashed, 16))
  return int_seeds


def _ToFloat(a: int, b: np.ndarray) -> np.ndarray:
  """Convert a%b where b is an int into a float on [-0.5, 0.5]."""
  return (np.int64(a) % b) / np.float64(b) - 0.5


def _R(dim: int, seed: int, *moreseeds: Any) -> np.ndarray:
  """Returns an orthonormal rotation matrix.

  Args:
    dim: size of the resulting matrix.
    seed: int seed. If set to 0, this function returns an identity matrix
      regardless of *moreseeds.
    *moreseeds: Additional parameters to include in the hash. Arguments are
      converted to strings first.

  Returns:
    Array of shape (dim, dim), representing a rotation matrix.
  """
  if seed == 0:
    return np.identity(dim)
  rng = np.random.default_rng(_IntSeeds(((seed, dim) + moreseeds)))
  return stats.special_ortho_group.rvs(dim, random_state=rng)


## BBOB Functions.
def Sphere(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Sphere function."""
  del seed
  return float(np.sum(arr * arr))


def Rastrigin(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Rastrigin function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z = np.matmul(_R(dim, seed, b"R"), arr)
  z = Tasy(ArrayMap(z, Tosz), 0.2)
  z = np.matmul(_R(dim, seed, b"Q"), z)
  z = np.matmul(LambdaAlpha(10.0, dim), z)
  z = np.matmul(_R(dim, seed, b"R"), z)
  if z.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z.shape}")
  z = z[:, 0]
  return float(10 * (dim - np.sum(np.cos(2 * math.pi * z))) +
               np.sum(z * z, axis=0))


def BuecheRastrigin(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB BuecheRastrigin function."""
  del seed
  dim = len(arr)
  arr.shape = (dim, 1)
  t = ArrayMap(arr, Tosz)
  l = SIndex(dim, arr) * t.flat

  term1 = 10 * (dim - np.sum(np.cos(2 * math.pi * l), axis=0))
  term2 = np.sum(l * l, axis=0)
  term3 = 100 * Fpen(arr)
  return float(term1 + term2 + term3)


def LinearSlope(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB LinearSlope function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  r = _R(dim, seed, b"R")
  z = np.matmul(r, arr)
  if z.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z.shape}")
  z = z[:, 0]
  result = 0.0
  for i in range(dim):
    s = 10**(i / float(dim - 1) if dim > 1 else 1)
    z_opt = 5 * np.sum(np.abs(r[i, :]))
    result += float(s * (z_opt - z[i]))
  return result


def AttractiveSector(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Attractive Sector function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  x_opt = np.array([1 if i % 2 == 0 else -1 for i in range(dim)])
  x_opt.shape = (dim, 1)
  z_vec = np.matmul(_R(dim, seed, b"R"), arr - x_opt)
  z_vec = np.matmul(LambdaAlpha(10.0, dim), z_vec)
  z_vec = np.matmul(_R(dim, seed, b"Q"), z_vec)

  result = 0.0
  for i in range(dim):
    z = z_vec[i, 0]
    s = 100 if z * x_opt[i] > 0 else 1
    result += (s * z)**2

  return math.pow(Tosz(result), 0.9)


def StepEllipsoidal(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB StepEllipsoidal function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z_hat = np.matmul(_R(dim, seed, b"R"), arr)
  z_hat = np.matmul(LambdaAlpha(10.0, dim), z_hat)
  z_tilde = np.array([
      math.floor(0.5 + z) if (z > 0.5) else (math.floor(0.5 + 10 * z) / 10)
      for z in z_hat.flat
  ])
  z_tilde = np.matmul(_R(dim, seed, b"Q"), z_tilde)
  s = 0.0
  for i, val in enumerate(z_tilde):
    exponent = 2.0 * float(i) / (dim - 1.0) if dim > 1.0 else 2.0
    s += 10.0**exponent * val**2
  value = max(abs(z_hat[0, 0]) / 1000, s)
  return 0.1 * value + Fpen(arr)


def RosenbrockRotated(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB RosenbrockRotated function."""
  dim = len(arr)
  r_x = np.matmul(_R(dim, seed, b"R"), arr)
  z = max(1.0, (dim**0.5) / 8.0) * r_x + 0.5 * np.ones((dim,))
  return float(
      sum([
          100.0 * (z[i]**2 - z[i + 1])**2 + (z[i] - 1)**2
          for i in range(dim - 1)
      ]))


def Ellipsoidal(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Ellipsoidal function."""
  del seed
  dim = len(arr)
  arr.shape = (dim, 1)
  z_vec = ArrayMap(arr, Tosz)
  s = 0.0
  for i in range(dim):
    exp = 6.0 * i / (dim - 1) if dim > 1 else 6.0
    s += float(10**exp * z_vec[i] * z_vec[i])
  return s


def Discus(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Discus function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  r_x = np.matmul(_R(dim, seed, b"R"), arr)
  z_vec = ArrayMap(r_x, Tosz)
  if z_vec.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z_vec.shape}")
  z_vec = z_vec[:, 0]
  return float(10**6 * z_vec[0] * z_vec[0]) + sum(
      [z * z for z in z_vec[1:].flat])


def BentCigar(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB BentCigar function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z_vec = np.matmul(_R(dim, seed, b"R"), arr)
  z_vec = Tasy(z_vec, 0.5)
  z_vec = np.matmul(_R(dim, seed, b"R"), z_vec)
  if z_vec.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z_vec.shape}")
  z_vec = z_vec[:, 0]
  return float(z_vec[0]**2) + 10**6 * np.sum(z_vec[1:]**2)


def SharpRidge(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB SharpRidge function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z_vec = np.matmul(_R(dim, seed, b"R"), arr)
  z_vec = np.matmul(LambdaAlpha(10, dim), z_vec)
  z_vec = np.matmul(_R(dim, seed, b"Q"), z_vec)
  return z_vec[0, 0]**2 + 100 * np.sum(z_vec[1:]**2)**0.5


def DifferentPowers(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB DifferentPowers function."""
  dim = len(arr)
  z = np.matmul(_R(dim, seed, b"R"), arr)
  s = 0.0
  for i in range(dim):
    exp = 2 + 4 * i / (dim - 1) if dim > 1 else 6
    s += abs(z[i])**exp
  return s**0.5


def Weierstrass(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Weierstrass function."""
  k_order = 12
  dim = len(arr)
  arr.shape = (dim, 1)
  z = np.matmul(_R(dim, seed, b"R"), arr)
  z = ArrayMap(z, Tosz)
  z = np.matmul(_R(dim, seed, b"Q"), z)
  z = np.matmul(LambdaAlpha(1.0 / 100.0, dim), z)
  if z.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z.shape}")
  z = z[:, 0]
  f0 = sum([0.5**k * math.cos(math.pi * 3**k) for k in range(k_order)])

  s = 0.0
  for i in range(dim):
    for k in range(k_order):
      s += 0.5**k * math.cos(2 * math.pi * (3**k) * (z[i] + 0.5))

  return float(10 * (s / dim - f0)**3) + 10 * Fpen(arr) / dim


def SchaffersF7(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Weierstrass function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  if dim == 1:
    return 0.0
  z = np.matmul(_R(dim, seed, b"R"), arr)
  z = Tasy(z, 0.5)
  z = np.matmul(_R(dim, seed, b"Q"), z)
  z = np.matmul(LambdaAlpha(10.0, dim), z)
  if z.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z.shape}")
  z = z[:, 0]

  s_arr = np.zeros(dim - 1)
  for i in range(dim - 1):
    s_arr[i] = float((z[i]**2 + z[i + 1]**2)**0.5)
  s = 0.0
  for i in range(dim - 1):
    s += s_arr[i]**0.5 + (s_arr[i]**0.5) * math.sin(50 * s_arr[i]**0.2)**2

  return (s / (dim - 1.0))**2 + 10 * Fpen(arr)


def SchaffersF7IllConditioned(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB SchaffersF7 Ill Conditioned."""
  dim = len(arr)
  arr.shape = (dim, 1)
  if dim == 1:
    return 0.0
  z = np.matmul(_R(dim, seed, b"R"), arr)
  z = Tasy(z, 0.5)
  z = np.matmul(_R(dim, seed, b"Q"), z)
  z = np.matmul(LambdaAlpha(1000.0, dim), z)
  if z.shape != (dim, 1):
    raise ValueError(f"z_vec should have shape (dim, 1), but got {z.shape}")
  z = z[:, 0]

  s_arr = np.zeros(dim - 1)
  for i in range(dim - 1):
    s_arr[i] = float((z[i]**2 + z[i + 1]**2)**0.5)
  s = 0.0
  for i in range(dim - 1):
    s += s_arr[i]**0.5 + (s_arr[i]**0.5) * math.sin(50 * s_arr[i]**0.2)**2

  return (s / (dim - 1.0))**2 + 10 * Fpen(arr)


def GriewankRosenbrock(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB GriewankRosenbrock function."""
  dim = len(arr)
  r_x = np.matmul(_R(dim, seed, b"R"), arr)
  # Slightly off BBOB documentation in order to center optima at origin.
  # Should be: max(1.0, (dim**0.5) / 8.0) * r_x + 0.5 * np.ones((dim,)).
  z_arr = max(1.0, (dim**0.5) / 8.0) * r_x + np.ones((dim,))
  s_arr = np.zeros(dim)
  for i in range(dim - 1):
    s_arr[i] = 100.0 * (z_arr[i]**2 - z_arr[i + 1])**2 + (z_arr[i] - 1)**2

  total = 0.0
  for i in range(dim - 1):
    total += (s_arr[i] / 4000.0 - math.cos(s_arr[i]))

  return (10.0 * total) / (dim - 1) + 10


def Schwefel(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Schwefel function."""
  del seed
  dim = len(arr)
  bernoulli_arr = np.array([pow(-1, i + 1) for i in range(dim)])
  x_opt = 4.2096874633 / 2.0 * bernoulli_arr
  x_hat = 2.0 * (bernoulli_arr * arr)  # Element-wise multiplication

  z_hat = np.zeros([dim, 1])
  z_hat[0, 0] = x_hat[0]
  for i in range(1, dim):
    z_hat[i, 0] = x_hat[i] + 0.25 * (x_hat[i - 1] - 2 * abs(x_opt[i - 1]))

  x_opt.shape = (dim, 1)
  z_vec = 100 * (
      np.matmul(LambdaAlpha(10, dim), z_hat - 2 * abs(x_opt)) + 2 * abs(x_opt))

  total = sum([z * math.sin(abs(z)**0.5) for z in z_vec.flat])

  return -(total / (100.0 * dim)) + 4.189828872724339 + 100 * Fpen(z_vec / 100)


def Katsuura(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Katsuura function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  r_x = np.matmul(_R(dim, seed, b"R"), arr)
  z_vec = np.matmul(LambdaAlpha(100.0, dim), r_x)
  z_vec = np.matmul(_R(dim, seed, b"Q"), z_vec)

  prod = 1.0
  for i in range(dim):
    s = 0.0
    for j in range(1, 33):
      s += abs(2**j * z_vec[i, 0] - round(2**j * z_vec[i, 0])) / 2**j
    prod *= (1 + (i + 1) * s)**(10.0 / dim**1.2)

  return (10.0 / dim**2) * prod - 10.0 / dim**2 + Fpen(arr)


def Lunacek(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Lunacek function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  mu0 = 2.5
  s = 1.0 - 1.0 / (2.0 * (dim + 20.0)**0.5 - 8.2)
  mu1 = -((mu0**2 - 1) / s)**0.5

  x_opt = np.array([mu0 / 2] * dim)
  x_hat = np.array([2 * arr[i, 0] * np.sign(x_opt[i]) for i in range(dim)])
  x_vec = x_hat - mu0
  x_vec.shape = (dim, 1)
  x_vec = np.matmul(_R(dim, seed, b"R"), x_vec)
  z_vec = np.matmul(LambdaAlpha(100, dim), x_vec)
  z_vec = np.matmul(_R(dim, seed, b"Q"), z_vec)

  s1 = sum([(val - mu0)**2 for val in x_hat])
  s2 = sum([(val - mu1)**2 for val in x_hat])
  s3 = sum([math.cos(2 * math.pi * z) for z in z_vec.flat])
  return min(s1, dim + s * s2) + 10.0 * (dim - s3) + 10**4 * Fpen(arr)


def Gallagher101Me(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Gallagher101 function."""
  dim = len(arr)
  arr.shape = (dim, 1)

  num_optima = 101
  optima_list = [np.zeros([dim, 1])]
  for i in range(num_optima - 1):
    vec = np.zeros([dim, 1])
    for j in range(dim):
      alpha = (i * dim + j + 1.0) / (dim * num_optima + 2.0)
      assert alpha > 0
      assert alpha < 1
      vec[j, 0] = -5 + 10 * alpha
    optima_list.append(vec)

  c_list = [LambdaAlpha(1000, dim)]
  for i in range(num_optima - 1):
    alpha = 1000.0**(2.0 * (i) / (num_optima - 2))
    c_mat = LambdaAlpha(alpha, dim) / (alpha**0.25)
    c_list.append(c_mat)

  rotation = _R(dim, seed, b"R")
  max_value = -1.0
  for i in range(num_optima):
    w = 10 if i == 0 else (1.1 + 8.0 * (i - 1.0) / (num_optima - 2.0))
    diff = np.matmul(rotation, arr - optima_list[i])
    e = np.matmul(diff.transpose(), np.matmul(c_list[i], diff))
    max_value = max(max_value, w * math.exp(-float(e.item()) / (2.0 * dim)))

  return Tosz(10.0 - max_value)**2 + Fpen(arr)


def Gallagher21Me(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Gallagher21 function."""
  dim = len(arr)
  arr.shape = (dim, 1)

  num_optima = 21
  optima_list = [np.zeros([dim, 1])]
  for i in range(num_optima - 1):
    vec = np.zeros([dim, 1])
    for j in range(dim):
      alpha = (i * dim + j + 1.0) / (dim * num_optima + 2.0)
      assert alpha > 0
      assert alpha < 1
      vec[j, 0] = -5 + 10 * alpha
    optima_list.append(vec)

  c_list = [LambdaAlpha(1000, dim)]
  for i in range(num_optima - 1):
    alpha = 1000.0**(2.0 * (i) / (num_optima - 2))
    c_mat = LambdaAlpha(alpha, dim) / (alpha**0.25)
    c_list.append(c_mat)

  rotation = _R(dim, seed, b"R")
  max_value = -1.0
  for i in range(num_optima):
    w = 10 if i == 0 else (1.1 + 8.0 * (i - 1.0) / (num_optima - 2.0))
    diff = np.matmul(rotation, arr - optima_list[i])
    e = np.matmul(diff.transpose(), np.matmul(c_list[i], diff))
    max_value = max(max_value, w * math.exp(-float(e) / (2.0 * dim)))

  return Tosz(10.0 - max_value)**2 + Fpen(arr)


## Additional BBOB-like functions to test exploration.


def NegativeSphere(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for BBOB Sphere function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z = np.matmul(_R(dim, seed, b"R"), arr)
  return float(100 + np.sum(z * z) - 2 * (z[0]**2))


def NegativeMinDifference(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for NegativeMinDifference function."""
  dim = len(arr)
  arr.shape = (dim, 1)
  z = np.matmul(_R(dim, seed, b"R"), arr)
  min_difference = 10000
  for i in range(len(z) - 1):
    min_difference = min(min_difference, z[i + 1] - z[i])
  return 10.0 - float(min_difference) + 1e-8 * float(sum(arr))


def FonsecaFleming(arr: np.ndarray, seed: int = 0) -> float:
  """Implementation for FonsecaFleming function."""
  del seed
  return 1.0 - float(np.exp(-np.sum(arr * arr)))


--- vizier/_src/benchmarks/experimenters/synthetic/bbob_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for bbob."""

import sys

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters.synthetic import bbob

from absl.testing import absltest
from absl.testing import parameterized


class BbobTest(parameterized.TestCase):

  @parameterized.parameters(
      {'scale_type': vz.ScaleType.LINEAR}, {'scale_type': vz.ScaleType.LOG}
  )
  def test_bbob_problem_statement_scale_type(self, scale_type):
    problem = bbob.DefaultBBOBProblemStatement(10, scale_type=scale_type)
    for p in problem.search_space.parameters:
      self.assertEqual(p.scale_type, scale_type)

  def test_rotation(self):
    rotation = bbob._R(5, sys.maxsize, 'A', b'XAFDAF', 3)
    rotation2 = bbob._R(5, sys.maxsize, 'A', b'XAFDAF', 3)
    np.testing.assert_allclose(rotation.T @ rotation, np.eye(5), atol=1e-5)
    np.testing.assert_array_equal(rotation, rotation2)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/branin.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Branin function."""
from typing import Sequence

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter


def _branin(x: np.ndarray) -> float:
  """Branin function.

  This function can accept batch shapes, although it is typed to return floats
  to conform to NumpyExperimenter API.

  Args:
    x: Shape (B*, 2) array.

  Returns:
    Shape (B*) array.
  """
  a = 1
  b = 5.1 / (4 * np.pi**2)
  c = 5 / np.pi
  r = 6
  s = 10
  t = 1 / (8 * np.pi)
  x1 = x[..., 0]
  x2 = x[..., 1]

  y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * np.cos(x1) + s
  return y


class Branin2DExperimenter(experimenter.Experimenter):
  """2D minimization function. See https://www.sfu.ca/~ssurjano/branin.html."""

  def __init__(self):
    self._impl = numpy_experimenter.NumpyExperimenter(
        _branin, self.problem_statement()
    )

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    self._impl.evaluate(suggestions)

  def problem_statement(self) -> vz.ProblemStatement:
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param("x1", -5, 10)
    problem.search_space.root.add_float_param("x2", 0, 15)
    problem.metric_information.append(
        vz.MetricInformation(name="value", goal=vz.ObjectiveMetricGoal.MINIMIZE)
    )
    return problem


--- vizier/_src/benchmarks/experimenters/synthetic/branin_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters.synthetic import branin
from vizier._src.benchmarks.testing import experimenter_testing

from absl.testing import absltest


class Branin2DExperimenterTest(absltest.TestCase):

  def test_branin_impl(self):
    np.testing.assert_allclose(
        branin._branin(
            np.array([[-np.pi, 12.275], [np.pi, 2.275], [9.42478, 2.475]])
        ),
        0.397887,
        atol=1e-5,
    )

  def test_experimenter_argmin(self):
    trial = vz.Trial(parameters={'x1': -np.pi, 'x2': 12.275})
    branin.Branin2DExperimenter().evaluate([trial])
    self.assertAlmostEqual(
        trial.final_measurement_or_die.metrics.get_value('value', np.nan),
        0.397887,
        places=5,
    )

  def test_experimenter(self):
    experimenter_testing.assert_evaluates_random_suggestions(
        self, branin.Branin2DExperimenter()
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/deb.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Multi-objective synthetic functions created by Deb et al.

Forked from
https://github.com/pytorch/botorch/blob/main/botorch/test_functions/multi_objective.py.
"""

from typing import Callable, Sequence
import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier.pyvizier import converters


# TODO: Subclass MultiObjectiveNumpyExperimenter.
class DHExperimenter(experimenter.Experimenter):
  """Multiobjective problem with two objectives (f0, f1) and d-dimensions.

  The functions are defined as:
    f0(x) = x0
    f1(x) = h(x) + g(x) * s(x) for DH1 and DH2
    f1(x) = h(x) * (g(x) + s(x)) for DH3 and DH4

  Reference:
    K. Deb and H. Gupta. Searching for Robust Pareto-Optimal Solutions in
    Multi-objective Optimization. Evolutionary Multi-Criterion Optimization,
    Springer-Berlin, pp. 150-164, 2005.
  """

  def __init__(
      self,
      h_fn: Callable[[np.ndarray], float],
      g_fn: Callable[[np.ndarray], float],
      s_fn: Callable[[np.ndarray], float],
      f1_fn: Callable[[float, float, float], float],  # Uses h, g, s.
      bounds: Sequence[tuple[float, float]],
  ):
    self._h_fn = h_fn
    self._g_fn = g_fn
    self._s_fn = s_fn
    self._f1_fn = f1_fn
    self._bounds = bounds

    self._converter = converters.TrialToArrayConverter.from_study_config(
        study_config=self.problem_statement(),
        scale=False,
        flip_sign_for_minimization_metrics=False,
    )

  def problem_statement(self) -> vz.ProblemStatement:
    problem = vz.ProblemStatement()
    problem.metric_information.append(
        vz.MetricInformation(name="f0", goal=vz.ObjectiveMetricGoal.MINIMIZE)
    )
    problem.metric_information.append(
        vz.MetricInformation(name="f1", goal=vz.ObjectiveMetricGoal.MINIMIZE)
    )
    for i, (min_val, max_val) in enumerate(self._bounds):
      problem.search_space.root.add_float_param(f"x{i}", min_val, max_val)

    return problem

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    features = self._converter.to_features(suggestions)
    for i, suggestion in enumerate(suggestions):
      feat = features[i]
      f0 = feat[0]
      f1 = self._f1_fn(self._h_fn(feat), self._g_fn(feat), self._s_fn(feat))
      suggestion.complete(vz.Measurement(metrics={"f0": f0, "f1": f1}))

  @classmethod
  def DH1(cls, num_dimensions: int) -> "DHExperimenter":
    if num_dimensions < 2:
      raise ValueError(f"num_dimensions must be >= 2, got {num_dimensions}.")
    h_fn = lambda x: 1 - x[0] ** 2
    g_fn = lambda x: np.sum(
        10 + x[1:] ** 2 - 10 * np.cos(4 * np.pi * x[1:]),
    )
    s_fn = lambda x: 1 / (0.2 + x[0]) + x[0] ** 2
    f1_fn = lambda h, g, s: h + g * s
    bounds = [(0, 1)] + [(-1, 1) for _ in range(num_dimensions - 1)]
    return cls(h_fn, g_fn, s_fn, f1_fn, bounds)

  @classmethod
  def DH2(cls, num_dimensions: int) -> "DHExperimenter":
    """Same as DH1 but s_fn has a different (10.0) constant."""
    if num_dimensions < 2:
      raise ValueError(f"num_dimensions must be >= 2, got {num_dimensions}.")
    h_fn = lambda x: 1 - x[0] ** 2
    g_fn = lambda x: np.sum(
        10 + x[1:] ** 2 - 10 * np.cos(4 * np.pi * x[1:]),
    )
    s_fn = lambda x: 1 / (0.2 + x[0]) + 10.0 * x[0] ** 2
    f1_fn = lambda h, g, s: h + g * s
    bounds = [(0, 1)] + [(-1, 1) for _ in range(num_dimensions - 1)]
    return cls(h_fn, g_fn, s_fn, f1_fn, bounds)

  @classmethod
  def DH3(cls, num_dimensions: int) -> "DHExperimenter":
    if num_dimensions < 3:
      raise ValueError(f"num_dimensions must be >= 3, got {num_dimensions}.")
    exp_arg1 = lambda x: -(((x[1] - 0.35) / 0.25) ** 2)
    exp_arg2 = lambda x: -(((x[1] - 0.85) / 0.03) ** 2)
    h_fn = lambda x: 2 - 0.8 * np.exp(exp_arg1(x)) - np.exp(exp_arg2(x))

    g_fn = lambda x: 50 * np.sum(x[2:] ** 2)
    s_fn = lambda x: 1 - np.sqrt(x[0])
    f1_fn = lambda h, g, s: h * (g + s)
    bounds = [(0, 1), (0, 1)] + [(-1, 1) for _ in range(num_dimensions - 2)]
    return cls(h_fn, g_fn, s_fn, f1_fn, bounds)

  @classmethod
  def DH4(cls, num_dimensions: int) -> "DHExperimenter":
    """Similar to DH3 but different h_fn."""
    if num_dimensions < 3:
      raise ValueError(f"num_dimensions must be >= 3, got {num_dimensions}.")
    exp_arg1 = lambda x: -(((np.sum(x[:2]) - 0.35) / 0.25) ** 2)
    exp_arg2 = lambda x: -(((np.sum(x[:2]) - 0.85) / 0.03) ** 2)
    h_fn = lambda x: 2 - x[0] - 0.8 * np.exp(exp_arg1(x)) - np.exp(exp_arg2(x))

    g_fn = lambda x: 50 * np.sum(x[2:] ** 2)
    s_fn = lambda x: 1 - np.sqrt(x[0])
    f1_fn = lambda h, g, s: h * (g + s)
    bounds = [(0, 1), (0, 1)] + [(-1, 1) for _ in range(num_dimensions - 2)]
    return cls(h_fn, g_fn, s_fn, f1_fn, bounds)


--- vizier/_src/benchmarks/experimenters/synthetic/deb_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier._src.benchmarks.experimenters.synthetic import deb
from vizier._src.benchmarks.testing import experimenter_testing
from absl.testing import absltest
from absl.testing import parameterized


class DHExperimenterTest(parameterized.TestCase):

  @parameterized.parameters(
      (deb.DHExperimenter.DH1,),
      (deb.DHExperimenter.DH2,),
      (deb.DHExperimenter.DH3,),
      (deb.DHExperimenter.DH4,),
  )
  def test_e2e_smoke(self, experimenter_cls):
    experimenter_testing.assert_evaluates_random_suggestions(
        self, experimenter_cls(num_dimensions=4)
    )


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/hartmann.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""HartMann function."""

import functools
from typing import Sequence
import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter
from vizier._src.benchmarks.experimenters import numpy_experimenter


def _hartmann(
    x: np.ndarray, alpha: np.ndarray, A: np.ndarray, P: np.ndarray
) -> float:
  # pylint:disable=invalid-name
  return -alpha @ np.exp(-np.sum(A * (x - P) ** 2, axis=1))


class HartmannExperimenter(experimenter.Experimenter):
  """General Hartmann minimization function."""

  # pylint:disable=invalid-name
  def __init__(self, alpha: np.ndarray, A: np.ndarray, P: np.ndarray):
    self._dimension = A.shape[-1]

    self._impl = numpy_experimenter.NumpyExperimenter(
        functools.partial(_hartmann, alpha=alpha, A=A, P=P),
        self.problem_statement(),
    )

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    self._impl.evaluate(suggestions)

  def problem_statement(self) -> vz.ProblemStatement:
    problem = vz.ProblemStatement()
    for i in range(1, self._dimension + 1):
      problem.search_space.root.add_float_param(f"x{i}", 0, 1)
    problem.metric_information.append(
        vz.MetricInformation(name="value", goal=vz.ObjectiveMetricGoal.MINIMIZE)
    )
    return problem

  @classmethod
  def from_3d(cls) -> "HartmannExperimenter":
    """See https://www.sfu.ca/~ssurjano/hart3.html."""
    return cls(
        alpha=np.array([1.0, 1.2, 3.0, 3.2]),
        A=np.array([
            [3, 10, 30],
            [0.1, 10, 35],
            [3, 10, 30],
            [0.1, 10, 35],
        ]),
        P=1e-4
        * np.array([
            [3689, 1170, 2673],
            [4699, 4387, 7470],
            [1091, 8732, 5547],
            [381, 5743, 8828],
        ]),
    )

  @classmethod
  def from_6d(cls) -> "HartmannExperimenter":
    """See https://www.sfu.ca/~ssurjano/hart6.html."""
    return cls(
        alpha=np.array([1.0, 1.2, 3.0, 3.2]),
        A=np.array([
            [10, 3, 17, 3.5, 1.7, 8],
            [0.05, 10, 17, 0.1, 8, 14],
            [3, 3.5, 1.7, 10, 17, 8],
            [17, 8, 0.05, 10, 0.1, 14],
        ]),
        P=1e-4
        * np.array([
            [1312, 1696, 5569, 124, 8283, 5886],
            [2329, 4135, 8307, 3736, 1004, 9991],
            [2348, 1451, 3522, 2883, 3047, 6650],
            [4047, 8828, 8732, 5743, 1091, 381],
        ]),
    )


--- vizier/_src/benchmarks/experimenters/synthetic/hartmann_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for hartmann."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters.synthetic import hartmann
from vizier._src.benchmarks.testing import experimenter_testing
from absl.testing import absltest


class Hartmann3DExperimenterTest(absltest.TestCase):

  def test_experimenter_argmin(self):
    trial = vz.Trial(
        parameters={
            f'x{i+1}': x for i, x in enumerate([0.114614, 0.555649, 0.852547])
        }
    )
    hartmann.HartmannExperimenter.from_3d().evaluate([trial])
    self.assertAlmostEqual(
        trial.final_measurement_or_die.metrics.get_value('value', np.nan),
        -3.86278,
        places=5,
    )

  def test_experimenter(self):
    experimenter_testing.assert_evaluates_random_suggestions(
        self, hartmann.HartmannExperimenter.from_3d()
    )


class Hartmann6DExperimenterTest(absltest.TestCase):

  def test_experimenter_argmin(self):
    trial = vz.Trial(
        parameters={
            f'x{i+1}': x
            for i, x in enumerate(
                [0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573]
            )
        }
    )
    hartmann.HartmannExperimenter.from_6d().evaluate([trial])
    self.assertAlmostEqual(
        trial.final_measurement_or_die.metrics.get_value('value', np.nan),
        -3.32237,
        places=5,
    )

  def test_experimenter(self):
    experimenter_testing.assert_evaluates_random_suggestions(
        self, hartmann.HartmannExperimenter.from_6d()
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/multiarm.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Multi-arm bandit environments.

Search space is pure 1-D categorical, and rewards are given by fixed
distributions.
"""

from typing import Mapping, Optional, Sequence

import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


def _default_multiarm_problem(arms: Sequence[str]) -> vz.ProblemStatement:
  """Returns default multi-arm problem statement."""
  problem = vz.ProblemStatement()
  problem.metric_information.append(
      vz.MetricInformation(name="reward", goal=vz.ObjectiveMetricGoal.MAXIMIZE)
  )
  problem.search_space.root.add_categorical_param("arm", feasible_values=arms)
  return problem


class BernoulliMultiArmExperimenter(experimenter.Experimenter):
  """Uses a mapping from arm to Bernoulli probability of success."""

  def __init__(
      self, arms_to_probs: Mapping[str, float], seed: Optional[int] = None
  ):
    self._arms_to_probs = arms_to_probs
    self._rng = np.random.RandomState(seed)

  def problem_statement(self) -> vz.ProblemStatement:
    return _default_multiarm_problem(list(self._arms_to_probs.keys()))

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    """Each arm has a fixed probability of outputting 0 or 1 reward."""
    for suggestion in suggestions:
      arm = suggestion.parameters["arm"].value
      prob = self._arms_to_probs[arm]
      reward = self._rng.choice([0, 1], p=[1 - prob, prob])
      suggestion.final_measurement = vz.Measurement(metrics={"reward": reward})


class FixedMultiArmExperimenter(experimenter.Experimenter):
  """Rewards are deterministic."""

  def __init__(self, arms_to_rewards: Mapping[str, float]):
    self._arms_to_rewards = arms_to_rewards

  def problem_statement(self) -> vz.ProblemStatement:
    return _default_multiarm_problem(list(self._arms_to_rewards.keys()))

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    for suggestion in suggestions:
      arm = suggestion.parameters["arm"].value
      reward = self._arms_to_rewards[arm]
      suggestion.final_measurement = vz.Measurement(metrics={"reward": reward})


--- vizier/_src/benchmarks/experimenters/synthetic/multiobjective_optproblems.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Wrapper around optproblems PyPI library for multiobjective problems.

See documentation:
https://ls11-www.cs.tu-dortmund.de/people/swessing/optproblems/doc/index.html#

WFG benchmark was used in MultiObjective TPE:
https://www.jair.org/index.php/jair/article/view/13188
"""

import json
import attrs
from optproblems import dtlz
from optproblems import wfg
from optproblems import zdt
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter_factory
from vizier._src.benchmarks.experimenters import numpy_experimenter


def _DefaultOptProblemStatement(
    dimension: int,
    num_objectives: int,
    *,
    min_value: float = 0.0,
    max_value: float = 1.0,
):
  """Returns default optproblems ProblemStatement."""
  problem = vz.ProblemStatement()
  for n in range(num_objectives):
    metric = vz.MetricInformation(
        name=f"f{n}", goal=vz.ObjectiveMetricGoal.MINIMIZE
    )
    problem.metric_information.append(metric)
  for d in range(dimension):
    problem.search_space.root.add_float_param(f"x{d}", min_value, max_value)
  return problem


EXPERIMENTER_FACTORY_KEY = "experimenter_factory"


@attrs.define
class WFGExperimenterFactory(
    experimenter_factory.SerializableExperimenterFactory
):
  """Multiobjective problem with variable number of objectives and dimensions.

  Reference:
    Huband, S.; Hingston, P.; Barone, L.; While, L. (2006). A review of
    multiobjective test problems and a scalable test problem toolkit. IEEE
    Transactions on Evolutionary Computation, vol.10, no.5, pp. 477-506.
  """

  name: str = attrs.field(
      default="", validator=attrs.validators.instance_of(str)
  )
  dim: int = attrs.field(
      default=1,
      validator=[attrs.validators.instance_of(int), attrs.validators.gt(0)],
  )
  num_objectives: int = attrs.field(
      default=2,
      validator=[attrs.validators.instance_of(int), attrs.validators.ge(2)],
  )

  def __attrs_post_init__(self):
    # k = "Position-related parameters". Must be divisible by (num_obj-1).
    k = self.num_objectives - 1
    if (self.dim - k) % 2 != 0:
      raise ValueError(
          f"dimensions - k must be even, got {self.dim - k} for k={k}."
      )
    self.k = k

  def __call__(self) -> numpy_experimenter.MultiObjectiveNumpyExperimenter:
    optprob_factory = getattr(wfg, self.name, None)
    if optprob_factory is None:
      raise ValueError(f"{self.name} is not a valid WFG problem in wfg.py")
    optprob: wfg.WFGBaseProblem = optprob_factory(
        self.num_objectives, self.dim, self.k
    )
    impl = optprob.objective_function
    problem = _DefaultOptProblemStatement(self.dim, self.num_objectives)
    return numpy_experimenter.MultiObjectiveNumpyExperimenter(impl, problem)

  def dump(self) -> vz.Metadata:
    metadata = vz.Metadata()
    metadata_dict = {
        "name": self.name,
        "dim": self.dim,
        "num_objectives": self.num_objectives,
    }
    metadata[EXPERIMENTER_FACTORY_KEY] = json.dumps(metadata_dict)
    return metadata

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> "WFGExperimenterFactory":
    metadata_dict = json.loads(metadata[EXPERIMENTER_FACTORY_KEY])
    return cls(**metadata_dict)


@attrs.define
class DTLZExperimenterFactory(
    experimenter_factory.SerializableExperimenterFactory
):
  """Multiobjective problem with variable number of objectives and dimensions.

  Reference:
    K. Deb, L. Thiele, M. Laumanns, E. Zitzler, A. Abraham, L. Jain, and
    R. Goldberg. Scalable test problems for evolutionary multi-objective
    optimization. Evolutionary Multiobjective Optimization, Springer-Verlag,
    pp. 105-145, 2005.
  """

  name: str = attrs.field(
      default="", validator=attrs.validators.instance_of(str)
  )
  dim: int = attrs.field(
      default=1,
      validator=[attrs.validators.instance_of(int), attrs.validators.gt(0)],
  )
  num_objectives: int = attrs.field(
      default=2,
      validator=[attrs.validators.instance_of(int), attrs.validators.ge(2)],
  )

  def __call__(self) -> numpy_experimenter.MultiObjectiveNumpyExperimenter:
    optprob_factory = getattr(dtlz, self.name, None)
    if optprob_factory is None:
      raise ValueError(f"{self.name} is not a valid DTLZ problem in dtlz.py")
    optprob: dtlz.DTLZBaseProblem = optprob_factory(
        self.num_objectives, self.dim
    )
    impl = optprob.objective_function
    problem = _DefaultOptProblemStatement(self.dim, self.num_objectives)
    return numpy_experimenter.MultiObjectiveNumpyExperimenter(impl, problem)

  def dump(self) -> vz.Metadata:
    metadata = vz.Metadata()
    metadata_dict = {
        "name": self.name,
        "dim": self.dim,
        "num_objectives": self.num_objectives,
    }
    metadata[EXPERIMENTER_FACTORY_KEY] = json.dumps(metadata_dict)
    return metadata

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> "DTLZExperimenterFactory":
    metadata_dict = json.loads(metadata[EXPERIMENTER_FACTORY_KEY])
    return cls(**metadata_dict)


@attrs.define
class ZDTExperimenterFactory(
    experimenter_factory.SerializableExperimenterFactory
):
  """Multiobjective problem with two objectives and d-dimensions.

  Reference:
    E. Zitzler, K. Deb, and L. Thiele. Comparison of multiobjective
    evolutionary algorithms: Empirical results. Evolutionary Computation, vol.
    8, no. 2,pp. 173-195, 2000.
  """

  name: str = attrs.field(
      default="", validator=attrs.validators.instance_of(str)
  )
  dim: int = attrs.field(
      default=1,
      validator=[attrs.validators.instance_of(int), attrs.validators.gt(0)],
  )

  def __call__(self) -> numpy_experimenter.MultiObjectiveNumpyExperimenter:
    optprob_factory = getattr(zdt, self.name, None)
    if optprob_factory is None:
      raise ValueError(f"{self.name} is not a valid ZDT problem in zdt.py")
    if optprob_factory == zdt.ZDT5:
      raise ValueError("ZDT5 does not allow variable dimensions.")
    optprob: zdt.ZDTBaseProblem = optprob_factory(self.dim)
    impl = optprob.objective_function
    problem = _DefaultOptProblemStatement(self.dim, 2)
    return numpy_experimenter.MultiObjectiveNumpyExperimenter(impl, problem)

  def dump(self) -> vz.Metadata:
    metadata = vz.Metadata()
    metadata_dict = {
        "name": self.name,
        "dim": self.dim,
    }
    metadata[EXPERIMENTER_FACTORY_KEY] = json.dumps(metadata_dict)
    return metadata

  @classmethod
  def recover(cls, metadata: vz.Metadata) -> "ZDTExperimenterFactory":
    metadata_dict = json.loads(metadata[EXPERIMENTER_FACTORY_KEY])
    return cls(**metadata_dict)


--- vizier/_src/benchmarks/experimenters/synthetic/multiobjective_optproblems_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier._src.benchmarks.experimenters.synthetic import multiobjective_optproblems as mopt
from vizier._src.benchmarks.testing import experimenter_testing
from absl.testing import absltest
from absl.testing import parameterized


class WFGxperimenterTest(parameterized.TestCase):

  @parameterized.parameters(
      ("WFG1",),
      ("WFG2",),
      ("WFG3",),
      ("WFG4",),
      ("WFG5",),
      ("WFG6",),
      ("WFG7",),
      ("WFG8",),
      ("WFG9",),
  )
  def test_e2e_smoke(self, name):
    exptr_factory = mopt.WFGExperimenterFactory(name, dim=6, num_objectives=3)
    experimenter_testing.assert_evaluates_random_suggestions(
        self, exptr_factory()
    )


class DTLZExperimenterTest(parameterized.TestCase):

  @parameterized.parameters(
      ("DTLZ1",),
      ("DTLZ2",),
      ("DTLZ3",),
      ("DTLZ4",),
      ("DTLZ5",),
      ("DTLZ6",),
      ("DTLZ7",),
  )
  def test_e2e_smoke(self, name):
    exptr_factory = mopt.DTLZExperimenterFactory(name, dim=4, num_objectives=3)
    experimenter_testing.assert_evaluates_random_suggestions(
        self, exptr_factory()
    )


class ZDTExperimenterTest(parameterized.TestCase):

  @parameterized.parameters(
      ("ZDT1",),
      ("ZDT2",),
      ("ZDT3",),
      ("ZDT4",),
      ("ZDT6",),
  )
  def test_e2e_smoke(self, name):
    exptr_factory = mopt.ZDTExperimenterFactory(name, dim=5)
    experimenter_testing.assert_evaluates_random_suggestions(
        self, exptr_factory()
    )


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/benchmarks/experimenters/synthetic/simplekd.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""SimpleKD experimenter.

The experimenter supports three flavors (corner, center, mixed), each of which
has a different optia location. The categorical parameter takes the same three
values, and the objective function depends on the value of the categorical
parameter which make it harder to optimize.
"""

from typing import Literal, Sequence, Union
import attrs
import numpy as np
from vizier import pyvizier as vz
from vizier._src.benchmarks.experimenters import experimenter


def _float_term(x_list: list[float]) -> float:
  """Computes the float term on a list of values.

  Args:
    x_list: Elements in the list correspond to a different dimension/Parameter.

  Returns:
    The float term accounting for all the float Parameters.
  """
  float_term = 0
  for x in x_list:
    float_term += min(
        1.6,
        (x > -0.8) * -((x - 1) ** 2)
        + (x <= -0.8) * (-(1.8**2) + 150 * (x + 0.8) ** 2),
    )
  return float_term


SimpleKDCategory = Literal['corner', 'center', 'mixed']


def _categorical_term(x: str, best_category: SimpleKDCategory) -> float:
  """Computes the categorical term."""
  if x != best_category:
    return 0
  elif x == 'corner':
    return 1
  elif x == 'center':
    return 1
  elif x == 'mixed':
    return 1.5
  raise NotImplementedError(f'Unknown categorical parameter: {x}')


_feasible_discrete_values = (1, 2, 5, 6, 8)


def _discrete_term(x_list: list[int]) -> float:
  """Computes the discrete term on a list of values."""
  discrete_term = 0
  for x in x_list:
    discrete_term += [1.2, 0.0, 0.6, 0.8, 1.0][
        _feasible_discrete_values.index(x)
    ]
  return discrete_term


def _int_term(x_list: list[int]) -> float:
  """Computes the int term on a list of values."""
  int_term = 0
  for x in x_list:
    int_term += np.power(x - 2.2, 2) / 2.0
  return int_term


@attrs.define
class SimpleKDExperimenter(experimenter.Experimenter):
  """Simple problem with arbitrary number of parameter of each type.

  SimpleKD is a carefully constructed function to trap an underexploring
  hillclimb algorithm to local optima. It is parameterized by "best_category",
  which optimization algorithm must get right in order to reach the optimum.

  Note that CATEGORICAL type has only a single parameter as there's no notion
  of hillclimb or direction for categorical parameter.

  When 'output_relative_error' is True (default case) the objective value is
  the relative error compared to the optimal objective value (which could be
  negative) and the objective goal is MINIMIZE. This option allows for using the
  experiementer results directly without needing for further processing.
  When 'output_relative_error' is False, the objecitve is the SimpleKd function
  value and the objective goal is MAXIMIZE.

  The "best_category" value refers to where the optimum occurs.
  """

  best_category: SimpleKDCategory = attrs.field()  # type: ignore
  num_float_param: int = 1
  num_discrete_param: int = 1
  num_int_param: int = 1
  output_relative_error: bool = True

  def __attrs_post_init__(self):
    if self.output_relative_error and abs(self.optimal_objective) < 1e-6:
      raise ValueError(
          f"Optimal objective is too small {self.optimal_objective}, can't"
          ' compute relative error.'
      )

  def evaluate(self, suggestions: Sequence[vz.Trial]) -> None:
    for suggestion in suggestions:
      param_values = self._get_param_values(suggestion)
      value = self._compute(param_values)
      if self.output_relative_error:
        value = abs((value - self.optimal_objective) / self.optimal_objective)
      suggestion.complete(vz.Measurement({'value': value}))

  def problem_statement(self) -> vz.ProblemStatement:
    problem = vz.ProblemStatement()
    for f in range(self.num_float_param):
      problem.search_space.root.add_float_param(f'float_{f}', -1, 1)
    for d in range(self.num_discrete_param):
      problem.search_space.root.add_discrete_param(
          f'discrete_{d}', _feasible_discrete_values
      )
    for i in range(self.num_int_param):
      problem.search_space.root.add_int_param(f'int_{i}', 1, 3)
    problem.search_space.root.add_categorical_param(
        'categorical', ('corner', 'center', 'mixed')
    )
    if self.output_relative_error:
      problem.metric_information.append(
          vz.MetricInformation(
              name='value', goal=vz.ObjectiveMetricGoal.MINIMIZE
          )
      )
    else:
      problem.metric_information.append(
          vz.MetricInformation(
              name='value', goal=vz.ObjectiveMetricGoal.MAXIMIZE
          )
      )
    return problem

  def _get_param_values(
      self, trial: vz.Trial
  ) -> dict[str, list[Union[float, int, str]]]:
    """Extract the trial parameter values by type."""

    float_list = []
    int_list = []
    discrete_list = []
    for index in range(self.num_float_param):
      float_list.append(trial.parameters[f'float_{index}'].value)
    for index in range(self.num_discrete_param):
      discrete_list.append(trial.parameters[f'discrete_{index}'].value)
    for index in range(self.num_int_param):
      int_list.append(trial.parameters[f'int_{index}'].value)

    return {
        'float': float_list,
        'int': int_list,
        'discrete': discrete_list,
        'categorical': [trial.parameters['categorical'].value],
    }

  def _compute(self, params: dict[str, list[Union[float, int, str]]]) -> float:
    """Computes the SimpleKD objective value."""
    if params['categorical'][0] == 'corner':
      return (
          _categorical_term(params['categorical'][0], self.best_category)
          + 0.8 * _float_term(params['float'])
          + _discrete_term(params['discrete'])
          + _int_term(params['int'])
      )
    elif params['categorical'][0] == 'center':
      return (
          _categorical_term(params['categorical'][0], self.best_category)
          - _float_term(params['float'])
          - _discrete_term(params['discrete'])
          - _int_term(params['int'])
      )
    elif params['categorical'][0] == 'mixed':
      return (
          _categorical_term(params['categorical'][0], self.best_category)
          + 0.8 * _float_term(params['float'])
          + _discrete_term(params['discrete'])
          - _int_term(params['int'])
      )
    else:
      raise NotImplementedError(
          f'Unknown categorical parameter: {params["categorical"]}'
      )

  @property
  def optimal_objective(self) -> float:
    """Computes the optimal objective value of a SimpleKD problem."""
    values = {}
    if self.best_category == 'corner':
      values['float'] = [-1.0 for _ in range(self.num_float_param)]
      values['int'] = [1 for _ in range(self.num_float_param)]
      values['discrete'] = [1 for _ in range(self.num_float_param)]
      values['categorical'] = ['corner']

    if self.best_category == 'center':
      values['float'] = [-0.8 for _ in range(self.num_float_param)]
      values['int'] = [2 for _ in range(self.num_float_param)]
      values['discrete'] = [2 for _ in range(self.num_float_param)]
      values['categorical'] = ['center']

    if self.best_category == 'mixed':
      values['float'] = [-1.0 for _ in range(self.num_float_param)]
      values['int'] = [2 for _ in range(self.num_float_param)]
      values['discrete'] = [1 for _ in range(self.num_float_param)]
      values['categorical'] = ['mixed']

    return self._compute(values)


--- vizier/_src/benchmarks/experimenters/synthetic/simplekd_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from absl.testing import parameterized
import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import grid
from vizier._src.benchmarks.experimenters.synthetic import simplekd
from vizier._src.benchmarks.runners import benchmark_runner
from vizier._src.benchmarks.runners import benchmark_state
from absl.testing import absltest


class SimpleKDExperimenterTest(parameterized.TestCase):

  @parameterized.product(
      best_category=['corner', 'center', 'mixed'],
      output_relative_error=[True, False],
  )
  def test_sweep(
      self,
      best_category: simplekd.SimpleKDCategory,
      output_relative_error: bool,
  ) -> None:
    experimenter = simplekd.SimpleKDExperimenter(
        best_category, output_relative_error=output_relative_error
    )
    runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[
            benchmark_runner.GenerateSuggestions(),
            benchmark_runner.EvaluateActiveTrials(),
        ],
        num_repeats=300,
    )
    state = benchmark_state.BenchmarkState(
        experimenter,
        benchmark_state.PolicySuggester.from_designer_factory(
            experimenter.problem_statement(),
            grid.GridSearchDesigner.from_problem,
        ),
    )
    runner.run(state)

  @parameterized.parameters(
      dict(best_category='corner'),
      dict(best_category='center'),
      dict(best_category='mixed'),
  )
  def test_compute_optimal_objective(
      self, best_category: simplekd.SimpleKDCategory
  ) -> None:
    exptr_simple4d = simplekd.SimpleKDExperimenter(best_category)
    categorical_values = ['corner', 'center', 'mixed']
    discrete_values = (1, 2, 5, 6, 8)
    integer_values = [1, 2, 3]
    continuous_values = np.linspace(-1, 1, 1001)

    opt_value = np.nan
    for cat_value in categorical_values:
      for disc_value in discrete_values:
        for cont_value in continuous_values:
          for int_value in integer_values:
            values = {
                'categorical': [cat_value],
                'discrete': [disc_value],
                'int': [int_value],
                'float': [cont_value],
            }
            opt_value = np.nanmax([exptr_simple4d._compute(values), opt_value])

    self.assertAlmostEqual(opt_value, exptr_simple4d.optimal_objective)

  @parameterized.parameters(
      dict(best_category='corner'),
      dict(best_category='center'),
      dict(best_category='mixed'),
  )
  def test_optimal_relative_error(
      self, best_category: simplekd.SimpleKDCategory
  ) -> None:
    exptr_simple4d = simplekd.SimpleKDExperimenter(
        best_category, output_relative_error=True
    )
    categorical_values = ['corner', 'center', 'mixed']
    discrete_values = (1, 2, 5, 6, 8)
    integer_values = [1, 2, 3]
    continuous_values = np.linspace(-1, 1, 1001)

    opt_rel_error = np.nan
    for cat_value in categorical_values:
      for disc_value in discrete_values:
        for cont_value in continuous_values:
          for int_value in integer_values:
            trial = vz.Trial({
                'categorical': cat_value,
                'discrete_0': disc_value,
                'int_0': int_value,
                'float_0': cont_value,
            })
            exptr_simple4d.evaluate([trial])
            if trial.final_measurement is None:
              raise ValueError('Final measurement is None.')
            rel_err = trial.final_measurement.metrics['value'].value
            opt_rel_error = np.nanmin([rel_err, opt_rel_error])

    self.assertAlmostEqual(opt_rel_error, 0.0)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/runners/benchmark_runner.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Benchmark runners to run benchmarks via modularized subroutines.

Modularization provides flexibility and clarity. Also, this allows easy
configuration of the protocol routine, which then can be applied to multiple
benchmarks via runner.run().

Ex: Typical Suggest + Evaluate loop all repeated for 7 iterations.

  # Define protocol (i.e. what should the benchmark do?).
  runner = benchmark_runner.BenchmarkRunner(
          benchmark_subroutines=[
              benchmark_runner.GenerateSuggestions(),
              benchmark_runner.EvaluateActiveTrials()
          ],
          num_repeats=7)

  # Initialize state (i.e. what problem and algorithm are we using?).
  benchmark_state =
  benchmark_runner.benchmark_state.BenchmarkState.from_designer_factory(
          designer_factory=designer_factory, experimenter=experimenter)

  # Runs benchmark protocol(s) and updates state.
  runner.run(benchmark_state)

Ex2: The power of modularization allows for complicated subroutines, such as
one that simulates the stochasticity of natural evaluation processes.

  # Allow for fixed-size batched Suggests and random-size evaluations.
  subroutines = []
  for _ in range(10):
    subroutines.append(benchmark_runner.GenerateSuggestions(num_suggestions=3)
    subroutines.append(benchmark_runner.EvaluateActiveTrial(
        num_trials_to_evaluate = np.randomInt(0, 10))
  runner = benchmark_runner.BenchmarkRunner(benchmark_subroutines=subroutines)
"""

import abc
import time
from typing import Optional, Sequence

from absl import logging
import attr
from vizier import pyvizier as vz
from vizier._src.benchmarks.runners import benchmark_state


class BenchmarkSubroutine(abc.ABC):
  """Abstraction for core benchmark routines.

  Benchmark protocols are modular alterations of BenchmarkState by reference.
  """

  @abc.abstractmethod
  def run(self, state: benchmark_state.BenchmarkState) -> None:
    """Abstraction to alter BenchmarkState by reference."""


@attr.define
class GenerateAndEvaluate(BenchmarkSubroutine):
  """Generate a fixed number of Suggestions and Evaluate them immediately.

  Use this Subroutine for simple benchmarks as it is computationally efficient.
  For benchmarks with complicated evaluation procedures, use a combination
  of other Subroutines.
  """

  # Number of total suggestions as a batch.
  batch_size: int = attr.field(
      default=1, validator=attr.validators.instance_of(int)
  )

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    suggestions = state.algorithm.suggest(self.batch_size)
    if not suggestions:
      logging.info(
          'Algorithm returned 0 suggestions. Expected: %s.',
          self.batch_size,
      )
    logging.info('Generated %s suggestions.', len(suggestions))
    state.experimenter.evaluate(list(suggestions))
    for t in suggestions:
      logging.info('Trial %s: %s', t.id, t.final_measurement)


@attr.define
class GenerateSuggestions(BenchmarkSubroutine):
  """Generate a fixed number of Suggestions as Active Trials."""

  # Number of total suggestions as a batch.
  batch_size: int = attr.field(
      default=1, validator=attr.validators.instance_of(int)
  )

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    suggestions = state.algorithm.suggest(self.batch_size)
    if not suggestions:
      logging.info(
          (
              'Suggestions did not generate %d suggestions'
              'because designer returned nothing.'
          ),
          self.batch_size,
      )


@attr.define
class FillActiveTrials(BenchmarkSubroutine):
  """Generate a number of Suggestions up to a limit of Active Trials."""

  # Upper limit of active Trials to be filled with new Suggestions.
  num_active_trials_limit: int = attr.field(
      default=1, validator=attr.validators.instance_of(int)
  )

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    active_trials = state.algorithm.supporter.GetTrials(
        status_matches=vz.TrialStatus.ACTIVE
    )
    if (
        num_suggestions := self.num_active_trials_limit - len(active_trials)
    ) > 0:
      suggestions = state.algorithm.suggest(num_suggestions)
      if not suggestions:
        logging.info(
            (
                'Suggestions did not generate %d suggestions'
                'to fill active Trials to %d '
                'because designer returned nothing.'
            ),
            num_suggestions,
            self.num_active_trials_limit,
        )


@attr.define
class EvaluateActiveTrials(BenchmarkSubroutine):
  """Evaluate a fixed number of Active Trials as Completed Trials."""

  # If None, this Evaluates all Active Trials.
  num_evaluations: Optional[int] = attr.field(default=None)

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    active_trials = state.algorithm.supporter.GetTrials(
        status_matches=vz.TrialStatus.ACTIVE
    )
    if self.num_evaluations is None:
      evaluated_trials = active_trials
    else:
      evaluated_trials = active_trials[: self.num_evaluations]

    state.experimenter.evaluate(evaluated_trials)
    logging.info('Evaluated %s trials.', len(evaluated_trials))
    for t in evaluated_trials:
      logging.info('Trial %s: %s', t.id, t.final_measurement)


@attr.define
class EvaluateAndAddPriorStudy(BenchmarkSubroutine):
  """Evaluate a fixed number of Active Trials as Completed Trials."""

  # Runner to use to mutate study.
  benchmark_runner: BenchmarkSubroutine = attr.field(
      kw_only=True,
      validator=attr.validators.instance_of(BenchmarkSubroutine),
  )
  # State factory.
  benchmark_state_factory: benchmark_state.BenchmarkStateFactory = attr.field(
      kw_only=True,
      validator=attr.validators.instance_of(
          benchmark_state.BenchmarkStateFactory
      ),
  )
  study_guid: Optional[str] = attr.field(
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
  )
  seed: Optional[int] = attr.field(
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(int)),
  )

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    prior_state = self.benchmark_state_factory(seed=self.seed)
    self.benchmark_runner.run(prior_state)
    prior_problem = prior_state.algorithm.supporter.GetStudyConfig()
    prior_trials = prior_state.algorithm.supporter.GetTrials()
    prior_study = vz.ProblemAndTrials(
        problem=prior_problem, trials=prior_trials
    )

    state.algorithm.supporter.SetPriorStudy(
        study=prior_study, study_guid=self.study_guid
    )


@attr.define
class BenchmarkRunner(BenchmarkSubroutine):
  """Run a sequence of subroutines, all repeated for a few iterations."""

  # A sequence of benchmark subroutines that alter BenchmarkState.
  benchmark_subroutines: Sequence[BenchmarkSubroutine] = attr.field()
  # Number of times to repeat applying benchmark_subroutines.
  num_repeats: int = attr.field(
      default=1, validator=attr.validators.instance_of(int)
  )

  def run(self, state: benchmark_state.BenchmarkState) -> None:
    """Run algorithm with benchmark subroutines with repetitions."""
    for iteration in range(self.num_repeats):
      start = time.time()
      for subroutine in self.benchmark_subroutines:
        subroutine.run(state)
      total_time = time.time() - start
      logging.info(
          'Completed BenchmarkRunner iteration %d out of %d (%f seconds)',
          iteration,
          self.num_repeats,
          total_time,
      )


--- vizier/_src/benchmarks/runners/benchmark_runner_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for base_runner."""

from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.experimenters import experimenter_factory
from vizier._src.benchmarks.runners import benchmark_runner
from vizier._src.benchmarks.runners import benchmark_state

from absl.testing import absltest
from absl.testing import parameterized


def _get_designer_benchmark_state_factory():
  dim = 10
  experimenter = experimenter_factory.BBOBExperimenterFactory('Sphere', dim)()

  def _designer_factory(config: vz.ProblemStatement, seed: int):
    return random.RandomDesigner(config.search_space, seed=seed)

  benchmark_state_factory = benchmark_state.DesignerBenchmarkStateFactory(
      designer_factory=_designer_factory, experimenter=experimenter
  )

  return benchmark_state_factory


class BaseRunnerTest(parameterized.TestCase):

  @parameterized.parameters(
      {
          'runner': benchmark_runner.BenchmarkRunner(
              benchmark_subroutines=[
                  benchmark_runner.GenerateSuggestions(),
                  benchmark_runner.EvaluateActiveTrials(),
              ],
              num_repeats=7,
          ),
          'expected_trials': 7,
      },
      {
          'runner': benchmark_runner.BenchmarkRunner(
              benchmark_subroutines=[benchmark_runner.GenerateAndEvaluate(10)],
              num_repeats=5,
          ),
          'expected_trials': 50,
      },
      {
          'runner': benchmark_runner.BenchmarkRunner(
              benchmark_subroutines=[
                  benchmark_runner.FillActiveTrials(10),
                  benchmark_runner.EvaluateActiveTrials(),
              ],
              num_repeats=5,
          ),
          'expected_trials': 50,
      },
  )
  def test_benchmark_run(self, runner, expected_trials):
    benchmark_state_factory = _get_designer_benchmark_state_factory()
    bench_state = benchmark_state_factory(seed=5)
    runner.run(bench_state)
    self.assertEmpty(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        )
    )
    all_trials = bench_state.algorithm.supporter.GetTrials()
    self.assertLen(all_trials, expected_trials)
    for trial in all_trials:
      self.assertEqual(trial.status, vz.TrialStatus.COMPLETED)

  def test_active_trials(self):
    benchmark_state_factory = _get_designer_benchmark_state_factory()
    bench_state = benchmark_state_factory(seed=5)
    runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[
            benchmark_runner.GenerateSuggestions(10),
            benchmark_runner.EvaluateActiveTrials(6),
        ],
        num_repeats=3,
    )
    runner.run(bench_state)
    self.assertLen(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        ),
        4 * 3,
    )
    self.assertLen(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.COMPLETED
        ),
        6 * 3,
    )

  def test_fill_active_trials(self):
    benchmark_state_factory = _get_designer_benchmark_state_factory()
    bench_state = benchmark_state_factory(seed=5)
    runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[
            benchmark_runner.FillActiveTrials(10),
            benchmark_runner.EvaluateActiveTrials(6),  # 6 Completions
            benchmark_runner.FillActiveTrials(3),  # No-op
            benchmark_runner.EvaluateActiveTrials(6),  # 4 Completions
            benchmark_runner.FillActiveTrials(10),
            benchmark_runner.EvaluateActiveTrials(6),  # 6 completions
        ]
    )
    runner.run(bench_state)
    self.assertLen(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        ),
        4,
    )
    self.assertLen(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.COMPLETED
        ),
        16,
    )

  def test_benchmark_run_from_exptr_factory(self):
    benchmark_state_factory = _get_designer_benchmark_state_factory()
    bench_state = benchmark_state_factory(seed=5)
    runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[benchmark_runner.GenerateAndEvaluate(10)],
        num_repeats=5,
    )
    runner.run(bench_state)
    self.assertEmpty(
        bench_state.algorithm.supporter.GetTrials(
            status_matches=vz.TrialStatus.ACTIVE
        )
    )
    all_trials = bench_state.algorithm.supporter.GetTrials()
    self.assertLen(all_trials, 50)
    for trial in all_trials:
      self.assertEqual(trial.status, vz.TrialStatus.COMPLETED)

  def test_add_prior(self):
    dim = 10
    exptr_factory = experimenter_factory.BBOBExperimenterFactory('Discus', dim)

    def _designer_factory(config: vz.ProblemStatement, seed: int):
      return random.RandomDesigner(config.search_space, seed=seed)

    prior_benchmark_state_factory = (
        benchmark_state.ExperimenterDesignerBenchmarkStateFactory(
            designer_factory=_designer_factory,
            experimenter_factory=exptr_factory,
        )
    )
    prior_runner = benchmark_runner.BenchmarkRunner(
        benchmark_subroutines=[benchmark_runner.GenerateAndEvaluate(10)],
        num_repeats=5,
    )
    prior_study_guid = 'prior'
    runner = benchmark_runner.EvaluateAndAddPriorStudy(
        benchmark_runner=prior_runner,
        benchmark_state_factory=prior_benchmark_state_factory,
        study_guid=prior_study_guid,
    )

    benchmark_state_factory = _get_designer_benchmark_state_factory()
    bench_state = benchmark_state_factory(seed=5)
    runner.run(bench_state)
    self.assertEmpty(bench_state.algorithm.supporter.GetTrials())

    prior_trials = bench_state.algorithm.supporter.GetTrials(
        study_guid=prior_study_guid
    )
    self.assertLen(prior_trials, 50)
    for trial in prior_trials:
      self.assertEqual(trial.status, vz.TrialStatus.COMPLETED)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/runners/benchmark_state.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Stateful algorithm suggesters with datastores to be used in benchmarks.

Definitions:
------------
- A 'suggester' generates suggestions and update its state based on the provided
suggestion results.

- A 'state' uses a suggester and experimenter to simulate a study. It calls
the suggester multiple times and evaluate the suggestions with the experimenter.
Note that the experimenter may be non-static.
"""

import abc
from typing import Optional, Protocol, Sequence

import attr
from vizier import algorithms as vza
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies.designer_policy import InRamDesignerPolicy
from vizier._src.benchmarks.experimenters.experimenter import Experimenter
from vizier._src.benchmarks.experimenters.experimenter_factory import ExperimenterFactory


@attr.define
class PolicySuggester:
  """Wraps a Policy and a datastore/supporter as an algorithm.

  NOTE: Updates to datastore should interface directly with supporter API. The
  policy will simply use the dataset that is in the supporter. This is the most
  faithful replication of the datastore vs suggest interaction in prod. Note
  that the algorithm/policy is updated with new Trials upon the suggest call.

  Typical usage:
    suggester = PolicySuggester(policy, supporter)
    # Trial ids are assigned by the supporter.
    suggestions = suggestion.suggest(...)
    trials = evaluate_and_complete_trials(suggestions)
    suggester.supporter.AddTrials(trials)
  """

  _policy: pythia.Policy = attr.field()
  _local_supporter: pythia.InRamPolicySupporter = attr.field()

  def suggest(self, batch_size: Optional[int]) -> Sequence[vz.Trial]:
    return self._local_supporter.SuggestTrials(
        self._policy, count=batch_size or 1
    )

  @property
  def supporter(self) -> pythia.InRamPolicySupporter:
    return self._local_supporter

  @classmethod
  def from_designer_factory(
      cls,
      problem: vz.ProblemStatement,
      designer_factory: vza.DesignerFactory[vza.Designer],
      supporter: Optional[pythia.InRamPolicySupporter] = None,
      seed: Optional[int] = None,
  ) -> 'PolicySuggester':
    """Initializes a PolicySuggester from Designer."""
    if supporter is None:
      supporter = pythia.InRamPolicySupporter(problem)
    # Wrap as policy and Policy
    policy = InRamDesignerPolicy(
        problem_statement=problem,
        supporter=supporter,
        designer_factory=designer_factory,
        seed=seed,
    )
    return PolicySuggester(policy=policy, local_supporter=supporter)


@attr.define
class BenchmarkState:
  """State of a benchmark run. It is altered via benchmark protocols."""

  experimenter: Experimenter
  algorithm: PolicySuggester


@attr.define(frozen=True)
class BenchmarkStateFactory(abc.ABC):
  """Factory class to generate new BenchmarkState."""

  @abc.abstractmethod
  def __call__(self, seed: Optional[int] = None) -> BenchmarkState:
    """Creates a new instance of BenchmarkFactory."""
    pass


@attr.define(frozen=True)
class ExperimenterDesignerBenchmarkStateFactory(BenchmarkStateFactory):
  """Generate new BenchmarkState from exptr/designer factory."""

  experimenter_factory: ExperimenterFactory
  designer_factory: vza.DesignerFactory[vza.Designer]

  def __call__(self, seed: Optional[int] = None) -> BenchmarkState:
    """Create a BenchmarkState from experimenter and designer factory."""
    experimenter = self.experimenter_factory()
    factory = DesignerBenchmarkStateFactory(
        experimenter=experimenter, designer_factory=self.designer_factory
    )
    return factory(seed=seed)


@attr.define(frozen=True)
class DesignerBenchmarkStateFactory(BenchmarkStateFactory):
  """Factory class to generate new BenchmarkState from designer factory."""

  experimenter: Experimenter
  designer_factory: vza.DesignerFactory[vza.Designer]

  def __call__(self, seed: Optional[int] = None) -> BenchmarkState:
    """Create a BenchmarkState from designer factory."""
    problem = self.experimenter.problem_statement()

    return BenchmarkState(
        experimenter=self.experimenter,
        algorithm=PolicySuggester.from_designer_factory(
            problem=problem,
            designer_factory=self.designer_factory,
            seed=seed,
        ),
    )


class SeededPolicyFactory(Protocol):
  """Factory to generate a policy with a seed."""

  def __call__(self, problem: vz.ProblemStatement, seed: int) -> pythia.Policy:
    ...


@attr.define(frozen=True)
class PolicyBenchmarkStateFactory(BenchmarkStateFactory):
  """Factory class to generate new BenchmarkState from policy factory."""

  experimenter: Experimenter
  policy_factory: SeededPolicyFactory

  def __call__(self, seed: Optional[int] = None) -> BenchmarkState:
    """Create a BenchmarkState from policy factory."""
    problem = self.experimenter.problem_statement()
    return BenchmarkState(
        experimenter=self.experimenter,
        algorithm=PolicySuggester(
            policy=self.policy_factory(problem, seed),
            local_supporter=pythia.InRamPolicySupporter(problem),
        ),
    )


--- vizier/_src/benchmarks/runners/benchmark_state_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier import algorithms as vza
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.runners import benchmark_state

from absl.testing import absltest


class BenchmarkStateTest(absltest.TestCase):

  def test_policy_suggester_active_trials_with_init(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('x', 0.0, 1.0)
    problem.search_space.root.add_float_param('y', 0.0, 1.0)
    problem.metric_information.append(
        vz.MetricInformation(
            name='maximize_metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )

    # Set up policy supporter with one Active and Completed Trial.
    policy_supporter = pythia.InRamPolicySupporter(problem)
    trials = [
        vz.Trial(parameters={'x': 0.5, 'y': 0.6}).complete(
            vz.Measurement(metrics={'maximize_metric': 0.3})
        ),
        vz.Trial(parameters={'x': 0.1, 'y': 0.2}),
    ]
    # The Completed and Active Trials should be updated in Designer.
    policy_supporter.AddTrials(trials)

    class DummyDesigner(vza.Designer):

      def __init__(self, problem_statement):
        self._designer = random.RandomDesigner(problem_statement.search_space)
        self.completed = []
        self.all_active = []

      def suggest(self, count):
        return self._designer.suggest(count)

      def update(self, completed, all_active):
        self.completed.extend(completed.trials)
        self.all_active = all_active.trials

    designer = DummyDesigner(problem)
    suggester = benchmark_state.PolicySuggester.from_designer_factory(
        problem, lambda _, **kwargs: designer, supporter=policy_supporter
    )

    suggestions = list(suggester.suggest(batch_size=5))
    self.assertLen(designer.completed, 1)
    self.assertLen(designer.all_active, 1)
    self.assertLen(suggestions, 5)

    # Report COMPLETED Trial back to supporter directly.
    completed_trial = suggestions[0].complete(
        vz.Measurement(metrics={'maximize_metric': 0.4})
    )
    suggester.supporter.AddTrials([completed_trial])
    suggester.suggest(1)
    self.assertLen(designer.completed, 2)
    self.assertLen(designer.all_active, 5)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/benchmarks/testing/experimenter_testing.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Assertions for testing experimenters."""

from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random
from vizier._src.benchmarks.experimenters import experimenter as experimenter_lib
from vizier._src.benchmarks.runners import benchmark_runner
from vizier._src.benchmarks.runners import benchmark_state


def assert_evaluates_random_suggestions(
    test,
    experimenter: experimenter_lib.Experimenter,
) -> None:
  """Asserts that random suggestions from the search space are valid."""
  runner = benchmark_runner.BenchmarkRunner(
      [benchmark_runner.GenerateAndEvaluate(10)], num_repeats=1
  )

  state = benchmark_state.BenchmarkState(
      experimenter=experimenter,
      algorithm=benchmark_state.PolicySuggester.from_designer_factory(
          experimenter.problem_statement(), random.RandomDesigner.from_problem
      ),
  )

  runner.run(state)

  test.assertLen(
      state.algorithm.supporter.GetTrials(
          status_matches=vz.TrialStatus.COMPLETED
      ),
      10,
  )


--- vizier/_src/jax/gp_bandit_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utilities for GP Bandit."""

import jax
from jax import numpy as jnp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types


def stochastic_process_model_loss_fn(
    params: types.ParameterDict,
    model: sp.StochasticProcessModel,
    data: types.ModelData,
    normalize: bool = False,
):
  """Loss function for a stochastic process model."""
  gp, mutables = model.apply(
      {'params': params},
      data.features,
      mutable=['losses', 'predictive'],
  )
  labels = data.labels.padded_array
  if len(gp.event_shape) == 1 and labels.shape[-1] == 1:
    labels = jnp.squeeze(data.labels.padded_array, axis=-1)
  loss = -gp.log_prob(
      labels,
      is_missing=data.labels.is_missing[0],
  ) + jax.tree_util.tree_reduce(jnp.add, mutables['losses'])
  if normalize:
    loss /= data.labels._original_shape[0]  # pylint: disable=protected-access
  return loss, dict()


def stochastic_process_model_setup(
    key: jax.Array,
    model: sp.StochasticProcessModel,
    data: types.ModelData,
):
  """Setup function for a stochastic process model."""
  return model.init(key, data.features)['params']


--- vizier/_src/jax/models/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Jax models."""


--- vizier/_src/jax/models/continuous_only_kernel.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""PSD Kernel for convenience with continuous-only data."""

import jax
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import types

tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


class ContinuousOnly(tfpk.PositiveSemidefiniteKernel):
  """Kernel for using only continuous data and discarding categorical.

  """

  def __init__(
      self,
      kernel: tfpk.PositiveSemidefiniteKernel,
  ):
    parameters = dict(locals())
    self._kernel = kernel

    super(ContinuousOnly, self).__init__(
        feature_ndims=tfpke.ContinuousAndCategoricalValues(
            kernel.feature_ndims, 1),
        dtype=tfpke.ContinuousAndCategoricalValues(
            kernel.dtype, types.INT_DTYPE),
        name='MaskFeatures',
        validate_args=False,
        parameters=parameters,
    )

  @property
  def kernel(self):
    return self._kernel

  def __getattr__(self, name):
    return getattr(self.kernel, name)

  def _apply(
      self, x1: types.ModelInput, x2: types.ModelInput, example_ndims=0
  ) -> jax.Array:
    return self.kernel._apply(x1.continuous, x2.continuous, example_ndims)  # pylint: disable=protected-access

  def _matrix(self, x1: types.ModelInput, x2: types.ModelInput) -> jax.Array:
    return self.kernel._matrix(x1.continuous, x2.continuous)  # pylint: disable=protected-access

  def matrix_over_all_tasks(
      self, x1: types.ModelInput, x2: types.ModelInput
  ) -> jax.Array:
    return self.kernel.matrix_over_all_tasks(x1.continuous, x2.continuous)  # pylint: disable=protected-access

  @classmethod
  def _parameter_properties(cls, dtype):
    return dict(
        kernel=tfp.internal.parameter_properties.BatchedComponentProperties(),
    )


def _continuous_to_cacv(x: jax.Array) -> tfpke.ContinuousAndCategoricalValues:
  return tfpke.ContinuousAndCategoricalValues(
      continuous=x, categorical=jnp.zeros(x.shape[:-1] + (0,))
  )


class EmptyCategoricalKernel(tfpk.PositiveSemidefiniteKernel):
  """Kernel that packs continuous data to ModelInput with empty categorical.

  If `k` is a PSDKernel that operates on `ModelInput` structures, then
  `EmptyCategoricalKernel(k)` is a kernel that operates on arrays of continuous
  data.

  Example:

  ```python
  tfpke = tfp.experimental.psd_kernels
  tfpk = tfp.math.psd_kernels

  # Define a kernel that operates on continuous and categorical values, with the
  # `categorical` field empty.
  kernel = tfpke.FeatureScaledWithCategorical(
      tfpk.ExponentiatedQuadratic(),
      scale_diag=tfpke.ContinuousAndCategoricalValues(
          continuous=jnp.ones([3]),
          categorical=jnp.ones([0])
      )
  )

  # Define an `EmptyCategoricalKernel` that operates on arrays of continuous
  # data.
  empty_cat_kernel = EmptyCategoricalKernel(k)
  xs = np.random.normal([10, 3])
  mat = empty_cat_kernel.matrix(xs, xs)  # Returns a [10, 10] matrix.
  ```
  """

  def __init__(
      self,
      kernel: tfpk.PositiveSemidefiniteKernel,
  ):
    parameters = dict(locals())
    self._kernel = kernel

    super(EmptyCategoricalKernel, self).__init__(
        feature_ndims=kernel.feature_ndims.continuous,
        dtype=kernel.dtype.continuous,
        name='EmptyCategoricalKernel',
        validate_args=False,
        parameters=parameters,
    )

  @property
  def kernel(self):
    return self._kernel

  def __getattr__(self, name):
    return getattr(self.kernel, name)

  def _apply(self, x1: jax.Array, x2: jax.Array, example_ndims: int = 0):
    return self.kernel._apply(  # pylint: disable=protected-access
        _continuous_to_cacv(x1), _continuous_to_cacv(x2), example_ndims
    )

  def _matrix(self, x1: jax.Array, x2: jax.Array):
    return self.kernel._matrix(  # pylint: disable=protected-access
        _continuous_to_cacv(x1), _continuous_to_cacv(x2)
    )

  @classmethod
  def _parameter_properties(cls, dtype):
    return dict(
        kernel=tfp.internal.parameter_properties.BatchedComponentProperties(),
    )


def _flatten(v):
  children = (v.kernel,)
  return (children, None)


def _unflatten(_, children):
  return ContinuousOnly(*children)


jax.tree_util.register_pytree_node(ContinuousOnly, _flatten, _unflatten)
jax.tree_util.register_pytree_node(EmptyCategoricalKernel, _flatten, _unflatten)

tfpke.MultiTaskKernel.register(ContinuousOnly)


--- vizier/_src/jax/models/gaussian_process_model.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Coroutine for a trainable Gaussian process with an ARD kernel."""

from typing import Any, Generator, Optional, Type, Union

import jax
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp_model
from vizier._src.jax import types

Array = Any
tfd = tfp.distributions
tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


class GaussianProcessARD(sp_model.ModelCoroutine):
  """Specifies an ARD Gaussian process model over continuous/categorical data.

  While `GaussianProcessARD` operates on a continuous domain,
  `GaussianProcessARDWithCategorical` operates on a domain that contains both
  continuous and categorical features. The GP's kernel, a
  `FeatureScaledWithCategorical` instance, operates on inputs of this type as
  well.

  `GaussianProcessARDWithCategorical` satisfies the `ModelCoroutine`
  Protocol. By default, `GaussianProcessARDWithCategorical` uses a Matern
  Five Halves kernel. Hyperparameter priors are LogNormal(0., 1.).

  #### Examples

  Build a Gaussian Process model with an ARD Exponentiated Quadratic kernel and
  LogNormal(0., 1.) hyperparameter priors for 5 continuous features and 3
  categorical features.

  ```python
  from tensorflow_probability.substrates import jax as tfp

  tfpk = tfp.math.psd_kernels
  tfpke = tfp.experimental.psd_kernels

  gen = GaussianProcessARDWithCategorical(
      dimension=types.ContinuousAndCategorical[int](
          continuous=5, categorical=3),
      kernel_class=tfpk.ExponentiatedQuadratic)
  ```
  """

  def __init__(
      self,
      dimension: types.ContinuousAndCategorical[int],
      kernel_class: Union[
          Type[tfpk.MaternFiveHalves],
          Type[tfpk.MaternThreeHalves],
          Type[tfpk.MaternOneHalf],
          Type[tfpk.Parabolic],
          Type[tfpk.ExponentiatedQuadratic],
      ] = tfpk.MaternFiveHalves,
      *,
      use_tfp_runtime_validation: bool = False,
      dtype: jnp.dtype = jnp.float64,
  ):
    """Initializes a `GaussianProcessARDWithCategorical`.

    Args:
      dimension: The dimensionality of the continuous/categorical features.
      kernel_class: The Gaussian process' kernel type.
      use_tfp_runtime_validation: If True, run additional runtime checks on the
        validity of the parameters and input for TFP objects (e.g. verify that
        amplitude and length scale parameters are positive). Runtime checks may
        be expensive and should be used only during development/debugging.
      dtype: Float dtype.
    """
    self.dimension = dimension
    self._kernel_class = kernel_class
    self._use_tfp_runtime_validation = use_tfp_runtime_validation
    self.dtype = dtype
    if dtype == jnp.float64 and not jax.config.read('jax_enable_x64'):
      raise ValueError(
          "x64 is not enabled for jax. Add jax_config.update('jax_enable_x64',"
          ' True) to your main'
      )

  def __call__(
      self, inputs: Optional[types.ModelInput] = None
  ) -> Generator[sp_model.ModelParameter, Array, tfd.GaussianProcess]:
    # TODO: Remove the following line when the linter bug is fixed.
    # pylint: disable=g-doc-return-or-yield
    """The coroutine that specifies the GP model.

    The inputs (and the index points of the returned Gaussian process) are
    instances of `tfpke.ContinuousAndCategoricalValues`, a `namedtuple` with
    a `"continuous"` field that contains continuous (float) data and a
    `"categorical"` field that contains categorical (int) data. The
    categorical data is encoded as integers (not one-hot encoded).

    Args:
      inputs: index_points to be provided to the GP.

    Yields:
      `ModelParameter`s describing the parameters to be declared in the Flax
        model.

    Returns:
      A `tfd.GaussianProcess` with the given index points.
    """
    # pytype: disable=not-callable  # jnp-type
    amplitude = yield sp_model.ModelParameter.from_prior(
        tfd.LogNormal(self.dtype(0.0), 1.0, name='amplitude'),
        constraint=sp_model.Constraint(bounds=(self.dtype(0.0), None)),
    )
    kernel = self._kernel_class(
        amplitude=amplitude,
        length_scale=self.dtype(1.0),
        validate_args=self._use_tfp_runtime_validation,
    )
    inverse_length_scale_continuous = yield sp_model.ModelParameter.from_prior(
        tfd.Sample(
            tfd.LogNormal(self.dtype(0.0), 1.0),
            sample_shape=(self.dimension.continuous,),
            name='inverse_length_scale_continuous',
        ),
        constraint=sp_model.Constraint(bounds=(self.dtype(0.0), None)),
    )
    inverse_length_scale_categorical = yield sp_model.ModelParameter.from_prior(
        tfd.Sample(
            tfd.LogNormal(self.dtype(0.0), 1.0),
            sample_shape=(self.dimension.categorical,),
            name='inverse_length_scale_categorical',
        ),
        constraint=sp_model.Constraint(
            bounds=(jnp.zeros(self.dimension.categorical, dtype=self.dtype),
                    None)
        ),
    )
    kernel = tfpke.FeatureScaledWithCategorical(
        kernel,
        inverse_scale_diag=tfpke.ContinuousAndCategoricalValues(
            inverse_length_scale_continuous, inverse_length_scale_categorical
        ),
        validate_args=self._use_tfp_runtime_validation,
    )
    observation_noise_variance = yield sp_model.ModelParameter.from_prior(
        tfd.LogNormal(self.dtype(0.0), 1.0,
                      name='observation_noise_variance'),
        constraint=sp_model.Constraint(bounds=(self.dtype(0.0), None)),
    )
    # pytype: enable=not-callable
    if inputs is not None:
      inputs = tfpke.ContinuousAndCategoricalValues(
          inputs.continuous.padded_array, inputs.categorical.padded_array
      )
    return tfd.GaussianProcess(
        kernel,
        index_points=inputs,
        observation_noise_variance=observation_noise_variance,
        validate_args=self._use_tfp_runtime_validation,
    )


--- vizier/_src/jax/models/gaussian_process_model_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for gaussian_process_model."""

import jax
from jax import random
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import types
from vizier._src.jax.models import gaussian_process_model as gp_model
from absl.testing import absltest

tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


class GaussianProcessARDTest(absltest.TestCase):

  def test_gp_model_with_categorical(self):
    cont_dim = 5
    cat_dim = 3
    num_obs = 10
    coro = gp_model.GaussianProcessARD(
        dimension=types.ContinuousAndCategorical[int](cont_dim, cat_dim),
        kernel_class=tfpk.ExponentiatedQuadratic,
        use_tfp_runtime_validation=True,
    )

    x_cont_key, x_cat_key, coro_key, sample_key = random.split(
        random.PRNGKey(0), num=4
    )
    x_cont = random.uniform(x_cont_key, shape=(num_obs, cont_dim),
                            dtype=np.float64)
    x_cat = random.randint(
        x_cat_key, shape=(num_obs, cat_dim), minval=0, maxval=5
    )
    x = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            x_cont, x_cont.shape, fill_value=np.nan
        ),
        categorical=types.PaddedArray.from_array(
            x_cat, x_cat.shape, fill_value=-1
        ),
    )
    gp, param_vals = _run_coroutine(coro(x), seed=coro_key)
    samples = gp.sample(100, seed=sample_key)
    self.assertSequenceEqual(gp.event_shape, [num_obs])
    self.assertEmpty(gp.batch_shape)
    self.assertTrue(np.isfinite(gp.log_prob(samples)).all())
    self.assertSameElements(
        param_vals.keys(),
        (
            'amplitude',
            'inverse_length_scale_continuous',
            'inverse_length_scale_categorical',
            'observation_noise_variance',
        ),
    )
    self.assertEmpty(param_vals['amplitude'].shape)
    self.assertEmpty(param_vals['observation_noise_variance'].shape)
    self.assertSequenceEqual(
        param_vals['inverse_length_scale_continuous'].shape, [cont_dim]
    )
    self.assertSequenceEqual(
        param_vals['inverse_length_scale_categorical'].shape, [cat_dim]
    )


def _run_coroutine(g, seed):
  param = next(g)
  param_vals = {}
  try:
    while True:
      seed, current_seed = random.split(seed)
      v = param.init_fn(current_seed)
      param_vals[param.name] = v
      param = g.send(v)
  except StopIteration as e:
    return e.value, param_vals


if __name__ == '__main__':
  jax.config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/jax/models/hebo_gp_model.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""HEBO GP models.


Faithful reimplementation of HEBO model based off of:
Paper: https://arxiv.org/abs/2012.03826
Repo: https://github.com/huawei-noah/HEBO.
"""

from typing import Generator, Optional
from flax import struct
import jax
from jax import numpy as jnp
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import continuous_only_kernel

tfd = tfp.distributions
tfb = tfp.bijectors
tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


@struct.dataclass
class VizierHeboGaussianProcess(sp.ModelCoroutine[tfd.GaussianProcess]):
  """Hebo Gaussian process model."""

  @classmethod
  def build_model(
      cls,
      features: types.ModelInput,
  ) -> sp.StochasticProcessModel:
    """Returns the model and loss function."""
    del features
    gp_coroutine = VizierHeboGaussianProcess()
    return sp.StochasticProcessModel(gp_coroutine)

  def __call__(
      self, inputs: Optional[types.ModelInput] = None
  ) -> Generator[sp.ModelParameter, jax.Array, tfd.GaussianProcess]:
    """Creates a generator.

    Args:
      inputs: tuple of (train_features, train_labels) train_features - array of
        dimension (num_examples, _feature_dim) train_labels - array of dimension
        (num_examples,)

    Yields:
      GaussianProcess whose event shape is `num_examples`.
    """
    epsilon = jnp.finfo(jnp.float64).resolution
    lim_val = jnp.float64(36.0)

    def _constraint_fn(x):
      return jnp.where(x > lim_val, x,
                       jnp.log1p(jnp.exp(jnp.clip(x, -lim_val,
                                                  lim_val)))) + epsilon

    def _inverse_constraint_fn(f):
      return jnp.where(f > lim_val, f, jnp.log(jnp.exp(f + 1e-20) - 1.))

    constraint_bijector = tfb.Inline(
        forward_fn=_constraint_fn,
        inverse_fn=_inverse_constraint_fn,
        forward_min_event_ndims=0)
    sigmoid = tfb.Sigmoid(jnp.float64(0.0), jnp.float64(10.0))

    # Signal variance
    gamma = tfd.Gamma(
        concentration=jnp.float64(0.5),
        rate=jnp.float64(1.0),
        name='signal_variance')
    signal_variance = yield sp.ModelParameter.from_prior(
        gamma,
        constraint=sp.Constraint((epsilon, None), bijector=constraint_bijector))

    # Observation noise variance
    noise_log_normal = tfd.LogNormal(
        loc=jnp.float64(-4.63), scale=0.5, name='observation_noise_variance')
    observation_noise_variance = yield sp.ModelParameter.from_prior(
        noise_log_normal,
        constraint=sp.Constraint((epsilon, None), bijector=constraint_bijector))

    # Kernel
    kernel = tfpk.MaternThreeHalves(
        amplitude=jnp.sqrt(signal_variance)) + tfpk.Linear()

    # Length scale
    length_scale = yield sp.ModelParameter.from_prior(
        tfd.LogNormal(
            loc=jnp.float64(0.0), scale=jnp.float64(1.0), name='length_scale'
        ),
        constraint=sp.Constraint((0.0, None), bijector=tfb.Exp()),
    )
    kernel = tfpk.FeatureScaled(kernel, scale_diag=length_scale)

    # Kumaraswamy input warping
    kumar_log_normal = tfd.LogNormal(loc=jnp.float64(0.0), scale=0.75)

    concentration0 = yield sp.ModelParameter.from_prior(
        kumar_log_normal.copy(name='concentration0'),
        constraint=sp.Constraint((0.0, 10.0), bijector=sigmoid),
    )

    concentration1 = yield sp.ModelParameter.from_prior(
        kumar_log_normal.copy(name='concentration1'),
        constraint=sp.Constraint((0.0, 10.0), bijector=sigmoid),
    )

    kernel = tfpk.KumaraswamyTransformed(kernel, concentration1, concentration0)

    kernel = continuous_only_kernel.ContinuousOnly(kernel)
    if inputs is not None:
      inputs = tfpke.ContinuousAndCategoricalValues(
          inputs.continuous.padded_array, inputs.categorical.padded_array
      )
    return tfd.GaussianProcess(
        kernel,
        index_points=inputs,
        observation_noise_variance=observation_noise_variance,
        cholesky_fn=None,
    )


--- vizier/_src/jax/models/hebo_gp_model_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for hebo_gp_model."""

from absl import logging
import jax
from jax import config
from jax import numpy as jnp
import optax
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import hebo_gp_model
from vizier.jax import optimizers

from absl.testing import absltest


VizierHeboGaussianProcess = hebo_gp_model.VizierHeboGaussianProcess


class VizierHeboGaussianProcessTest(absltest.TestCase):

  def setUp(self):
    super(VizierHeboGaussianProcessTest, self).setUp()
    self.x_obs = jnp.array([[
        0.2941264, 0.29313548, 0.68817519, 0.37502566, 0.48356813, 0.34127283
    ], [
        0.66218224, 0.70770083, 0.6901334, 0.66787973, 0.5400858, 0.52721233
    ], [
        0.88469647, 0.50593371, 0.83160862, 0.58674892, 0.42145673, 0.31749428
    ], [
        0.39976682, 0.59517741, 0.73295106, 0.6084903, 0.54891015, 0.44338632
    ], [
        0.8354305, 0.87605574, 0.47855956, 0.48174861, 0.37685449, 0.38348768
    ], [
        0.55608455, 0.72781129, 0.52432913, 0.44291417, 0.3816395, 0.326599
    ], [
        0.24689187, 0.50979672, 0.67604857, 0.45172594, 0.34994392, 0.75239792
    ], [
        0.71007257, 0.60896354, 0.29270877, 0.74683367, 0.50169051, 0.74480515
    ], [
        0.9193235, 0.24393112, 0.63868591, 0.43271524, 0.43339578, 0.59413154
    ], [0.51850627, 0.62689204, 0.76134879, 0.65990021, 0.82350868, 0.7429215]],
                           dtype=jnp.float64)

    self.y_obs = jnp.array(
        [
            0.55552674,
            -0.29054829,
            -0.04703586,
            0.0217839,
            0.15445438,
            0.46654119,
            0.12255823,
            -0.19540335,
            -0.11772564,
            -0.44447326,
        ],
        dtype=jnp.float64,
    )[:, jnp.newaxis]

  def test_log_prob_and_loss(self):
    inputs = types.ModelInput(
        continuous=types.PaddedArray.as_padded(self.x_obs),
        categorical=types.PaddedArray.as_padded(
            jnp.zeros((self.x_obs.shape[0], 0)).astype(jnp.int32),
        ),
    )
    data = types.ModelData(
        features=inputs,
        labels=types.PaddedArray.as_padded(self.y_obs),
    )
    model = sp.CoroutineWithData(
        hebo_gp_model.VizierHeboGaussianProcess(), data=data
    )
    key, init_key, init_keys = jax.random.split(jax.random.PRNGKey(2), 3)
    init_params = model.setup(init_key)
    optimize = optimizers.OptaxTrain(optax.adam(5e-3), epochs=500, verbose=True)
    constraints = sp.get_constraints(model)
    params, metrics = optimize(
        init_params=jax.vmap(model.setup)(jax.random.split(init_keys, 20)),
        loss_fn=model.loss_with_aux,
        rng=key,
        constraints=constraints,
    )

    self.assertGreater(
        model.loss_with_aux(init_params), model.loss_with_aux(params)
    )
    losses_every_50 = metrics['loss'][::50]
    self.assertTrue((losses_every_50[1:] < losses_every_50[:-1]).all())

    logging.info('Optimal parameters: %s', params)
    final_loss = model.loss_with_aux(params)[0]
    logging.info('Optimal loss fn: %s', final_loss)
    self.assertLess(final_loss, 0.3)


if __name__ == '__main__':
  config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/jax/models/mask_features.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""PSD Kernel for masking out dimensions."""

import jax
import jax.numpy as jnp
from tensorflow_probability.substrates import jax as tfp

tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


class MaskFeatures(tfpk.PositiveSemidefiniteKernel):
  """Kernel for masking out features by setting them to zero.

  By masking out feature dimensions to zero (or to any other finite value), we
  ensure that when the Stochastic Process Model aware of this masking ingests
  these features (which may be NaN), we will get finite gradients. See
  https://github.com/tensorflow/probability/blob/main/discussion/where-nan.pdf
  for more details.
  """

  def __init__(
      self,
      kernel: tfpk.PositiveSemidefiniteKernel,
      dimension_is_missing: tfpke.ContinuousAndCategoricalValues,
  ):
    parameters = dict(locals())
    self._kernel = kernel
    self._dimension_is_missing = dimension_is_missing

    def _transformation_fn(x, *_):
      return jax.tree_util.tree_map(
          lambda x_, d: jnp.where(d, jnp.zeros_like(x_), x_),
          x,
          dimension_is_missing,
      )

    self._mask_kernel = tfpk.FeatureTransformed(kernel, _transformation_fn)

    super(MaskFeatures, self).__init__(
        feature_ndims=kernel.feature_ndims,
        dtype=kernel.dtype,
        name='MaskFeatures',
        validate_args=False,
        parameters=parameters,
    )

  @property
  def kernel(self):
    return self._kernel

  @property
  def dimension_is_missing(self):
    return self._dimension_is_missing

  def _apply(self, x1, x2, example_ndims=0):
    return self._mask_kernel._apply(x1, x2, example_ndims)  # pylint: disable=protected-access

  def _matrix(self, x1, x2):
    return self._mask_kernel._matrix(x1, x2)  # pylint: disable=protected-access

  @classmethod
  def _parameter_properties(cls, dtype):
    return dict(
        kernel=tfp.internal.parameter_properties.BatchedComponentProperties(),
        dimension_is_missing=(
            tfp.internal.parameter_properties.ParameterProperties(
                event_ndims=lambda self: self.kernel.feature_ndims
            )
        ),
    )


def _mask_features_flatten(v):
  children = (v.kernel, v.dimension_is_missing)
  return (children, None)


def _mask_features_unflatten(_, children):
  return MaskFeatures(*children)


jax.tree_util.register_pytree_node(
    MaskFeatures, _mask_features_flatten, _mask_features_unflatten
)


--- vizier/_src/jax/models/mask_features_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for MaskFeatures."""

from absl.testing import parameterized
import jax
import jax.numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax.models import mask_features

from absl.testing import absltest

tfpke = tfp.experimental.psd_kernels


class MaskFeaturesTest(parameterized.TestCase):

  @parameterized.parameters(1, 2, 3, 4)
  def testMaskingAndGradient(self, half_dims: int):
    xs_cont = np.random.randn(10, half_dims)
    xs_cont = np.concatenate(
        [xs_cont, np.full([10, half_dims], np.nan)], axis=-1
    )
    # Generate some integer-encoded categorical data with values between 0 and
    # 5.
    xs_cat = np.broadcast_to(
        np.arange(10)[:, jnp.newaxis] // 2,
        shape=(10, half_dims),
    )
    xs_cat = np.concatenate([xs_cat, np.full([10, half_dims], -1)], axis=-1)
    missing_dim = np.array([False] * half_dims + [True] * half_dims)
    x = tfpke.ContinuousAndCategoricalValues(xs_cont, xs_cat)

    def f(s):
      kernel = tfp.math.psd_kernels.MaternThreeHalves()
      kernel = tfpke.FeatureScaledWithCategorical(
          kernel,
          scale_diag=s,
      )

      kernel = mask_features.MaskFeatures(
          kernel,
          dimension_is_missing=tfpke.ContinuousAndCategoricalValues(
              continuous=missing_dim, categorical=missing_dim
          ),
      )
      return jnp.sum(kernel.matrix(x, x))

    scale_diag = tfpke.ContinuousAndCategoricalValues(
        np.random.uniform(low=1.0, high=10.0, size=(half_dims * 2,)),
        np.random.uniform(low=1.0, high=10.0, size=(half_dims * 2,)),
    )
    value, grad = jax.value_and_grad(f)(scale_diag)
    self.assertTrue(np.all(~np.isnan(value)))
    self.assertTrue(np.all(~np.isnan(grad.continuous)))
    self.assertTrue(np.all(grad.continuous[~missing_dim] != 0))
    self.assertTrue(np.all(grad.continuous[missing_dim] == 0))
    self.assertTrue(np.all(~np.isnan(grad.categorical)))
    self.assertTrue(np.all(grad.categorical[~missing_dim] != 0))
    self.assertTrue(np.all(grad.categorical[missing_dim] == 0))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/jax/models/multitask_tuned_gp_models.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Collection of multitask GP models."""

import enum
import functools
from typing import Any, Generator, Optional, Union

from flax import struct
import jax
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import continuous_only_kernel
from vizier._src.jax.models import mask_features


tfb = tfp.bijectors
tfd = tfp.distributions
tfde = tfp.experimental.distributions
tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


class MultiTaskType(enum.Enum):
  """The form of the MultiTask GP."""

  # Use a TFP MultiTask GP with an Independent kernel. A base (prior) kernel
  # matrix is shared across all tasks, and the overall kernel matrix is the
  # Kronecker product of the base kernel matrix and the identity.
  INDEPENDENT = 'independent'

  # Use a TFP MultiTask GP with a Separable kernel. The prior kernel matrix
  # is the Kronecker product of a base kernel matrix and a task kernel matrix
  # (assumed to be full-covariance).
  SEPARABLE_NORMAL_TASK_KERNEL_PRIOR = 'separable_normal_task_kernel_prior'

  # Use an LKJ-distributed prior for the (full-covariance) Separable task
  # kernel.
  SEPARABLE_LKJ_TASK_KERNEL_PRIOR = 'separable_lkj_task_kernel_prior'

  # Use a diagonal matrix for the Separable task kernel.
  SEPARABLE_DIAG_TASK_KERNEL_PRIOR = 'separable_diag_task_kernel_prior'


def build_task_kernel_scale_linop(
    num_tasks: int,
    multitask_type: MultiTaskType,
) -> Generator[sp.ModelParameter, jax.Array, tfp.tf2jax.linalg.LinearOperator]:
  """Builds a Separable MultiTask GP's task kernel scale LinearOperator.

  Args:
    num_tasks: The number of tasks.
    multitask_type: The type of MultiTask GP.

  Yields:
    Model parameters for the task kernel scale and a LinearOperator representing
    the task kernel scale.
  """
  if multitask_type == MultiTaskType.SEPARABLE_DIAG_TASK_KERNEL_PRIOR:
    correlation_diag = yield sp.ModelParameter.from_prior(
        tfd.Sample(
            tfd.Uniform(low=jnp.float64(1e-6), high=1.0),
            sample_shape=num_tasks,
            name='correlation_diag',
        ),
        constraint=sp.Constraint(
            bounds=(1e-6, 1.0),
            bijector=tfb.Sigmoid(low=jnp.float64(1e-6), high=1.0),
        ),
    )
    task_kernel_scale_linop = tfp.tf2jax.linalg.LinearOperatorDiag(
        correlation_diag
    )
  elif multitask_type == MultiTaskType.SEPARABLE_LKJ_TASK_KERNEL_PRIOR:
    # Generate parameters for the Cholesky of the task kernel matrix,
    # which accounts for correlations between tasks.
    num_task_kernel_entries = tfb.CorrelationCholesky().inverse_event_shape(
        [num_tasks, num_tasks]
    )
    correlation_cholesky_vec = yield sp.ModelParameter(
        init_fn=lambda key: tfd.Sample(  # pylint: disable=g-long-lambda
            tfd.Normal(jnp.float64(0.0), 1.0), num_task_kernel_entries
        ).sample(seed=key),
        # Use `jnp.copy` to prevent tracers leaking from bijector cache.
        regularizer=lambda x: -tfd.CholeskyLKJ(  # pylint: disable=g-long-lambda
            dimension=num_tasks, concentration=1.0
        ).log_prob(tfb.CorrelationCholesky()(jnp.copy(x))),
        name='task_kernel_correlation_cholesky_vec',
    )

    task_kernel_correlation_cholesky = tfb.CorrelationCholesky()(
        jnp.copy(correlation_cholesky_vec)
    )

    task_kernel_scale_vec = yield sp.ModelParameter(
        init_fn=functools.partial(
            jax.random.uniform,
            shape=(num_tasks,),
            dtype=jnp.float64,
            minval=1e-6,
            maxval=1.0,
        ),
        constraint=sp.Constraint(
            bounds=(1e-6, 1.0),
            bijector=tfb.Sigmoid(low=jnp.float64(1e-6), high=1.0),
        ),
        name='task_kernel_sqrt_diagonal',
    )
    task_kernel_cholesky = (
        task_kernel_correlation_cholesky * task_kernel_scale_vec[:, jnp.newaxis]
    )

    # Build the `LinearOperator` object representing the task kernel matrix,
    # to parameterize the Separable kernel.
    task_kernel_scale_linop = tfp.tf2jax.linalg.LinearOperatorLowerTriangular(
        task_kernel_cholesky
    )
  elif multitask_type == MultiTaskType.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR:
    # Generate parameters for the Cholesky of the task kernel matrix;
    # accounts for correlations between tasks. The task kernel matrix must
    # be positive definite, so we construct it via a Cholesky factor.
    # Define the prior of the kernel task matrix to be centered at the
    # identity.
    prior_mean = jnp.eye(num_tasks, dtype=jnp.float64)
    prior_mean_vec = tfb.FillTriangular().inverse(prior_mean)
    prior_mean_batched = jnp.broadcast_to(prior_mean_vec, prior_mean_vec.shape)

    task_kernel_cholesky_entries = yield sp.ModelParameter.from_prior(
        tfd.Independent(
            tfd.Normal(prior_mean_batched, 1.0),
            reinterpreted_batch_ndims=1,
            name='task_kernel_cholesky_entries',
        )
    )

    # Apply a bijector to pack the task kernel entries into a lower
    # triangular matrix and ensure the diagonal is positive.
    task_kernel_bijector = tfb.Chain([
        tfb.TransformDiagonal(
            tfb.Chain([tfb.Shift(jnp.float64(1e-3)), tfb.Softplus()])
        ),
        tfb.FillTriangular(),
    ])
    task_kernel_cholesky = task_kernel_bijector(
        jnp.copy(task_kernel_cholesky_entries)
    )

    # Build the `LinearOperator` object representing the task kernel
    # matrix, to parameterize the Separable kernel.
    task_kernel_scale_linop = tfp.tf2jax.linalg.LinearOperatorLowerTriangular(
        task_kernel_cholesky
    )
  else:
    raise ValueError(f'Unsupported multitask type: {multitask_type}')

  return task_kernel_scale_linop


@struct.dataclass
class VizierMultitaskGaussianProcess(
    sp.ModelCoroutine[Union[tfd.GaussianProcess, tfde.MultiTaskGaussianProcess]]
):
  """Multitask GP model using priors from Vizier's tuned GP."""

  _feature_dim: types.ContinuousAndCategorical[int] = struct.field(
      pytree_node=False
  )
  _num_tasks: int = struct.field(pytree_node=False)
  _multitask_type: MultiTaskType = struct.field(
      default=MultiTaskType.INDEPENDENT, kw_only=True, pytree_node=False
  )
  _use_retrying_cholesky: bool = struct.field(
      default=True, kw_only=True, pytree_node=False
  )
  _boundary_epsilon: float = struct.field(default=1e-12, kw_only=True)

  def _log_uniform_init(
      self,
      low: Union[float, np.floating],
      high: Union[float, np.floating],
      shape: tuple[int, ...] = tuple(),
  ) -> sp.InitFn:
    r"""Take log-uniform sample in the constraint and map it back to \R.

    Args:
      low: Parameter lower bound.
      high: Parameter upper bound.
      shape: Returned array has this shape. Each entry in the returned array is
        an i.i.d sample.

    Returns:
      Randomly sampled array.
    """

    def sample(key: Any) -> jnp.ndarray:
      unif = jax.random.uniform(key, shape, dtype=jnp.float64)
      return jnp.exp(unif * jnp.log(high / low) + jnp.log(low))

    return sample

  def __call__(
      self, inputs: Optional[types.ModelInput] = None
  ) -> Generator[
      sp.ModelParameter,
      jax.Array,
      Union[tfd.GaussianProcess, tfde.MultiTaskGaussianProcess],
  ]:
    """Creates a generator.

    Args:
      inputs: Floating array of dimension (num_examples, _feature_dim).

    Yields:
      Multitask GP hyperparameters.
    """

    eps = self._boundary_epsilon
    observation_noise_bounds = (np.float64(1e-10 - eps), 1.0 + eps)
    amplitude_bounds = (np.float64(1e-3 - eps), 10.0 + eps)
    continuous_ones = np.ones((self._feature_dim.continuous), dtype=np.float64)
    continuous_length_scale_bounds = (
        continuous_ones * (1e-2 - eps),
        continuous_ones * 1e2 + eps,
    )
    categorical_ones = np.ones(
        (self._feature_dim.categorical), dtype=np.float64
    )
    categorical_length_scale_bounds = (
        categorical_ones * (1e-2 - eps),
        categorical_ones * 1e2 + eps,
    )

    signal_variance = yield sp.ModelParameter(
        init_fn=self._log_uniform_init(*amplitude_bounds),
        constraint=sp.Constraint(
            amplitude_bounds,
            tfb.SoftClip(*amplitude_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: 0.01 * jnp.log(x / 0.039) ** 2,
        name='signal_variance',
    )
    kernel = tfpk.MaternFiveHalves(amplitude=jnp.sqrt(signal_variance))
    continuous_length_scale_squared = yield sp.ModelParameter(
        init_fn=self._log_uniform_init(
            *continuous_length_scale_bounds,
            shape=(self._feature_dim.continuous,),
        ),
        constraint=sp.Constraint(
            continuous_length_scale_bounds,
            tfb.SoftClip(*continuous_length_scale_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: jnp.sum(0.01 * jnp.log(x / 0.5) ** 2),
        name='continuous_length_scale_squared',
    )
    categorical_length_scale_squared = yield sp.ModelParameter(
        init_fn=self._log_uniform_init(
            *categorical_length_scale_bounds,
            shape=(self._feature_dim.categorical,),
        ),
        constraint=sp.Constraint(
            categorical_length_scale_bounds,
            tfb.SoftClip(*categorical_length_scale_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: jnp.sum(0.01 * jnp.log(x / 0.5) ** 2),
        name='categorical_length_scale_squared',
    )

    kernel = tfpke.FeatureScaledWithCategorical(
        kernel,
        scale_diag=tfpke.ContinuousAndCategoricalValues(
            jnp.sqrt(continuous_length_scale_squared),
            jnp.sqrt(categorical_length_scale_squared),
        ),
    )

    bias_amplitude = yield sp.ModelParameter.from_prior(
        tfd.Normal(
            jnp.zeros(shape=tuple(), dtype=jnp.float64),
            1.0,
            name='bias_amplitude',
        ),
    )
    slope_amplitude = yield sp.ModelParameter.from_prior(
        tfd.Normal(
            jnp.zeros(shape=tuple(), dtype=jnp.float64),
            1.0,
            name='slope_amplitude',
        ),
    )
    shift = yield sp.ModelParameter.from_prior(
        tfd.Normal(
            jnp.zeros(shape=tuple(), dtype=jnp.float64),
            1.0,
            name='shift',
        )
    )
    kernel = kernel + continuous_only_kernel.ContinuousOnly(
        tfpk.FeatureScaled(
            tfpk.Linear(
                slope_amplitude=slope_amplitude,
                bias_amplitude=bias_amplitude,
                shift=shift,
            ),
            scale_diag=jnp.sqrt(continuous_length_scale_squared),
        )
    )

    observation_noise_variance = yield sp.ModelParameter(
        init_fn=self._log_uniform_init(*observation_noise_bounds),
        constraint=sp.Constraint(
            observation_noise_bounds,
            tfb.SoftClip(*observation_noise_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: 0.01 * jnp.log(x / 0.0039) ** 2,
        name='observation_noise_variance',
    )

    cholesky_fn = None
    # When cholesky fails, increase jitters and retry.
    if self._use_retrying_cholesky:
      retrying_cholesky = functools.partial(
          tfp.experimental.distributions.marginal_fns.retrying_cholesky,
          jitter=np.float64(1e-4),
          max_iters=5,
      )
      cholesky_fn = lambda matrix: retrying_cholesky(matrix)[0]

    if inputs is not None:
      kernel = mask_features.MaskFeatures(
          kernel,
          dimension_is_missing=tfpke.ContinuousAndCategoricalValues(
              continuous=inputs.continuous.is_missing[1],
              categorical=inputs.categorical.is_missing[1],
          ),
      )

      inputs = tfpke.ContinuousAndCategoricalValues(
          inputs.continuous.padded_array,
          inputs.categorical.padded_array,
      )

    # Creates multitask kernel from single-task kernel. Defaults to INDEPENDENT
    # for multitask kernel.
    if self._multitask_type == MultiTaskType.INDEPENDENT:
      multitask_kernel = tfpke.Independent(self._num_tasks, kernel)
    else:
      task_kernel_scale_linop = yield from build_task_kernel_scale_linop(
          self._num_tasks, self._multitask_type
      )
      multitask_kernel = tfpke.Separable(
          self._num_tasks,
          base_kernel=kernel,
          task_kernel_scale_linop=task_kernel_scale_linop,
      )

    return tfde.MultiTaskGaussianProcess(
        multitask_kernel,
        index_points=inputs,
        observation_noise_variance=observation_noise_variance,
        cholesky_fn=cholesky_fn,
    )


--- vizier/_src/jax/models/multitask_tuned_gp_models_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from absl.testing import parameterized
import jax
from jax import config
import numpy as np
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier.jax import optimizers

from absl.testing import absltest


class MultitaskTunedGpModelsTest(parameterized.TestCase):

  def _generate_xys(self):
    # x_obs has shape [10, 6] and y_obs [10, 2]
    x_obs = np.array(
        [
            [
                0.2941264,
                0.29313548,
                0.68817519,
                0.37502566,
                0.48356813,
                0.34127283,
            ],
            [
                0.66218224,
                0.70770083,
                0.6901334,
                0.66787973,
                0.5400858,
                0.52721233,
            ],
            [
                0.88469647,
                0.50593371,
                0.83160862,
                0.58674892,
                0.42145673,
                0.31749428,
            ],
            [
                0.39976682,
                0.59517741,
                0.73295106,
                0.6084903,
                0.54891015,
                0.44338632,
            ],
            [
                0.8354305,
                0.87605574,
                0.47855956,
                0.48174861,
                0.37685449,
                0.38348768,
            ],
            [
                0.55608455,
                0.72781129,
                0.52432913,
                0.44291417,
                0.3816395,
                0.326599,
            ],
            [
                0.24689187,
                0.50979672,
                0.67604857,
                0.45172594,
                0.34994392,
                0.75239792,
            ],
            [
                0.71007257,
                0.60896354,
                0.29270877,
                0.74683367,
                0.50169051,
                0.74480515,
            ],
            [
                0.9193235,
                0.24393112,
                0.63868591,
                0.43271524,
                0.43339578,
                0.59413154,
            ],
            [
                0.51850627,
                0.62689204,
                0.76134879,
                0.65990021,
                0.82350868,
                0.7429215,
            ],
        ],
        dtype=np.float64,
    )
    y_obs = np.transpose(
        np.array(
            [
                [
                    0.55552674,
                    -0.29054829,
                    -0.04703586,
                    0.0217839,
                    0.15445438,
                    0.46654119,
                    0.12255823,
                    -0.19540335,
                    -0.11772564,
                    -0.44447326,
                ],
                [
                    1.55552674,
                    -1.29054829,
                    -1.04703586,
                    1.0217839,
                    1.15445438,
                    1.46654119,
                    1.12255823,
                    -1.19540335,
                    -1.11772564,
                    -1.44447326,
                ],
            ],
            dtype=np.float64,
        )
    )
    return x_obs, y_obs

  @parameterized.parameters(
      multitask_tuned_gp_models.MultiTaskType.INDEPENDENT,
      multitask_tuned_gp_models.MultiTaskType.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR,
      multitask_tuned_gp_models.MultiTaskType.SEPARABLE_LKJ_TASK_KERNEL_PRIOR,
      multitask_tuned_gp_models.MultiTaskType.SEPARABLE_DIAG_TASK_KERNEL_PRIOR,
  )
  def test_masking_works(self, multitask_type):
    # Mask three dimensions and four observations.
    x_obs, y_obs = self._generate_xys()
    categorical_dim = 3
    continuous_dim = 9
    data = types.ModelData(
        features=types.ModelInput(
            continuous=types.PaddedArray.from_array(
                x_obs, target_shape=(10, continuous_dim), fill_value=1.0
            ),
            categorical=types.PaddedArray.from_array(
                np.zeros((9, 0), dtype=types.INT_DTYPE),
                target_shape=(10, categorical_dim),
                fill_value=1,
            ),
        ),
        # y_obs has shape [10, 2]
        labels=types.PaddedArray.as_padded(y_obs),
    )
    model1 = sp.CoroutineWithData(
        multitask_tuned_gp_models.VizierMultitaskGaussianProcess(
            _feature_dim=types.ContinuousAndCategorical[int](
                continuous_dim, categorical_dim
            ),
            _num_tasks=2,
            _multitask_type=multitask_type,
        ),
        data=data,
    )

    modified_data = types.ModelData(
        features=types.ModelInput(
            continuous=data.features.continuous.replace_fill_value(np.nan),
            categorical=data.features.categorical.replace_fill_value(-1),
        ),
        labels=data.labels,
    )
    model2 = sp.CoroutineWithData(
        multitask_tuned_gp_models.VizierMultitaskGaussianProcess(
            _feature_dim=types.ContinuousAndCategorical[int](
                continuous_dim, categorical_dim
            ),
            _num_tasks=2,
            _multitask_type=multitask_type,
        ),
        data=modified_data,
    )

    # Check that the model loss and optimal parameters are independent of those
    # dimensions and observations.
    optimize = optimizers.JaxoptScipyLbfgsB()
    rng, init_rng = jax.random.split(jax.random.PRNGKey(2), 2)
    optimal_params1, _ = optimize(
        init_params=jax.vmap(model1.setup)(jax.random.split(init_rng, 1)),
        loss_fn=model1.loss_with_aux,
        rng=rng,
        constraints=sp.get_constraints(model1),
    )
    optimal_params2, _ = optimize(
        init_params=jax.vmap(model2.setup)(jax.random.split(init_rng, 1)),
        loss_fn=model2.loss_with_aux,
        rng=rng,
        constraints=sp.get_constraints(model2),
    )

    for key in optimal_params1:
      self.assertTrue(
          np.all(np.equal(optimal_params1[key], optimal_params2[key])),
          msg=f'{key} parameters were not equal.',
      )
    self.assertEqual(
        model1.loss_with_aux(optimal_params1)[0],
        model2.loss_with_aux(optimal_params2)[0],
    )


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/jax/models/tuned_gp_models.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Collection of well-tuned GP models."""

# TODO: Add Ax/BoTorch GP.

import functools
from typing import Any, Generator, Optional, Union

from flax import struct
import jax
from jax import numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import continuous_only_kernel
from vizier._src.jax.models import mask_features
from vizier._src.jax.models import multitask_tuned_gp_models

tfb = tfp.bijectors
tfd = tfp.distributions
tfde = tfp.experimental.distributions
tfpk = tfp.math.psd_kernels
tfpke = tfp.experimental.psd_kernels


def _log_uniform_init(
    low: Union[float, np.floating],
    high: Union[float, np.floating],
    shape: tuple[int, ...] = tuple(),
) -> sp.InitFn:
  r"""Take log-uniform sample in the constraint and map it back to \R.

  Args:
    low: Parameter lower bound.
    high: Parameter upper bound.
    shape: Returned array has this shape. Each entry in the returned array is an
      i.i.d sample.

  Returns:
    Randomly sampled array.
  """

  def sample(key: Any) -> jnp.ndarray:
    unif = jax.random.uniform(key, shape, dtype=jnp.float64)
    return jnp.exp(unif * jnp.log(high / low) + jnp.log(low))

  return sample


@struct.dataclass
class _ConstantMeanFn:
  """Implements a constant GP mean function."""

  constant: jax.Array

  def __call__(self, x: types.ModelInput) -> jax.Array:
    del x
    return self.constant


@struct.dataclass
class VizierGaussianProcess(sp.ModelCoroutine[tfd.GaussianProcess]):
  """Vizier's tuned GP with categorical parameters.

  See __call__ method documentation.

  Attributes:
    _boundary_epsilon: We expand the constraints by this number so that the
      values exactly at the boundary can be mapped to unconstrained space. i.e.
      we are trying to avoid SoftClip(low=1e-2, high=1.).inverse(1e-2) giving
      NaN.
  """

  _dim: types.ContinuousAndCategorical[int] = struct.field(pytree_node=False)
  _num_metrics: int = struct.field(pytree_node=False)
  _use_retrying_cholesky: bool = struct.field(
      pytree_node=False, default=True, kw_only=True
  )
  _boundary_epsilon: float = struct.field(default=1e-12, kw_only=True)
  _linear_coef: Optional[float] = struct.field(default=None, kw_only=True)
  _multitask_type: multitask_tuned_gp_models.MultiTaskType = struct.field(
      default=multitask_tuned_gp_models.MultiTaskType.INDEPENDENT,
      kw_only=True,
  )

  def __attrs_post_init__(self):
    if self._num_metrics < 1:
      raise ValueError(
          'Number of metrics must be at least 1, got: {self._num_metrics}'
      )

  @classmethod
  def build_model(
      cls,
      data: types.ModelData,
      *,
      use_retrying_cholesky: bool = True,
      linear_coef: Optional[float] = None,
      multitask_type: multitask_tuned_gp_models.MultiTaskType = (
          multitask_tuned_gp_models.MultiTaskType.INDEPENDENT
      ),
  ) -> sp.StochasticProcessModel:
    """Returns a StochasticProcessModel for the GP."""
    gp_coroutine = VizierGaussianProcess(
        _dim=types.ContinuousAndCategorical[int](
            data.features.continuous.padded_array.shape[-1],
            data.features.categorical.padded_array.shape[-1],
        ),
        _num_metrics=data.labels.shape[-1],
        _use_retrying_cholesky=use_retrying_cholesky,
        _linear_coef=linear_coef,
        _multitask_type=multitask_type,
    )
    return sp.StochasticProcessModel(gp_coroutine)

  def __call__(
      self,
      inputs: Optional[types.ModelInput] = None,
  ) -> Generator[sp.ModelParameter, jax.Array, tfd.GaussianProcess]:
    """Creates a generator.

    Args:
      inputs: ContinuousAndCategoricalArray with array shapes of: (num_examples,
        continuous_feature_dim), (num_examples, categorical_feature_dim).

    Yields:
      GaussianProcess whose event shape is `num_examples` for single-metric GP
      and MultiTaskGaussianProcess with event shape
      `[num_examples, num_metrics]` for multimetric GP.
    """
    eps = self._boundary_epsilon
    observation_noise_bounds = (np.float64(1e-10 - eps), 1.0 + eps)
    amplitude_bounds = (np.float64(1e-3 - eps), 10.0 + eps)
    continuous_ones = np.ones((self._dim.continuous), dtype=np.float64)
    continuous_length_scale_bounds = (
        continuous_ones * (1e-2 - eps),
        continuous_ones * 1e2 + eps,
    )
    categorical_ones = np.ones((self._dim.categorical), dtype=np.float64)
    categorical_length_scale_bounds = (
        categorical_ones * (1e-2 - eps),
        categorical_ones * 1e2 + eps,
    )

    signal_variance = yield sp.ModelParameter(
        init_fn=_log_uniform_init(*amplitude_bounds),
        constraint=sp.Constraint(
            amplitude_bounds,
            tfb.SoftClip(*amplitude_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: 0.01 * jnp.log(x / 0.039) ** 2,
        name='signal_variance',
    )
    kernel = tfpk.MaternFiveHalves(amplitude=jnp.sqrt(signal_variance))

    continuous_length_scale_squared = yield sp.ModelParameter(
        init_fn=_log_uniform_init(
            *continuous_length_scale_bounds, shape=(self._dim.continuous,)
        ),
        constraint=sp.Constraint(
            continuous_length_scale_bounds,
            tfb.SoftClip(*continuous_length_scale_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: jnp.sum(0.01 * jnp.log(x / 0.5) ** 2),
        name='continuous_length_scale_squared',
    )
    categorical_length_scale_squared = yield sp.ModelParameter(
        init_fn=_log_uniform_init(
            *categorical_length_scale_bounds,
            shape=(self._dim.categorical,),
        ),
        constraint=sp.Constraint(
            categorical_length_scale_bounds,
            tfb.SoftClip(*categorical_length_scale_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: jnp.sum(0.01 * jnp.log(x / 0.5) ** 2),
        name='categorical_length_scale_squared',
    )
    kernel = tfpke.FeatureScaledWithCategorical(
        kernel,
        scale_diag=tfpke.ContinuousAndCategoricalValues(
            jnp.sqrt(continuous_length_scale_squared),
            jnp.sqrt(categorical_length_scale_squared),
        ),
    )
    mean_fn = None
    if self._linear_coef is not None:
      # Add linear kernel.
      # We do not need to tune bias here because we are tuning mean_fn.
      slopes = yield sp.ModelParameter(
          init_fn=_log_uniform_init(*amplitude_bounds),
          constraint=sp.Constraint(
              amplitude_bounds,
              tfb.SoftClip(*amplitude_bounds, hinge_softness=1e-2),
          ),
          regularizer=lambda x: 0.01 * jnp.log(x / 0.039) ** 2,
          name='linear_slope_amplitude',
      )
      shift = yield sp.ModelParameter(
          init_fn=jax.random.normal,
          regularizer=lambda x: 0.5 * x**2,
          name='linear_shift',
      )
      kernel += continuous_only_kernel.ContinuousOnly(
          tfpk.FeatureScaled(
              tfpk.Linear(
                  slope_amplitude=self._linear_coef * slopes,
                  shift=self._linear_coef * shift,
              ),
              scale_diag=jnp.sqrt(continuous_length_scale_squared),
          )
      )

      # mean function output must broadcast to
      # `[batch_shape, num_observations]`, where `batch_shape` is the
      # (possibly empty) batch shape of the other hyperparameters. Initializing
      # `mean_fn_constant` with an array of shape `[1]` gives the mean function
      # output a shape of `[batch_shape, 1]`, ensuring that batch dimensions
      # line up properly.
      mean_fn_constant = yield sp.ModelParameter(
          init_fn=lambda k: jax.random.normal(
              key=k,
              shape=[1] if self._num_metrics == 1 else [1, self._num_metrics],
          ),
          regularizer=lambda x: 0.5 * jnp.sum(x**2),
          name='mean_fn',
      )

      mean_fn = _ConstantMeanFn(mean_fn_constant * self._linear_coef)

    if inputs is not None:
      # Ensure features are zero for this kernel. This will also ensure the
      # length scales are not trainable, since there will be no signal from
      # these dimensions.
      kernel = mask_features.MaskFeatures(
          kernel,
          dimension_is_missing=tfpke.ContinuousAndCategoricalValues(
              continuous=inputs.continuous.is_missing[1],
              categorical=inputs.categorical.is_missing[1],
          ),
      )
      inputs = tfpke.ContinuousAndCategoricalValues(
          continuous=inputs.continuous.padded_array,
          categorical=inputs.categorical.padded_array,
      )

    observation_noise_variance = yield sp.ModelParameter(
        init_fn=_log_uniform_init(*observation_noise_bounds),
        constraint=sp.Constraint(
            observation_noise_bounds,
            tfb.SoftClip(*observation_noise_bounds, hinge_softness=1e-2),
        ),
        regularizer=lambda x: 0.01 * jnp.log(x / 0.0039) ** 2,
        name='observation_noise_variance',
    )
    cholesky_fn = None
    # When cholesky fails, increase jitters and retry.
    if self._use_retrying_cholesky:
      retrying_cholesky = functools.partial(
          tfp.experimental.distributions.marginal_fns.retrying_cholesky,
          jitter=np.float64(1e-4),
          max_iters=5,
      )
      cholesky_fn = lambda matrix: retrying_cholesky(matrix)[0]

    if self._num_metrics > 1:
      if (
          self._multitask_type
          == multitask_tuned_gp_models.MultiTaskType.INDEPENDENT
      ):
        multitask_kernel = tfpke.Independent(self._num_metrics, kernel)
      else:
        task_kernel_scale_linop = (
            yield from multitask_tuned_gp_models.build_task_kernel_scale_linop(
                self._num_metrics, self._multitask_type
            )
        )
        multitask_kernel = tfpke.Separable(
            self._num_metrics,
            base_kernel=kernel,
            task_kernel_scale_linop=task_kernel_scale_linop,
        )
      return tfde.MultiTaskGaussianProcess(
          multitask_kernel,
          index_points=inputs,
          observation_noise_variance=observation_noise_variance,
          cholesky_fn=cholesky_fn,
          mean_fn=mean_fn,
      )
    else:
      return tfd.GaussianProcess(
          kernel,
          index_points=inputs,
          observation_noise_variance=observation_noise_variance,
          cholesky_fn=cholesky_fn,
          mean_fn=mean_fn,
      )


--- vizier/_src/jax/models/tuned_gp_models_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for tuned_gp_models."""

from absl import logging
import equinox as eqx
import jax
import numpy as np
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.models import multitask_tuned_gp_models
from vizier._src.jax.models import tuned_gp_models
from vizier.jax import optimizers

from absl.testing import absltest
from absl.testing import parameterized

tfb = tfp.bijectors
mt_type = multitask_tuned_gp_models.MultiTaskType


class VizierGpTest(parameterized.TestCase):

  def _generate_xys(self, num_metrics: int):
    x_obs = np.array(
        [
            [
                0.2941264,
                0.29313548,
                0.68817519,
                0.37502566,
                0.48356813,
                0.34127283,
            ],
            [
                0.66218224,
                0.70770083,
                0.6901334,
                0.66787973,
                0.5400858,
                0.52721233,
            ],
            [
                0.88469647,
                0.50593371,
                0.83160862,
                0.58674892,
                0.42145673,
                0.31749428,
            ],
            [
                0.39976682,
                0.59517741,
                0.73295106,
                0.6084903,
                0.54891015,
                0.44338632,
            ],
            [
                0.8354305,
                0.87605574,
                0.47855956,
                0.48174861,
                0.37685449,
                0.38348768,
            ],
            [
                0.55608455,
                0.72781129,
                0.52432913,
                0.44291417,
                0.3816395,
                0.326599,
            ],
            [
                0.24689187,
                0.50979672,
                0.67604857,
                0.45172594,
                0.34994392,
                0.75239792,
            ],
            [
                0.71007257,
                0.60896354,
                0.29270877,
                0.74683367,
                0.50169051,
                0.74480515,
            ],
            [
                0.9193235,
                0.24393112,
                0.63868591,
                0.43271524,
                0.43339578,
                0.59413154,
            ],
            [
                0.51850627,
                0.62689204,
                0.76134879,
                0.65990021,
                0.82350868,
                0.7429215,
            ],
        ],
        dtype=np.float64,
    )
    y_obs = np.tile(
        np.array(
            [
                0.55552674,
                -0.29054829,
                -0.04703586,
                0.0217839,
                0.15445438,
                0.46654119,
                0.12255823,
                -0.19540335,
                -0.11772564,
                -0.44447326,
            ],
            dtype=np.float64,
        )[
            :, np.newaxis
        ],  # Added a new axis to be compatible with `np.tile`.
        (1, num_metrics),
    )
    return x_obs, y_obs

  # TODO: Define generic assertions for loss values/masking in
  # coroutines.
  @parameterized.parameters(
      # Pads two observations.
      dict(num_metrics=1, num_obs=12),
      # No observations are padded because multimetric GP does not support
      # observation padding.
      dict(num_metrics=2, num_obs=10),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR,
      ),
  )
  def test_masking_works(
      self,
      num_metrics: int,
      num_obs: int,
      multitask_type: mt_type = mt_type.INDEPENDENT,
  ):
    x_obs, y_obs = self._generate_xys(num_metrics)
    data = types.ModelData(
        features=types.ModelInput(
            # Pads three continuous dimensions.
            continuous=types.PaddedArray.from_array(
                x_obs, target_shape=(num_obs, 9), fill_value=1.0
            ),
            categorical=types.PaddedArray.from_array(
                np.zeros((9, 0), dtype=types.INT_DTYPE),
                target_shape=(num_obs, 2),
                fill_value=1,
            ),
        ),
        labels=types.PaddedArray.from_array(
            y_obs, target_shape=(num_obs, num_metrics), fill_value=np.nan
        ),
    )
    model1 = sp.CoroutineWithData(
        tuned_gp_models.VizierGaussianProcess(
            types.ContinuousAndCategorical[int](9, 2),
            num_metrics,
            _multitask_type=multitask_type,
        ),
        data=data,
    )

    modified_data = types.ModelData(
        features=types.ModelInput(
            continuous=data.features.continuous.replace_fill_value(np.nan),
            categorical=data.features.categorical.replace_fill_value(-1),
        ),
        labels=data.labels,
    )
    model2 = sp.CoroutineWithData(
        tuned_gp_models.VizierGaussianProcess(
            types.ContinuousAndCategorical[int](9, 2),
            num_metrics,
            _multitask_type=multitask_type,
        ),
        data=modified_data,
    )

    # Check that the model loss and optimal parameters are independent of those
    # dimensions and observations.
    optimize = optimizers.JaxoptScipyLbfgsB()
    rng, init_rng = jax.random.split(jax.random.PRNGKey(2), 2)
    optimal_params1, _ = optimize(
        init_params=jax.vmap(model1.setup)(jax.random.split(init_rng, 1)),
        loss_fn=model1.loss_with_aux,
        rng=rng,
        constraints=sp.get_constraints(model1),
    )
    optimal_params2, _ = optimize(
        init_params=jax.vmap(model2.setup)(jax.random.split(init_rng, 1)),
        loss_fn=model2.loss_with_aux,
        rng=rng,
        constraints=sp.get_constraints(model2),
    )

    for key in optimal_params1:
      self.assertTrue(
          np.all(np.equal(optimal_params1[key], optimal_params2[key])),
          msg=f'{key} parameters were not equal.',
      )
    self.assertEqual(
        model1.loss_with_aux(optimal_params1)[0],
        model2.loss_with_aux(optimal_params2)[0],
    )

  @parameterized.parameters(
      # Pads two observations.
      dict(num_metrics=1, num_obs=12),
      # No observations are padded because multimetric GP does not support
      # observation padding.
      dict(num_metrics=2, num_obs=10),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=3,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR,
      ),
  )
  def test_good_log_likelihood(
      self,
      num_metrics: int,
      num_obs: int,
      multitask_type: mt_type = mt_type.INDEPENDENT,
  ):
    # We use a fixed random seed for sampling categorical data (and continuous
    # data from `_generate_xys`, above) so that the same data is used for every
    # test run.
    rng, init_rng, cat_rng = jax.random.split(jax.random.PRNGKey(2), 3)
    x_cont_obs, y_obs = self._generate_xys(num_metrics)
    data = types.ModelData(
        features=types.ModelInput(
            continuous=types.PaddedArray.from_array(
                x_cont_obs, target_shape=(num_obs, 9), fill_value=np.nan
            ),
            categorical=types.PaddedArray.from_array(
                jax.random.randint(
                    cat_rng,
                    shape=(num_obs, 3),
                    minval=0,
                    maxval=3,
                    dtype=types.INT_DTYPE,
                ),
                target_shape=(num_obs, 5),
                fill_value=-1,
            ),
        ),
        labels=types.PaddedArray.from_array(
            y_obs, target_shape=(num_obs, num_metrics), fill_value=np.nan
        ),
    )
    target_loss = -0.2
    model = sp.CoroutineWithData(
        tuned_gp_models.VizierGaussianProcess(
            types.ContinuousAndCategorical[int](9, 5),
            num_metrics,
            _multitask_type=multitask_type,
        ),
        data=data,
    )
    optimize = optimizers.JaxoptScipyLbfgsB()
    constraints = sp.get_constraints(model)
    optimal_params, metrics = optimize(
        init_params=jax.vmap(model.setup)(jax.random.split(init_rng, 50)),
        loss_fn=model.loss_with_aux,
        rng=rng,
        constraints=constraints,
    )
    logging.info('Optimal: %s', optimal_params)
    logging.info('Loss: %s', metrics['loss'])
    self.assertLess(np.min(metrics['loss']), target_loss)

  @parameterized.parameters(
      # Pads two observations.
      dict(num_metrics=1, num_obs=12),
      # No observations are padded because multimetric GP does not support
      # observation padding.
      dict(num_metrics=2, num_obs=10),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_NORMAL_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_LKJ_TASK_KERNEL_PRIOR,
      ),
      dict(
          num_metrics=2,
          num_obs=10,
          multitask_type=mt_type.SEPARABLE_DIAG_TASK_KERNEL_PRIOR,
      ),
  )
  def test_good_log_likelihood_linear(
      self,
      num_metrics: int,
      num_obs: int,
      multitask_type: mt_type = mt_type.INDEPENDENT,
  ):
    """Tests that the GP with linear coef after ARD has good log likelihood.

    The tests use a fixed random seed for sampling categorical data (and
    continuous data from `_generate_xys`, above) so that the same data is used
    for every test run.

    Args:
      num_metrics: Number of metrics.
      num_obs: Number of observations.
      multitask_type: The type of multitask GP to test.
    """
    rng, init_rng, cat_rng = jax.random.split(jax.random.PRNGKey(2), 3)
    x_cont_obs, y_obs = self._generate_xys(num_metrics)
    data = types.ModelData(
        features=types.ModelInput(
            continuous=types.PaddedArray.from_array(
                x_cont_obs, target_shape=(num_obs, 9), fill_value=np.nan
            ),
            categorical=types.PaddedArray.from_array(
                jax.random.randint(
                    cat_rng,
                    shape=(num_obs, 3),
                    minval=0,
                    maxval=3,
                    dtype=types.INT_DTYPE,
                ),
                target_shape=(num_obs, 5),
                fill_value=-1,
            ),
        ),
        labels=types.PaddedArray.from_array(
            y_obs, target_shape=(num_obs, num_metrics), fill_value=np.nan
        ),
    )
    target_loss = -0.2
    model = sp.CoroutineWithData(
        tuned_gp_models.VizierGaussianProcess(
            types.ContinuousAndCategorical[int](9, 5),
            num_metrics,
            _linear_coef=1.0,
            _multitask_type=multitask_type,
        ),
        data=data,
    )
    optimize = optimizers.JaxoptScipyLbfgsB(
        optimizers.LbfgsBOptions(maxiter=100)
    )
    constraints = sp.get_constraints(model)
    best_n = 7
    optimal_params, metrics = optimize(
        init_params=jax.vmap(model.setup)(jax.random.split(init_rng, 50)),
        loss_fn=model.loss_with_aux,
        rng=rng,
        constraints=constraints,
        best_n=best_n,
    )
    logging.info('Optimal: %s', optimal_params)
    logging.info('Loss: %s', metrics['loss'])
    self.assertLess(np.min(metrics['loss']), target_loss)

    best_models = sp.StochasticProcessWithCoroutine(
        coroutine=model.coroutine, params=optimal_params
    )
    predictive = eqx.filter_jit(best_models.precompute_predictive)(data)
    n_pred_features = 6
    cont_pred_feature_dim = 6
    cont_pred_feature_dim_padded = 9
    cat_pred_feature_dim = 3
    cat_pred_feature_dim_padded = 5
    pred_features = types.ModelInput(
        continuous=types.PaddedArray.from_array(
            np.random.normal(size=(n_pred_features, cont_pred_feature_dim)),
            target_shape=(n_pred_features, cont_pred_feature_dim_padded),
            fill_value=np.nan,
        ),
        categorical=types.PaddedArray.from_array(
            np.random.randint(
                3,
                size=(n_pred_features, cat_pred_feature_dim),
                dtype=types.INT_DTYPE,
            ),
            target_shape=(n_pred_features, cat_pred_feature_dim_padded),
            fill_value=-1,
        ),
    )
    y_pred_mean = predictive.predict(pred_features).mean()
    self.assertEqual(
        y_pred_mean.shape,
        (best_n, n_pred_features) + ((num_metrics,) if num_metrics > 1 else ()),
    )


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  jax.config.update('jax_enable_x64', True)
  jax.config.update('jax_threefry_partitionable', False)
  absltest.main()


--- vizier/_src/jax/optimizers/core.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""High-level wrappers for stochastic process hyperparameter optimizers."""

import functools
from typing import Generic, Optional, Protocol, TypeVar

from absl import logging
import chex
import jax
from jax import numpy as jnp
from vizier._src.jax import stochastic_process_model as sp


Params = TypeVar('Params', bound=chex.ArrayTree)


class LossFunction(Protocol, Generic[Params]):
  """Evaluates model params and returns (loss, dict of auxiliary metrics)."""

  def __call__(self, params: Params) -> tuple[jax.Array, chex.ArrayTree]:
    """Evaluates model params and returns (loss, dict of auxiliary metrics)."""
    pass


class LossAndGradFunction(Protocol, Generic[Params]):
  """Computes loss and gradient."""

  def __call__(
      self, params: Params
  ) -> tuple[tuple[jax.Array, chex.ArrayTree], jax.Array]:
    """Returns (loss, tree of auxiliary metrics), grad."""


class Optimizer(Protocol[Params]):
  """Optimizes the LossFunction.

  Example:

  ```python
  setup: Setup = lambda rng: jax.random.uniform(rng, minval=-5, maxval=5)

  def loss_fn(xs):  # satisfies `LossFunction` Protocol
    xs = jax.nn.sigmoid(xs)
    return jnp.cos(xs * 5 * 2 * jnp.pi) * (2 - 5 * (xs - 0.5)**2), dict()

  optimize = optimizers.OptaxTrain(optax.adam(5e-3), epochs=500, verbose=True)
  rngs = jax.random.split(jax.random.PRNGKey(0), 51)
  optimal_params, metrics = optimize(
      init_params=jax.vmap(setup)(rngs[1:]),
      loss_fn=loss_fn,
      rng=rngs[0])
  ```
  """

  def __call__(
      self,
      init_params: Params,
      loss_fn: LossFunction[Params],
      rng: jax.Array,
      *,
      constraints: Optional[sp.Constraint] = None,
      best_n: Optional[int] = None,
  ) -> tuple[Params, chex.ArrayTree]:
    """Optimizes a LossFunction expecting Params as input.

    When constraint bijectors are applied, note that the returned parameters are
    in the constrained space (the parameter domain), not the unconstrained space
    over which the optimization takes place.

    If the Optimizer uses lower and upper bounds, then it is responsible for
    converting `None` bounds to `+inf` or `-inf` as necessary.

    Args:
      init_params: A set of initial points represented as a batched Pytree. All
        leaf nodes are expected to have the same leading dimension, which is the
        batch size.
      loss_fn: Evaluates a point.
      rng: JAX PRNGKey.
      constraints: Parameter constraints.
      best_n: If not None, returns a pytree with a batch dimension [best_n].

    Returns:
      Tuple containing optimal input in the constrained space and optimization
      metrics.
    """


def get_best_params(
    losses: jax.Array,
    all_params: chex.ArrayTree,
    *,
    best_n: Optional[int] = None,
) -> chex.ArrayTree:
  """Returns the top `best_n` parameters that minimize the losses.

  Args:
    losses: Shape (N,) array
    all_params: ArrayTree whose leaves have shape (N, ...)
    best_n: Integer greater than or equal to 1. If None, squeezes the leading
      dimension.

  Returns:
    Top `best_n` parameters.
  """
  argsorted = jnp.argsort(losses)
  if not best_n:
    best_idx = argsorted[:1]
  else:
    best_idx = argsorted[:best_n]

  logging.info('Best loss(es): %s at retry %s', losses[best_idx], best_idx)
  optimal_params = jax.tree_util.tree_map(lambda p: p[best_idx], all_params)
  if best_n is None:
    optimal_params = jax.tree.map(
        functools.partial(jnp.squeeze, axis=0), optimal_params
    )
  return optimal_params


--- vizier/_src/jax/optimizers/core_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests on core.py."""


import chex
from jax import numpy as jnp
from vizier._src.jax.optimizers import core
from absl.testing import absltest


class GetBestParamsTest(absltest.TestCase):

  def test_best_n_none(self):
    losses = jnp.array([
        3.0,
        1.0,
        4.0,
        2.0,
        5.0,
    ])
    all_params = {
        'a': losses + 0.1,
        'b': jnp.stack([losses + 0.2, losses + 0.3], axis=1),
    }
    actual = core.get_best_params(losses, all_params, best_n=None)
    expected = {'a': jnp.array(1.1), 'b': jnp.array([1.2, 1.3])}

    chex.assert_trees_all_close(actual, expected)

  def test_best_n_2(self):
    losses = jnp.array([
        3.0,
        1.0,
        4.0,
        2.0,
        5.0,
    ])
    all_params = {
        'a': losses + 0.1,
        'b': jnp.stack([losses + 0.2, losses + 0.3], axis=1),
    }
    actual = core.get_best_params(losses, all_params, best_n=2)
    expected = {
        'a': jnp.array([1.1, 2.1]),
        'b': jnp.array([[1.2, 1.3], [2.2, 2.3]]),
    }

    chex.assert_trees_all_close(actual, expected)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/jax/optimizers/jaxopt_wrappers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""High-level wrappers for stochastic process hyperparameter optimizers."""

import datetime
import time
from typing import Any, Optional

from absl import logging
import attr
import chex
import equinox as eqx
from flax import struct
import jax
from jax import numpy as jnp
import jaxopt
import tree
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax.optimizers import core


@struct.dataclass
class LbfgsBOptions:
  """L-BFGS-B is a version of L-BFGS that incorporates box constraints.

  https://digital.library.unt.edu/ark:/67531/metadc666315/m2/1/high_res_d/204262.pdf

  Attributes:
    num_line_search_steps: Maximum number of line search steps.
    random_restarts: Must be positive; number of random initializations for the
      optimization.
    tol: Tolerance for stopping criteria.
    maxiter: Max number of iterations.
    best_n: Number of best values to return from the initializations; must be
      less than or equal to `random_restarts`.
  """

  num_line_search_steps: int = struct.field(kw_only=True, default=20)
  # TODO: Remove this since now it's a part of optimizer API.
  random_restarts: Optional[int] = struct.field(
      kw_only=True, default=None, pytree_node=False
  )
  tol: float = struct.field(kw_only=True, default=1e-8)
  maxiter: int = struct.field(kw_only=True, default=50)
  # TODO: Remove best_n. Now it's a part of optimizer API.
  best_n: Optional[int] = struct.field(
      kw_only=True, default=None, pytree_node=False
  )


def _is_leaf(x: float) -> bool:
  return jax.tree_util.treedef_is_leaf(jax.tree_util.tree_structure(x))


def _none_to_inf(b: float, inf: float, params: chex.ArrayTree):
  """Converts None bounds to inf or -inf to pass to the optimizer."""
  if b is None:
    b = inf
  if _is_leaf(b):
    # Broadcast scalars to the parameter structure.
    return tree.map_structure(lambda x: b * jnp.ones_like(x), params)
  else:
    # For structured bounds, replace `None`s in the structure with `inf`.
    return tree.map_structure(
        lambda b_, x: jnp.ones_like(x) * (inf if b_ is None else b_), b, params
    )


# TODO: Remove support for broadcasting array -> arraytree
# and reduce maintenance burden. Alternatively, move to a separate file
# so other libraries can use it too.
def _get_bounds(
    params: core.Params,
    constraints: Optional[sp.Constraint],
) -> Optional[tuple[chex.ArrayTree, chex.ArrayTree]]:
  """Returns (Lower, upper) ArrayTrees with the same shape as params."""
  if constraints is None:
    return None
  else:
    lb = _none_to_inf(constraints.bounds[0], -jnp.inf, params)
    ub = _none_to_inf(constraints.bounds[1], jnp.inf, params)
    logging.info(
        'constraints\n: %s converted to bounds:\n %s', constraints, (lb, ub)
    )
    return (lb, ub)


def _unbatch_params(batched_params: core.Params) -> list[core.Params]:
  batch_size = jax.tree_util.tree_leaves(batched_params)[0].shape[0]
  return list(
      map(
          lambda i: jax.tree.map(lambda p: p[i], batched_params),
          range(batch_size),
      )
  )


@attr.define
class JaxoptScipyLbfgsB(core.Optimizer[core.Params]):
  """Jaxopt's wrapper for scipy L-BFGS-B optimizer."""

  _options: LbfgsBOptions = attr.field(default=LbfgsBOptions())
  _speed_test: bool = attr.field(kw_only=True, default=False)
  _max_duration: Optional[datetime.timedelta] = None

  def __call__(
      self,
      init_params: core.Params,
      loss_fn: core.LossFunction[core.Params],
      rng: jax.Array,
      *,
      constraints: Optional[sp.Constraint] = None,
      best_n: Optional[int] = None,
  ) -> tuple[core.Params, chex.ArrayTree]:
    # L-BFGS-B is deterministic given initial parameters.
    del rng

    # L-BFGS-B may be used on unconstrained problems (in which case it is
    # slightly different from L-BFGS, in that it uses the Cauchy point/subspace
    # minimization to choose the line search direction). Bounds must be None or
    # a tuple of size 2. The tuple must contain lower/upper bounds, which may be
    # None or a pytree with the same structure as the model parameters returned
    # by setup (otherwise Jaxopt will raise an error).
    jax.monitoring.record_event('/vizier/jax/optimizers/scipylbfgsb/called')
    lbfgsb = jaxopt.ScipyBoundedMinimize(
        fun=loss_fn,
        method='L-BFGS-B',
        maxiter=self._options.maxiter,
        options={
            'gtol': self._options.tol,
            'maxls': self._options.num_line_search_steps,
        },
        jit=False,
        has_aux=True,
        value_and_grad=eqx.filter_jit(
            eqx.filter_value_and_grad(loss_fn, has_aux=True)
        ),
    )
    train_times = []
    losses = []
    params = []
    metrics = {}

    init_params = _unbatch_params(init_params)
    bounds = _get_bounds(init_params[0], constraints)

    logging.info(
        'Using SCIPY L-BFGS-B w/ %d initializations: %s',
        len(init_params),
        init_params,
    )
    for p in init_params:
      if self._max_duration is not None and train_times:
        expected_worst_case_duration = max(train_times) + sum(train_times)
        if expected_worst_case_duration >= self._max_duration.total_seconds():
          logging.warn(
              'Maximum duration of %s is expected to be surpassed.'
              ' Completed only %s initializations out of %s.',
              self._max_duration,
              len(params),
              len(init_params),
          )
          break
      start_time = time.time()
      position, opt_state = lbfgsb.run(init_params=p, bounds=bounds)
      train_times.append(time.time() - start_time)
      losses.append(opt_state.fun_val)
      params.append(position)
      logging.info(
          'Loss: %s, last step time: %s, ScipyMinimizeInfo: %s',
          opt_state.fun_val,
          train_times[-1],
          opt_state,
      )
    losses = jnp.asarray(losses)
    all_params = jax.tree_util.tree_map(lambda *x: jnp.stack(x), *params)

    metrics['loss'] = losses[jnp.newaxis, :]
    if self._speed_test:
      metrics['train_time'] = train_times
    return (
        core.get_best_params(losses, all_params, best_n=best_n),
        metrics,
    )

  # TODO: Remove dependencies on this property.
  @property
  def best_n(self) -> int:
    return self._options.best_n or 1


def _run_parallel_lbfgs(
    loss_fn: core.LossFunction[core.Params],
    init_params_batch: core.Params,
    *,
    bounds: Optional[tuple[chex.ArrayTree, chex.ArrayTree]],
    options: LbfgsBOptions,
) -> tuple[core.Params, Any]:
  """Called by JaxoptLbfgsB."""

  def _run_one_lbfgs(
      init_params: core.Params,
  ) -> tuple[core.Params, Any]:
    lbfgsb = jaxopt.LBFGSB(
        fun=loss_fn,
        maxls=options.num_line_search_steps,
        tol=options.tol,
        maxiter=options.maxiter,
        has_aux=True,
    )
    return lbfgsb.run(init_params=init_params, bounds=bounds)

  # We chose map over vmap because some of the lbfgs runs may terminate early.
  # pmap is also not fit for our use case, because we typically have a single
  # processor.
  return jax.lax.map(_run_one_lbfgs, init_params_batch)


@attr.define
class JaxoptLbfgsB(core.Optimizer[core.Params]):
  """Jaxopt's L-BFGS-B optimizer.

  Jaxopt calls the Scipy or Jax implementation of L-BFGS-B.
  L-BFGS-B is a version of L-BFGS that incorporates box constraints on
  parameters.
  https://digital.library.unt.edu/ark:/67531/metadc666315/m2/1/high_res_d/204262.pdf

  Attributes:
    num_line_search_steps: Maximum number of line search steps.
    random_restarts: Must be positive; number of random initializations for the
      optimization.
    best_n: Number of best values to return from the initializations; must be
      less than or equal to `random_restarts`.
     use_scipy: Uses the scipy version of L-BFGS-B. If False, uses the pure JAX
       L-BFGS-B in Jaxopt, which runs on accelerators.
     tol: Tolerance for stopping criteria.
     maxiter: Max number of iterations.
    _speed_test: If True, return speed test results.
  """

  _options: LbfgsBOptions = attr.field(default=LbfgsBOptions())
  _speed_test: bool = attr.field(kw_only=True, default=False)

  def __call__(
      self,
      init_params: core.Params,
      loss_fn: core.LossFunction[core.Params],
      rng: jax.Array,
      *,
      constraints: Optional[sp.Constraint] = None,
      best_n: Optional[int] = None,
  ) -> tuple[core.Params, chex.ArrayTree]:
    metrics = {}
    logging.info('Using JAX L-BFGS-B w/ RNG: %s', rng)
    start_time = time.time()
    bounds = _get_bounds(_unbatch_params(init_params)[0], constraints)
    lbfgsb = jaxopt.LBFGSB(
        fun=eqx.filter_jit(loss_fn),
        value_and_grad=eqx.filter_value_and_grad(loss_fn, has_aux=True),
        maxls=self._options.num_line_search_steps,
        tol=self._options.tol,
        maxiter=self._options.maxiter,
        has_aux=True,
        jit=True,
        unroll=True,
    )

    all_params, opt_states = jax.lax.map(
        lambda p: lbfgsb.run(init_params=p, bounds=bounds),
        init_params,
    )  # pylint: disable=g-long-lambda

    losses = jnp.asarray(opt_states.value)
    metrics['train_time'] = time.time() - start_time
    metrics['loss'] = losses[jnp.newaxis, :]
    return (
        core.get_best_params(losses, all_params, best_n=best_n),
        metrics,
    )

  # TODO: Remove dependencies on this property.
  @property
  def best_n(self) -> int:
    return self._options.best_n or 1

  @property
  def jittable(self) -> bool:
    return True


--- vizier/_src/jax/optimizers/jaxopt_wrappers_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for optimizers."""

import datetime

from absl.testing import parameterized
import jax
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax.optimizers import jaxopt_wrappers as jw
from vizier._src.jax.optimizers.testing import sinusoidal

from absl.testing import absltest

tfb = tfp.bijectors


class JaxoptWrappersTest(
    parameterized.TestCase, sinusoidal.ConvergenceTestMixin
):

  @parameterized.product(
      (
          dict(bounds=None, nest_constraint=True),
          dict(bounds=((-4.0, None)), nest_constraint=True),
          dict(bounds=((None, 5.0)), nest_constraint=False),
          dict(bounds=((-3.0, 3.0)), nest_constraint=True),
      ),
      cls=(jw.JaxoptScipyLbfgsB, jw.JaxoptLbfgsB),
  )
  def test_sinusoidal(self, cls, bounds, nest_constraint):
    constraints = sinusoidal.bounds_to_constraints(bounds, nest_constraint)
    self.assert_converges(
        cls(jw.LbfgsBOptions()),
        constraints=constraints,
        threshold=1.0,
        random_restarts=20,
    )

  def test_max_duration(self):
    optimizer = jw.JaxoptScipyLbfgsB(
        max_duration=datetime.timedelta(seconds=0),
        speed_test=True,
    )
    random_restarts = 3
    rngs = jax.random.split(jax.random.PRNGKey(1), random_restarts + 1)
    _, metrics = optimizer(
        init_params=jax.vmap(sinusoidal.setup)(rngs[1:]),
        loss_fn=jax.jit(sinusoidal.loss_fn),
        rng=rngs[0],
    )
    # Check that there's only one training instead of 3.
    self.assertLen(metrics['train_time'], 1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/jax/optimizers/optax_wrappers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""High-level wrappers for stochastic process hyperparameter optimizers."""

# TODO: Add optimizers that are frequently used in the literature.

from typing import Optional
from absl import logging
import attr
import chex
import jax
from jax import numpy as jnp
import optax
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax.optimizers import core

tfb = tfp.bijectors

OptState = chex.ArrayTree


@attr.define
class OptaxTrain(core.Optimizer[core.Params]):
  """Wraps an Optax optimizer.

  It's recommended to use this optimizer with a loss function that normalizes by
  the number of observations. The unnormalized loss function for parameters `p`
  is typically of the form

  ```None
  -gp_likelihood(observations | p) + regularization(p)
  ```

  where regularization may be a negative prior log probability. The likelihood
  term is approximately proportional to the number of observations. As the
  number of observations changes over the course of a study, dividing the loss
  by this number helps ensure that loss values are roughly of the same order of
  magnitude, such that a constant learning rate may be used for gradient-based
  optimizers. Vizier library models make this adjustment automatically.

  Attributes:
    optimizer: Optax optimizer such as `optax.adam(1e-2)`.
    epochs: Number of train epochs.
    verbose: If >=1, logs the train progress. If >=2, logs the gradients.
  """

  optimizer: optax.GradientTransformation = attr.field()
  epochs: int = attr.field(kw_only=True)
  verbose: int = attr.field(kw_only=True, default=0, converter=int)

  # TODO: Prevent retracing.
  def __call__(
      self,
      init_params: core.Params,
      loss_fn: core.LossFunction[core.Params],
      rng: jax.Array,
      *,
      constraints: Optional[sp.Constraint] = None,
      best_n: Optional[int] = None,
  ) -> tuple[core.Params, chex.ArrayTree]:
    if constraints is None or constraints.bijector is None:
      bijector = None
      unconstrained_loss_fn = loss_fn
    else:
      bijector = constraints.bijector
      unconstrained_loss_fn = lambda x: loss_fn(bijector(x))

    grad_fn = jax.value_and_grad(unconstrained_loss_fn, has_aux=True)

    def _setup_all(init_params: core.Params) -> tuple[core.Params, OptState]:
      """Sets up both model params and optimizer state."""
      params = (
          bijector.inverse(init_params) if bijector is not None else init_params
      )
      opt_state = self.optimizer.init(params)
      return params, opt_state

    def _train_step(
        params: core.Params, opt_state: OptState
    ) -> tuple[core.Params, OptState, chex.ArrayTree]:
      """One train step."""
      (loss, metrics), grads = grad_fn(params)
      logging.log_if(logging.INFO, 'gradients: %s', self.verbose >= 2, grads)
      updates, opt_state = self.optimizer.update(grads, opt_state, params)
      params = optax.apply_updates(params, updates)
      metrics['loss'] = loss
      return params, opt_state, metrics

    params, opt_state = jax.vmap(_setup_all)(init_params)
    train_step = jax.vmap(_train_step)

    logging.info('Initialized parameters. %s',
                 jax.tree.map(lambda x: x.shape, params))

    # See https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
    train_step = jax.jit(train_step, donate_argnums=[0, 1])
    metrics = []
    for epoch in range(self.epochs):
      params, opt_state, step_metrics = train_step(params, opt_state)
      logging.log_if(
          logging.INFO,
          'Epoch %s: metrics: %s',
          self.verbose >= 1,
          epoch,
          step_metrics,
      )
      metrics.append(step_metrics)

    # Convert `metrics` from a list of dicts to a dict of arrays with leftmost
    # dimension corresponding to train steps.
    outer_treedef = jax.tree_util.tree_structure([0] * self.epochs)
    transposed_metrics = jax.tree_util.tree_transpose(
        outer_treedef, jax.tree_util.tree_structure(metrics[0]), metrics
    )
    metrics = jax.tree_util.tree_map(
        jnp.array,
        transposed_metrics,
        is_leaf=lambda x: jax.tree_util.tree_structure(x) == outer_treedef,
    )

    final_losses = metrics['loss'][-1]
    logging.info('Final loss: %s', final_losses)
    best_params = core.get_best_params(final_losses, params, best_n=best_n)
    if bijector is not None:
      best_params = bijector(best_params)
    return (best_params, metrics)


--- vizier/_src/jax/optimizers/optax_wrappers_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for optimizers."""

from absl.testing import parameterized
import jax
import optax
from tensorflow_probability.substrates import jax as tfp
from vizier._src.jax.optimizers.testing import sinusoidal
from vizier.jax import optimizers

from absl.testing import absltest

jax.config.update('jax_threefry_partitionable', False)

tfb = tfp.bijectors


class OptaxWrapperTest(parameterized.TestCase, sinusoidal.ConvergenceTestMixin):

  @parameterized.parameters(
      (None,),
      ((-4.0, None),),
      ((None, 5.0),),
      ((-3.0, 3.0),),
  )
  def test_sinusoidal(self, bounds):
    constraints = sinusoidal.bounds_to_constraints(bounds, nest_constraint=True)
    self.assert_converges(
        optimizers.OptaxTrain(optax.adam(5e-2), epochs=100, verbose=True),
        constraints=constraints,
        threshold=5e-3 if bounds is None else 1.0,
        random_restarts=200,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/jax/optimizers/testing/sinusoidal.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Sinusodial function for testing."""

from typing import Optional
import unittest

from absl import logging
import chex
import jax
import jax.numpy as jnp
import numpy as np
from tensorflow_probability.substrates import jax as tfp
import tree
from vizier._src.jax import stochastic_process_model as sp
from vizier._src.jax import types
from vizier._src.jax.optimizers import core
from vizier._src.jax.types import Bounds


tfb = tfp.bijectors


@chex.assert_max_traces(1)
def loss_fn(inputs):
  # Sinusoidal function minimized at all inputs being zero. (min value=-2)
  # There are lots of local optima between [-3, 3]
  xs = jnp.concatenate(jax.tree_util.tree_flatten(inputs)[0], axis=-1)
  xs = jax.nn.sigmoid(xs)
  return (
      jnp.mean(jnp.cos(xs * 5 * 2 * jnp.pi) * (2 - 5 * (xs - 0.5) ** 2)) + 2,
      dict(),
  )


def setup(rng):
  rng1, rng2 = jax.random.split(rng, 2)
  return {
      'x1': jax.random.uniform(rng1, shape=(1,), minval=-3, maxval=3),
      'x2': jax.random.uniform(rng2, shape=(2,), minval=-3.0, maxval=3.0),
  }


def _make_constraints(b: Optional[types.Bounds]) -> Optional[chex.ArrayTree]:
  if b is None:
    return None
  return {'x1': jnp.array([b]), 'x2': jnp.array([b, b])}


def bounds_to_constraints(
    bounds: Optional[Bounds],
    nest_constraint: bool = True,
) -> Optional[sp.Constraint]:
  if bounds is None:
    return None
  if nest_constraint:
    bounds = tree.map_structure(_make_constraints, bounds)
  return sp.Constraint.create(bounds, tfb.SoftClip)


class ConvergenceTestMixin(unittest.TestCase):
  """Mixin for subclasses."""

  def assert_converges(
      self,
      optimize: core.Optimizer,
      *,
      constraints: sp.Constraint,
      threshold: float = 5e-3,
      random_restarts: int,
  ) -> None:
    """Assert the optimizer converges."""
    rngs = jax.random.split(jax.random.PRNGKey(1), random_restarts + 1)
    optimal_params, metrics = optimize(
        init_params=jax.vmap(setup)(rngs[1:]),
        loss_fn=jax.jit(loss_fn),
        rng=rngs[0],
        constraints=constraints,
    )
    logging.info('Optimal: %s', optimal_params)

    self.assertLessEqual(loss_fn(optimal_params)[0], threshold)
    if metrics['loss'].shape[0] > 1:
      np.testing.assert_array_less(
          metrics['loss'][-1, :], metrics['loss'][0, :]
      )

    if (constraints is not None) and (constraints.bounds is not None):
      bounds = constraints.bounds
      for y_, b_ in zip(tree.flatten(optimal_params), tree.flatten(bounds[0])):
        if b_ is not None:
          np.testing.assert_array_less(b_, y_)
      for y_, b_ in zip(tree.flatten(optimal_params), tree.flatten(bounds[1])):
        if b_ is not None:
          np.testing.assert_array_less(y_, b_)


--- vizier/_src/jax/stochastic_process_model.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Flax module for a trainable stochastic process."""

import abc
from typing import Any, Callable, Generator, Generic, Optional, Protocol, TypeVar

from absl import logging
import attr
import chex
import equinox as eqx
from flax import config as flax_config
from flax import linen as nn
from flax import struct
import jax
from jax import numpy as jnp
from jax import tree_util
from jax.typing import ArrayLike
from tensorflow_probability.substrates import jax as tfp
import tree
from vizier._src.jax import types

flax_config.update('flax_return_frozendict', False)

tfd = tfp.distributions
tfb = tfp.bijectors
tfde = tfp.experimental.distributions
tfpke = tfp.experimental.psd_kernels

_D = TypeVar('_D', bound=tfd.Distribution)


class InitFn(Protocol):
  """Protocol for Flax parameter initialization functions."""

  @abc.abstractmethod
  def __call__(self, rng: jax.Array) -> jax.Array:
    pass


@struct.dataclass
class Constraint:
  """Class specifying parameter constraints.

  `ModelParameter`s may optionally contain a `Constraint` object that specifies
  the lower/upper bounds of the parameter and a bijector that maps from the
  space of all real numbers to the interval between the lower and upper bounds.

  Attributes:
    bounds: When expressing the bounds of a single parameter (array), `bounds`
      is a tuple of arrays containing (lower, upper) bounds for the parameter.
      When expressing the bounds for a structure of parameters (such as a Flax
      parameters dict), `bounds` is a tuple of structures of the same form as
      the parameters structure, containing lower and upper bounds of
      corresponding parameters. If `bounds` is None, the parameter (or all
      parameters in the structure) are unbounded. If the tuple of bounds
      contains None, then the parameter (or corresponding parameter in the
      structure) is unbounded (from below if None is in the first element of the
      tuple, and above if None is in the second element of the tuple).
    bijector: A TFP bijector-like constraint function mapping the parameter (or
      structure of parameters) from an unconstrained space to the parameter
      domain. A value of `None` is equivalent to the identity function. A
      bijector that maps structures of parameters will typically be an instance
      of `tfb.JointMap`.
  """

  bounds: Optional[types.Bounds] = struct.field(default=None)
  bijector: Optional[tfb.Bijector] = struct.field(default=None)

  @classmethod
  def create(
      cls,
      bounds: types.Bounds,
      bijector_fn: Callable[
          [Optional[ArrayLike], Optional[ArrayLike]], tfb.Bijector
      ],
  ) -> 'Constraint':
    """Factory that builds a `Constraint` from bounds and a bijector fn.

    The constraint's bijector is created by mapping `bijector_fn` over the tree
    structure of the lower/upper bounds. The bijector will operate on the same
    nested structure as the bounds.

    Args:
      bounds: A tuple of lower and upper bounds (may be ArrayTrees matching the
        tree structure of the parameters to be constrained).
      bijector_fn: A callable that takes a lower and upper bound (Arrays) as
        args and returns a TFP bijector that maps from an unconstrained space to
        the parameter domain.

    Returns:
      constraint: A Constraint object.

    Example:

    ```python
    lower = {'a': jnp.array(0.0), 'b': jnp.array(-1.0)}
    upper = {'a': jnp.array(10.0), 'b': None}
    constraint = Constraint.create((lower, upper), bijector_fn=tfb.SoftClip)
    x = {'a': jnp.array(-0.5), 'b': jnp.array(3.0)}
    constraint.bijector(x)  # => {'a': Array(0.47), 'b': Array(3.02)}
    ```
    """
    if all(
        tree_util.treedef_is_leaf(tree_util.tree_structure(x)) for x in bounds
    ):
      bijector = bijector_fn(*bounds)
    else:
      lb, ub = bounds
      if lb is None:
        if ub is None:
          raise ValueError(
              'At least one of lower and upper bounds must be specified.'
          )
        f = lambda u: bijector_fn(None, u)
        args = (ub,)
      elif ub is None:
        f = lambda l: bijector_fn(l, None)
        args = (lb,)
      else:
        f = bijector_fn
        args = bounds
      # Use dm-tree instead of jax.tree_util to handle structures containing
      # None.
      bijector = tfb.JointMap(tree.map_structure(f, *args))
    return Constraint(bounds=bounds, bijector=bijector)


@attr.frozen
class ModelParameter:
  """Class specifying a surrogate model parameter.

  Attributes:
    name: Also used as the Flax parameter name.
    init_fn: Initializes parameter values.
    constraint: Parameter constraint.
    regularizer: Regularizes the parameter.
  """

  name: str = attr.field()
  init_fn: InitFn = attr.field()
  constraint: Optional[Constraint] = attr.field(default=None)
  regularizer: Callable[[ArrayLike], jax.Array] = attr.field(
      kw_only=True, default=lambda x: jnp.zeros([], dtype=x.dtype)
  )

  @classmethod
  def from_prior(cls,
                 prior: tfd.Distribution,
                 constraint: Optional[Constraint] = None) -> 'ModelParameter':
    """Builds a `ModelParameter` from a `tfd.Distribution`.

    If `constraint` or `constraint.bijector` is None, then the constraint
    bijector is assumed to be the prior distribution's default event space
    bijector. See
    https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Distribution#experimental_default_event_space_bijector

    Args:
      prior: Parameter prior distribution.
      constraint: The parameter constraint.

    Returns:
      model_parameter: The parameter specification with the given prior.
    """
    if constraint is None or constraint.bounds is None:
      bounds = None
    else:
      bounds = constraint.bounds
    if constraint is None or constraint.bijector is None:
      bijector = prior.experimental_default_event_space_bijector()
    else:
      bijector = constraint.bijector

    sample = lambda seed: prior.sample(seed=seed)
    init_fn = sample
    if bounds is not None:
      init_fn = lambda s: jnp.clip(sample(s), *bounds)
    return ModelParameter(
        init_fn=init_fn,
        name=prior.name,
        constraint=Constraint(bounds=bounds, bijector=bijector),
        # TODO: `jnp.copy` is used to bypass TFP bijector caching;
        # otherwise JAX tracers are leaked from JIT-ted code.
        regularizer=lambda x: -prior.log_prob(jnp.copy(x)),
    )


ModelParameterGenerator = Generator[ModelParameter, jax.Array, _D]


class ModelCoroutine(Protocol, Generic[_D]):
  """`Protocol` to avoid inheritance.

  The coroutine pattern allows the `ModelParameter` objects, and the assembly of
  parameters into the kernel and stochastic process, to be specified
  simultaneously. The `StochasticProcessModel` Flax module runs the coroutine
  to initialize Flax parameters and build stochastic process objects.

  When a `ModelCoroutine` is called, it returns a generator-iterator, which
  should be iterated to build the `ModelParameter`s and the stochastic process
  object. See the full protocol below.
  """

  def __call__(
      self,
      inputs: Optional[types.ModelInput] = None,
      **kwargs,
  ) -> ModelParameterGenerator[_D]:
    """Coroutine function to be called from `StochasticProcessModel`.

    The coroutine is implemented via an enhanced generator
    (https://peps.python.org/pep-0342/). The generator-iterator returned by this
    method corresponds to the pytype
    `Generator[YieldType, SendType, ReturnType]`. (Python also has a newer, more
    flexible `Coroutine` type declared with `async`/`await` syntax. Here, when
    we reference "coroutines," we're referring to the simpler, more restrictive
    generator-based implementation.)

    The expected protocol is to run the coroutine for two different use cases:

    1) To build the Flax model.
    2) To implement Flax model forward passes.

    During (1), a new Flax model parameter is declared with the `name` and
    `init_fn` of each `ModelParameter` yielded by the generator. The initial
    values of each Flax parameter are generated by the `init_fn` and then sent
    into the generator as the left-hand sides of the yield statements. Once all
    `ModelParameter`s are yielded, the generator raises a `StopIteration`, and
    `StopIteration.value` contains a `tfd.Distribution` representing a
    stochastic process (e.g. `tfd.GaussianProcess` or `tfd.StudentTProcess`).
    During Flax module initialization, the returned `tfd.Distribution` is
    ignored.

    During (2), for each `ModelParameter` yielded by the generator, the Flax
    module accesses the Flax parameter of the same name, regularizes it (if
    applicable), sends the value into the generator, and stores the value of the
    regularization loss in a Flax mutable variable collection. Once all
    `ModelParameter`s are yielded, the generator raises a `StopIteration`, and
    `StopIteration.value` contains a `tfd.Distribution` on the provided index
    points. The module's `__call__` method returns this distribution.

    Example:

    ```python
    # Define a coroutine for a simple Gaussian Process model with trainable
    # kernel amplitude and observation noise variance.
    def model_coroutine(inputs=None):
      amplitude_constraint = Constraint(
          bounds=(jnp.zeros([]), None), bijector=tfb.Exp())
      amplitude = yield ModelParameter(
          init_fn=jax.random.exponential,
          regularizer=lambda x: 1e-3 * x**2,
          constraint=amplitude_constraint,
          name='amplitude')
      kernel = tfpk.ExponentiatedQuadratic(amplitude=amplitude)
      observation_noise = yield ModelParameter.from_prior(
          tfd.LogNormal(0.0, 1.0, name='observation_noise'),
          constraint=Constraint(bounds=(jnp.zeros([]), None)))
      return tfd.GaussianProcess(kernel=kernel, index_points=inputs,
          observation_noise_variance=observation_noise)
    ```

    Args:
      inputs: An ArrayTree of index points or None.
      **kwargs:
    """
    pass


def _squeeze_to_event_dims(
    dist: tfd.Distribution, labels: jax.Array
) -> jax.Array:
  """Squeezes the singleton `metrics` dimension from `labels`, if applicable."""
  if len(dist.event_shape) == 1 and labels.shape == (dist.event_shape[0], 1):
    return jnp.squeeze(labels, axis=-1)
  return labels


class StochasticProcessModel(nn.Module):
  """Builds a Stochastic Process Flax module.

  The module is instantiated with a coroutine in the pattern of
  `ModelCoroutine` and represents a trainable stochastic process
  (typically a `tfd.GaussianProcess` or `tfd.StudentTProcess`.)

  The module may also be passed a `mean_fn`, which is evaluated at the input
  points and returns the mean of the stochastic process (default is a constant
  zero mean).

  Examples:

  ```python
  from jax import random

  # Simulate some observed data.
  dim = 3
  x_observed = random.uniform(random.PRNGKey(0), shape=(20, dim))
  y_observed = x_observed.sum(axis=-1)

  # Build a GP module. `coro` follows the `ModelCoroutine` protocol.
  coro = GaussianProcessARD(dimension=dim)
  gp_model = StochasticProcessModel(coroutine=coro)

  # Initialize the Flax parameters.
  init_params = gp_model.init(random.PRNGKey(1), x_observed)

  # Build a GP with `x_observed` as index points. By default, `apply` invokes
  # the Flax module's `__call__` method.
  gp, regularization_losses = gp_model.apply(
      init_params,
      x_observed,
      mutable=('losses',))

  # Run the expensive computation (often a Cholesky decomposition) necessary to
  # compute the GP posterior predictive, and return the expensive intermediates
  # as mutable state.
  _, pp_state = gp_model.apply(
      {'params': init_state['params']},
      x_observed,
      y_observed,
      method=gp_model.precompute_predictive,
      mutable=('predictive'))

  # Now, posterior predictive GPs over different sets of index points,
  # conditioned on the observed data `x_observed` and `y_observed`, can be built
  # without recomputing the Cholesky decomposition.
  x_predicted = random.uniform(random.PRNGKey(2), shape=(5, dim))
  pp_dist = gp_model.apply(
      {'params': init_state['params'], **pp_state},
      x_predicted,
      x_observed,
      y_observed,
      method=gp_model.posterior_predictive)
  ```
  """

  coroutine: ModelCoroutine

  # `None` is zero-mean.
  mean_fn: Optional[Callable[[types.ModelInput], jax.Array]] = None

  def setup(self):
    """Builds module parameters."""
    generator = self.coroutine()
    try:
      p: ModelParameter = next(generator)
      while True:
        # Declare a Flax variable with the name and initialization function from
        # the `ModelParameter`.
        param: jax.Array = self.param(p.name, p.init_fn)
        p: ModelParameter = generator.send(param)
    except StopIteration:
      # Ignore the return value from the generator since this method only builds
      # the Flax parameters.
      pass

  def __call__(self, x: types.ModelInput, **kwargs) -> _D:
    """Returns a stochastic process distribution.

    If the Flax module's `apply` method is called with `mutable=True` or
    `mutable=('losses,')` regularization losses are additionally returned.

    Args:
      x: ArrayTree of index points in the constrained space.
      **kwargs:

    Returns:
      dist: `tfd.Distribution` instance with x as index points.
    """
    gen = self.coroutine(inputs=x, **kwargs)
    if self.is_initializing() and isinstance(self.mean_fn, nn.Module):
      # If mean_fn is a module, call it so its parameters are initialized.
      _ = self.mean_fn(x)  # pylint: disable=not-callable
    try:
      p: ModelParameter = next(gen)
      while True:
        # "params" is the name that `nn.Module` gives to the collection of read-
        # only variables.
        param: jax.Array = self.get_variable('params', p.name)
        if p.regularizer:
          self.sow(  # `sow` stores a value in a collection.
              'losses',
              f'{p.name}_regularization',
              p.regularizer(param),
              reduce_fn=lambda _, b: b,
          )
        p = gen.send(param)
    except StopIteration as e:
      # After the generator is exhausted, it raises a `StopIteration` error. The
      # `StopIteration` object has a property `value` of type `_D`.
      gp = e.value
      if self.mean_fn is None:
        tfp_mean_fn = None
      else:
        # TODO: Decide on a better pattern for incorporating
        # mean_fn.
        # pylint: disable=protected-access
        def tfp_mean_fn(x_: tfpke.ContinuousAndCategoricalValues):
          return self.mean_fn(
              types.ModelInput(
                  types.PaddedArray(
                      x_.continuous,
                      fill_value=x.continuous.fill_value,
                      _original_shape=(
                          x_.continuous.shape[0],
                          x.continuous._original_shape[1],
                      ),
                      _mask=x.continuous._mask,
                      _nopadding_done=x.continuous._nopadding_done,
                  ),
                  types.PaddedArray(
                      x_.categorical,
                      fill_value=x.categorical.fill_value,
                      _original_shape=(
                          x_.categorical.shape[0],
                          x.categorical._original_shape[1],
                      ),
                      _mask=x.categorical._mask,
                      _nopadding_done=x.categorical._nopadding_done,
                  ),
              )
          )
          # pylint: enable=protected-access

      return gp.copy(mean_fn=tfp_mean_fn)

  def precompute_predictive(
      self,
      x_observed: types.ModelInput,  # Could just take ModelData
      y_observed: types.PaddedArray,
  ) -> None:
    """Builds a stochastic process regression model conditioned on observations.

    The mutable variable returned by this method as auxiliary output should be
    passed as state to `posterior_predictive`. This avoids repeated, expensive
    operations (often Cholesky decompositions) when computing the posterior
    predictive.

    Args:
      x_observed: Index points on which to condition the posterior predictive.
      y_observed: Observations on which to condition the posterior predictive.
    """
    # Call the `tfd.Distribution` object's `posterior_predictive` method. This
    # triggers an expensive computation, typically a Cholesky decomposition, and
    # returns a new `tfd.Distribution` representing the posterior predictive.
    # Expensive intermediates are stored in the `precomputed_cholesky` Flax
    # variable and returned as auxiliary output.
    kwargs = {}
    if not y_observed._nopadding_done:  # pylint: disable=protected-access
      kwargs['observations_is_missing'] = y_observed.is_missing[0]

    prior = self(x_observed)
    observations = _squeeze_to_event_dims(prior, y_observed.padded_array)
    predictive_dist = prior.posterior_predictive(
        predictive_index_points=None, observations=observations, **kwargs
    )
    # pylint: disable=protected-access
    cached_predictive_intermediates = {
        '_precomputed_divisor_matrix_cholesky': (
            predictive_dist._precomputed_divisor_matrix_cholesky
        ),
        '_precomputed_solve_on_observation': (
            predictive_dist._precomputed_solve_on_observation
        ),
    }
    # pylint: enable=protected-access
    self.sow(
        'predictive',
        'precomputed_cholesky',
        cached_predictive_intermediates,
        reduce_fn=lambda _, b: b,
    )

  def posterior_predictive(
      self,
      x_predictive: types.ModelInput,
      x_observed: types.ModelInput,
      y_observed: types.PaddedArray,
  ) -> _D:
    """Returns a posterior predictive stochastic process.

    The posterior predictive distribution over the function values at
    `x_predictive`, typically a `tfd.GaussianProcessRegressionModel` or
    `tfd.StudentTProcessRegressionModel`, is built using the mutable variable in
    `predictive/precomputed_cholesky`. This avoids repeated, expensive
    computation (often Cholesky decompositions of the kernel matrix for observed
    data). See the class docstring for how to use `precompute_predictive` in
    combination with `posterior_predictive`.

    Args:
      x_predictive: Predictive index points.
      x_observed: Index points on which to condition the posterior predictive.
      y_observed: Observations on which to condition the posterior predictive.

    Returns:
      pp_dist: The posterior predictive distribution over `x_predictive`.
    """
    if not self.has_variable('predictive', 'precomputed_cholesky'):
      raise ValueError(
          'The mutable variable returned by '
          '`precompute_predictive` must be passed into `predict`. '
          'See the class docstring for an example.'
      )
    # Access the precomputed values stored in the Flax variable, and build the
    # distribution object over the predictive index points (avoiding
    # recomputation).
    cached_intermediates = self.get_variable(
        'predictive', 'precomputed_cholesky'
    )
    kwargs = cached_intermediates
    if not y_observed._nopadding_done:  # pylint: disable=protected-access
      kwargs = kwargs | {'observations_is_missing': y_observed.is_missing[0]}

    predictive_index_points = tfpke.ContinuousAndCategoricalValues(
        continuous=x_predictive.continuous.padded_array,
        categorical=x_predictive.categorical.padded_array,
    )

    prior = self(x_observed)
    observations = _squeeze_to_event_dims(prior, y_observed.padded_array)
    return prior.posterior_predictive(
        observations=observations,
        predictive_index_points=predictive_index_points,
        **kwargs,
    )


class VectorToArrayTree(tfb.Chain):
  """Bijector that converts a vector to a dict like `params`.

  The bijector splits the vector, reshapes the splits, and then packs the splits
  into a dictionary. The bijector's `inverse` method does the reverse.

  It also has aliases `to_params` and `to_vectors` for `forward()` and
  `inverse()` methods.

  Example:

  ```python
  params = {'a': [4.0, 3.0], 'b': -2.0, 'c': [[6.0]]}
  bij = VectorToArrayTree(params)
  bij.inverse(params)  #  => [4.0, 3.0, -2.0, 6.0]
  bij([0.0, 1.0, 2.0, 3.0])  # => {'a': [0.0, 1.0], 'b': 2.0, 'c': [[3.0]]}
  ```
  """

  def __init__(
      self, arraytree: types.ParameterDict, *, validate_args: bool = False
  ):
    """Init.

    Args:
      arraytree: A nested structure of Arrays.
      validate_args: If True, does runtime validation. It may be slow.
    """
    parameters = dict(locals())
    flat, treedef = tree_util.tree_flatten(arraytree)
    restructure = tfb.Restructure(
        output_structure=tree_util.tree_unflatten(treedef, range(len(flat))))
    reshape = tfb.JointMap(
        [tfb.Reshape(event_shape_out=f.shape) for f in flat],
        validate_args=validate_args,
    )
    split = tfb.Split([x.size for x in flat], validate_args=validate_args)
    super().__init__(
        [restructure, reshape, split],
        parameters=parameters,
        name='VectorToArrayTree',
        validate_args=validate_args,
    )

  def to_params(self, vector: ArrayLike) -> types.ParameterDict:
    return self.forward(vector)

  def to_vector(self, params: types.ParameterDict) -> jax.Array:
    return self.inverse(params)


def get_constraints(
    model: StochasticProcessModel, x: Optional[Any] = None
) -> Constraint:
  """Gets the parameter constraints from a StochasticProcessModel.

  If the model contains trainable Flax variables besides those defined by the
  coroutine (for example, if `mean_fn` is a Flax module), the non-coroutine
  variables are assumed to be unconstrained (the bijector passes them through
  unmodified, and their lower/upper bounds are `None`). In this case `x` must be
  passed, so that the structure of the non-coroutine parameters dict(s) can be
  generated. `Constraint` objects for models with constrained parameters aside
  from those defined in the coroutine must be built manually.

  This method runs the coroutine, collects the parameter constraints, and
  returns a new Constraint object in which the lower/upper bounds are dicts
  (corresponding to the parameters dict) and the bijector operates on dicts. The
  object may be passed to a Vizier Optimizer to constrain the parameters for
  optimization.

  Example:

  ```python
  def model_coroutine(inputs=None):
    amplitude_constraint = Constraint(
        bounds=(jnp.array(0.1), None))
    length_scale_constraint = Constraint.create(
        bounds=(jnp.array(0.0), jnp.array(10.0)),
        bijector_fn=tfb.Sigmoid)

    amplitude = yield ModelParameter.from_prior(
        tfd.LogNormal(0.0, 1.0, name='amplitude'),
        constraint=amplitude_constraint)
    length_scale = yield ModelParameter(
        init_fn=jax.random.exponential,
        regularizer=lambda x: 1e-3 * x**2,
        constraint=length_scale_constraint,
        name='length_scale')
    kernel = tfpk.ExponentiatedQuadratic(
        amplitude=amplitude, length_scale=length_scale)
    return tfd.GaussianProcess(kernel, index_points=inputs)

  model = StochasticProcessModel(model_coroutine)
  constraint = GetConstraints(model)
  constraint.bijector
  # => tfb.JointMap({'amplitude': tfb.Exp(),
                    'length_scale': tfb.Sigmoid(0.0, 10.0)})

  constraint.bounds
  # => ({'amplitude': jnp.array(0.1), 'length_scale': jnp.array(0.0)},
  #     {'amplitude': None, 'length_scale': jnp.array(10.0)})
  ```

  Args:
    model: A `StochasticProcessModel` instance.
    x: An input that can be passed to `model.lazy_init`. `x` must be of the same
      structure as the model inputs and may contain arrays or `ShapeDtypeStruct`
      instances (see flax.linen.Module.lazy_init docs). If `model` contains Flax
      variables aside from those defined by `model.coroutine` (e.g. in a
      trainable `mean_fn`) then this arg is required.

  Returns:
    constraint: A `Constraint` instance expressing constraints on the parameters
      specified by `coroutine`.
  """

  # Run the coroutine to extract constraints for the model parameters defined in
  # the coroutine.
  gen = model.coroutine()
  k = jax.random.PRNGKey(0)
  lower = {}
  upper = {}
  bijectors = {}
  try:
    p = next(gen)
    while True:
      v = p.init_fn(k)
      if p.constraint is None or p.constraint.bounds is None:
        lower[p.name] = None
        upper[p.name] = None
      else:
        lower[p.name] = p.constraint.bounds[0]
        upper[p.name] = p.constraint.bounds[1]
      if p.constraint is None or p.constraint.bijector is None:
        bijectors[p.name] = tfb.Identity()
      else:
        bijectors[p.name] = p.constraint.bijector
      p = gen.send(v)
  except StopIteration:
    pass

  # `tfb.JointMap` applies a structure of bijectors to a parallel structure of
  # inputs. Define a `JointMap` bijector that maps an unconstrained parameters
  # dict to a constrained parameters dict with a dict of bijectors (all dicts
  # are keyed by parameter names).
  bijector = tfb.JointMap(bijectors=bijectors)
  if x is not None:
    # Get the parameters dict keys, if any, that do not come from the coroutine
    # (e.g. `mean_fn` parameters).
    params = model.lazy_init(jax.random.PRNGKey(0), x)['params']
    non_coroutine_keys = set(params.keys()) - set(bijectors.keys())

    # Define a bijector that ignores (applies an identity transformation to)
    # non-coroutine parameters.
    if non_coroutine_keys:
      logging.info(
          (
              'Defining a constraint object that ignores the following'
              'non-coroutine parameters: %s'
          ),
          non_coroutine_keys,
      )

      def _wrap_bijector_method_to_ignore_non_coro(f):
        """Wrap bijector methods to pass non-coroutine params through."""

        def _f(p):
          p_ = p.copy()
          non_coroutine_params = {k: p_.pop(k) for k in non_coroutine_keys}
          y = f(p_)
          y.update(non_coroutine_params)
          return y

        return _f

      def _bijector_fldj_with_non_coro(p):
        """Non-coroutine params do not affect the FLDJ."""
        p_ = {k: v for k, v in p.items() if k not in non_coroutine_keys}
        return bijector.forward_log_det_jacobian(p_)

      bijector_forward_min_event_ndims = bijector.forward_min_event_ndims.copy()
      # Populate `lower` and `upper` bounds dicts with `None` values for entries
      # corresponding to non-coroutine params.
      for k in non_coroutine_keys:
        lower[k] = tree.map_structure(lambda _: None, params[k])
        upper[k] = tree.map_structure(lambda _: None, params[k])
        bijector_forward_min_event_ndims[k] = tree.map_structure(
            lambda _: 0, params[k]
        )
      bijector = tfb.Inline(
          forward_fn=_wrap_bijector_method_to_ignore_non_coro(bijector.forward),
          inverse_fn=_wrap_bijector_method_to_ignore_non_coro(bijector.inverse),
          forward_log_det_jacobian_fn=_bijector_fldj_with_non_coro,
          forward_min_event_ndims=bijector_forward_min_event_ndims,
      )
    else:
      # If the model doesn't have params aside from those defined by the
      # coroutine, its params should have the same structure as `bijectors`
      # (this assertion failing indicates a bug).
      try:
        tree.assert_same_structure(params, bijectors)
      except ValueError as exc:
        raise ValueError(
            '`params` and `bijectors` should have the same nested structure.'
            f'Saw: `params={params}` and `bijectors={bijectors}`'
        ) from exc

  return Constraint((lower, upper), bijector=bijector)


class PrecomputedPredictive(eqx.Module):
  """Precomputed model for prediction.

  Conceptually, this module corresponds to the posterior distribution obtained
  by updating `prior` with `observed_data`.

  Also see `StochasticProcessWithCoroutine.precompute_predictive`.

  Attributes:
    prior: Defines the prior distribution.
    observed_data: Observations with batch shape [B].
  """

  prior: 'StochasticProcessWithCoroutine'
  observed_data: types.ModelData
  precomputed_divisor_matrix_cholesky: jax.Array
  precomputed_solve_on_observation: jax.Array

  @property
  def _posterior_kwargs(self):
    if self.observed_data.labels.shape[-1] > 1:
      # If there is more than one metric, assume we are using a MultiTask GP, in
      # which case `observations_is_missing` should be a 2D array instead of a
      # vector.
      # TODO: Make `_mask` public on PaddedArray.
      observations_is_missing = ~self.observed_data.labels._mask  # pylint: disable=protected-access
    else:
      # If there is one metric, then squeeze out the `num_metrics` dimension
      # (which has size 1).
      observations_is_missing = self.observed_data.labels.is_missing[0]
    return dict(
        _precomputed_divisor_matrix_cholesky=self.precomputed_divisor_matrix_cholesky,
        _precomputed_solve_on_observation=self.precomputed_solve_on_observation,
        observations=self.observed_data.labels.padded_array,
        observations_is_missing=observations_is_missing,
    )

  def predict(self, x_predictive: types.ModelInput) -> tfd.Distribution:
    """Returns the posterior distribution on index points `xs`.

    Args:
      x_predictive: Array(Tree) of batch shape [B].

    Returns:
      Distribution with sample shape [B].
    """
    return self._predict(x_predictive)

  def _predict(
      self,
      x_predictive: types.ModelInput,
      expand_batch_dim: bool = False,
  ) -> tfd.Distribution:
    """Returns the posterior distribution on index points `xs`.

    Args:
      x_predictive: Array(Tree) of batch shape [B].
      expand_batch_dim: If True, expand the dimensions of `x_predictive` so that
        it broadcasts with the distribution's batch shape. This is useful for
        predictions on a batch of predictive points using an ensemble GP.

    Returns:
      Distribution with sample shape [B].
    """
    x_pred_tfp = tfpke.ContinuousAndCategoricalValues(
        continuous=x_predictive.continuous.padded_array,
        categorical=x_predictive.categorical.padded_array,
    )
    if expand_batch_dim:
      x_pred_tfp = tfpke.ContinuousAndCategoricalValues(
          continuous=x_pred_tfp.continuous[:, jnp.newaxis, ...],
          categorical=x_pred_tfp.categorical[:, jnp.newaxis, ...],
      )
    prior_gp = self.prior(self.observed_data.features)
    kwargs = self._posterior_kwargs.copy()
    kwargs['observations'] = _squeeze_to_event_dims(
        prior_gp, kwargs['observations']
    )
    return prior_gp.posterior_predictive(
        predictive_index_points=x_pred_tfp, **kwargs
    )


class UniformEnsemblePredictive(eqx.Module):
  """Uniform ensemble of predictive models.

  Ensembles the `predictives` with equal weights.
  """

  predictives: PrecomputedPredictive

  def predict(self, xs: types.ModelInput) -> tfd.Distribution:
    return self.predict_with_aux(xs)[0]

  def predict_with_aux(
      self, xs: types.ModelInput
  ) -> tuple[tfd.Distribution, chex.ArrayTree]:
    # If `xs` has a batch dimension and the predictive distribution is
    # ensembled, we expand xs' batch shape so as not to collide with the
    # ensemble dimensions. (`vmap` cannot currently be used on functions
    # returning TFP distributions).
    has_batched_hparams = (
        self.predictives.precomputed_solve_on_observation.ndim > 1
    )
    expand_x = len(xs.continuous.shape) == 3 and has_batched_hparams
    dist = self.predictives._predict(xs, expand_batch_dim=expand_x)  # pylint: disable=protected-access
    if has_batched_hparams:
      return (
          tfd.MixtureSameFamily(
              tfd.Categorical(logits=jnp.zeros(dist.batch_shape[-1])), dist
          ),
          {
              'components_mean': dist.mean().T,
              'components_stddev': dist.stddev().T,
          },
      )
    return dist, {}


def _initialize_params(
    coroutine: ModelCoroutine, rng: jax.Array
) -> chex.ArrayTree:
  """Randomly initializes a coroutine's parameters."""
  gen = coroutine()
  params = {}
  try:
    p: ModelParameter = next(gen)
    while True:
      # Declare a Flax variable with the name and initialization function from
      # the `ModelParameter`.
      rng, init_rng = jax.random.split(rng)
      param = p.init_fn(init_rng)
      params[p.name] = param
      p: ModelParameter = gen.send(param)
  except StopIteration:
    return params


class StochasticProcessWithCoroutine(eqx.Module):
  """Fully parameterized stochastic process model."""

  coroutine: ModelCoroutine = eqx.field(static=True)
  params: chex.ArrayTree

  @classmethod
  def initialize(cls, coroutine: ModelCoroutine, *, rng: jax.Array):
    """Builds module parameters."""
    # return cls(coroutine, params=_initialize_params(coroutine, rng))
    # TODO: Put back the line above. This code currently uses
    # flax's init routine to exactly match the setup so we can replicate
    # the trajectories from pre-refactoring.
    return cls(
        coroutine, StochasticProcessModel(coroutine).init(rng, None)['params']
    )

  def call_with_aux(
      self, x: types.ModelInput, /
  ) -> tuple[tfd.Distribution, chex.ArrayTree]:
    """Returns the joint distribution and auxiliary information.

    Args:
      x:

    Returns:
      (dist, aux)
      dist: Distribution
      aux: A dict where all the leaf nodes in the subtree aux[`losses`] contain
        regularization losses.
    """
    gen = self.coroutine(x)
    params = self.params
    params_loss = dict()
    try:
      p: ModelParameter = next(gen)
      while True:
        if p.regularizer:
          params_loss[p.name] = p.regularizer(params[p.name])
        # "params" is the name that `nn.Module` gives to the collection of read-
        # only variables.
        p = gen.send(params[p.name])
    except StopIteration as e:
      # After the generator is exhausted, it raises a `StopIteration` error. The
      # `StopIteration` object has a property `value` of type `_D`.
      return e.value, {'losses': params_loss}

  def __call__(self, x: types.ModelInput) -> tfd.Distribution:
    return self.call_with_aux(x)[0]

  def loss_with_aux(
      self,
      data: types.ModelData,
      seed: Optional[jax.Array] = None,
  ) -> tuple[jax.Array, chex.ArrayTree]:
    dist, aux = self.call_with_aux(data.features)

    # If `seed` is provided, that implies `dist.log_prob` is a stochastic
    # approximation that requires a random seed.
    log_prob_kwargs = {}
    if seed is not None:
      log_prob_kwargs['key'] = seed

    labels = _squeeze_to_event_dims(dist, data.labels.padded_array)
    # TODO: Enable `is_missing` for MTGP.
    if isinstance(dist, tfde.MultiTaskGaussianProcess):
      logging.warning(
          'Using a multitask GP; note that padding/masking is not yet supported'
          'in `log_prob`.'
      )
      nll_data = -dist.log_prob(labels, **log_prob_kwargs)
    else:
      nll_data = -dist.log_prob(
          labels, is_missing=data.labels.is_missing[0], **log_prob_kwargs
      )
    loss = nll_data + jax.tree_util.tree_reduce(jnp.add, aux['losses'])
    return loss, aux

  def precompute_predictive(
      self, data: types.ModelData
  ) -> PrecomputedPredictive:
    jax.monitoring.record_event(
        '/vizier/jax/coroutine_with_params/precompute_predictive/traced'
    )
    if jnp.size(data.labels.padded_array) == 0:
      # TFP `retrying_cholesky` does not handle empty observations.
      prior = self(data.features).copy(cholesky_fn=None)
    else:
      prior = self(data.features)

    observations = _squeeze_to_event_dims(prior, data.labels.padded_array)
    if isinstance(prior, tfde.MultiTaskGaussianProcess):
      # TODO: Make `_mask` public on PaddedArray.
      observations_is_missing = ~data.labels._mask  # pylint: disable=protected-access
    else:
      observations_is_missing = data.labels.is_missing[0]
    predictive = prior.posterior_predictive(
        predictive_index_points=None,
        observations=observations,
        observations_is_missing=observations_is_missing,
    )
    # pylint: disable=protected-access
    return PrecomputedPredictive(
        self,
        data,
        predictive._precomputed_divisor_matrix_cholesky,
        predictive._precomputed_solve_on_observation,
    )


class CoroutineWithData(eqx.Module):
  """Utility module for training (ARD).

  Has setup() and loss_with_aux() that can be readily `eqx.filter_jit`ed
  """

  coroutine: ModelCoroutine = eqx.field(static=True)
  data: types.ModelData

  def constraints(self):
    return get_constraints(StochasticProcessModel(self.coroutine))

  def setup(self, rng: jax.Array):
    jax.monitoring.record_event('/vizier/jax/coroutine_with_data/setup/traced')
    return StochasticProcessWithCoroutine.initialize(
        self.coroutine, rng=rng
    ).params

  def loss_with_aux(
      self, params, seed: Optional[jax.Array] = None
  ) -> tuple[jax.Array, chex.ArrayTree]:
    jax.monitoring.record_event(
        '/vizier/jax/coroutine_with_data/loss_with_aux/traced'
    )
    return StochasticProcessWithCoroutine(self.coroutine, params).loss_with_aux(
        self.data, seed=seed
    )


--- vizier/_src/jax/stochastic_process_model_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for stochastic_process_model."""

import functools
from typing import Optional
from unittest import mock

from absl import logging
from absl.testing import parameterized
import equinox as eqx
from flax import linen as nn
import jax
from jax import config
from jax import numpy as jnp
from jax import random
import numpy as np
from tensorflow_probability.substrates import jax as tfp
import tree
from vizier._src.jax import stochastic_process_model as sp_model
from vizier._src.jax import types
from vizier._src.jax.models import mask_features

from absl.testing import absltest


tfb = tfp.bijectors
tfd = tfp.distributions
tfpk = tfp.math.psd_kernels
tfde = tfp.experimental.distributions
tfpke = tfp.experimental.psd_kernels


def _kernel_coroutine(dtype=np.float64):
  amplitude = yield sp_model.ModelParameter(
      init_fn=lambda k: random.exponential(k, dtype=dtype),
      constraint=sp_model.Constraint(bounds=(np.zeros([], dtype=dtype), None)),
      regularizer=lambda x: 1e-3 * x**2,
      name='amplitude',
  )
  one = np.array([1.0]).astype(dtype)
  inverse_length_scale_continuous = yield sp_model.ModelParameter.from_prior(
      tfd.Exponential(rate=one, name='inverse_length_scale_continuous')
  )
  inverse_length_scale_categorical = yield sp_model.ModelParameter.from_prior(
      tfd.Exponential(rate=one, name='inverse_length_scale_categorical')
  )
  inverse_length_scale = tfpke.ContinuousAndCategoricalValues(
      inverse_length_scale_continuous, inverse_length_scale_categorical
  )
  kernel = tfpk.ExponentiatedQuadratic(amplitude=amplitude, validate_args=True)
  return tfpke.FeatureScaledWithCategorical(
      kernel, inverse_length_scale, validate_args=True
  )


def _test_coroutine(
    inputs: Optional[types.ModelInput] = None,
    num_tasks=1,
    dtype=np.float64,
):
  """A coroutine that follows the `ModelCoroutine` protocol."""
  kernel = yield from _kernel_coroutine(dtype=dtype)
  if inputs is not None:
    kernel = mask_features.MaskFeatures(
        kernel,
        dimension_is_missing=tfpke.ContinuousAndCategoricalValues(
            continuous=inputs.continuous.is_missing[1],
            categorical=inputs.categorical.is_missing[1],
        ),
    )
    inputs = tfpke.ContinuousAndCategoricalValues(
        inputs.continuous.padded_array, inputs.categorical.padded_array
    )
  if num_tasks == 1:
    return tfd.StudentTProcess(
        df=np.array(5.0).astype(dtype),
        kernel=kernel,
        index_points=inputs,
        observation_noise_variance=np.ones([], dtype=dtype),
        validate_args=True,
    )

  multi_task_kernel = tfpke.Independent(num_tasks=num_tasks, base_kernel=kernel)
  return tfde.MultiTaskGaussianProcess(
      kernel=multi_task_kernel,
      index_points=inputs,
      observation_noise_variance=np.ones([], dtype=dtype),
      validate_args=True,
  )


def _make_inputs(
    key,
    dtype=jnp.float64,
    num_continuous=5,
    num_categorical=3,
    num_tasks=1,
    num_observed=15,
    num_predictive=8,
    predictive_batch=(),
    pad_obs=0,
    pad_continuous_dim=0,
    pad_categorical_dim=0,
    pad_tasks=0,
):
  cont_obs_key, cat_obs_key, cont_pred_key, cat_pred_key, y_key = random.split(
      key, num=5
  )
  x_observed_cont = random.uniform(
      cont_obs_key, shape=(num_observed, num_continuous), dtype=dtype
  )
  x_observed_cat = random.randint(
      cat_obs_key, shape=(num_observed, num_categorical), minval=0, maxval=6
  )
  x_observed = types.ModelInput(
      continuous=types.PaddedArray.from_array(
          x_observed_cont,
          (num_observed + pad_obs, num_continuous + pad_continuous_dim),
          fill_value=np.nan,
      ),
      categorical=types.PaddedArray.from_array(
          x_observed_cat,
          (num_observed + pad_obs, num_categorical + pad_categorical_dim),
          fill_value=-1,
      ),
  )

  y_observed = random.uniform(
      y_key, shape=(num_observed, num_tasks), dtype=dtype
  )
  y_observed = types.PaddedArray.from_array(
      y_observed,
      (num_observed + pad_obs, num_tasks + pad_tasks),
      fill_value=np.nan,
  )

  x_predictive_cont = random.uniform(
      cont_pred_key,
      shape=predictive_batch + (num_predictive, num_continuous),
      dtype=dtype,
  )
  x_predictive_cat = random.randint(
      cat_pred_key,
      shape=predictive_batch + (num_predictive, num_categorical),
      minval=0,
      maxval=6,
  )

  x_predictive = types.ModelInput(
      continuous=types.PaddedArray.from_array(
          x_predictive_cont,
          predictive_batch
          + (num_predictive + pad_obs, num_continuous + pad_continuous_dim),
          fill_value=np.nan,
      ),
      categorical=types.PaddedArray.from_array(
          x_predictive_cat,
          predictive_batch
          + (num_predictive + pad_obs, num_categorical + pad_categorical_dim),
          fill_value=-1,
      ),
  )
  return x_observed, y_observed, x_predictive


class MeanFn(nn.Module):

  @nn.compact
  def __call__(self, x):
    x = nn.relu(nn.Dense(3)(x.continuous.padded_array))
    x = nn.Dense(1)(x)
    return jnp.squeeze(x, axis=-1)


class StochasticProcessModelTest(parameterized.TestCase):

  @parameterized.named_parameters(
      # TODO: Fix support for f32.
      {
          'testcase_name': 'continuous_only',
          'input_kwargs': dict(
              num_categorical=0, pad_obs=3, pad_continuous_dim=2
          ),
      },
      {
          'testcase_name': 'multitask_continuous',
          'input_kwargs': dict(
              num_categorical=2,
              num_tasks=3,
          ),
      },
      {
          'testcase_name': 'continuous_categorical',
          'input_kwargs': dict(
              pad_continuous_dim=5, pad_categorical_dim=4, pad_obs=7
          ),
      },
      {'testcase_name': 'continuous_categorical_nopad', 'input_kwargs': dict()},
      # TODO: Enable `is_missing` in TFP Multitask GP.
      # {
      #     'testcase_name': 'continuous_categorical_multitask',
      #     'input_kwargs': dict(
      #         num_tasks=3,
      #         pad_tasks=2,
      #         pad_obs=5,
      #     )
      # },
  )
  def test_stochastic_process_model(self, input_kwargs):
    init_key, data_key, sample_key = jax.random.split(random.PRNGKey(0), num=3)
    x_observed, y_observed, x_predictive = _make_inputs(
        data_key, **input_kwargs
    )
    dtype = y_observed.padded_array.dtype
    model_coroutine = functools.partial(
        _test_coroutine,
        num_tasks=input_kwargs.get('num_tasks', 1),
        dtype=dtype,
    )
    model = sp_model.StochasticProcessModel(coroutine=model_coroutine)

    init_state = model.init(init_key, x_observed)
    dist, losses = model.apply(init_state, x_observed, mutable=('losses',))
    if isinstance(dist, tfde.MultiTaskGaussianProcess):
      lp = dist.log_prob(y_observed.padded_array)
    else:
      lp = dist.log_prob(
          y_observed.padded_array[:, 0], is_missing=y_observed.is_missing[0]
      )
    self.assertTrue(np.isfinite(lp))
    self.assertEqual(lp.dtype, dtype)

    # Check regularization loss values.
    gen = model_coroutine()
    p = next(gen)
    params = dict(init_state['params'])
    losses = dict(losses['losses'])
    try:
      while True:
        value = params[p.name]
        param_loss = losses[f'{p.name}_regularization']
        self.assertTrue(np.isfinite(value))
        self.assertEqual(value.dtype, dtype)
        self.assertTrue(np.isfinite(param_loss))
        self.assertAlmostEqual(p.regularizer(value), param_loss)
        p = gen.send(value)
    except StopIteration:
      pass

    _, pp_state = model.apply({'params': params},
                              x_observed,
                              y_observed,
                              method=model.precompute_predictive,
                              mutable=('predictive',))

    state = {'params': params, **pp_state}
    x_pred = types.ModelInput(
        continuous=x_predictive.continuous.replace_fill_value(0.0),
        categorical=x_predictive.categorical.replace_fill_value(0),
    )
    pp_dist = model.apply(
        state,
        x_pred,
        x_observed,
        y_observed,
        method=model.posterior_predictive,
    )
    pp_log_prob = pp_dist.log_prob(pp_dist.sample(seed=sample_key))
    self.assertTrue(np.isfinite(pp_log_prob).all())
    self.assertEqual(pp_log_prob.dtype, dtype)

  @parameterized.named_parameters(
      {
          'testcase_name': 'continuous_with_mean_fn',
          'num_tasks': 1,
          'use_mean_fn': True,
      },
      {
          'testcase_name': 'multitask_continuous',
          'num_tasks': 3,
      },
      {
          'testcase_name': 'multitask_continuous_categorical',
          'num_tasks': 3,
      },
  )
  def test_jax_transformations(self, num_tasks, use_mean_fn=False):
    init_key, data_key, prior_key, post_key = jax.random.split(
        random.PRNGKey(0), num=4
    )
    num_observed = 20
    x_observed, y_observed, x_predictive = _make_inputs(
        data_key,
        num_tasks=num_tasks,
        num_observed=num_observed,
        predictive_batch=(12,),
        dtype=np.float32,
    )
    model = sp_model.StochasticProcessModel(
        coroutine=functools.partial(
            _test_coroutine,
            num_tasks=num_tasks,
            dtype=np.float32,
        ),
        mean_fn=MeanFn() if use_mean_fn else None,
    )

    batch_size = 10
    keys = jax.random.split(init_key, num=batch_size)
    init_state = jax.vmap(lambda k: model.init(k, x_observed))(keys)
    s = jax.jit(
        jax.vmap(
            lambda s: model.apply(  # pylint: disable=g-long-lambda
                s, x_observed, mutable=('losses',)
            )[0].sample(seed=prior_key)
        )
    )(init_state)
    sample_shape = (
        (batch_size, num_observed)
        if num_tasks == 1
        else (batch_size, num_observed, num_tasks)
    )
    self.assertEqual(s.shape, sample_shape)

    _, pp_state = jax.jit(
        jax.vmap(
            lambda p: model.apply(  # pylint: disable=g-long-lambda
                p,
                x_observed,
                y_observed,
                method=model.precompute_predictive,
                mutable=('predictive',),
            )
        )
    )(init_state)

    state = {**init_state, **pp_state}

    @jax.jit
    @jax.vmap
    def _posterior_sample_and_log_prob(s):
      dist = model.apply(
          s,
          x_predictive,
          x_observed,
          y_observed,
          method=model.posterior_predictive,
      )[0]
      sample = dist.sample(seed=post_key)
      return dist.log_prob(sample)

    pp_log_prob = _posterior_sample_and_log_prob(state)
    log_prob_grad = jax.grad(
        lambda s: jnp.sum(_posterior_sample_and_log_prob(s))
    )(state)
    for g in tree.flatten(log_prob_grad['params']):
      self.assertTrue(np.isfinite(g).all())
    self.assertTrue(np.isfinite(pp_log_prob).all())
    self.assertEqual(pp_log_prob.shape, (batch_size,))

  def test_cholesky_not_recomputed(self):
    # Need num_observed == num_predictive so the mock Cholesky works for both
    # observed and predictive.
    x_obs, y_obs, x = _make_inputs(
        jax.random.PRNGKey(5), np.float32, num_observed=12, num_predictive=12
    )
    chol = np.eye(12, dtype=np.float32)
    mock_cholesky_fn = mock.Mock(return_value=chol)

    def _coro_with_mock_cholesky(inputs=None):
      stp = yield from _test_coroutine(inputs=inputs, dtype=np.float32)
      # The `copy` method of TFP distributions rebuilds the distribution object
      # with the given constructor kwarg(s) replaced.
      return stp.copy(cholesky_fn=mock_cholesky_fn)

    model = sp_model.StochasticProcessModel(coroutine=_coro_with_mock_cholesky)
    keys = jax.random.split(jax.random.PRNGKey(0), num=8)
    params = jax.vmap(lambda k: model.init(k, x_obs))(keys)
    _, pp_state = jax.vmap(
        lambda p: model.apply(  # pylint: disable=g-long-lambda
            p,
            x_obs,
            y_obs,
            method=model.precompute_predictive,
            mutable=('predictive',),
        )
    )(params)
    mock_cholesky_fn.assert_called_once()

    # Sampling from the posterior predictive should invoke cholesky_fn only
    # once (on the kernel matrix for the predictive index points).
    mock_cholesky_fn.reset_mock()
    state = {**params, **pp_state}
    _ = jax.jit(
        jax.vmap(
            lambda s: model.apply(  # pylint: disable=g-long-lambda
                s, x, x_obs, y_obs, method=model.posterior_predictive
            ).sample(seed=jax.random.PRNGKey(0))
        )
    )(state)
    mock_cholesky_fn.assert_called_once()

    # Sample new data and compute the posterior predictive.
    mock_cholesky_fn.reset_mock()
    x_obs2, y_obs2, x2 = _make_inputs(
        jax.random.PRNGKey(6), np.float32, num_observed=12, num_predictive=12
    )
    params2 = model.init(jax.random.PRNGKey(7), x_obs2)
    _, pp_state2 = model.apply(
        params2,
        x_obs2,
        y_obs2,
        method=model.precompute_predictive,
        mutable=('predictive',),
    )
    _ = jax.jit(
        lambda s: model.apply(  # pylint: disable=g-long-lambda
            s, x2, x_obs2, y_obs2, method=model.posterior_predictive
        ).sample(seed=jax.random.PRNGKey(0))
    )({**params2, **pp_state2})
    # Two Cholesky calls; one for observed, one for predictive.
    self.assertEqual(mock_cholesky_fn.call_count, 2)

    # Assert that `model.posterior_predictive`, when called on the original
    # dataset, does not recompute the Cholesky.
    mock_cholesky_fn.reset_mock()
    state = {**params, **pp_state}
    _ = jax.jit(
        jax.vmap(
            lambda s: model.apply(  # pylint: disable=g-long-lambda
                s, x, x_obs, y_obs, method=model.posterior_predictive
            ).sample(seed=jax.random.PRNGKey(0))
        )
    )(state)
    mock_cholesky_fn.assert_called_once()

  def test_stochastic_process_model_with_mean_fn(self):

    data_key, init_key, vmap_key = random.split(random.PRNGKey(0), num=3)
    x_observed, y_observed, x_predictive = _make_inputs(
        data_key,
        num_continuous=3,
        num_categorical=0,
        num_observed=20,
        num_predictive=50,
    )

    # Use `jnp.squeeze` to remove the singleton dimension of the output of
    # `mean_fn`, so that it has shape `(num_observed,)`.
    mean_fn = MeanFn()
    model = sp_model.StochasticProcessModel(
        coroutine=_test_coroutine, mean_fn=mean_fn)

    init_state = model.init(init_key, x_observed)
    stp = model.apply(init_state, x_observed, mutable=False)
    lp = stp.log_prob(y_observed.padded_array[:, 0])
    self.assertTrue(np.isfinite(lp))

    # The mean of the GP should equal `mean_fn` evaluated at the index points.
    np.testing.assert_allclose(
        stp.mean(),
        mean_fn.apply({'params': init_state['params']['mean_fn']}, x_observed))

    _, pp_state = model.apply({'params': init_state['params']},
                              x_observed,
                              y_observed,
                              method=model.precompute_predictive,
                              mutable=('predictive',))

    state = {'params': init_state['params'], **pp_state}
    pp_dist = model.apply(
        state,
        x_predictive,
        x_observed,
        y_observed,
        method=model.posterior_predictive,
    )
    pp_mean = pp_dist.mean()

    self.assertTrue(
        np.isfinite(
            pp_dist.log_prob(x_predictive.continuous.padded_array.sum(axis=-1))
        ).all()
    )
    self.assertSequenceEqual(pp_mean.shape, (50,))
    self.assertTrue(np.isfinite(pp_mean).all())

    # Test that vmap works.
    def log_prob(p):
      return model.apply(p, x_observed, mutable=('losses',))[0].log_prob(
          y_observed.padded_array
      )

    batch_size = 20
    keys = random.split(vmap_key, num=batch_size)
    params = jax.vmap(lambda k: model.init(k, x_observed))(keys)
    lp = jax.vmap(log_prob)(params)
    self.assertLen(lp, batch_size)
    self.assertTrue(np.isfinite(lp).all())


class VectorToArrayTreeTest(absltest.TestCase):

  def test_vector_to_array_tree(self):
    k0, k1 = random.split(random.PRNGKey(0))
    params = {
        'foo': random.uniform(k0, shape=(2, 3)),
        'bar': np.array(0.3),
        'baz': random.uniform(k1, shape=(6,))
    }
    bijector = sp_model.VectorToArrayTree(params)
    self.assertEqual(bijector.inverse(params).shape, (13,))

    v = np.ones([13])
    struct = bijector.forward(v)
    self.assertSameElements(list(struct.keys()), list(params.keys()))
    for k in params:
      self.assertEqual(struct[k].shape, params[k].shape)


class ModelParameterTest(absltest.TestCase):

  def test_parameter_from_prior(self):
    prior = tfd.Gamma(concentration=1., rate=1.)
    param = sp_model.ModelParameter.from_prior(prior=prior)
    samples = param.init_fn(random.PRNGKey(0))
    regularization = param.regularizer(samples)
    x = np.linspace(-10.0, 10.0, 20)
    self.assertTrue(np.isfinite(regularization))
    self.assertEmpty(regularization.shape)
    self.assertNotEqual(regularization, 0.)
    self.assertTrue((param.constraint.bijector(x) > 0.0).all())


class ConstraintTest(parameterized.TestCase):

  @parameterized.parameters(
      (-1.0, None),
      (0.0, 10.0),
      (None, 1.0),
      ({'a': -1.0, 'b': 0.0}, {'a': 1.0, 'b': 10.0}),
      ({'a': None, 'b': 0.0}, {'a': 1.0, 'b': None}),
      (None, {'a': 1.0, 'b': 2.0}),
  )
  def test_create_constraint(self, lower, upper):
    make_array = lambda x: None if x is None else jnp.array(x)
    lower = jax.tree_util.tree_map(make_array, lower)
    upper = jax.tree_util.tree_map(make_array, upper)
    constraint = sp_model.Constraint.create(
        bounds=(lower, upper), bijector_fn=tfb.SoftClip
    )
    x_part = jnp.linspace(-5.0, 5.0, 10)
    x = tree.map_structure(lambda _: x_part, lower or upper)
    y = constraint.bijector(x)
    for y_, b_ in zip(tree.flatten(y), tree.flatten(lower)):
      if b_ is not None:
        self.assertTrue((y_ > b_).all())
    for y_, b_ in zip(tree.flatten(y), tree.flatten(upper)):
      if b_ is not None:
        self.assertTrue((y_ < b_).all())

  @parameterized.parameters((None,), (MeanFn,))
  def test_get_constraints(self, mean_fn):
    if mean_fn:
      mean_fn = mean_fn()

    x, *_ = _make_inputs(jax.random.PRNGKey(0))
    model = sp_model.StochasticProcessModel(
        coroutine=_test_coroutine, mean_fn=mean_fn
    )

    constraint = sp_model.get_constraints(model, x=x)
    unconstrained_p = model.lazy_init(jax.random.PRNGKey(0), x)['params']
    p = constraint.bijector(unconstrained_p)
    lower, upper = constraint.bounds

    tree.assert_same_structure(p, unconstrained_p)
    for y_, b_ in zip(tree.flatten(p), tree.flatten(lower)):
      if b_ is not None:
        self.assertTrue((y_ >= b_).all())
    for y_, b_ in zip(tree.flatten(p), tree.flatten(upper)):
      if b_ is not None:
        self.assertTrue((y_ <= b_).all())


@functools.lru_cache(maxsize=None)
def _test_data() -> tuple[types.ModelData, types.ModelInput]:
  x_observed, y_observed, x_predictive = _make_inputs(
      jax.random.PRNGKey(0),
      num_continuous=5,
      num_categorical=2,
      num_observed=13,
      num_predictive=7,
      pad_continuous_dim=2,
      pad_categorical_dim=3,
  )
  return types.ModelData(features=x_observed, labels=y_observed), x_predictive


class EquinoxModulesTest(absltest.TestCase):

  def testCoroutineWithData(self):
    data = _test_data()[0]
    data = types.ModelData(
        features=types.ModelInput(
            data.features.continuous.replace_fill_value(0.0),
            data.features.categorical.replace_fill_value(0),
        ),
        labels=data.labels,
    )
    wrapper = sp_model.CoroutineWithData(_test_coroutine, data)

    # Test setup and loss
    params = wrapper.setup(jax.random.PRNGKey(0))
    self.assertTrue(np.all(np.isfinite(wrapper.loss_with_aux(params)[0])))

    # Test constraint
    constraint = wrapper.constraints()
    p = constraint.bijector(params)
    lower, upper = constraint.bounds

    tree.assert_same_structure(p, params)
    for y_, b_ in zip(tree.flatten(p), tree.flatten(lower)):
      if b_ is not None:
        self.assertTrue((y_ >= b_).all())
    for y_, b_ in zip(tree.flatten(p), tree.flatten(upper)):
      if b_ is not None:
        self.assertTrue((y_ <= b_).all())

  def testStochasticProcessWithCoroutine(self):
    model = sp_model.StochasticProcessWithCoroutine.initialize(
        _test_coroutine, rng=jax.random.PRNGKey(0)
    )
    dist, _ = model.call_with_aux(_test_data()[0].features)
    self.assertSequenceEqual(dist.event_shape, (13,))
    self.assertSequenceEqual(dist.batch_shape, tuple())

    data = _test_data()[0]
    data = types.ModelData(
        features=types.ModelInput(
            data.features.continuous.replace_fill_value(0.0),
            data.features.categorical.replace_fill_value(0),
        ),
        labels=data.labels,
    )
    loss, aux = model.loss_with_aux(data)
    logging.info('%s, %s', loss, aux)
    logging.info('model:%s', model.params)
    self.assertTrue(np.isfinite(loss))

  def testPrecomputePredictive(self):
    model = sp_model.StochasticProcessWithCoroutine.initialize(
        _test_coroutine, rng=jax.random.PRNGKey(0)
    )
    predictive = model.precompute_predictive(_test_data()[0])
    dist = predictive.predict(_test_data()[1])
    self.assertSequenceEqual(dist.event_shape, (7,))
    self.assertSequenceEqual(dist.batch_shape, tuple())

    x_obs_empty, y_obs_empty, y_pred = _make_inputs(
        key=jax.random.PRNGKey(0), num_observed=0, num_predictive=7
    )
    predictive_empty_obs = model.precompute_predictive(
        types.ModelData(x_obs_empty, y_obs_empty)
    )
    dist = predictive_empty_obs.predict(y_pred)
    self.assertSequenceEqual(dist.event_shape, (7,))
    self.assertSequenceEqual(dist.batch_shape, tuple())


class UniformEnsemblePrecomputePredictiveTest(parameterized.TestCase):

  def test_no_batch_shape(self):
    model = sp_model.StochasticProcessWithCoroutine.initialize(
        _test_coroutine, rng=jax.random.PRNGKey(0)
    )
    predictive = model.precompute_predictive(_test_data()[0])
    dist = predictive.predict(_test_data()[1])
    self.assertSequenceEqual(dist.event_shape, (7,))
    self.assertSequenceEqual(dist.batch_shape, tuple())

  @parameterized.parameters(dict(n=5), dict(n=1))
  def test_batch_shape_n(self, n):
    model = jax.vmap(
        eqx.Partial(
            sp_model.StochasticProcessWithCoroutine.initialize,
            coroutine=_test_coroutine,
        )
    )(rng=jax.random.split(jax.random.PRNGKey(0), n))

    predictive = model.precompute_predictive(_test_data()[0])
    dist = predictive.predict(_test_data()[1])
    self.assertSequenceEqual(dist.event_shape, (7,))
    self.assertSequenceEqual(dist.batch_shape, (n,))

    # Put it in the UniformEnsemblePredictive and the batch dimension is gone.
    ensemble = sp_model.UniformEnsemblePredictive(predictive)
    dist = ensemble.predict(_test_data()[1])
    self.assertSequenceEqual(dist.event_shape, (7,))
    self.assertSequenceEqual(dist.batch_shape, tuple())


if __name__ == '__main__':
  config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/_src/jax/types.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Types library for vizier/_src/jax."""

from typing import Any, Generic, Iterable, Mapping, Optional, Sequence, TypeVar, Union
import equinox as eqx
from flax import struct
from flax.core import scope as flax_scope
import jax
from jax import numpy as jnp
from jax.typing import ArrayLike
import jaxtyping as jt
import numpy as np

# Integer dtype for categorical data.
INT_DTYPE = np.int32


# We define our own Array type since `jax.typing.Array` and `chex.Array` both
# include scalar types, which result in type errors when array
# methods/properties like `.shape` are accessed.
Array = Union[np.ndarray, jax.Array]


# TODO: Add a `concatenate` method.
class PaddedArray(eqx.Module):
  """Padded Array as a pytree.

  There is no validation done in `__init__`. In order to get a validated
  instance, Use a converter object or a factory method outside the jit scope.

  Attributes:
    padded_array:
    _original_shape: 1D array of length equal to `padded_array.ndims`. The k-th
      element must be less than or equal to `padded_array.shape[k]`.
    _nopadding_done: Must be set to True iff `_original_shape` is the same as
      padded_array.shape.
    _mask: Same shape as padded_array. True where padded_array's value is the
      original array value as opposed to the fill value.
  """

  # TODO: Rename "padded_array" to a shorter name like "padded".
  padded_array: jt.Shaped[jax.Array, '...'] = eqx.field(converter=jnp.asarray)
  fill_value: Any = eqx.field(converter=jnp.asarray)
  # TODO: Make `_original_shape` public.
  _original_shape: jt.Int[jax.Array, '_'] = eqx.field(converter=jnp.asarray)

  # TODO: Make _mask an inferred property. The current
  # implementation is not memory efficient.
  _mask: jt.Bool[jax.Array, '...'] = eqx.field(converter=jnp.asarray)
  _nopadding_done: bool = eqx.field(static=True, default=False, kw_only=True)

  @property
  def shape(self) -> tuple[int, ...]:
    """Returns the shape of the padded array."""
    return self.padded_array.shape

  def replace_array(self, array: jt.Shaped[jax.Array, '...']) -> 'PaddedArray':
    """Replaces the original array values, maintaining the padding."""
    return PaddedArray.from_array(array, self.shape, fill_value=self.fill_value)

  def replace_fill_value(self, fill_value: Any) -> 'PaddedArray':
    """Replaces the padded fill values."""
    # TODO: Consider optimizing when fill_value == self.fill_value.
    if self._nopadding_done:
      return self
    else:
      return PaddedArray(
          jnp.where(self._mask, self.padded_array, fill_value),
          fill_value=fill_value,
          _original_shape=self._original_shape,
          _mask=self._mask,
          _nopadding_done=self._nopadding_done,
      )

  @classmethod
  def as_padded(cls, array: jt.Shaped[jax.Array, '...']) -> 'PaddedArray':
    """Converts a regular array as PaddedArray type, with no actual padding.

    NOTE This is implemented as a separate method instead of setting default
    for `fill_value=np.nan` in `from_array` method. `jnp.pad`` automatically
    casts `nan` to `0` for integer arrays, which can cause unexpected
    behavior, and we want people to always explicitly set the fill value
    unless they know for sure that padding would not occur.

    Args:
      array:

    Returns:
      PaddedArray.
    """
    return PaddedArray.from_array(array, array.shape, fill_value=np.nan)

  @classmethod
  def from_array(
      cls,
      array: jt.Shaped[jax.Array, '...'],
      target_shape: Sequence[int],
      *,
      fill_value: Any
  ) -> 'PaddedArray':
    """Factory function to guarantee a self-consistent creation."""
    spec = [(0, dnew - d) for d, dnew in zip(array.shape, target_shape)]
    mask_array = jnp.pad(
        jnp.ones_like(array, dtype=bool), spec, constant_values=False
    )
    new_array = jnp.pad(array, spec, constant_values=fill_value)
    return PaddedArray(
        padded_array=new_array,
        fill_value=fill_value,
        _original_shape=array.shape,
        _mask=mask_array,
        _nopadding_done=array.shape == target_shape,
    )

  @property
  def is_missing(self) -> tuple[jt.Bool[jax.Array, '...']]:
    """Mask per dimension padded. Arrays have shape [N1], [N2], ..., [Nk]."""
    return tuple(
        jnp.arange(s1) >= s2
        for s1, s2 in zip(self.padded_array.shape, self._original_shape)
    )

  def unpad(self) -> jt.Shaped[jax.Array, '...']:
    """Can be used in jit scope only if original shape == padded shape."""
    if self._nopadding_done:
      return self.padded_array
    return jax.lax.slice(
        self.padded_array,
        [0] * self.padded_array.ndim,
        self._original_shape.tolist(),
    )


ArrayTree = Union[ArrayLike, Iterable['ArrayTree'], Mapping[Any, 'ArrayTree']]

# An ArrayTree that allows None values.
ArrayTreeOptional = Union[
    ArrayLike,
    Iterable[Optional['ArrayTreeOptional']],
    Mapping[Any, Optional['ArrayTreeOptional']],
]
ParameterDict = flax_scope.Collection
ModelState = flax_scope.VariableDict


_T = TypeVar('_T')


@struct.dataclass
class ContinuousAndCategorical(Generic[_T]):
  continuous: _T
  categorical: _T


# TODO: Make it a private type inside jnp_converters.py.
ContinuousAndCategoricalArray = ContinuousAndCategorical[Array]

ModelInput = ContinuousAndCategorical[PaddedArray]


class ModelData(eqx.Module):
  features: ModelInput
  labels: PaddedArray


# Tuple representing a box constraint of the form (lower, upper) bounds.
Bounds = tuple[Optional[ArrayTreeOptional], Optional[ArrayTreeOptional]]


# TODO: Deprecate it in favor of
# PrecomputedPredictive for full predictive state including cholesky, and
# StochasticProcessWithCoroutine for computing log probs.
@struct.dataclass
class GPState:
  """State that changes at each iteration."""

  data: ModelData
  model_state: ModelState


--- vizier/_src/jax/types_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from absl.testing import parameterized
import equinox as eqx
import jax
from jax import numpy as jnp
import numpy as np
from vizier._src.jax import types as vt
from absl.testing import absltest


# pylint: disable=g-long-lambda
class PaddedArrayTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(
          factory=lambda arr: vt.PaddedArray.from_array(
              arr, arr.shape, fill_value=-1
          )
      ),
      dict(factory=vt.PaddedArray.as_padded),
  )
  def test_padding_to_the_same_shape(self, factory) -> None:
    """Tests the case where no padding dimensions are added.

    Args:
      factory: Takes an array and returns a padded array.
    """
    arr = jnp.ones([3, 2], dtype=jnp.int32)
    parr = factory(arr)
    # The padded shape matches the original shape, which means we can unpad
    # within a jit scope.
    self.assertSequenceEqual(
        eqx.filter_jit(parr.unpad)().shape,
        arr.shape,
    )

    np.testing.assert_array_equal(parr.padded_array, arr)
    np.testing.assert_array_equal(parr.is_missing[0], [False, False, False])
    np.testing.assert_array_equal(parr.is_missing[1], [False, False])
    np.testing.assert_array_equal(
        parr.replace_fill_value(-999).padded_array == -999, ~parr._mask
    )

  @parameterized.parameters(dict(fill_value=0), dict(fill_value=-1))
  def test_padding_to_a_different_shape(self, fill_value):
    arr = jnp.ones([3, 2], dtype=jnp.int32)
    # If the padded shape does not match the original shape, then we can't unpad
    # within a jit scope.
    target_shape = (5, 3)
    parr = vt.PaddedArray.from_array(arr, target_shape, fill_value=fill_value)
    with self.assertRaises(jax.errors.ConcretizationTypeError):
      eqx.filter_jit(parr.unpad)()

    expected = np.asarray(
        [
            [1, 1, 0],
            [1, 1, 0],
            [1, 1, 0],
            [0, 0, 0],
            [0, 0, 0],
        ],
        dtype=arr.dtype,
    )
    expected[expected == 0] = fill_value
    np.testing.assert_array_equal(parr.padded_array, expected)
    np.testing.assert_array_equal(parr.unpad(), arr)
    np.testing.assert_array_equal(
        parr.is_missing[0], [False, False, False, True, True]
    )
    np.testing.assert_array_equal(parr.is_missing[1], [False, False, True])
    np.testing.assert_array_equal(
        parr.replace_fill_value(-999).padded_array == -999, ~parr._mask
    )


if __name__ == "__main__":
  absltest.main()


--- vizier/_src/jax/xla_pareto.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Fast pareto frontier computation using Jax."""
import functools

import jax
from jax import numpy as jnp
import jaxtyping as jt
import numpy as np
from vizier._src.pyvizier.multimetric import pareto_optimal


def _is_dominated(
    y1: jt.Float[jt.Array, "M"],
    y2: jt.Float[jt.Array, "M"],
    strict: bool = True,
) -> jt.Bool[jt.Array, ""]:
  """True if y2 > y1 (or y2 >= y1 if strict is False) every coordinate."""
  dominated_or_equal = jnp.all(y1 <= y2)
  if strict:
    return dominated_or_equal & jnp.any(y2 > y1)
  else:
    return dominated_or_equal


@functools.partial(jax.jit, static_argnames="strict")
def _is_pareto_optimal_against(
    yy: jt.Float[jt.Array, "B1 M"],
    baseline: jt.Float[jt.Array, "B2 M"],
    *,
    strict: bool,
) -> jt.Bool[jt.Array, "B1"]:
  """Computes if nothing in `baseline` dominates `yy`.

  Args:
    yy: array of shape [B1, M] where M is number of metrics.
    baseline: array of shape [B2, M] where M is number of metrics.
    strict: If true, strict dominance is used.

  Returns:
    Boolean array of shape [B1]
  """
  jax_dominated_mv = jax.vmap(
      functools.partial(_is_dominated, strict=strict), (None, 0), 0
  )  #  ([b,a], [a]) -> [b]
  jax_dominated_mm = jax.vmap(jax_dominated_mv, (0, None),
                              0)  #  ([b,a], [c,a]) -> [b,c]
  return jnp.logical_not(jnp.any(jax_dominated_mm(yy, baseline),
                                 axis=-1))  # N_y, N_b -> N_y


def is_frontier(
    ys: jt.Float[jt.ArrayLike, "B M"],
    *,
    num_shards: int = 10,
    verbose: bool = False,
) -> jt.Bool[jt.ArrayLike, "B"]:
  """Efficiently compute `_is_pareto_optimal_against(ys, ys, strict=True)`.

  Divide `ys` into shards and gradually trim down the candidates.

  Args:
    ys: Array of shape [B, M] where M is number of metrics.
    num_shards: Each sharding results in filtering, i.e. indexing the array with
      boolean vector. This operation can be very expensive and dominate the cost
      of computation. Use a moderate number sublinear in B, e.g. log(B).
    verbose:

  Returns:
    Boolean numpy Array of shape [B].
  """
  idx = np.linspace(0, ys.shape[0], num_shards).astype(np.int32)
  idx = list(reversed(idx))
  # Initialize candidates with all points
  frontier = np.ones(ys.shape[0], dtype=np.bool_)

  for begin, end in zip(idx[1:], idx[:-1]):
    candidates = ys[frontier]
    # Filter candidates by comparing against the slice.
    if verbose:
      print(f"Compare {len(candidates)} against {begin}:{end}.")
    tt = _is_pareto_optimal_against(candidates, ys[begin:end], strict=True)
    frontier[frontier] = tt
  return frontier


class JaxParetoOptimalAlgorithm(pareto_optimal.BaseParetoOptimalAlgorithm):
  """Jax-based functions for calculating pareto frontiers."""

  def is_pareto_optimal(self, points: np.ndarray) -> np.ndarray:
    """Jax-enabled Pareto frontier algorithm. See base class."""
    return np.array(is_frontier(points), dtype=bool)

  def is_pareto_optimal_against(self, points: np.ndarray,
                                dominating_points: np.ndarray,
                                strict: bool) -> np.ndarray:
    """Jax-enabled optimality/domination algorithm. See base class."""
    return np.array(
        _is_pareto_optimal_against(points, dominating_points, strict=strict),
        dtype=bool,
    )


def get_frontier(
    ys: jt.Float[jt.ArrayLike, "B M"],
    *,
    num_shards: int = 10,
    verbose: bool = True,
) -> jnp.ndarray:
  """Efficiently compute `ys[_is_pareto_optimal_against(ys, ys, strict=True)]` using iterative filtering.

  Divide `ys` into shards and gradually trim down the candidates.
  `get_frontier` doesn't call `is_frontier`, because `get_frontier` runs faster
  by not slicing the full `ys` every iteration.

  Args:
    ys: Array of shape [B, M] where M is number of metrics.
    num_shards: Each sharding results in filtering, i.e. indexing the array with
      boolean vector. This operation can be very expensive and dominate the cost
      of computation. Use a moderate number sublinear in B, e.g. log(B).
    verbose:

  Returns:
    Array of shape [B, M].
  """
  idx = np.linspace(0, ys.shape[0], num_shards).astype(np.int32)
  idx = list(reversed(idx))
  # Initialize candidates with all points
  candidates = jnp.asarray(ys)
  for begin, end in zip(idx[1:], idx[:-1]):
    # Filter candidates by comparing against the slice.
    if verbose:
      # Use print. This method won't run in production anyways.
      print(f"Compare {len(candidates)} against {begin}:{end}.")
    tt = _is_pareto_optimal_against(candidates, ys[begin:end], strict=True)
    candidates = candidates[tt]
  return candidates


@jax.jit
def pareto_rank(ys: jt.Float[jt.ArrayLike, "B M"]) -> jt.Int[jt.ArrayLike, "B"]:
  """Returns the pareto rank."""
  jax_dominated_mv = jax.vmap(
      functools.partial(_is_dominated, strict=True), (None, 0), 0
  )  #  ([b,a], [a]) -> [b]
  jax_dominated_mm = jax.vmap(
      jax_dominated_mv, (0, None), 0
  )  #  ([b,a], [c,a]) -> [b,c]
  domination_matrix = jax_dominated_mm(ys, ys)
  return jnp.sum(domination_matrix, axis=1)


def _cum_hypervolume_origin(
    points: jt.Float[jt.ArrayLike, "B M"], vector: jt.Float[jt.Array, "... M"]
) -> jt.Float[jt.Array, "B"]:
  """Returns a randomized approximation of the cumulative dominated hypervolume.

  See Section 3, Lemma 5 of https://arxiv.org/pdf/2006.04655.pdf for a fuller
  explanation of the technique. This assumes the reference point is the
  origin.

  NOTE: This returns an unnormalized hypervolume.

  Args:
    points: Any set of points with shape (num_points, dimension).
    vector: A vector of length dimension.

  Returns:
    Approximated cumulative dominated hypervolume of points[:i]. Length is
    num_points.
  """
  ratios = points / vector
  coordinate_min_ratio = jnp.min(ratios, axis=1)
  return jax.lax.cummax(coordinate_min_ratio, axis=0)**len(vector)


@jax.jit
def jax_cum_hypervolume_origin(
    points: jt.Float[jt.ArrayLike, "B M"], vectors: jt.Float[jt.Array, "B2 M"]
) -> jt.Float[jt.Array, "B B2"]:
  #  ([B,M], [B2,M]) -> [B,B2]
  cum_hypervolume_mm = jax.vmap(_cum_hypervolume_origin, (None, 0), 0)
  return jnp.mean(cum_hypervolume_mm(points, vectors), axis=0)


--- vizier/_src/jax/xla_pareto_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for xla_pareto."""

import numpy as np
from vizier._src.jax import xla_pareto
from vizier._src.pyvizier.multimetric import hypervolume

from absl.testing import absltest


class XlaParetoTest(absltest.TestCase):

  def test_get_pareto(self):
    ys = np.random.random([20000, 2])
    is_frontier = xla_pareto.is_frontier(ys, num_shards=5)
    frontier = xla_pareto.get_frontier(ys, num_shards=5)

    # Two implementations should be consistent.
    self.assertEqual(frontier.shape[0], np.sum(is_frontier))

    # Sort by one of the dimensions and the other dimension must be sorted
    # in the reverse order.
    sid = frontier[:, 0].argsort()

    np.testing.assert_array_equal(frontier[sid, 1].argsort(),
                                  np.arange(len(frontier)-1, -1, -1))

  def test_get_pareto_rank(self):
    ys = np.array([[6, 6], [4, 8], [2, 6], [3, 6], [1, 8], [7, 0], [4, 4],
                   [9, 6], [5, 10], [8, 10]])
    np.testing.assert_array_equal(
        xla_pareto.pareto_rank(ys), np.array([2, 2, 6, 5, 3, 2, 5, 0, 1, 0]))

  def test_cum_hypervolume_origin(self):
    dim = 3
    vectors = abs(
        np.array(np.random.uniform(size=(100, dim)), dtype=np.float32))
    points = abs(
        np.array(np.random.uniform(size=(1000, dim)), dtype=np.float32))

    np.testing.assert_array_almost_equal(
        hypervolume._cum_hypervolume_origin(points, vectors),
        xla_pareto.jax_cum_hypervolume_origin(points, vectors),
        decimal=2)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/algorithms.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Classes for specifying Vizier algorithms."""
import pyglove as pg


class PseudoAlgorithm(pg.DNAGenerator):
  """Base class for algorithms that delegate actual logics to others."""

  def _setup(self) -> None:
    raise RuntimeError(f'{self!r} should be used with \'vizier\' backend.')

  def _propose(self) -> pg.DNA:
    self._should_never_trigger('propose')
    return pg.DNA(0)

  def _feedback(self, unused_dna: pg.DNA, unused_reward: float) -> None:
    self._should_never_trigger('feedback')

  def _should_never_trigger(self, method_name):
    raise RuntimeError(
        f'`{method_name}` of pseudo algorithm {self!r} should never be called.')


@pg.members(
    [(
        'name',
        pg.typing.Enum(
            'DEFAULT',
            [
                'DEFAULT',
                # Algorithms shared across Vizier platforms.
                'GRID_SEARCH',
                'RANDOM_SEARCH',
                # Google-Vizier only algorithms.
                # TODO: copybara away these algorithms for OSS.
                'CMAES',
                'GP_BANDIT',
                'LINEAR_COMBINATION_SEARCH',
                'HYPER_LCS',
                # OSS-Vizier only algorithms:
                'BOCS',
                'CMA_ES',
                'HARMONICA',
                'NSGA2',
                'QUASI_RANDOM_SEARCH',
            ]),
        'Name of Vizier predefined algorithm.')],
    metadata={'init_arg_list': ['name']})  # pylint: disable=bad-continuation
class BuiltinAlgorithm(PseudoAlgorithm):
  """Vizier built-in algorithm."""

  @property
  def name(self) -> str:
    return self.sym_init_args.name

  @property
  def multi_objective(self) -> bool:
    return self.name in ('DEFAULT', 'GP_BANDIT', 'LINEAR_COMBINATION_SEARCH',
                         'RANDOM_SEARCH')


# TODO: implement the details according to Vizier early stopping policy
# later. Move to stopping.py
class BuiltinEarlyStoppingPolicy(pg.tuning.EarlyStoppingPolicy):
  """Vizier built-in early stopping policy."""


--- vizier/_src/pyglove/backend.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Cross-Platform Backend."""
import enum
import functools
import getpass
import random
import threading
import time
from typing import Any, Dict, Optional, Sequence, Type, Union

from absl import logging
import attrs
import pyglove as pg
from vizier import pyvizier as vz
from vizier._src.pyglove import algorithms
from vizier._src.pyglove import client
from vizier._src.pyglove import constants
from vizier._src.pyglove import converters
from vizier._src.pyglove import core
from vizier._src.pyglove import pythia as pyglove_pythia
from vizier.client import client_abc


TunerPolicy = pyglove_pythia.TunerPolicy
BuiltinAlgorithm = algorithms.BuiltinAlgorithm

ExpandedStudyName = client.ExpandedStudyName
StudyKey = client.StudyKey


class TunerMode(enum.Enum):
  """Mode for Tuner."""

  # Automatic select primary mode when study is new or study TUNER_ID equals
  # to current tuner ID (for failover) or the primary tuner is not accessible
  # via its BNS. Otherwise select secondary mode.
  AUTO = 0

  # Work as primary tuner, which will host pythia service when algorithm is
  # non-Vizier-builtin. When using Vizier built-in algorithms, all tuners are
  # in secondary mode.
  PRIMARY = 1

  # Work as secondary tuner, which can query or stop a tuner task but not
  # hosting pythia service.
  SECONDARY = 2


# Global policy cache.
_global_policy_cache: Dict[StudyKey, TunerPolicy] = dict()


@attrs.define(auto_attribs=False)
class VizierBackend(pg.tuning.Backend):
  """Vizier backend."""

  # Class-level variables.
  default_owner: str = getpass.getuser()
  default_study_prefix: Optional[str] = None
  tuner_cls: Type[client.VizierTuner] = attrs.field()

  # Instance-level variables.

  #
  # Flags passed from `pg.sample`:
  #

  # Worker group - workers that belong to the same group will share the same
  # Vizier client ID.
  _group: Union[None, int, str] = attrs.field()

  # Max number of examples to sample.
  _num_examples: Optional[int] = attrs.field()

  # Prior study IDs for transfer learning.
  _prior_study_ids: Optional[Sequence[str]] = attrs.field()

  # If True, add the completed trials from prior studies. Otherwise, simply
  # warm up the algorithm using these trials without adding them to the current
  # study.
  _add_prior_trials: bool = attrs.field()

  #
  # Internal states.
  #

  _tuner: client.VizierTuner
  _dna_spec: pg.DNASpec
  _algorithm: pg.geno.DNAGenerator = attrs.field()
  _early_stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = (
      attrs.field()
  )

  _study_owner: str = attrs.field()
  _study_name: ExpandedStudyName = attrs.field()
  _converter: converters.VizierConverter = attrs.field()

  _study: client_abc.StudyInterface = attrs.field()
  _suggestion_generator: Any = attrs.field()

  _run_mode: TunerMode = attrs.field()
  _auto_election_thread: Optional[threading.Thread] = attrs.field()
  _is_active: bool = attrs.field()

  def __init__(
      self,
      name: Optional[str],
      group: Union[None, int, str],
      dna_spec: pg.DNASpec,
      algorithm: pg.DNAGenerator,
      metrics_to_optimize: Sequence[str],
      early_stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = None,
      num_examples: Optional[int] = None,
      study_owner: Optional[str] = None,
      prior_study_ids: Optional[Sequence[str]] = None,
      add_prior_trials: bool = False,
      is_chief: Optional[bool] = None,
  ):
    self._tuner = self.tuner_cls()

    self._dna_spec = dna_spec
    self._algorithm = algorithm
    self._early_stopping_policy = early_stopping_policy

    self._group = group
    self._num_examples = num_examples
    self._prior_study_ids = prior_study_ids or tuple()
    self._add_prior_trials = add_prior_trials

    self._study_owner = study_owner or self.default_owner
    self._study_name = self._expand_name(name)

    # Set up converter based on the search space and metrics to optimize.
    self._converter = converters.VizierConverter.from_dna_spec(
        dna_spec, metrics_to_optimize
    )

    # Detecting mode based on is_chef flag.
    if is_chief:
      mode = TunerMode.PRIMARY
    elif is_chief is None:
      mode = TunerMode.AUTO
    else:
      mode = TunerMode.SECONDARY

    self._run_mode = mode

    # Setup the Vizier study and the suggestion generator.
    is_chief = self._setup_study()
    self._suggestion_generator = self._create_suggestion_generator()

    # Start Pythia if needed.
    self._auto_election_thread = None
    self._is_active = True

    if self._need_pythia_service:
      pg.logging.info(
          "Study '%s/%s' will be served by Pythia service hosted on %r. "
          'Algorithm=%r, EarlyStoppingPolicy=%r.',
          self._study_owner,
          self._study_name,
          self._get_chief_tuner_id(),
          self._algorithm,
          self._early_stopping_policy,
      )

      if is_chief:
        pg.logging.info(
            "Starting hosting Pythia service for study '%s/%s' as PRIMARY. ",
            self._study_owner,
            self._study_name,
        )
        self._start_pythia()
      if self._run_mode == TunerMode.AUTO:
        self._auto_election_thread = threading.Thread(
            target=self._auto_elect_primary_if_needed
        )
        self._auto_election_thread.start()
    else:
      pg.logging.info(
          'Pythia service is not required as both the search algorithm and '
          'early stopping policy are either Vizier built-in or served from '
          'remote Pythia endpoint. Algorithm=%r, EarlyStoppingPolicy=%r.',
          self._algorithm,
          self._early_stopping_policy,
      )

  @property
  def _host_pythia_algorithm(self) -> bool:
    """Returns True if the algorithm is hosted on Pythia."""
    return not isinstance(self._algorithm, algorithms.PseudoAlgorithm)

  @property
  def _need_pythia_service(self) -> bool:
    """Returns True if pythia service is needed."""
    return self._host_pythia_algorithm

  def _setup_study(self) -> bool:
    """Sets up Vizier study, returns True if current worker is the chief."""
    study_descriptor = f'{self._study_owner}/{self._study_name}'

    try:
      self._study = self._tuner.load_study(self._study_owner, self._study_name)
      pg.logging.info(
          'Connecting tuner to existing study %r... ', study_descriptor
      )

      #
      # Ensure the client-side search space matches with the server-side search
      # space.
      #
      stored_dna_spec = self._get_stored_dna_spec()

      # CustomDecisionPoint could be non-serializable and non-compariable.
      if pg.contains(self._dna_spec, pg.geno.CustomDecisionPoint):
        pg.logging.info(
            'There is a CustomDecisionPoint. Skipping the check to ensure the '
            'client-side search space matches the server-side search space.'
        )
      elif pg.eq(self._dna_spec, stored_dna_spec):
        pg.logging.info(
            'Verified that the client-side search space matches the '
            'server-side.'
        )
      else:
        raise ValueError(
            'The client-side search space is different from the search space '
            'from the study stored in Vizier. Try launching the experiment '
            'with a different study name. '
            f'Diff: {pg.diff(stored_dna_spec, self._dna_spec)}.'
        )

      chief_tuner_id = self._get_chief_tuner_id()

      if self._run_mode != TunerMode.SECONDARY and not self._tuner.ping_tuner(
          chief_tuner_id
      ):
        # NOTE(daiyip): there could be a race condition that multiple workers
        # elect themselves as the new primary, and all of them regard
        # themselves as the elected. When this happens, multiple workers
        # may be hosting the Pythia service for this study. However, the study
        # will use one address (BNS of the latest elected worker) as the
        # Pythia endpoint. Besides, all states are stored in the study,
        # restart of workers will pick up these states from the study.
        # This is done by replaying existing trials to recompute the states of
        # the search algorithm. Therefore, it does not matter which worker is
        # chosen to serve the study, which should work equally well.
        # On the other hand, it is cheap to serve a PythiaService without
        # incoming queries. Therefore, we do not handle this race conditions
        # with expensive distributed locks.
        chief_tuner_id = self._register_self_as_primary()

      is_chief = chief_tuner_id == self._tuner_id
      if self._run_mode == TunerMode.SECONDARY and is_chief:
        raise ValueError(
            f'Inconsistent primary tuner: {self._tuner_id!r} is running as '
            f'secondary but study {study_descriptor!r} indicates otherwise.'
        )
      elif self._run_mode == TunerMode.PRIMARY and not is_chief:
        raise ValueError(
            f'Inconsistent tuner mode: {self._tuner_id!r} is running in '
            f'PRIMARY mode but study {study_descriptor!r} already has a '
            f'different primary {chief_tuner_id!r}.\n'
            'Please check if you are launching different experiments using '
            'the same Vizier study.'
        )

      pg.logging.info(
          'Tuner is running in %s mode with existing study %r.',
          'PRIMARY' if is_chief else 'SECONDARY',
          study_descriptor,
      )

      return is_chief
    except client_abc.ResourceNotFoundError:
      # Study does not exist.
      if self._run_mode == TunerMode.SECONDARY:
        pg.logging.info(
            'Start tuner as secondary. Waiting for study %r to be created...',
            study_descriptor,
        )
        self._study = self._wait_for_study(self._study_owner, self._study_name)
      else:
        mode_str = 'PRIMARY' if self._run_mode == TunerMode.PRIMARY else 'AUTO'
        pg.logging.info(
            'Start tuner in %s mode. Attempting to create study %r...',
            mode_str,
            study_descriptor,
        )

        # Create a new study.
        problem = self._converter.problem_or_dummy
        problem.metadata.ns(constants.METADATA_NAMESPACE)[
            constants.STUDY_METADATA_KEY_TUNER_ID
        ] = self._tuner_id
        self._study = self._tuner.create_study(
            problem,
            self._converter,
            self._study_owner,
            self._study_name,
            self._algorithm,
            self._early_stopping_policy,
        )

      # Perform post-study actions.
      is_chief = self._tuner_id == self._get_chief_tuner_id()
      if is_chief:
        self._on_study_created()

      pg.logging.info(
          'Tuner is running in %s mode with newly created study %r.',
          'PRIMARY' if is_chief else 'SECONDARY',
          study_descriptor,
      )
      return is_chief

  def _on_study_created(self):
    # For multi-thread scenario, `local_tuner_id` will be the same for
    # all the worker threads, therefore there is a chance that multiple
    # worker threads consider themselves as PRIMARY. This does not matter
    # since there is only one Pythia service shared across them.
    if self._add_prior_trials:
      # Trials are added to the study directly upon creation.
      trials: Sequence[vz.Trial] = self._load_prior_trials()
      for trial in trials:
        self._study.add_trial(trial)

  def _create_suggestion_generator(self):
    """Creates a suggestion generator."""
    while (
        self._num_examples is None
        or len(list(self._study.trials())) < self._num_examples
    ):
      trials = self._study.suggest(
          count=1, client_id=self._tuner.get_group_id(self._group)
      )
      if not trials:
        return
      for trial in trials:
        yield core.Feedback(trial, self._converter)

  def _start_pythia(self) -> None:
    """Starts Pythia service."""
    # Start pythia service.
    self._tuner.start_pythia_service(_global_policy_cache)

    # Set up the policy.
    self._algorithm.setup(self._dna_spec)
    prior_trials = tuple()
    if not self._add_prior_trials:
      # Trials are added to the algorithm only.
      prior_trials: Sequence[vz.Trial] = self._load_prior_trials()
    policy = self._create_policy(prior_trials)
    # Connect the service with policy.
    self._register(self._study_owner, self._study_name, policy)

  def _create_policy(
      self,
      prior_trials: Sequence[vz.Trial],
  ) -> TunerPolicy:
    """Creates a pythia policy."""
    if prior_trials:

      def get_trial_history(vizier_trials):
        for trial in vizier_trials:
          tuner_trial = core.VizierTrial(self._converter, trial)
          reward = tuner_trial.get_reward_for_feedback(
              self._converter.metrics_to_optimize
          )
          yield (tuner_trial.dna, reward)

      self._algorithm.recover(get_trial_history(prior_trials))

    return TunerPolicy(
        self._tuner.pythia_supporter(self._study),
        self._converter,
        self._algorithm,
        early_stopping_policy=self._early_stopping_policy,
    )

  def _get_stored_dna_spec(self) -> pg.DNASpec:
    metadata = self._study.materialize_problem_statement().metadata.ns(
        constants.METADATA_NAMESPACE
    )
    try:
      return converters.restore_dna_spec(
          metadata[constants.STUDY_METADATA_KEY_DNA_SPEC]
      )
    except KeyError as e:
      raise RuntimeError(
          f'Metadata {constants.STUDY_METADATA_KEY_DNA_SPEC} does not exist '
          f'in study: {self._study.resource_name}.'
      ) from e

  def _get_chief_tuner_id(self) -> str:
    metadata = self._study.materialize_problem_statement().metadata.ns(
        constants.METADATA_NAMESPACE
    )
    try:
      return str(metadata[constants.STUDY_METADATA_KEY_TUNER_ID])
    except KeyError as e:
      raise RuntimeError(
          f'Metadata {constants.STUDY_METADATA_KEY_TUNER_ID} does not exist '
          f'in study: {self._study.resource_name}.'
      ) from e

  @functools.cached_property
  def _tuner_id(self) -> str:
    """Returns the tuner id of current ."""
    return self._tuner.get_tuner_id(self._algorithm)

  def _register_self_as_primary(self) -> str:
    metadata = vz.Metadata()
    metadata.ns(constants.METADATA_NAMESPACE)[
        constants.STUDY_METADATA_KEY_TUNER_ID
    ] = self._tuner_id
    self._study.update_metadata(metadata)
    self._tuner.use_pythia_for_study(self._study)
    return self._tuner_id

  def _auto_elect_primary_if_needed(self) -> None:
    """Automatically elect primary tuner if previous primary is offline."""
    assert self._need_pythia_service
    assert self._run_mode == TunerMode.AUTO

    while self._is_active:
      chief_tuner_id = self._get_chief_tuner_id()
      if not self._tuner.ping_tuner(chief_tuner_id):
        # NOTE(daiyip): there could be a race condition that multiple workers
        # elect themselves as the new primary, and all of them regard
        # themselves as the elected. When this happens, multiple workers
        # may be hosting the Pythia service for this study. However, the study
        # will use one address (BNS of the latest elected worker) as the
        # Pythia endpoint. Besides, all states are stored in the study,
        # restart of workers will pick up these states from the study.
        # This is done by replaying existing trials to recompute the states of
        # the search algorithm. Therefore, it does not matter which worker is
        # chosen to serve the study, which should work equally well.
        # On the other hand, it is cheap to serve a PythiaService without
        # incoming queries. Therefore, we do not handle this race conditions
        # with expensive distributed locks.
        new_chief_tuner_id = self._register_self_as_primary()
        self._is_primary = new_chief_tuner_id == self._tuner_id
        if self._is_primary:
          pg.logging.warning(
              'Primary tuner has been switched from %s to %s.',
              chief_tuner_id,
              new_chief_tuner_id,
          )
          self._start_pythia()
      time.sleep(random.randint(50, 70))

  def next(self) -> pg.tuning.Feedback:
    """Gets the next tuning feedback object."""
    try:
      trial = next(self._suggestion_generator)  # pytype: disable=wrong-arg-types
      return core.Feedback(self._study.get_trial(trial.id), self._converter)
    except StopIteration as e:
      self._is_active = False
      raise e

  def _wait_for_study(
      self, owner: str, name: ExpandedStudyName
  ) -> client_abc.StudyInterface:
    """Wait for the study in a loop."""
    while True:
      try:
        return self._tuner.load_study(owner, name)
      except client_abc.ResourceNotFoundError:
        logging.info(
            'Study %s (owner=%s) does not exist. Retrying after 10 seconds.',
            name,
            owner,
        )
        time.sleep(10)
      except Exception as e:  # pylint:disable=broad-except
        logging.warn(
            'Could not look up study: %s. Retrying after 60 seconds', e
        )
        time.sleep(60)

  def _load_prior_trials(self) -> list[vz.Trial]:
    trials = []
    for prior in self._prior_study_ids:
      trials.extend(
          self._tuner.load_prior_study(prior)
          .trials(vz.TrialFilter(status=vz.TrialStatus.COMPLETED))
          .get()
      )
    return trials

  def _register(
      self, owner: str, name: ExpandedStudyName, policy: TunerPolicy
  ) -> None:
    """Registers the algorithm for a specific study."""
    study_key = StudyKey(owner, name)

    if study_key in _global_policy_cache:
      existing = _global_policy_cache[study_key]
      if existing.algorithm != policy.algorithm:
        raise ValueError(
            f'Different algorithms are used for the same study {study_key!r}. '
            f'Previous: {existing.algorithm!r}, Current: {policy.algorithm!r}.'
        )
      if existing.early_stopping_policy != policy.early_stopping_policy:
        raise ValueError(
            'Different early stopping policy are used for the same study '
            f'{study_key!r}. Previous: {existing.early_stopping_policy!r}, '
            f'Current: {policy.early_stopping_policy!r}.'
        )

    _global_policy_cache[study_key] = policy

  #
  # Class methods.
  #

  @classmethod
  def use_study_prefix(cls, study_prefix: Optional[str]):
    cls.default_study_prefix = study_prefix or ''

  @classmethod
  def _expand_name(cls, name: Optional[str]) -> ExpandedStudyName:
    """Expand the pyglove study name into the full name in Vizier DB.

    Args:
      name: Name as passed into pyglove.

    Returns:
      Study name to use for Vizier interactions.
    """
    components = []
    if cls.default_study_prefix:
      components.append(cls.default_study_prefix)
    if name:
      components.append(name)
    return ExpandedStudyName('.'.join(components))

  @classmethod
  def _get_study_resource_name(cls, name: str) -> str:
    """Use for testing only."""
    return cls.tuner_cls.load_study(
        owner=cls.default_owner,
        name=ExpandedStudyName(name),
    ).resource_name

  @classmethod
  def poll_result(
      cls, name: str, study_owner: Optional[str] = None
  ) -> pg.tuning.Result:
    """Polls result of a study."""
    return core.Result.from_study(
        cls.tuner_cls.load_study(
            study_owner or cls.default_owner, cls._expand_name(name)
        )
    )


--- vizier/_src/pyglove/client.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Abstraction for Vizier Tuner."""

import abc
import threading
from typing import Dict, NewType, Optional, Union

import attrs
import pyglove as pg
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.pyglove import converters
from vizier._src.pyglove import pythia as pg_pythia
from vizier.client import client_abc


# Pyglove-Vizier interactions only accept ExpandedStudyName. It is distinguished
# from the name that is passed into Backend.__init__ which does not include
# prefix.
ExpandedStudyName = NewType('ExpandedStudyName', str)


@attrs.define(eq=True, frozen=True)
class StudyKey:
  """Simple immutable structure used for looking up policies in a dict."""
  _owner: str
  _name: ExpandedStudyName


PolicyCache = Dict[StudyKey, pg_pythia.TunerPolicy]


class VizierTuner(abc.ABC):
  """Abstraction for Vizier tuner.

  Each platform (e.g. Google, OSS, Vertex) should have their own implementation
  of this abstraction.
  """
  # NOTE(chansoo): All of the methods can technically become a classmethod.

  def __init__(self):
    self._pythia_lock = threading.Lock()

  @abc.abstractmethod
  def get_tuner_id(self, algorithm: pg.DNAGenerator) -> str:
    """Get identifier of this tuner instance."""

  def start_pythia_service(self, policy_cache: PolicyCache) -> None:
    """Start pythia service on current machine."""
    with self._pythia_lock:
      self._start_pythia_service(policy_cache)

  @abc.abstractmethod
  def _start_pythia_service(
      self, policy_cache: dict[StudyKey, pg_pythia.TunerPolicy]
  ) -> None:
    """Starts pythia service _only if_ there is no pythia service running.

    Args:
      policy_cache: The pythia service will have the reference to policies
        created by the backend. It is expected to use the study keys to find
        the policies, instead of creating new ones from scratch.
    """

  @abc.abstractmethod
  def load_prior_study(self, resource_name: str) -> client_abc.StudyInterface:
    """Loads prior study identified by the resource name."""

  @abc.abstractmethod
  def create_study(
      self,
      problem: vz.ProblemStatement,
      converter: converters.VizierConverter,
      owner: str,
      name: str,
      algorithm: pg.DNAGenerator,
      stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = None,
  ) -> client_abc.StudyInterface:
    """Creates a new study."""

  @abc.abstractmethod
  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:
    """Get worker ID."""

  @abc.abstractmethod
  def ping_tuner(self, tuner_id: str) -> bool:
    """See if the tuner is alive."""

  @abc.abstractmethod
  def pythia_supporter(
      self, study: client_abc.StudyInterface
  ) -> pythia.PolicySupporter:
    """Creates a pythia policy supporter for this study."""

  @abc.abstractmethod
  def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:
    """Uses current Pythia service to serve the input study."""

  @classmethod
  @abc.abstractmethod
  def load_study(
      cls, owner: str, name: ExpandedStudyName
  ) -> client_abc.StudyInterface:
    """Loads a study identified by (owner, name) pair."""


--- vizier/_src/pyglove/constants.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Constants."""

PARAMETER_NAME_ROOT = '$'
STUDY_METADATA_KEY_TUNER_ID = 'PRIMARY_TUNER_ID'
STUDY_METADATA_KEY_DNA_SPEC = 'TUNER_DNA_SPEC'
STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC = 'USES_EXTERNAL_DNA_SPEC'
TRIAL_METADATA_KEY_DNA_METADATA = 'DNA_METADATA'
TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS = 'CUSTOM_TYPE_DECISIONS'
TRIAL_METADATA_KEYS = frozenset(
    [TRIAL_METADATA_KEY_DNA_METADATA, TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS])
FROM_VIZIER_STUDY_HINT = 'from_vizier_study'
METADATA_NAMESPACE = 'pyglove'
RELATED_LINKS_SUBNAMESPACE = 'related_links'
DUMMY_PARAMETER_NAME = 'dummy_parameter'
DUMMY_PARAMETER_VALUE = 'UNUSED'

REWARD_METRIC_NAME = 'reward'

STUDY_METADATA_KEYS = frozenset([
    STUDY_METADATA_KEY_TUNER_ID,
    STUDY_METADATA_KEY_DNA_SPEC,
    STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC,
])


--- vizier/_src/pyglove/converters.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""PyVizier module. See VizierConverter class."""

import base64
import datetime
import json
import lzma
import numbers
from typing import Any, Dict, List, Optional, Sequence, Literal

from absl import logging
import attr
import pyglove as pg
from vizier import pyvizier as vz
from vizier._src.pyglove import constants


def _to_json_str_compressed(value: Any) -> str:
  """Serialize (maybe) symbolic object to compressed JSON value."""
  return base64.b64encode(
      lzma.compress(json.dumps(
          pg.to_json(value)).encode('utf-8'))).decode('ascii')


def _parameter_with_external_type(
    val: vz.ParameterValueTypes,
    external_type: vz.ExternalType) -> vz.ParameterValueTypes:
  """Converts a parameter value to proper external type."""
  if external_type == vz.ExternalType.BOOLEAN:
    # We output strings 'True' or 'False', not booleans themselves.
    # because BOOLEAN is interally CATEGORICAL.
    return val
  elif external_type == vz.ExternalType.INTEGER:
    return int(val)
  elif external_type == vz.ExternalType.FLOAT:
    return float(val)
  else:
    return val


def _make_decision_point(
    parameter_config: vz.ParameterConfig) -> pg.geno.DecisionPoint:
  """Make a decision point (DNASpec) out from a parameter config."""

  # NOTE(daiyip): We set the name of each decision point instead of its
  # location with parameter name.
  #
  # Why? For conditional space, the ID of a decision point is a
  # path of locations from the root to the leaf node. For example, if there
  # are two parameters - a parent with location 'a' and a child with location
  # 'b', the ID for the child will be 'a.b'. However, for external (
  # non-PyGlove created) study, the parameter name for the child does not
  # follow this pattern. The solution is to use the ``name`` property of
  # `DNASpec`, which allows the user to access hierarchical decision
  # points by name, also DNA supports to_dict/from_dict based on the decision
  # point names instead of their IDs. Therefore, we can minimize the
  # difference between a PyGlove created study and an external study.

  name = parameter_config.name
  if parameter_config.type == vz.ParameterType.DOUBLE:
    # Create `pg.geno.Float` which does not have child spaces.
    min_value, max_value = parameter_config.bounds
    return pg.geno.Float(min_value, max_value, name=name)
  elif parameter_config.type in (vz.ParameterType.CATEGORICAL,
                                 vz.ParameterType.DISCRETE,
                                 vz.ParameterType.INTEGER):
    # Create `pg.geno.Choices` with possible child spaces.
    candidates = []
    literal_values = []
    for val in parameter_config.feasible_values:
      child_decision_points = []
      if parameter_config.child_parameter_configs:
        for child_pc in parameter_config.child_parameter_configs:
          if val in child_pc.matching_parent_values:
            child_decision_points.append(_make_decision_point(child_pc))
      candidates.append(pg.geno.Space(child_decision_points))
      literal_values.append(
          _parameter_with_external_type(val, parameter_config.external_type))
    return pg.geno.Choices(
        1, candidates, literal_values=literal_values, name=name)
  else:
    raise ValueError(
        f'Parameter Config Type {parameter_config.type!r} is not supported.')


def _to_dna_spec(search_space: vz.SearchSpace) -> pg.DNASpec:
  return pg.geno.Space(
      [_make_decision_point(pc) for pc in search_space.parameters])


def _to_search_space(dna_spec: pg.DNASpec) -> vz.SearchSpace:
  """Converts a DNASpec to Vizier search space.

  Args:
    dna_spec:

  Returns:
    Vizier search space.

  Raises:
    NotImplementedError: If no part of the spec can be converted to a Vizier
      parameter.
  """

  def _parameter_name(path: pg.KeyPath) -> str:
    # NOTE(daiyip): Vizier doesn't support empty name, thus we use a
    # special parameter name for the hyper value at root.
    return path.path if path else constants.PARAMETER_NAME_ROOT

  def _categories(spec: pg.geno.Choices) -> List[str]:
    return [spec.format_candidate(i) for i in range(len(spec.candidates))]

  def _category_value(spec: pg.geno.Choices, index: int) -> str:
    assert index < len(spec.candidates)
    return spec.format_candidate(index)

  def _add_dna_spec(root: vz.SearchSpaceSelector, path: pg.KeyPath,
                    spec: pg.DNASpec) -> None:
    """Convert a DNASpec node with parent choice to a list of parameters.

    Args:
      root: The DNA spec is added to this root.
      path: Root path of current DNA spec.
      spec: Current DNA spec.
    """
    if isinstance(spec, pg.geno.Space):
      for elem in spec.elements:
        _add_dna_spec(root, path + elem.location, elem)
    elif isinstance(spec, pg.geno.Choices):
      is_discrete = all(
          isinstance(v, numbers.Number) for v in spec.literal_values
      ) and len(set(spec.literal_values)) == len(spec.literal_values)

      for choice_idx in range(spec.num_choices):
        choice_path = path
        if spec.num_choices > 1:
          choice_path = choice_path + choice_idx

        if is_discrete:
          unique_feasible_points = sorted(set(spec.literal_values))
          root.add_discrete_param(
              name=_parameter_name(choice_path),
              # We sort the literal values since Vizier requires the feasible
              # points of a discrete parameter to be in increasing order.
              # The sorting has no impact to the trial parameter -> DNA
              # conversion since for numeric literal value, the conversion
              # is value based.
              feasible_values=unique_feasible_points,
          )
          if unique_feasible_points != spec.literal_values:
            logging.warning(
                'Candidates for parameter %r have been reordered/deduped from '
                '%s to %s to meet the sorted/distinct requirement for discrete '
                'parameter specifiction.',
                _parameter_name(choice_path),
                spec.literal_values,
                unique_feasible_points)
        else:
          new_parameter: vz.SearchSpaceSelector = root.add_categorical_param(
              name=_parameter_name(choice_path),
              feasible_values=_categories(spec))
          for candidate_idx, candidate in enumerate(spec.candidates):
            candidate_path = choice_path + pg.geno.ConditionalKey(
                candidate_idx, len(spec.candidates)
            )
            child: vz.SearchSpaceSelector = new_parameter.select_values(
                [_category_value(spec, candidate_idx)])
            _add_dna_spec(child, candidate_path, candidate)
    elif isinstance(spec, pg.geno.Float):
      root.add_float_param(
          name=_parameter_name(path),
          scale_type=get_scale_type(spec.scale),
          min_value=spec.min_value,
          max_value=spec.max_value)
    elif isinstance(spec, pg.geno.CustomDecisionPoint):
      # For CustomDecisionPoint, there is not a corresponding parameter type
      # in Vizier since its value is a variable string. In such case the
      # parameter value will be put into metadata.
      logging.info(
          'Encountered custom decision point %s, which will not be shown '
          'in Vizier dashboard.',
          _parameter_name(path),
      )
    else:
      raise NotImplementedError(
          f'Spec has unknown type. This Should never happen. Spec: {spec}')

  search_space = vz.SearchSpace()
  _add_dna_spec(search_space.root, pg.KeyPath(), dna_spec)

  if not search_space.parameters:
    raise NotImplementedError(
        'No part of the dna spec could be represented as a Vizier parameter.')
  return search_space


def get_scale_type(scale: Optional[str]) -> Optional[vz.ScaleType]:
  """Returns scale type based on scale string."""
  if scale in [None, 'linear']:
    return vz.ScaleType.LINEAR
  elif scale == 'log':
    return vz.ScaleType.LOG
  elif scale == 'rlog':
    return vz.ScaleType.REVERSE_LOG
  else:
    raise ValueError(f'Unsupported scale type: {scale!r}')


def get_pyglove_metadata(trial: vz.Trial) -> dict[str, Any]:
  """Extracts only the pyglove-related metadata into a simple dict."""
  metadata = dict()

  # NOTE(daiyip): This is to keep backward compatibility for Cloud NAS service,
  # which might loads trials from studies created in the old NAS pipeline for
  # transfer learning.
  for key, value in trial.metadata.items():
    if key in constants.TRIAL_METADATA_KEYS:
      metadata[key] = pg.from_json_str(value)

  for key, value in trial.metadata.ns(constants.METADATA_NAMESPACE).items():
    if key not in constants.TRIAL_METADATA_KEYS and value is not None:
      metadata[key] = pg.from_json_str(value)
  return metadata


def get_pyglove_study_metadata(problem: vz.ProblemStatement) -> pg.Dict:
  """Extracts only the pyglove-related metadata into a simple dict."""
  metadata = pg.Dict()
  pg_metadata = problem.metadata.ns(constants.METADATA_NAMESPACE)
  for key, value in pg_metadata.items():
    if key not in constants.STUDY_METADATA_KEYS and value is not None:
      metadata[key] = pg.from_json_str(value)
  return metadata


@attr.frozen
class VizierConverter:
  """Converts between PyGlove DNA and Vizier Trial.

  It can be initialized from a pg.DNASpec or vz.SearchSpace. It handles
  conversions between pg.DNA and vz.Trial.

  NOTE: Use a factory instead of __init__. There are two factories:
    VizierConverter.from_problem(...)
    VizierConverter.from_dna_spec(...)

  CAVEAT: The set of search spaces that can be described pg.DNASpec and
  vz.SearchSpace does not fully overlap. (e.g. DNASpec does not support
  discrete doubles and integers) As a result,
  `VizierConverter.from_dna_spec(VizierConverter.from_search_space(s).dna_spec)`
  is not the same as `VizierConverter.from_search_space(s)`.

  If VizierConverter is created from a DNA spec that can't be represented
  in Vizier search space, then `_problem` has a dummy search space and
  effectively has no purpose beyond saving dna_spec in metadata.
  In this case, `vizier_conversion_error` has a non-None value.
  """

  _dna_spec: pg.DNASpec = attr.field()
  _problem: vz.ProblemStatement = attr.field()
  _uses_external_dna_spec: bool = attr.field()

  vizier_conversion_error: Optional[Exception] = attr.field(
      default=None, kw_only=True)

  def __attrs_post_init__(self):
    # Store the dna spec in the metadata.
    self._problem.metadata.ns(constants.METADATA_NAMESPACE)[
        constants.STUDY_METADATA_KEY_DNA_SPEC] = _to_json_str_compressed(
            self._dna_spec)
    self._problem.metadata.ns(constants.METADATA_NAMESPACE)[
        constants.STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC
    ] = pg.to_json_str(self._uses_external_dna_spec)

  @property
  def metrics_to_optimize(self) -> Sequence[str]:
    metrics = []
    for m in self._problem.metric_information:
      if m.goal == vz.ObjectiveMetricGoal.MAXIMIZE:
        metrics.append(m.name)
      else:
        metrics.append(f'negative_{m.name}')
    return metrics

  @classmethod
  def from_problem(cls, problem: vz.ProblemStatement) -> 'VizierConverter':
    """Creates from vizier problem."""
    # TODO: Check this implementation
    json_str_compressed = problem.metadata.ns(constants.METADATA_NAMESPACE).get(
        constants.STUDY_METADATA_KEY_DNA_SPEC, None)

    if json_str_compressed is not None:
      dna_spec = restore_dna_spec(json_str_compressed)
    else:
      dna_spec = _to_dna_spec(problem.search_space)

    if bad_metrics := tuple(
        filter(lambda m: m.goal != vz.ObjectiveMetricGoal.MAXIMIZE,
               problem.metric_information)):
      raise ValueError(
          f'All goals must MAXIMIZE. Offending metrics: {bad_metrics}')
    if bad_metrics := tuple(
        filter(lambda m: m.type != vz.MetricType.OBJECTIVE,
               problem.metric_information)):
      logging.warning('All goals must be OBJECTIVE. Offending metrics: %s',
                      bad_metrics)

    uses_external_dna_spec = (
        json_str_compressed is not None
        and pg.from_json_str(
            problem.metadata.ns(constants.METADATA_NAMESPACE).get(
                constants.STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC, 'true'
            )
        )
    )
    return cls(dna_spec, problem, uses_external_dna_spec)

  @classmethod
  def from_dna_spec(
      cls,
      dna_spec: pg.DNASpec,
      metrics_to_maximize: Sequence[str] = (constants.REWARD_METRIC_NAME,)
  ) -> 'VizierConverter':
    """Create from dna spec."""
    problem = vz.ProblemStatement()
    for name in metrics_to_maximize:
      problem.metric_information.append(
          vz.MetricInformation(name, goal=vz.ObjectiveMetricGoal.MAXIMIZE))

    try:
      problem.search_space = _to_search_space(dna_spec)
      return cls(dna_spec, problem, True)
    except NotImplementedError as e:
      # Add a dummy parameter.
      problem.search_space.root.add_categorical_param(
          constants.DUMMY_PARAMETER_NAME,
          feasible_values=[constants.DUMMY_PARAMETER_VALUE])
      logging.info(
          'The provided DNA spec cannot be converted to a '
          'Vizier search space. Vizier algorithms cannot be used. '
          'Error was: %s', e)
      return cls(dna_spec, problem, True, vizier_conversion_error=e)

  @property
  def dna_spec(self) -> pg.DNASpec:
    return self._dna_spec

  @property
  def problem(self) -> vz.ProblemStatement:
    """Raises an error if the dna spec cannot be represented in Vizier."""
    if self.vizier_conversion_error:
      raise self.vizier_conversion_error
    return self._problem

  @property
  def uses_external_dna_spec(self) -> bool:
    """Returns True if the dna spec is provided from external."""
    return self._uses_external_dna_spec

  @property
  def problem_or_dummy(self) -> vz.ProblemStatement:
    """Returns dummy if the dna spec cannot be represented in Vizier."""
    return self._problem

  @property
  def search_space(self) -> vz.SearchSpace:
    if self.vizier_conversion_error:
      raise NotImplementedError(
          f'Vizier algorithms cannot work with the dna spec. '
          f'Error was: {self.vizier_conversion_error}')
    return self._problem.search_space

  def _process_key_value(self, key: str, value: vz.ParameterValueTypes):
    if key == constants.PARAMETER_NAME_ROOT:
      key = ''
    if self.dna_spec.hints == constants.FROM_VIZIER_STUDY_HINT:
      if not isinstance(value, str):
        value = float(value)  # Integers are always converted to doubles.
      if isinstance(self.dna_spec[key], pg.geno.Choices):
        value = repr(value)
    return key, value

  def _parameters_to_dict(self,
                          trial: vz.Trial) -> Dict[str, vz.ParameterValueTypes]:
    return dict({
        self._process_key_value(k, v)
        for k, v in trial.parameters.as_dict().items()
    })

  def to_dna(self, trial: vz.Trial) -> pg.DNA:
    """Extract DNA from vizier trial."""
    decision_dict = self._parameters_to_dict(trial)
    if len(
        decision_dict) == 1 and constants.DUMMY_PARAMETER_NAME in decision_dict:
      decision_dict = {}

    custom_decisions_str = trial.metadata.ns(constants.METADATA_NAMESPACE).get(
        constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS, None)
    if custom_decisions_str is not None:
      custom_decisions = pg.from_json_str(custom_decisions_str)
      assert isinstance(custom_decisions, dict)
      decision_dict.update(custom_decisions)

    dna = pg.DNA.from_dict(
        decision_dict, self.dna_spec, use_ints_as_literals=True)

    # Restore DNA metadata if present
    dna_metadata = trial.metadata.ns(constants.METADATA_NAMESPACE).get(
        constants.TRIAL_METADATA_KEY_DNA_METADATA, None
    )

    if dna_metadata is None:
      # NOTE(daiyip): To be compatible with V1 pipeline for transfer learning,
      # we also try to read DNA_METADATA stored under the global (empty)
      # namespace.
      dna_metadata = trial.metadata.get(
          constants.TRIAL_METADATA_KEY_DNA_METADATA, None
      )

    if dna_metadata is not None:
      dna.rebind(
          metadata=pg.from_json_str(dna_metadata),
          skip_notification=True,
          raise_on_no_change=False,
      )
    else:
      logging.warn('DNA metadata is None for trial: %s', trial)
    return dna

  def to_trial(self, dna: pg.DNA, *,
               fallback: Literal['raise_error', 'return_dummy']) -> vz.Trial:
    """Converts DNA to vizier Trial.

    Args:
      dna:
      fallback: Decides the behavior when Vizier Search space is ill-formed from
        the DNA spec. 'raise_error': Raises an error. 'return_dummy': Returns a
        dummy trial.

    Returns:
      Vizier Trial.

    Raises:
      NotImplementedError: DNA has no valid representation as a Vizier trial.
    """
    trial = vz.Trial()
    trial.description = str(dna)
    trial.metadata.ns(constants.METADATA_NAMESPACE)[
        constants.TRIAL_METADATA_KEY_DNA_METADATA] = pg.to_json_str(
            dna.metadata)

    # Custom decision.
    def is_custom(x):
      return isinstance(x, pg.geno.CustomDecisionPoint)

    custom_decisions = dna.to_dict(filter_fn=is_custom)
    if custom_decisions:
      trial.metadata.ns(constants.METADATA_NAMESPACE)[
          constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS] = pg.to_json_str(
              custom_decisions)

    if self.vizier_conversion_error:
      if fallback == 'return_dummy':
        trial.parameters[
            constants.DUMMY_PARAMETER_NAME] = constants.DUMMY_PARAMETER_VALUE
      else:
        raise self.vizier_conversion_error
    else:

      def is_discrete(x):
        return (
            isinstance(x, pg.geno.Choices)
            and all(isinstance(v, numbers.Number) for v in x.literal_values)
            and len(set(x.literal_values)) == len(x.literal_values)
        )

      if self._uses_external_dna_spec:
        key_type, value_type = 'id', 'choice_and_literal'
      else:
        key_type, value_type = 'name_or_id', 'literal'

      # Update (non-discrete) configured parameters from DNA.
      # We still write discrete parameter values in 'choice_and_literal' format
      # though their values will be overriden later, to preserve the order
      # of parameters.
      configured_parameters = dna.to_dict(
          key_type=key_type,
          value_type=value_type,
          filter_fn=lambda x: not is_custom(x),
      )

      # Update (discrete) configured parameters from DNA.
      # Force converting literal values to float to match DISCRETE parameter
      # expectation.
      discrete_parameters = {
          k: float(v)
          for k, v in dna.to_dict(
              key_type=key_type, value_type='literal', filter_fn=is_discrete
          ).items()
      }
      configured_parameters.update(discrete_parameters)

      if not configured_parameters:
        configured_parameters[constants.DUMMY_PARAMETER_NAME] = (
            constants.DUMMY_PARAMETER_VALUE
        )

      for name, value in configured_parameters.items():
        trial.parameters[name or constants.PARAMETER_NAME_ROOT] = (
            vz.ParameterValue(value)
        )
    return trial

  def to_tuner_measurement(
      self, vizier_measurement: Optional[vz.Measurement]
  ) -> Optional[pg.tuning.Measurement]:
    """Convert Vizier measurement to tuner Measurement."""
    if not vizier_measurement:
      return None
    tuner_measurement = pg.tuning.Measurement(
        step=int(vizier_measurement.steps),
        elapse_secs=vizier_measurement.elapsed_secs,
        reward=vizier_measurement.metrics.get_value(
            constants.REWARD_METRIC_NAME, 0.))
    tuner_measurement.metrics = {
        m: vizier_measurement.metrics.get_value(m, 0.)
        for m in vizier_measurement.metrics
    }
    return tuner_measurement

  def to_tuner_trial(self, vizier_trial: vz.Trial) -> pg.tuning.Trial:
    return pg.tuning.Trial(
        id=vizier_trial.id,
        description=vizier_trial.description,
        dna=self.to_dna(vizier_trial),
        metadata=dict(vizier_trial.metadata.ns(constants.METADATA_NAMESPACE)),
        related_links=dict(
            vizier_trial.metadata.ns(constants.METADATA_NAMESPACE).ns(
                constants.RELATED_LINKS_SUBNAMESPACE
            )
        ),
        measurements=vizier_trial.measurements,
        final_measurement=self.to_tuner_measurement(
            vizier_trial.final_measurement
        ),
        status=self._to_tuner_trial_status(vizier_trial.status),
        created_time=int(  # pylint: disable=g-long-ternary
            vizier_trial.creation_time.replace(
                tzinfo=datetime.timezone.utc
            ).timestamp()
        )
        if vizier_trial.creation_time
        else None,
        completed_time=int(  # pylint: disable=g-long-ternary
            vizier_trial.completion_time.replace(
                tzinfo=datetime.timezone.utc
            ).timestamp()
        )
        if vizier_trial.completion_time
        else None,
        infeasible=vizier_trial.infeasible,
    )

  def _to_tuner_trial_status(self, status: vz.TrialStatus) -> str:
    """Convert Vizier trial status to tuner trial status."""
    return 'PENDING' if status == vz.TrialStatus.ACTIVE else status.name


def restore_dna_spec(json_str_compressed: str) -> pg.DNASpec:
  """Restores DNASpec from compressed JSON str."""
  return pg.from_json(
      json.loads(lzma.decompress(base64.b64decode(json_str_compressed)))
  )


--- vizier/_src/pyglove/converters_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for converters."""

import datetime
import pyglove as pg
from vizier import pyvizier as vz
from vizier._src.pyglove import constants
from vizier._src.pyglove import converters
from absl.testing import absltest
from absl.testing import parameterized


class VizierCreatedSearchSpaceTest(parameterized.TestCase):
  """Tests for search space created from vz.SearchSpace."""

  def _search_space(self) -> vz.SearchSpace:
    ss = vz.SearchSpace()
    root = ss.root
    root.add_float_param('double', -1., 1.)
    root.add_int_param('int', 2, 4)
    root.add_discrete_param('discrete_int', (-2, 4, 7, 8))
    root.add_discrete_param('discrete_double', (-2.1, 4.1, 7.1, 8.1))
    root.add_categorical_param('categorical', ('a', 'b', 'c'))
    return ss

  def test_dna_spec(self):
    vc = converters.VizierConverter.from_problem(
        vz.ProblemStatement(self._search_space()))
    self.assertLen(vc.dna_spec, 5)
    self.assertNotEmpty(
        vc.problem.metadata.ns(constants.METADATA_NAMESPACE)[
            constants.STUDY_METADATA_KEY_DNA_SPEC])
    self.assertEqual(
        vc.problem.metadata.ns(constants.METADATA_NAMESPACE)[
            constants.STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC
        ],
        'false',
    )

  def test_dna_to_trial(self):
    vc = converters.VizierConverter.from_problem(
        vz.ProblemStatement(self._search_space()))
    self.assertDictEqual(
        vc.to_trial(vc.dna_spec.first_dna(),
                    fallback='raise_error').parameters.as_dict(), {
                        'double': -1,
                        'int': 2,
                        'discrete_int': -2,
                        'discrete_double': -2.1,
                        'categorical': 'a'
                    })

  def test_vizier_trial_to_tuner_trial(self):
    vc = converters.VizierConverter.from_problem(
        vz.ProblemStatement(self._search_space())
    )
    trial = vz.Trial()
    trial.id = 1
    trial.description = 'A test trial'
    trial.parameters['double'] = -0.5
    trial.parameters['int'] = 3
    trial.parameters['discrete_int'] = 7
    trial.parameters['discrete_double'] = 4.1
    trial.parameters['categorical'] = 'a'
    self.assertEqual(
        vc.to_tuner_trial(trial),
        pg.tuning.Trial(
            id=1,
            description=trial.description,
            dna=pg.DNA([-0.5, 1, 2, 1, 0]),
            related_links={},
            measurements=[],
            final_measurement=None,
            status='PENDING',
            created_time=int(  # pylint: disable=g-long-ternary
                trial.creation_time.replace(
                    tzinfo=datetime.timezone.utc
                ).timestamp()
            )
            if trial.creation_time
            else None,
            completed_time=None,
        ),
    )

  @parameterized.parameters((7, None), (7.0, 'pyglove'))
  def test_trial_to_dna(self, discrete_int_value, metadata_ns):
    vc = converters.VizierConverter.from_problem(
        vz.ProblemStatement(search_space=self._search_space()))
    trial = vz.Trial()
    trial.parameters['double'] = -.5
    trial.parameters['int'] = 3
    trial.parameters['discrete_int'] = discrete_int_value
    trial.parameters['discrete_double'] = 4.1
    trial.parameters['categorical'] = 'a'

    metadata = trial.metadata.ns(metadata_ns) if metadata_ns else trial.metadata
    metadata[constants.TRIAL_METADATA_KEY_DNA_METADATA] = '{"log_prob": 1.0}'
    dna = vc.to_dna(trial)
    self.assertEqual(dna, pg.DNA([-0.5, 1, 2, 1, 0]))
    self.assertEqual(dna.metadata, dict(log_prob=1.0))


class PyGloveCreatedSearchSpaceTest(parameterized.TestCase):
  """Tests for PyGlove created search space."""

  def _dna_spec(self) -> pg.DNASpec:

    class CustomTypeDecision(pg.hyper.CustomHyper):

      def custom_decode(self, dna):
        return dna.value

    search_space = pg.Dict(
        a=CustomTypeDecision(),
        b=pg.manyof(2, [0, 0, 2, 2]),
        x=pg.oneof([3, 2]),
        y=pg.floatv(0.1, 2.0),
        z=pg.oneof(['foo', 'bar']),
    )
    return pg.dna_spec(search_space)

  def test_search_space(self):
    vc = converters.VizierConverter.from_dna_spec(self._dna_spec())
    # Custom-type decision point `a` is not part of the search space.
    # But `b` has two decision points.
    self.assertLen(vc.search_space.parameters, 5)
    self.assertNotEmpty(
        vc.problem.metadata.ns(constants.METADATA_NAMESPACE)[
            constants.STUDY_METADATA_KEY_DNA_SPEC])
    self.assertEqual(
        vc.problem.metadata.ns(constants.METADATA_NAMESPACE)[
            constants.STUDY_METADATA_KEY_USES_EXTERNAL_DNA_SPEC
        ],
        'true',
    )

  def test_dna_to_trial(self):
    dna_spec = self._dna_spec()
    vc = converters.VizierConverter.from_dna_spec(dna_spec)
    dna = pg.DNA(['abc', [1, 2], 0, 0.5, 1], spec=dna_spec)
    trial = vc.to_trial(dna, fallback='raise_error')
    self.assertLen(trial.parameters, 5)
    self.assertEqual(trial.parameters['b[0]'], vz.ParameterValue('1/4 (0)'))
    self.assertEqual(trial.parameters['b[1]'], vz.ParameterValue('2/4 (2)'))
    self.assertEqual(trial.parameters['x'], vz.ParameterValue(3))
    self.assertEqual(trial.parameters['y'], vz.ParameterValue(0.5))
    self.assertEqual(trial.parameters['z'], vz.ParameterValue('1/2 (\'bar\')'))
    self.assertEqual(
        pg.from_json_str(
            trial.metadata.ns(constants.METADATA_NAMESPACE)
            [constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS]), {'a': 'abc'})

  def test_trial_to_dna(self):
    vc = converters.VizierConverter.from_dna_spec(self._dna_spec())
    trial = vz.Trial()
    trial.parameters['x'] = 2.0
    trial.parameters['b[0]'] = '1/4 (0)'
    trial.parameters['b[1]'] = '2/4 (2)'
    trial.parameters['y'] = 0.5
    trial.parameters['z'] = '1/2 (\'bar\')'
    trial.metadata.ns(constants.METADATA_NAMESPACE)[
        constants.TRIAL_METADATA_KEY_CUSTOM_TYPE_DECISIONS] = pg.to_json_str(
            {'a': 'abc'})
    self.assertEqual(vc.to_dna(trial), pg.DNA(['abc', [1, 2], 1, 0.5, 1]))

  def test_dna_trial_conversion(self):
    dna_spec_str = (
        '/Td6WFoAAATm1rRGAgAhARYAAAB0L+Wj4DRRAd1dAD2Ih+dECUOcj7RXvfBPYS0OyzAc2w'
        '+PsBIJ1+EKYkDTqQ48R4Lre1bjAsSR5xLCA3KFq2tR8zHqH2SfrVoBUCrdm1gXQLM7PnGC'
        'z52uZFScSqVSvmJfgDEysdKpt//4+0S1c3hId0yS85IVlpXp68vfx+7pUOGCrns7aqQMnQ'
        'lU6+71UcejsPe7GyuCOnl1hRUMWBJ4TlvES+DZG5v9bDPB84v3vtEMuj3e6qGl67y1Un6F'
        '/P2g9qXeBt7DO2zcGdFjdJbkhdQBAFM/+5YsTXP7N5hy6Ih0FYlLUh0SoALlEwGfqyzR1D'
        'QSyZWoCYilaf67UqdgusN6+RSJ8CXOgNPcMZEUwCd1gvzqOKlqTKu4tELJWh/vhO4pWGmQ'
        'T/DC6xlS9/6s0o1nzYO5hOQwOXmZ7G+VTuxi7k267ALMIsn197fMQVnSjNCVCCr1vkheyV'
        'p9wLygytvhWS0lXtlzNGFQMB3z/62OC6A3dz4hM52CACxaHyjS3WtYOGNUDHoMGew0wiSj'
        '/OIAlJ7aQXaz5kXtH8OsfqXTOWecW0DnhtWtKg8nolR0rqU7y6uoPWX+7HQYnzdZDOeoSK'
        'ZtDCKMRUam52iceU9o+1o4SB6XtWsO3JkOQ+5ZTwZ6zgqQAAAAAAB4E+94/gOmcgAB+QPS'
        'aAAAVg3xfLHEZ/sCAAAAAARZWg=='
    )
    dna_spec = converters.restore_dna_spec(dna_spec_str)
    converter = converters.VizierConverter.from_dna_spec(dna_spec)
    dna = pg.DNA(
        [
            2,
            1,
            [3, 4],
            1,
            0,
            [4, 5],
            1,
            [1, 2],
            0,
            [2, 3],
            0,
            [0, 4],
            0,
            [1, 9],
            0,
            [2, 5],
            [4, 1, 3, 2, 0],
        ],
        spec=dna_spec,
    )
    trial = converter.to_trial(dna, fallback='raise_error')
    self.assertTrue(pg.eq(converter.to_dna(trial), dna))


class GetPyGloveMetadataTest(absltest.TestCase):

  def test_metadata_from_pyglove_namespace(self):
    """Tests getting metadata from PyGlove namespace."""
    t = vz.Trial()
    t.metadata.ns(constants.METADATA_NAMESPACE)['foo'] = '1.0'
    self.assertEqual(converters.get_pyglove_metadata(t), {'foo': 1.0})

  def test_metadata_from_empty_namespace(self):
    """Tests getting metadata from empty namespace."""
    t = vz.Trial()
    t.metadata['DNA_METADATA'] = '{"log_prob": 1.0}'
    t.metadata['should_exclude'] = 'abc'
    self.assertEqual(
        converters.get_pyglove_metadata(t), {'DNA_METADATA': {'log_prob': 1.0}}
    )


class MakeParameterConfigTest(absltest.TestCase):
  """Tests for make parameter configs from DNASpec."""

  def testScale(self):
    """Test scale type conversion."""
    self.assertEqual(converters.get_scale_type(None), vz.ScaleType.LINEAR)
    self.assertEqual(converters.get_scale_type('linear'), vz.ScaleType.LINEAR)
    self.assertEqual(converters.get_scale_type('log'), vz.ScaleType.LOG)
    self.assertEqual(
        converters.get_scale_type('rlog'), vz.ScaleType.REVERSE_LOG)
    with self.assertRaisesRegex(
        ValueError, 'Unsupported scale type'):
      converters.get_scale_type('unknown')

  def testHyperValueAsRoot(self):
    """Test using hyper value as search space root."""
    search_space = pg.oneof([1, 2])
    actual = converters.VizierConverter.from_dna_spec(
        pg.dna_spec(search_space)).search_space
    expected = vz.SearchSpace()
    expected.root.add_discrete_param('$', [1, 2])
    self.assertEqual(actual, expected)

  def testFlatSearchSpace(self):
    """Test flat search spaces."""

    class CustomDecisionPoint(pg.geno.CustomDecisionPoint):

      def custom_decode(self, dna):
        return dna.value

    search_space = pg.Dict(
        # custom decision point 'a' will not be inserted as a part of
        # parameter config.
        a=CustomDecisionPoint(),
        b=pg.oneof([2, 1]),
        x=pg.oneof([2, 2, 1]),
        y=[pg.floatv(1e-6, 1.0, scale='log')],
        z=pg.Dict(p=pg.manyof(2, ['foo', 'bar'])),
    )
    actual = converters.VizierConverter.from_dna_spec(
        pg.dna_spec(search_space)).search_space
    expected = vz.SearchSpace()
    root = expected.root
    # Feasible points of discrete params are sorted.
    root.add_discrete_param('b', [1, 2])
    root.add_categorical_param('x', ['0/3 (2)', '1/3 (2)', '2/3 (1)'])
    root.add_float_param('y[0]', 1e-6, 1.0, scale_type=vz.ScaleType.LOG)
    root.add_categorical_param('z.p[0]', ['0/2 (\'foo\')', '1/2 (\'bar\')'])
    root.add_categorical_param('z.p[1]', ['0/2 (\'foo\')', '1/2 (\'bar\')'])

    self.assertEqual(actual, expected)

  def testHierarchicalSearchSpace(self):
    """Test hierarchical search space."""
    search_space = pg.oneof([[pg.oneof([1, 2])],
                             pg.Dict(x=pg.floatv(0.1, 0.5)),
                             pg.manyof(2,
                                       [pg.oneof(['foo', 'bar']), True, False])
                            ])
    actual = converters.VizierConverter.from_dna_spec(
        pg.dna_spec(search_space)).search_space

    expected = vz.SearchSpace()
    root = expected.root
    categories = [
        '0/3 ([0: OneOf(candidates=[0: 1, 1: 2])])',
        '1/3 ({x=Float(min_value=0.1, max_value=0.5)})',
        ('2/3 (ManyOf(num_choices=2, candidates=[0: OneOf('
         'candidates=[0: \'foo\', 1: \'bar\']), 1: True, 2: False]))')
    ]
    dollar = root.add_categorical_param('$', categories)
    dollar.select_values([categories[0]]).add_discrete_param(
        '[=0/3][0]',
        [
            1,
            2,
        ],
    )
    dollar.select_values([categories[1]]).add_float_param(
        name='[=1/3].x',
        min_value=0.1,
        max_value=0.5,
        scale_type=vz.ScaleType.LINEAR,
    )

    branch2 = dollar.select_values([categories[2]])

    def _add_children(name):
      branch2.add_categorical_param(
          name,
          [
              '0/3 (OneOf(candidates=[0: \'foo\', 1: \'bar\']))', '1/3 (True)',
              '2/3 (False)'
          ],
      ).select_values(['0/3 (OneOf(candidates=[0: \'foo\', 1: \'bar\']))'
                      ]).add_categorical_param(
                          f'{name}[=0/3]',
                          [
                              '0/2 (\'foo\')',
                              '1/2 (\'bar\')',
                          ],
                      )

    _add_children('[=2/3][0]')
    _add_children('[=2/3][1]')
    self.assertEqual(
        expected, actual, msg=f'expected={expected}, actual={actual}')


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/core.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Defines core components for tuner integration."""

import collections
import contextlib
import datetime
import typing
from typing import Any, Optional, Sequence

from absl import logging
import attr
import pyglove as pg
from vizier import pyvizier as vz
from vizier._src.pyglove import constants
from vizier._src.pyglove import converters
from vizier.client import client_abc


def _trial_status_legacy_value(status: vz.TrialStatus) -> str:
  # PENDING was renamed to ACTIVE.
  if status == vz.TrialStatus.ACTIVE:
    return 'PENDING'
  return status.value


class VizierTrial(pg.tuning.Trial):
  """Override Trial to lazy load DNA and metadata upon access.

  When we construct a `Trial` object, it doesn't pop up DNA, measurements and
  metadata from vizier trial proto immediately. This is because that a study
  may consists of thousands of trials, if we load them at construction time, it
  would take minutes, which is not acceptable. So we made the `Trial` object
  lazily load these properties upon access, reducing the construction time into
  a few seconds.
  """

  # Here we explicitly override the __init__ method managed by PyGlove,
  # for we want to pass in `converter` and `trial` which are not managed by
  # PyGlove.
  @pg.explicit_method_override
  def __init__(
      self, converter: converters.VizierConverter, trial: vz.Trial, **kwargs
  ):
    completed_time = (
        int(trial.completion_time.timestamp()) if trial.completion_time else 0
    )
    created_time = (
        int(trial.creation_time.timestamp()) if trial.creation_time else 0
    )

    super().__init__(
        dna=pg.DNA(None),
        id=trial.id,
        description=trial.description,
        final_measurement=converter.to_tuner_measurement(
            trial.final_measurement
        ),
        status=_trial_status_legacy_value(trial.status),
        created_time=created_time,
        completed_time=completed_time,
        infeasible=trial.infeasible,
        **kwargs,
    )
    self._converter = converter
    self._trial = trial

  @property
  def dna(self) -> pg.DNA:
    """Returns lazy loaded DNA."""
    temp_dna = self.sym_init_args.dna
    if temp_dna.value is None and not temp_dna.children:
      self.sym_init_args.dna = self._converter.to_dna(self._trial)
    return self.sym_init_args.dna

  @property
  def metadata(self) -> dict[str, Any]:
    """Returns lazy loaded metadata."""
    if not self.sym_init_args.metadata and self._trial:
      self.sym_init_args.metadata = converters.get_pyglove_metadata(self._trial)
    return self.sym_init_args.metadata

  @property
  def related_links(self) -> dict[str, str]:
    """Returns lazy loaded related links."""
    if not self.sym_init_args.related_links and self._trial:
      self.sym_init_args.related_links = dict(
          self._trial.metadata.ns(constants.METADATA_NAMESPACE).ns(
              constants.RELATED_LINKS_SUBNAMESPACE
          )
      )
    return self.sym_init_args.related_links

  @property
  def measurements(self) -> list[pg.tuning.Measurement]:
    """Returns lazy loaded measurements."""
    if not self.sym_init_args.measurements:
      self.sym_init_args.measurements = [
          self._converter.to_tuner_measurement(m)
          for m in self._trial.measurements
      ]
    return self.sym_init_args.measurements

  def format(self, *args, **kwargs):
    """Fetch lazy bound properties before print."""
    # NOTE(daiyip): `format` depends on the symbolic attributes to generate
    # the string representation. Since the following symbolic attributes are
    # lazily assigned upon property accesses, we prefetch them before calling
    # the `format`. Otherwise, the symbolic attributes are just default values
    # set at __init__ time.
    _, _, _, _ = self.dna, self.measurements, self.metadata, self.related_links
    return super().format(*args, **kwargs)


def _parse_namespace_from_key(
    encoded_key: str, default_ns: vz.Namespace
) -> tuple[vz.Namespace, str]:
  """From ':ns:key' to (ns, key)."""
  ns_and_key = tuple(vz.Namespace.decode(encoded_key))
  if not ns_and_key:
    raise ValueError(
        f'String did not parse into namespace and key: {encoded_key}'
    )
  elif len(ns_and_key) == 1:
    return (default_ns, ns_and_key[-1])
  else:
    return (vz.Namespace(ns_and_key[:-1]), ns_and_key[-1])


class Feedback(pg.tuning.Feedback):
  """Tuning feedback for a vizier trial."""

  def __init__(
      self,
      vizier_trial: client_abc.TrialInterface,
      converter: converters.VizierConverter,
  ):
    """Constructor.

    Args:
      vizier_trial: Vizier trial (cross-platform).
      converter: Vizier-Pyglove converter.
    """
    super().__init__(converter.metrics_to_optimize)
    self._converter = converter
    self._trial_client = vizier_trial
    self._trial = self._trial_client.materialize()
    self._dna_spec = converter.dna_spec
    self._discard_reward = 'reward' not in converter.metrics_to_optimize

  @property
  def id(self) -> int:
    """Gets Trial ID as ID."""
    return self._trial_client.id

  @property
  def dna(self) -> pg.DNA:
    """Gets DNA of current trial."""
    return self._converter.to_dna(self._trial)

  def get_trial(self) -> pg.tuning.Trial:
    """Gets current trial with all fields up-to-date."""
    self._trial = self._trial_client.materialize()
    return VizierTrial(self._converter, self._trial)

  @property
  def checkpoint_to_warm_start_from(self) -> Optional[str]:
    """Gets checkpoint path to warm start from. Refreshes `_trial`."""
    return None

  @contextlib.contextmanager
  def _maybe_race_condition(self, message: str):
    """Raise race condition error when error message matches with regex."""
    try:
      yield
    # TODO: once pyvizier expose common error types, we should
    # change `Exception` to a narrower error type.
    except Exception as e:  # pylint:disable=broad-except
      if message in str(e):
        raise pg.tuning.RaceConditionError(str(e)) from e
      else:
        raise

  def _add_measurement(
      self,
      reward: Optional[float],
      metrics: dict[str, float],
      step: int,
      checkpoint_path: Optional[str],
      elapse_secs: float,
  ) -> None:
    """Reports tuning measurement to the pg.tuning."""
    if reward is not None and not self._discard_reward:
      metrics |= {'reward': reward}
    with self._maybe_race_condition('Measurements can only be added to'):
      self._trial_client.add_measurement(
          vz.Measurement(metrics, elapsed_secs=elapse_secs, steps=step)
      )

  def set_metadata(self, key: str, value: Any, per_trial: bool = True) -> None:
    """Sets metadata for current trial or current sampling."""
    md = vz.Metadata()
    md.ns(constants.METADATA_NAMESPACE)[key] = pg.to_json_str(value)
    if per_trial:
      self._trial_client.update_metadata(md)
    else:
      self._trial_client.study.update_metadata(md)

  def get_metadata(self, key: str, per_trial: bool = True) -> Optional[Any]:
    """Gets metadata for current trial or current sampling.

    Args:
      key: A key to the Trial or StudyConfig metadata.  Vizier treats this as
        {namespace}:{key} where colons in {key} are escaped with a backslash,
        and {namespace} is encoded per vizier.pyvizier.Namespace.encode (i.e.
        colons are escaped and namespace components are separated by colons).
        But for the special case of the empty namespace, this simplifies to be
        just the key string.
      per_trial: True if you want to see per-trial metadata for the current
        Trial; false for study-related metadata.

    Returns:
      A metadata item, interpreted as JSON format.
    """
    abs_ns, key_in_ns = _parse_namespace_from_key(
        key, default_ns=vz.Namespace([constants.METADATA_NAMESPACE])
    )
    if per_trial:
      value = self._trial.metadata.abs_ns(abs_ns).get(key_in_ns, None)
    else:
      value = (
          self._trial_client.study.materialize_problem_statement()
          .metadata.abs_ns(abs_ns)
          .get(key_in_ns, None)
      )
    return pg.from_json_str(value) if value is not None else None

  def add_link(self, name: str, url: str) -> None:
    """Adds related link."""
    md = vz.Metadata()
    md.ns(constants.METADATA_NAMESPACE).ns(
        constants.RELATED_LINKS_SUBNAMESPACE
    )[name] = url
    self._trial_client.update_metadata(md)

  def done(
      self,
      metadata: Optional[dict[str, Any]] = None,
      related_links: Optional[dict[str, str]] = None,
  ) -> None:
    """Marks current tuning trial as done, and export final object."""
    metadata = metadata or {}
    related_links = related_links or {}
    for key, value in metadata.items():
      self.set_metadata(key, value)
    for key, value in related_links.items():
      self.add_link(key, value)
    self._trial_client.complete()
    self._trial = self._trial_client.materialize()

  def skip(self, reason: Optional[str] = None) -> None:
    """Skips current trial without providing feedback to the controller."""
    self._trial_client.complete(infeasible_reason=reason or 'skipped')

  def should_stop_early(self) -> bool:
    """Tells whether this trial should be stopped."""
    return self._trial_client.check_early_stopping()

  def end_loop(self) -> None:
    """Ends current search loop."""
    self._trial_client.study.set_state(vz.StudyState.ABORTED)


@attr.define(repr=False)
class Result(pg.tuning.Result):
  """Vizier tuner progress."""

  _converter: converters.VizierConverter = attr.field()
  _problem: vz.ProblemStatement = attr.field()
  _study: client_abc.StudyInterface = attr.field()
  _metadata: pg.Dict = attr.field()
  _best_trial: pg.tuning.Trial = attr.field()
  _trials: Sequence[pg.tuning.Trial] = attr.field()
  _num_trials_by_status: dict[str, int] = attr.field()
  _last_update_time: datetime.datetime = attr.field(
      factory=datetime.datetime.now
  )

  @classmethod
  def from_study(cls, study: client_abc.StudyInterface):
    # Recover DNA spec
    logging.info('from study..')
    problem = study.materialize_problem_statement()
    logging.info('got metadata.')
    metadata = converters.get_pyglove_study_metadata(problem)
    converter = converters.VizierConverter.from_problem(problem)

    logging.info('Getting trials..')
    best_trial = VizierTrial(converter, next(study.optimal_trials().get()))
    tuner_trials = [VizierTrial(converter, t) for t in study.trials().get()]
    logging.info('Got trials...')
    num_trials_by_status = dict(
        collections.Counter(t.status for t in tuner_trials)
    )

    return cls(
        converter,
        problem,
        study,
        metadata,
        best_trial,
        tuner_trials,
        num_trials_by_status,
    )

  @property
  def last_updated(self) -> datetime.datetime:
    """Last update time."""
    return self._last_update_time

  @property
  def is_active(self) -> bool:
    """Returns whether tuner is active."""
    state = self._study.materialize_state()
    active = state == vz.StudyState.ACTIVE
    logging.info('is_active was called. state:%s, active:%s', state, active)
    return active

  @property
  def metadata(self) -> dict[str, Any]:
    """Gets metadata for current sampling."""
    return self._metadata

  @property
  def best_trial(self) -> Optional[pg.tuning.Trial]:
    """Returns the best trial."""
    return self._best_trial

  @property
  def trials(self) -> Sequence[pg.tuning.Trial]:
    """Returns trials."""
    return self._trials

  def format(
      self,
      compact: bool = False,
      verbose: bool = True,
      root_indent: int = 0,
      **kwargs,
  ):
    # Return summary.
    status_field = pg.tuning.Trial.__schema__.get_field('status')
    assert status_field is not None
    possible_status = set(
        typing.cast(pg.typing.Enum, status_field.value).values
    )
    study = f'{self._study.resource_name}'
    # TODO: Add link to the study.
    status = {
        s: f'{self._num_trials_by_status[s]}/{len(self._trials)}'
        for s in possible_status
        if s in self._num_trials_by_status
    }
    json_repr = dict(study=study, status=status)
    if self._best_trial:
      json_repr['best_trial'] = dict(
          id=self._best_trial.id,
          reward=self._best_trial.final_measurement.reward,
          step=self._best_trial.final_measurement.step,
          dna=self._best_trial.dna.format(compact=True),
      )
    return pg.format(json_repr, compact, False, root_indent, **kwargs)


--- vizier/_src/pyglove/e2e_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""End-to-end tests for Vizier backend for PyGlove."""

import pyglove as pg
from vizier._src.pyglove import oss_vizier
from vizier._src.service import clients as pyvizier_clients
from absl.testing import absltest


pyvizier_clients.environment_variables.servicer_use_sql_ram()


class PygloveTest(absltest.TestCase):
  """Tests for using Vizier as PyGlove backend."""

  def test_sample(self):
    oss_vizier.init('my_study')

    examples = []
    for x, f in pg.sample(
        pg.oneof([1, 2, 3]), pg.geno.Random(seed=1), num_examples=3
    ):
      f(x)
      examples.append(x)

    self.assertEqual(examples, [1, 3, 1])
    result = pg.poll_result('')
    self.assertLen(result.trials, 3)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/oss_vizier.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tuner implementation based on Open Source Vizier."""

from concurrent import futures
import os
import threading
from typing import Optional, Union

from absl import logging
import attr
import grpc
import portpicker
import pyglove as pg
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.pyglove import algorithms
from vizier._src.pyglove import backend
from vizier._src.pyglove import client
from vizier._src.pyglove import converters
from vizier._src.service import clients as pyvizier_clients
from vizier._src.service import constants
from vizier._src.service import policy_factory as service_policy_factory_lib
from vizier._src.service import pythia_service
from vizier._src.service import pythia_service_pb2_grpc
from vizier._src.service import resources
from vizier._src.service import service_policy_supporter
from vizier._src.service import stubs_util
from vizier._src.service import types as vizier_types
from vizier._src.service import vizier_client
from vizier.client import client_abc
from vizier.service import pyvizier as svz

from google.protobuf import empty_pb2

BuiltinAlgorithm = algorithms.BuiltinAlgorithm
ExpandedStudyName = client.ExpandedStudyName
PolicyCache = client.PolicyCache
StudyKey = client.StudyKey


class PyGlovePolicyFactory(pythia.PolicyFactory):
  """PolicyFactory for OSSVizierTuner."""

  def __init__(self, policy_cache: PolicyCache):
    self._policy_cache = policy_cache

  def __call__(
      self, problem_statement, algorithm, policy_supporter, study_name
  ):
    study_resource = resources.StudyResource.from_name(study_name)
    study_key = StudyKey(
        study_resource.owner_id, ExpandedStudyName(study_resource.study_id)
    )
    if study_key in self._policy_cache:
      logging.info(
          'StudyKey %s was found in cache. Using it as the policy.', study_key
      )
      return self._policy_cache[study_key]

    # Use default Vizier algorithms if not using PyGlove poliices.
    logging.info(
        'StudyKey %s was not found in cache. Using default policy factory.'
    )

    default_policy_factory = service_policy_factory_lib.DefaultPolicyFactory()
    return default_policy_factory(
        problem_statement, algorithm, policy_supporter, study_name
    )


@attr.define
class _VizierServices:
  """Vizier services hub.

  This class is intended to be used as a singleton and not directly exposed to
  the user. Instead, users should always call `vizier.pyglove.init` to set up
  the vizier services stub, which calls `_VizierServices.use_vizier_service` and
  `_VizierServices.set_pythia_port` underlying.
  """

  _vizier_endpoint: Optional[str] = None
  _vizier_service: Optional[vizier_types.VizierService] = None
  _pythia_port: Optional[int] = None
  _pythia_server: Optional[grpc.Server] = None
  _pythia_servicer: Optional[pythia_service.PythiaServicer] = None

  def reset_for_testing(self) -> None:
    """Resets the services for testing purpose."""
    self._vizier_endpoint = None
    self._vizier_service = None
    self._pythia_port = None
    self._pythia_server = None
    self._pythia_servicer = None

  def use_vizier_service(self, endpoint: Optional[None]) -> None:
    """Uses vizier service specified by endpoint."""
    endpoint = endpoint or constants.NO_ENDPOINT
    if self._vizier_endpoint is not None and self._vizier_endpoint != endpoint:
      raise ValueError(
          'Cannot use different vizier endpoints in the same process. '
          f'Previous={self._vizier_endpoint}, New={endpoint}.'
      )
    self._vizier_endpoint = endpoint
    if self._vizier_service is not None:
      return
    if endpoint != constants.NO_ENDPOINT:
      pyvizier_clients.environment_variables.server_endpoint = (
          self._vizier_endpoint
      )
    self._vizier_service = vizier_client.create_vizier_servicer_or_stub()

  @property
  def vizier_service(self) -> vizier_types.VizierService:
    """Returns current vizier service."""
    assert self._vizier_service, 'call `use_vizier_service` first.'
    return self._vizier_service

  def set_pythia_port(self, pythia_port: Optional[int] = None) -> None:
    if (
        self._pythia_port is not None
        and pythia_port is not None
        and self._pythia_port != pythia_port
    ):
      raise ValueError(
          'Cannot use different pythia ports in the same process. '
          f'Previous={self._pythia_port}, New={pythia_port}'
      )
    self._pythia_port = pythia_port or portpicker.pick_unused_port()

  @property
  def pythia_endpoint(self) -> str:
    """Returns the endpoint for Pythia service."""
    assert self._pythia_port is not None, 'Please call `set_pythia_port` first.'
    return f'{os.uname()[1]}:{self._pythia_port}'

  def start_pythia_service(self, policy_cache: PolicyCache) -> None:
    """Start pythia service with a policy cache."""
    if self._pythia_server is not None:
      return

    policy_factory = PyGlovePolicyFactory(policy_cache)
    self._pythia_servicer = pythia_service.PythiaServicer(
        self._vizier_service, policy_factory
    )
    self._pythia_server = grpc.server(futures.ThreadPoolExecutor(max_workers=1))
    pythia_service_pb2_grpc.add_PythiaServiceServicer_to_server(
        self._pythia_servicer, self._pythia_server
    )
    self._pythia_server.add_insecure_port(self.pythia_endpoint)
    self._pythia_server.start()


# Global vizier services hub.
_services = _VizierServices()


class _OSSVizierTuner(client.VizierTuner):
  """OSS Vizier tuner for pyglove."""

  def get_tuner_id(self, algorithm: pg.DNAGenerator) -> str:
    """See parent class."""
    del algorithm
    # We use hostname plus thread ID as tuner ID, so we could test
    # distributed tuning scenarios using multi-threading.
    return f'{threading.get_ident()}@{_services.pythia_endpoint}'

  def _start_pythia_service(self, policy_cache: PolicyCache) -> None:
    """See parent class."""
    _services.start_pythia_service(policy_cache)

  def load_prior_study(self, resource_name: str) -> client_abc.StudyInterface:
    """See parent class."""
    return pyvizier_clients.Study.from_resource_name(resource_name)

  @classmethod
  def load_study(
      cls, owner: str, name: ExpandedStudyName
  ) -> client_abc.StudyInterface:
    """See parent class."""
    return pyvizier_clients.Study.from_owner_and_id(owner, name)

  def _configure_algorithm(
      self, study_config: svz.StudyConfig, algorithm: pg.DNAGenerator
  ) -> None:
    """Configure algorithm for a study."""
    if isinstance(algorithm, algorithms.BuiltinAlgorithm):
      study_config.algorithm = algorithm.name
    else:
      study_config.algorithm = 'EXTERNAL_PYTHIA_SERVICE'
    study_config.pythia_endpoint = _services.pythia_endpoint

  def create_study(
      self,
      problem: vz.ProblemStatement,
      converter: converters.VizierConverter,
      owner: str,
      name: str,
      algorithm: pg.DNAGenerator,
      stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = None,
  ) -> client_abc.StudyInterface:
    """See parent class."""
    study_config = svz.StudyConfig.from_problem(problem)
    if converter.vizier_conversion_error:
      study_config.observation_noise = svz.ObservationNoise.HIGH
    self._configure_algorithm(study_config, algorithm)
    logging.info(
        'Created OSS Vizier study with owner: %s, name: %s', owner, name
    )
    return pyvizier_clients.Study.from_study_config(
        study_config, owner=owner, study_id=name
    )

  def get_group_id(self, group_id: Union[None, int, str] = None) -> str:
    """See parent class."""
    if group_id is None:
      hostname = os.uname()[1]
      thread_id = threading.get_ident()
      return f'{thread_id}@{hostname}'
    elif isinstance(group_id, int):
      return f'group:{group_id}'
    elif isinstance(group_id, str):
      return group_id  # pytype: disable=bad-return-type

  def ping_tuner(self, tuner_id: str) -> bool:
    # We treat `tuner_id` as the Pythia endpoint.
    assert '@' in tuner_id, tuner_id
    pythia_endpoint = tuner_id.split('@')[1]
    try:
      stubs_util.create_pythia_server_stub(pythia_endpoint, timeout=3).Ping(
          empty_pb2.Empty()
      )
      return True
    except (grpc.RpcError, grpc.FutureTimeoutError):
      return False

  def pythia_supporter(
      self, study: client_abc.StudyInterface
  ) -> pythia.PolicySupporter:
    return service_policy_supporter.ServicePolicySupporter(
        study.resource_name, _services.vizier_service
    )

  def use_pythia_for_study(self, study: client_abc.StudyInterface) -> None:
    pythia_endpoint = _services.pythia_endpoint
    metadata = svz.StudyConfig.pythia_endpoint_metadata(pythia_endpoint)
    study.update_metadata(metadata)


def init(
    study_prefix: Optional[str] = None,
    vizier_endpoint: Optional[str] = None,
    pythia_port: Optional[int] = None,
) -> None:
  """Init OSS Vizier backend.

  Args:
    study_prefix: An optional string that will be used as the prefix for the
      study names created by `pg.sample` throughout the application. This allows
      users to change the study names across multiple runs of the same binary
      through this single venue, instead of modifying the `name` argument of
      every `pg.sample` invocation.
    vizier_endpoint: An optional string in format of <hostname>:<port>, as the
      Vizier service address to connect to. If None, an in-process Vizier
      service will be created for local tuning scenarios.
    pythia_port: An optional port used for hosting the Pythia service. If None,
      the port will be automatically picked.
  """
  _services.use_vizier_service(vizier_endpoint)
  _services.set_pythia_port(pythia_port)
  backend.VizierBackend.use_study_prefix(study_prefix)
  pg.tuning.set_default_backend('oss_vizier')


@pg.tuning.add_backend('oss_vizier')
class OSSVizierBackend(backend.VizierBackend):
  """PyGlove backend that uses OSS Vizier."""

  tuner_cls = _OSSVizierTuner


--- vizier/_src/pyglove/oss_vizier_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for pyglove.tuner.vizier2.oss_vizier_test."""
import datetime
import os

from absl import logging
from vizier._src.pyglove import oss_vizier as vizier
from vizier._src.pyglove import vizier_test_lib
from vizier._src.service import constants
from vizier._src.service import vizier_server

from absl.testing import absltest


class OSSVizierSampleTest(vizier_test_lib.SampleTest):

  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    server = vizier_server.DefaultVizierServer(
        host=os.uname()[1],
        database_url=constants.SQL_MEMORY_URL,
        early_stop_recycle_period=datetime.timedelta(seconds=0.0),
    )
    logging.info(server.endpoint)
    vizier._services.reset_for_testing()
    vizier.init(vizier_endpoint=server.endpoint)
    cls.server = server
    logging.info('Vizier service has been set up!')

  def __init__(self, *args, **kwargs):
    super().__init__(
        vizier.OSSVizierBackend,
        *args,
        builtin_multiobjective_algorithm_to_test='NSGA2',
        **kwargs,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/performance_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Large-scale stress tests (multiple workers, multithreading) for PyGlove-Vizier integration."""
import multiprocessing.pool
import os
import random
import time
from absl import logging
import pyglove as pg

from vizier._src.pyglove import oss_vizier as vizier
from vizier._src.service import constants
from vizier._src.service import vizier_server

from absl.testing import absltest
from absl.testing import parameterized


NUM_TRIALS_PER_WORKER = 10
NUM_WORKERS = 10


class PerformanceTest(parameterized.TestCase):

  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    server = vizier_server.DefaultVizierServer(
        host=os.uname()[1], database_url=constants.SQL_MEMORY_URL
    )
    logging.info(server.endpoint)
    vizier._services.reset_for_testing()
    vizier.init(vizier_endpoint=server.endpoint)
    cls.server = server
    logging.info('Vizier service is set up!')

  @parameterized.parameters(
      (multiprocessing.pool.ThreadPool,),
      # (multiprocessing.Pool,),  # Fails currently.
  )
  def test_multiple_workers(self, pool_creator):
    def work_fn(worker_id: int):
      del worker_id
      algorithm = pg.evolution.regularized_evolution()
      for _, feedback in pg.sample(
          pg.Dict(x=pg.oneof([1, 2, 3])),
          algorithm=algorithm,
          num_examples=NUM_TRIALS_PER_WORKER,
          name='performance_testing',
      ):
        feedback(reward=random.random())

    with pool_creator(NUM_WORKERS) as pool:
      start = time.time()
      pool.map(work_fn, range(NUM_WORKERS))
      end = time.time()

    logging.info(
        'For %d workers to evaluate %d trials each, it took %f seconds total.',
        NUM_WORKERS,
        NUM_TRIALS_PER_WORKER,
        end - start,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/pythia.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""TunerPolicy."""

from typing import Optional, Sequence, cast

from absl import logging
import attr
import pyglove as pg
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies import trial_caches
from vizier._src.pyglove import constants
from vizier._src.pyglove import converters
from vizier._src.pyglove import core


@attr.define
class TunerPolicy(pythia.Policy):
  """Pythia policy for custom multi-trial tuner algorithm.

  Note that study_config should be used if the user needs the Trials to
  faithfully use the parameter names from the original StudyConfig.
  """

  supporter: pythia.PolicySupporter = attr.field()
  _converter: converters.VizierConverter = attr.field()
  algorithm: pg.geno.DNAGenerator = attr.field()
  early_stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = attr.field(
      default=None
  )

  def __attrs_post_init__(self):
    # Initialize the caches. Unlike Vizier's Pythia service implementation,
    # Pyglove's pythia service maintains a single Policy instance for each
    # study (unless the binary crashes). We can therefore use an in-ram cache
    # and avoid re-loading the same trials over and over again.
    self._suggestion_cache = trial_caches.IdDeduplicatingTrialLoader(
        self.supporter, include_intermediate_measurements=False
    )
    self._stopping_cache = trial_caches.IdDeduplicatingTrialLoader(
        self.supporter, include_intermediate_measurements=True
    )

  @property
  def _metric_names(self) -> Sequence[str]:
    return self._converter.metrics_to_optimize

  def _update(self, tuner_trial: pg.tuning.Trial) -> bool:
    """Update a single tuner Trial.

    Args:
      tuner_trial: If the trial id was previously seen, update is no-op.

    Returns:
      True if the trial was added.
    """
    logging.info(
        'Updating TunerTrial id=%s to algorithm: %s',
        tuner_trial.id,
        self.algorithm,
    )
    reward = tuner_trial.get_reward_for_feedback(self._metric_names)
    if reward is not None:
      self.algorithm.feedback(tuner_trial.dna, reward)
      return True
    return False

  def suggest(self, request: pythia.SuggestRequest) -> pythia.SuggestDecision:
    logging.info('TunerPolicy:suggest started')
    newly_completed_trials = self._suggestion_cache.get_newly_completed_trials(
        request.max_trial_id
    )
    n_trials_updated = 0
    metadata_updates = vz.MetadataDelta()
    for vizier_trial in newly_completed_trials:
      tuner_trial = core.VizierTrial(self._converter, vizier_trial)
      metadata = dict(tuner_trial.dna.metadata)  # make a deep copy.
      if self._update(tuner_trial):
        n_trials_updated += 1
        # Serialize DNA metadata to Vizier trial metadata upon change.
        if pg.ne(tuner_trial.dna.metadata, metadata):
          metadata_updates.assign(
              namespace=constants.METADATA_NAMESPACE,
              key=constants.TRIAL_METADATA_KEY_DNA_METADATA,
              value=pg.to_json_str(tuner_trial.dna.metadata),
              trial=vizier_trial,
          )

    new_trials = []
    count = request.count or 1

    logging.info(
        (
            'The algorithm is updated with %s new trials out of %s newly'
            ' completed trials. It has seen %s trials total. Now generating %s'
            ' new suggestions.'
        ),
        n_trials_updated,
        len(newly_completed_trials),
        self._suggestion_cache.num_incorporated_trials,
        count,
    )
    for _ in range(request.count or 1):
      try:
        dna = self.algorithm.propose()
      except StopIteration:
        logging.info(
            'Search algorithm %s has no new suggestions.', self.algorithm
        )
        break

      if dna.spec is None:
        dna.use_spec(self._converter.dna_spec)
      trial = self._converter.to_trial(dna, fallback='return_dummy')
      new_trials.append(trial)
      logging.info(
          'algorithm : %s proposed DNA: %s which is translated to trial: %s',
          self.algorithm,
          repr(dna),
          trial,
      )

    logging.info('TunerPolicy:suggest ended')
    return pythia.SuggestDecision(new_trials, metadata_updates)

  def early_stop(
      self, request: pythia.EarlyStopRequest
  ) -> pythia.EarlyStopDecisions:
    if self.early_stopping_policy is None:
      return pythia.EarlyStopDecisions()

    early_stopping_policy = cast(
        pg.tuning.EarlyStoppingPolicy, self.early_stopping_policy
    )

    newly_completed_trials = self._stopping_cache.get_newly_completed_trials(
        request.max_trial_id
    )
    for vizier_trial in newly_completed_trials:
      tuner_trial = core.VizierTrial(self._converter, vizier_trial)
      # For completed trials, we do not need the actual stopping decision but
      # we still have to feed them in and update the stopping policy's state.
      early_stopping_policy.should_stop_early(tuner_trial)

    active_trials = self._stopping_cache.get_active_trials()

    decisions = pythia.EarlyStopDecisions()
    for vizier_trial in active_trials:
      tuner_trial = core.VizierTrial(self._converter, vizier_trial)
      should_stop = early_stopping_policy.should_stop_early(tuner_trial)
      decisions.decisions.append(
          pythia.EarlyStopDecision(
              vizier_trial.id,
              should_stop=should_stop,
              reason='Pyglove stopping policy stopped the trial.',
          )
      )
    return decisions


def create_policy(
    supporter: pythia.PolicySupporter,
    problem_statement: vz.ProblemStatement,
    algorithm: pg.geno.DNAGenerator,
    early_stopping_policy: Optional[pg.tuning.EarlyStoppingPolicy] = None,
    prior_trials: Optional[Sequence[vz.Trial]] = None,
) -> pythia.Policy:
  """Creates a Pythia policy that uses PyGlove algorithms."""
  converter = converters.VizierConverter.from_problem(problem_statement)

  # Bind the algorithm with the search space before usage.
  algorithm.setup(converter.dna_spec)

  # Warm up algorithm if prior trials are present.
  if prior_trials:

    def get_trial_history():
      for trial in prior_trials:
        tuner_trial = core.VizierTrial(converter, trial)
        reward = tuner_trial.get_reward_for_feedback(
            converter.metrics_to_optimize
        )
        yield (tuner_trial.dna, reward)

    algorithm.recover(get_trial_history())

  return TunerPolicy(supporter, converter, algorithm, early_stopping_policy)


--- vizier/_src/pyglove/pythia_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for pythia."""

import functools

import mock
import pyglove as pg
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.testing import test_runners
from vizier._src.pyglove import converters
from vizier._src.pyglove import pythia as pg_pythia
from vizier._src.pyglove import vizier_test_lib

from absl.testing import absltest

_RandomAlgorithm = vizier_test_lib.RandomAlgorithm


class TunerPolicyTest(absltest.TestCase):

  def test_stopping_policy(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('x', 0.0, 1.0)
    problem.metric_information.append(
        vz.MetricInformation(name='r', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
    )

    supporter = pythia.InRamPolicySupporter(problem)

    m = mock.create_autospec(pg.tuning.EarlyStoppingPolicy, instance=True)
    policy = pg_pythia.create_policy(
        supporter,
        problem,
        algorithm=_RandomAlgorithm(),
        early_stopping_policy=m,
    )

    supporter.AddTrials([
        vz.Trial(
            parameters={'x': 0.0},
            final_measurement=vz.Measurement({'r': 1.0}),
        ),
        vz.Trial(
            parameters={'x': 0.0},
            final_measurement=vz.Measurement({'r': 1.0}),
        ),
        vz.Trial(
            parameters={'x': 0.0},
            final_measurement=vz.Measurement({'r': 1.0}),
        ),
        vz.Trial(
            parameters={'x': 0.0},
            measurements=[vz.Measurement({'r': 1.0})],
        ),
    ])

    descriptor = supporter.study_descriptor()

    m.should_stop_early.return_value = False
    res = policy.early_stop(
        pythia.EarlyStopRequest(study_descriptor=descriptor, trial_ids=[4])
    )
    self.assertFalse(res.decisions[0].should_stop)

    m.should_stop_early.return_value = True
    res = policy.early_stop(
        pythia.EarlyStopRequest(study_descriptor=descriptor, trial_ids=[4])
    )
    self.assertTrue(res.decisions[0].should_stop)
    self.assertEqual(m.should_stop_early.call_count, 5)  # 3 + 1 + 1

  def test_random_algorithm_on_simple_search_space(self):
    """geno.Random wrapped into TunerPolicy should generate valid trials."""
    # Get a DNA spec.
    rewards = []

    def foo():
      r = pg.oneof([1, 2]) + pg.floatv(3., 4.)
      branch = pg.oneof(['negate', 'no-op'])
      if branch == 'negate':
        r = -r
      rewards.append(r)
      return r

    search_space = pg.hyper.DynamicEvaluationContext()
    with search_space.collect():
      foo()

    # Create a vizier converter.
    converter = converters.VizierConverter.from_dna_spec(
        search_space.dna_spec, ('',)
    )

    # Create pyglove algorithm.
    algorithm = _RandomAlgorithm()
    algorithm.setup(search_space.dna_spec)
    policy_factory = functools.partial(
        pg_pythia.TunerPolicy,
        converter=converter,
        algorithm=algorithm,
    )
    # Test the policy.
    tester = test_runners.RandomMetricsRunner(
        converter.problem,
        batch_size=5,
        validate_parameters=True,
    )
    tester.run_policy(policy_factory)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyglove/vizier_test_lib.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Libraries for testing different Tuners on the shared backend."""
# pylint:disable=protected-access
# pylint:disable=invalid-name
import inspect
import threading
from typing import Type

from absl import flags
from absl import logging
import pyglove as pg
from vizier._src.pyglove import backend

from absl.testing import absltest

FLAGS = flags.FLAGS


class RandomAlgorithm(pg.DNAGenerator):
  """Dummy algorithm for testing."""

  def setup(self, dna_spec: pg.DNASpec):
    super().setup(dna_spec)
    self._random = pg.geno.Random(seed=1)
    self._random.setup(dna_spec)

  def _propose(self) -> pg.DNA:
    return self._random.propose()


@pg.members([
    ('max_trial_id', pg.typing.Int()),
    ('max_step', pg.typing.Int()),
])
class SizeLimitingStoppingPolicy(pg.tuning.EarlyStoppingPolicy):
  """Policy that stops trials when either ID or step is larger."""

  def _on_bound(self):
    super()._on_bound()
    self.requested_trial_steps = []
    self.stopped_trial_steps = []

  def should_stop_early(self, trial: pg.tuning.Trial) -> bool:
    if trial.status == 'COMPLETED':
      return False

    measurement = trial.measurements[-1]
    self.requested_trial_steps.append((trial.id, measurement.step))
    should_stop = (
        trial.id > self.max_trial_id or measurement.step > self.max_step
    )
    if should_stop:
      self.stopped_trial_steps.append((trial.id, measurement.step))
    return should_stop


class VizierTest(absltest.TestCase):
  """Base class for Vizier-based tests."""

  def __init__(
      self,
      backend_class: Type[backend.VizierBackend], *args, **kwargs
  ):
    self._backend_class = backend_class
    super().__init__(*args, **kwargs)


class SampleTest(VizierTest):
  """Tests for PyGlove sampling with Vizier backend."""

  def setUp(self):
    super().setUp()
    self._backend_class.use_study_prefix(self.id())

  def __init__(
      self,
      backend_class: Type[backend.VizierBackend],
      *args,
      builtin_multiobjective_algorithm_to_test: str,
      **kwargs):
    self._builtin_multiobjective_algorithm_to_test = (
        builtin_multiobjective_algorithm_to_test)
    super().__init__(backend_class, *args, **kwargs)

  def testSample(self):
    """Test `pg.sample` with Vizier backend."""
    self._backend_class.use_study_prefix('distributed_sampling')

    rewards_a = []
    algorithm = RandomAlgorithm()
    for a, fa in pg.sample(
        pg.Dict(x=pg.oneof([1, 2, 3])),  # Outer search space.
        algorithm=algorithm,
        num_examples=2,
        name='a',
    ):
      rs = []
      for b, fb in pg.sample(
          pg.Dict(y=pg.oneof([3, 4, 5, 6, 7])),  # Inner space.
          algorithm=backend.BuiltinAlgorithm('DEFAULT'),
          num_examples=3,
          name='a%d_b' % fa.id,
      ):
        logging.info('Sampling a=%s, fa=%s, b=%s, fb=%s', a, fa, b, fb)
        r = a.x + b.y
        fb(r)
        rs.append(r)
      ra = sum(rs) / len(rs)
      fa(ra)
      rewards_a.append(ra)

    # Check alogrithm state are updated.
    self.assertEqual(algorithm.num_proposals, 2)

    # Check studies are created.
    logging.info('Check studies.')
    self._backend_class._get_study_resource_name('distributed_sampling.a')
    self._backend_class._get_study_resource_name('distributed_sampling.a1_b')
    self._backend_class._get_study_resource_name('distributed_sampling.a2_b')

    # Check tuning results from each study.
    logging.info('Check tuning results from each study.')
    trials_a = pg.tuning.poll_result('a').trials
    self.assertLen(trials_a, 2)
    self.assertEqual([t.final_measurement.reward for t in trials_a], rewards_a)
    self.assertLen(pg.tuning.poll_result('a1_b').trials, 3)
    self.assertLen(pg.tuning.poll_result('a2_b').trials, 3)

    logging.info('Check that constant space raises an error.')
    # Non-deterministic value.
    with self.assertRaisesRegex(ValueError, "'space' is a constant value"):
      next(
          pg.sample(
              pg.Dict(x=1),  # a fixed value.
              algorithm=pg.geno.Random(seed=1),
              name='c',
          )
      )

  @absltest.skip('Pythia cannot accept empty trials.')
  def testSampleWithSearchAlgorithmStopIteration(self):
    """Test `pg.sample` with algorithm-side stop iteration."""
    self._backend_class.use_study_prefix('algorithm-stop-iteration')
    for sd, f in pg.sample(
        pg.Dict(x=pg.oneof(range(3)), y=pg.oneof(range(4))), pg.geno.Sweeping()
    ):
      f(sd.x + sd.y)
    self.assertLen(pg.tuning.poll_result('').trials, 3 * 4)

    # Continue from previous one.
    # We create a new Sweeping instance to test recovery of controller state.
    for _, _ in pg.sample(
        pg.Dict(x=pg.oneof(range(3)), y=pg.oneof(range(4))), pg.geno.Sweeping()
    ):
      assert False, 'should never happen'

    self.assertLen(pg.tuning.poll_result('').trials, 3 * 4)

  def testSampleWithRaceCondition(self):
    """Test `pg.sample` with race condition among co-workers."""
    self._backend_class.use_study_prefix('coworkers-with-race-conditions')
    _, f = next(pg.sample(pg.oneof([1, 2, 3]), pg.geno.Random(seed=1)))

    f(1)
    with self.assertRaisesRegex(
        pg.tuning.RaceConditionError,
        '.*Measurements can only be added to.*',
    ):
      f.add_measurement(0.1)

    with f.ignore_race_condition():
      f.add_measurement(0.1)

  def testSampleWithControllerSideUpdateOfDNAMetadata(self):
    """Test `pg.sample` with controller-side DNA metadata update."""
    self._backend_class.use_study_prefix('sampling_with_evolution')

    def write_metadata(dna_list):
      return [dna.set_metadata('foo', 1) for dna in dna_list]

    algorithm = pg.evolution.Evolution(
        (
            pg.evolution.Identity()[-1]
            >> pg.evolution.mutators.Uniform()
            >> write_metadata
        ),
        population_init=(pg.geno.Random(), 5),
    )

    for i, (x, fx) in enumerate(
        pg.sample(
            pg.oneof(range(100)),
            algorithm=algorithm,
            num_examples=10,
        )
    ):
      if i >= 5:
        self.assertEqual(fx.dna.metadata.foo, 1)
      fx(x)

    # Check alogrithm state are updated.
    self.assertEqual(algorithm.num_proposals, 10)

    # The feedback operation on the last trial is not yet called, since
    # it will be triggered by the next call to `GetNewSuggestions`.
    self.assertEqual(algorithm.num_feedbacks, 9)

    # Check studies are created.
    self._backend_class._get_study_resource_name('sampling_with_evolution')

    # Check the DNA metadata for each trial.
    trials = pg.tuning.poll_result('').trials
    self.assertLen(trials, 10)
    for i, t in enumerate(trials[:-1]):
      expected_metadata = {
          'proposal_id': i + 1,
          'generation_id': max(i - 5 + 1, 0) + 1,
          'feedback_sequence_number': i + 1,
          'initial_population': i < 5,
          'reward': float(t.dna.value),
      }
      if i >= 5:
        expected_metadata['foo'] = 1
      self.assertEqual(t.dna.metadata, expected_metadata)

    self.assertEqual(
        trials[-1].dna.metadata,
        {
            'proposal_id': 10,
            'generation_id': 6,
            'initial_population': False,
            'foo': 1,
        },
    )

  def testSamplingWithControllerSideEvaluation(self):
    """Test sampling with controller evaluated reward."""
    self._backend_class.use_study_prefix(
        'sampling_with_controller_side_evaluation'
    )

    def controller_side_evaluate(dna_list):
      return [pg.evolution.set_fitness(dna, 1.0) for dna in dna_list]

    algorithm = pg.evolution.Evolution(
        (
            pg.evolution.Identity()[-1]
            >> pg.evolution.mutators.Uniform()
            >> controller_side_evaluate
        ),
        population_init=(pg.geno.Sweeping(), 5),
    )

    client_side_evaluated = []
    for x, fx in pg.sample(
        pg.oneof(range(100)), algorithm=algorithm, num_examples=10
    ):
      client_side_evaluated.append(x)
      fx(x)

    # Check the DNA metadata for each trial.
    trials = pg.tuning.poll_result('').trials
    self.assertLen(trials, 10)
    for i, t in enumerate(trials[:-1]):
      self.assertEqual(
          t.dna.metadata,
          {
              'proposal_id': i + 1,
              'generation_id': max(i - 5 + 1, 0) + 1,
              'feedback_sequence_number': i + 1,
              'initial_population': i < 5,
              'reward': float(t.dna.value) if i < 5 else 1.0,
          },
          msg=f'for i={i}, t={t}',
      )

    # Check alogrithm state are updated.
    self.assertEqual(algorithm.num_proposals, 10)

    # The feedback operation on the last trial is not yet called, since
    # it will be triggered by the next call to `GetNewSuggestions`.
    self.assertEqual(algorithm.num_feedbacks, 9)

    # Check studies are created.
    self._backend_class._get_study_resource_name(
        'sampling_with_controller_side_evaluation'
    )

    # Only the initial population is evaluated at client side.
    self.assertEqual(client_side_evaluated, list(range(5)))

  def testSamplingWithMultiObjectiveAlgorithm(self):
    """Test sampling with multi-objective alogirthm."""
    # Test sampling with Vizier built-in algorithm.
    self._backend_class.use_study_prefix('multi-objective-sampling-vizier')
    sample1 = pg.sample(
        pg.oneof([1, 2]),
        backend.BuiltinAlgorithm(
            self._builtin_multiobjective_algorithm_to_test),
        metrics_to_optimize=['accuracy', 'latency'],
    )
    _, f = next(sample1)
    f(metrics={'accuracy': 0.9, 'latency': 0.5})

    class DummyMultiObjectiveAlgorithm(pg.DNAGenerator):
      """Test sampling with custom multi-objective algorithm."""

      @property
      def multi_objective(self):
        return True

      def setup(self, dna_spec):
        super().setup(dna_spec)
        self.rewards = []

      def _propose(self):
        return pg.DNA(1)

      def _feedback(self, dna, reward):
        self.rewards.append(reward)

    self._backend_class.use_study_prefix('multi-objective-sampling-custom')
    algo = DummyMultiObjectiveAlgorithm()
    sample2 = pg.sample(
        pg.oneof([1, 2]),
        algo,
        metrics_to_optimize=['reward', 'accuracy', 'latency'],
    )

    _, f = next(sample2)
    f(reward=0.0, metrics={'accuracy': 0.9, 'latency': 0.5})

    # In Vizier, feedback is done while the next example is requested.
    # Therefore, we call next sample to trigger the feedback call to the
    # first example.
    _, _ = next(sample2)
    self.assertEqual(algo.rewards, [(0.0, 0.9, 0.5)])

  def testSampleWithCustomTermination(self):
    """Test `pg.sample` with custom termination."""
    self._backend_class.use_study_prefix(None)
    hyper_value = pg.Dict(x=pg.oneof([1, 2, 3]))
    for x, f in pg.sample(
        hyper_value,
        algorithm=pg.geno.Random(seed=1),
        name='custom_termination',
    ):
      # Always invoke the feedback function in order to advance
      # to the next trial.
      if f.id == 1:
        f.skip()
      else:
        f.set_metadata('x', 'foo')
        f.set_metadata('y', True, per_trial=False)
        f.add_link('filepath', '/file/old_path')
        f(
            x.x,
            metrics={'accuracy': 0.9},
            metadata={'z': 'bar'},
            related_links={'filepath': '/file/path_%d' % f.id},
        )
      if f.id == 2:
        f.end_loop()
        logging.info('Ending loop')
        break

    result = pg.tuning.poll_result('custom_termination')
    self.assertFalse(result.is_active)
    self.assertEqual(result.metadata, {'y': True})
    self.assertLen(result.trials, 2)
    self.assertTrue(result.trials[0].infeasible)
    self.assertFalse(result.trials[1].infeasible)
    self.assertEqual(result.trials[1].measurements[0].reward, 3.0)
    self.assertEqual(
        result.trials[1].measurements[0].metrics,
        {
            # NOTE(daiyip): Vizier adds the reward to the metric with an empty
            # metric name.
            'reward': 3.0,
            'accuracy': 0.9,
        },
    )
    self.assertEqual(result.trials[1].metadata, {'x': 'foo', 'z': 'bar'})
    self.assertEqual(
        result.trials[1].related_links, {'filepath': '/file/path_2'}
    )
    self.assertEqual(
        str(result),
        inspect.cleandoc(
            """
        {
          'study': '%s',
          'status': {
            'COMPLETED': '2/2'
          },
          'best_trial': {
            'id': 2,
            'reward': 3.0,
            'step': 0,
            'dna': 'DNA(2)'
          }
        }"""
            % (
                self._backend_class._get_study_resource_name(
                    'custom_termination'
                )
            )
        ),
    )

    t = result.best_trial
    t.description = None
    self.assertEqual(
        str(t),
        inspect.cleandoc(
            """
        VizierTrial(
          id = 2,
          description = None,
          dna = DNA(
            value = 2,
            children = [],
            metadata = {}
          ),
          status = 'COMPLETED',
          final_measurement = Measurement(
            step = 0,
            elapse_secs = %s,
            reward = 3.0,
            metrics = {
              accuracy = 0.9,
              reward = 3.0
            },
            checkpoint_path = None
          ),
          infeasible = False,
          measurements = [
            0 : Measurement(
              step = 0,
              elapse_secs = %s,
              reward = 3.0,
              metrics = {
                accuracy = 0.9,
                reward = 3.0
              },
              checkpoint_path = None
            )
          ],
          metadata = {
            x = 'foo',
            z = 'bar'
          },
          related_links = {
            filepath = '/file/path_2'
          },
          created_time = %d,
          completed_time = %d
        )"""
            % (
                t.final_measurement.elapse_secs,
                t.measurements[0].elapse_secs,
                t.created_time,
                t.completed_time,
            )
        ),
    )

  def testSampleWithEarlyStopping(self):
    """Test sample with early stopping."""
    # Stop trial early if either the trial id or trial-step is greater than 1.
    early_stopping_policy = SizeLimitingStoppingPolicy(
        max_trial_id=1, max_step=1
    )
    actually_stopped = []
    for x, f in pg.sample(
        pg.oneof([1, 2, 3]),
        algorithm=RandomAlgorithm(),
        early_stopping_policy=early_stopping_policy,
        num_examples=2,
    ):
      skipped = False
      for i in range(4):
        f.add_measurement(x, step=i)
        # Add a minimal sleep to ensure that the measurement is added to DB.
        if f.should_stop_early():
          f.skip()
          skipped = True
          actually_stopped.append((f.id, i))
          break
      if not skipped:
        f.done()

    self.assertEqual(
        early_stopping_policy.requested_trial_steps,
        [(1, 0), (1, 1), (1, 2), (2, 0)],
    )
    self.assertEqual(
        early_stopping_policy.stopped_trial_steps, actually_stopped
    )

    self.assertEqual(
        early_stopping_policy.stopped_trial_steps, [(1, 2), (2, 0)]
    )

  def testSampleWithMultipleWorkers(self):
    """Test `pg.sample` with multiple workers."""
    self._backend_class.use_study_prefix(None)

    hyper_value = pg.Dict(x=pg.oneof([1, 2, 3]))

    # Create a new study for sample using Random.
    sample1 = pg.sample(
        hyper_value,
        algorithm=pg.geno.Random(seed=1),
        name='distributed_sampling2',
        group=0,
    )

    # `sample2` works on the same sampling queue with `sample1`
    # but with different trials by having the same `name` with
    # a different `group`.
    sample2 = pg.sample(
        hyper_value,
        algorithm=pg.geno.Random(seed=1),
        name='distributed_sampling2',
        group=1,
    )

    # `sample3` works on the same sampling queue and the same trials
    # with `sample1` by having the same `name` and `group`.
    sample3 = pg.sample(
        hyper_value,
        algorithm=pg.geno.Random(seed=1),
        name='distributed_sampling2',
        group=0,
    )

    # Make sure sampling with different worker IDs get different trial IDs.
    _, f1 = next(sample1)
    self.assertEqual(f1.id, 1)
    f1.set_metadata('x', 1)
    f1.set_metadata('y', RandomAlgorithm(), per_trial=False)

    # X is not serializable via `pg.to_json_str()`.
    class X:
      pass

    with self.assertRaisesRegex(
        ValueError, 'Cannot convert local class .* to JSON'
    ):
      f1.set_metadata('z', X)

    _, f2 = next(sample2)
    self.assertEqual(f2.id, 2)
    self.assertIsNone(f2.get_metadata('x'))
    self.assertEqual(f2.get_metadata('y', per_trial=False), RandomAlgorithm())

    _, f3 = next(sample3)
    self.assertEqual(f3.id, 1)
    self.assertEqual(f3.get_metadata('x'), 1)
    self.assertEqual(f3.get_metadata('y', per_trial=False), RandomAlgorithm())
    # Update the value of 'x'.
    f3.set_metadata('x', 2)
    f3.set_metadata('z', 'foo')
    f3.add_measurement(0.1, step=1)

    # Make sure sampling within the same worker get the same trial IDs before
    # feedback.
    _, f1b = next(sample1)
    self.assertEqual(f1b.id, 1)

    # Make sure `f1.trial` reflect the measurement added by f3.
    trial = f1.get_trial()
    self.assertLen(trial.measurements, 1)
    self.assertEqual(
        trial.measurements[0].reward,
        0.1,
        msg=f'Measurement was: {trial.measurements}',
    )

    logging.info('Trial metadata: %s', trial.metadata)
    self.assertEqual(
        trial.metadata.z, 'foo', msg=f'Metadata was: {trial.metadata}'
    )
    # Mark trial 1 as done.
    f1(0.5)
    # Check the value of metadata 'x' is updated (by f3).
    self.assertEqual(f1.get_metadata('x'), 2)
    self.assertEqual(f3.get_trial().status, 'COMPLETED')

    # NOTE(daiyip): trial 1 is now COMPLETED after `f1(0)`, therefore, calling
    # `f1b(reward)` will attempt to add a new measurement to the completed
    # trial, triggering a Vizier error.
    with self.assertRaisesRegex(
        pg.tuning.RaceConditionError,
        '.*Measurements can only be added to.*',
    ):
      f1b(1)

    # Make sure sampling within the same worker get different trial IDs after
    # previous trial is done.
    _, f1c = next(sample1)
    self.assertEqual(f1c.id, 3)
    _, f3b = next(sample3)
    self.assertEqual(f3b.id, 3)

    # After calling `end_loop`, all samplings (even with pending ones)
    # shall stop.
    f2.end_loop()
    with self.assertRaises(StopIteration):
      next(sample1)

    with self.assertRaises(StopIteration):
      next(sample2)

    with self.assertRaises(StopIteration):
      next(sample3)

  def testSampleWithDifferentSearchSpace(self):
    """Test client-side search space mismatch with server-side search space."""
    self._backend_class.use_study_prefix('different_search_space_vizier')

    ssd1 = pg.Dict(x=pg.oneof([1, 2, 3]))
    sample1 = pg.sample(ssd1, pg.geno.Random())
    x, f = next(sample1)
    f(x.x)

    ssd2 = pg.Dict(x=pg.oneof([1, 2]))
    sample2 = pg.sample(ssd2, pg.geno.Random())
    with self.assertRaisesRegex(ValueError, '.*different.*search space.*'):
      next(sample2)

  def testSampleWithCustomHyper(self):
    """Test sample with custom hyper."""
    self._backend_class.use_study_prefix('custom_hyper_vizier')

    class VariableString(pg.hyper.CustomHyper):
      """A custom decision type that represents a variable-length string."""

      def custom_decode(self, dna):
        return dna.value

    @pg.geno.dna_generator
    def init_population(dna_spec):
      yield pg.DNA('abc', spec=dna_spec)

    class StringRepeater(pg.evolution.Mutator):

      def mutate(self, dna):  # type: ignore
        return pg.DNA(dna.value + dna.value, spec=dna.spec)

    algo = pg.evolution.Evolution(
        StringRepeater(),
        population_init=init_population.partial(),
        population_update=pg.evolution.selectors.Last(1),
    )

    sample = pg.sample(VariableString(), algo)
    x, f = next(sample)
    self.assertEqual(x, 'abc')
    f(len(x))

    x, f = next(sample)
    self.assertEqual(x, 'abcabc')
    f(len(x))

    x, f = next(sample)
    self.assertEqual(x, 'abcabcabcabc')
    f(len(x))

  def testAllProposedTrialAreDeliveredToWorkers(self):
    """Test that all proposed trials are delivered to the workers."""
    self._backend_class.use_study_prefix('proposal_delivery')
    searchable_list = pg.List(
        [
            pg.one_of([1, 2, 3]),
            pg.one_of([-1, 0, 1]),
            pg.one_of([1, 2, 0]),
        ]
        * 5
    )
    algorithm = pg.evolution.regularized_evolution()

    def worker_fun():
      for l, f in pg.sample(searchable_list, algorithm, num_examples=100):
        reward = float(sum(l))
        f(reward)

    workers = [threading.Thread(target=worker_fun) for _ in range(10)]
    for w in workers:
      w.start()
    for w in workers:
      w.join()

    result = pg.tuning.poll_result('')
    for t in result.trials:
      self.assertEqual(t.id, t.dna.metadata.proposal_id)


--- vizier/_src/pythia/local_policy_supporters.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Policy supporters that keep data in RAM."""

import copy
import datetime
from typing import Iterable, List, Optional, Sequence
import uuid

from absl import logging
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier._src.pythia import policy
from vizier._src.pythia import policy_supporter
from vizier.pyvizier import converters
from vizier.pyvizier import multimetric


# TODO: Keep the Pareto frontier trials.
@attr.s(frozen=False, init=True, slots=True)
class InRamPolicySupporter(policy_supporter.PolicySupporter):
  """Runs a fresh Study in RAM using a Policy.

  InRamPolicySupporter acts as a limited vizier service + client that runs in
  RAM. Trials can only be added and never removed.

  Example of using a policy to run a Study for 100 iterations, 1 trial each:
    runner = InRamPolicySupporter(my_study_config)
    policy = MyPolicy(runner)
    for _ in range(100):
      trials = runner.SuggestTrials(policy, count=1)
      if not trials:
        break
      for t in trials:
        t.complete(vz.Measurement(
            {'my_objective': my_objective(t)}, inplace=True))

  Attributes:
    study_config: Study config.
    study_guid: Unique identifier for the current study.
  """

  study_config: vz.ProblemStatement = attr.ib(
      init=True,
      validator=attr.validators.instance_of(vz.ProblemStatement),
      on_setattr=attr.setters.frozen,
  )
  study_guid: str = attr.ib(
      init=True,
      kw_only=True,
      default='',
      converter=str,
      on_setattr=attr.setters.frozen,
  )
  _trials: dict[int, vz.Trial] = attr.ib(
      init=False, factory=dict, on_setattr=attr.setters.frozen
  )
  # Dictionary of study_guid to ProblemAndTrials.
  prior_studies: dict[str, vz.ProblemAndTrials] = attr.ib(
      init=False, factory=dict
  )

  def __str__(self) -> str:
    return (
        f'InRamPolicySupporter(study_guid={self.study_guid},'
        f' num_trials={len(self.trials)})'
    )

  @property
  def trials(self) -> Sequence[vz.Trial]:
    return list(self._trials.values())

  def study_descriptor(self) -> vz.StudyDescriptor:
    return vz.StudyDescriptor(
        self.study_config,
        guid=self.study_guid,
        max_trial_id=max(self._trials.keys()) if self._trials else 0,
    )

  def GetStudyConfig(
      self, study_guid: Optional[str] = None
  ) -> vz.ProblemStatement:
    if study_guid is None or study_guid == self.study_guid:
      return self.study_config
    elif study_guid in self.prior_studies:
      return self.prior_studies[study_guid].problem
    else:
      raise KeyError(f'Study does not exist in InRamSupporter: {study_guid}')

  def GetTrials(
      self,
      *,
      study_guid: Optional[str] = None,
      trial_ids: Optional[Iterable[int]] = None,
      min_trial_id: Optional[int] = None,
      max_trial_id: Optional[int] = None,
      status_matches: Optional[vz.TrialStatus] = None,
      include_intermediate_measurements: bool = True,
  ) -> List[vz.Trial]:
    """Returns trials by reference to allow changing their status and attributes."""
    self.CheckCancelled('GetTrials')
    if study_guid is not None and study_guid != self.study_guid:
      if study_guid not in self.prior_studies:
        raise KeyError(f'Study Guid does not exist {study_guid}')

      candidate_trials = self.prior_studies[study_guid].trials
    else:
      candidate_trials = self.trials

    trial_id_set = None
    if trial_ids is not None:
      trial_id_set = set(trial_ids)
    output: List[vz.Trial] = []
    for t in candidate_trials:
      if status_matches is not None and t.status != status_matches:
        continue
      if min_trial_id is not None and t.id < min_trial_id:
        continue
      if max_trial_id is not None and t.id > max_trial_id:
        continue
      if trial_id_set is not None and t.id not in trial_id_set:
        continue
      # NOTE: we ignore include_intermediate_measurements and always enclude
      # them.  That should be safe, and avoids a nasty conflict with the
      # pass-by-reference philosophy for Trials (you can't delete the
      # intermediate measurements without deleting them everywhere, and you
      # can't copy the trial without breaking desired reference connections).
      output.append(t)
    return output

  def CheckCancelled(self, note: Optional[str] = None) -> None:
    pass

  def TimeRemaining(self) -> datetime.timedelta:
    return datetime.timedelta(seconds=100.0)

  def _UpdateMetadata(self, delta: vz.MetadataDelta) -> None:
    """Assign metadata to trials."""
    for ns in delta.on_study.namespaces():
      self.study_config.metadata.abs_ns(ns).update(delta.on_study.abs_ns(ns))

    for tid, metadatum in delta.on_trials.items():
      if not tid > 0:
        raise ValueError(f'Bad Trial Id: {tid}')
      for ns in metadatum.namespaces():
        self._trials[tid].metadata.abs_ns(ns).update(metadatum.abs_ns(ns))

  # TODO: Return `count` trials for multi-objectives, when
  # `count` exceeds the size of the Pareto frontier.
  def GetBestTrials(self, *, count: Optional[int] = None) -> List[vz.Trial]:
    """Returns optimal trials.

    Single-objective study:
      * If `count` is unset, returns all tied top trials.
      * If `count` is set, returns top `count` trials, breaking ties
           arbitrarily.

    Multi-objective study:
      * If `count` is unset, returns all Pareto optimal trials.
      * If `count` is set, returns up to `count` of Pareto optimal trials that
          are arbitrarily selected.

    Args:
      count: If unset, returns Pareto optimal trials only. If set, returns the
        top "count" trials.

    Returns:
      Best trials.
    """
    if not self.study_config.metric_information.of_type(
        vz.MetricType.OBJECTIVE):
      raise ValueError('Requires at least one objective metric.')

    # Add safety warping and remove safety metrics from conversion.
    safety_checker = multimetric.SafetyChecker(
        self.study_config.metric_information
    )
    warped_trials = safety_checker.warp_unsafe_trials(
        copy.deepcopy(self.trials)
    )
    config_without_safe = copy.deepcopy(self.study_config)
    config_without_safe.metric_information = (
        self.study_config.metric_information.exclude_type(vz.MetricType.SAFETY)
    )
    converter = converters.TrialToArrayConverter.from_study_config(
        config_without_safe,
        flip_sign_for_minimization_metrics=True,
        dtype=np.float32,
    )

    if self.study_config.is_single_objective:
      # Single metric: Sort and take top N.
      count = count or 1  # Defaults to 1.
      labels = converter.to_labels(warped_trials).squeeze()
      sorted_idx = np.argsort(-labels)  # np.argsort sorts in ascending order.
      return list(np.asarray(self.trials)[sorted_idx[:count]])
    else:
      algorithm = multimetric.FastParetoOptimalAlgorithm()
      is_optimal = algorithm.is_pareto_optimal(
          points=converter.to_labels(warped_trials)
      )
      return list(np.asarray(self.trials)[is_optimal][:count])

  def SetPriorStudy(
      self, study: vz.ProblemAndTrials, study_guid: Optional[str] = None
  ) -> str:
    if study_guid is None:
      # Assign study_guid using unique identifier.
      study_guid = f'prior_{uuid.uuid1()}'
      if study_guid in self.prior_studies:
        raise RuntimeError(f'Cannot set unique id: {study_guid}')

    if study_guid in self.prior_studies:
      logging.warning('Prior study already exists with guid %s ', study_guid)

    self.prior_studies[study_guid] = study
    return study_guid

  def AddTrials(self, trials: Sequence[vz.Trial]) -> None:
    """Assigns ids to the trials and add them to the supporter (by reference).

    New IDs are always assigned in increasing order from the max id unless:
      * If an incoming Trial has an ID that matches an ACTIVE Trial, the ACTIVE
    Trial is replaced and the COMPLETED Trial keeps its ID.
      * If an incoming Trial has an ID that matches an COMPLETE Trial, no
    updates are done and there is a warning.

    Args:
      trials: Incoming Trials to add.
    """
    existing_trial_ids = self._trials.keys()
    next_trial_id = max(existing_trial_ids) + 1 if existing_trial_ids else 1
    for trial in trials:
      if trial.id in existing_trial_ids:
        if self._trials[trial.id].status == vz.TrialStatus.ACTIVE:
          self._trials[trial.id] = trial
        elif self._trials[trial.id].status == vz.TrialStatus.COMPLETED:
          logging.warning(
              'COMPLETED Trial %s cannot be overwritten by %s',
              self._trials[trial.id],
              trial,
          )
        continue

      trial.id = next_trial_id
      self._trials[next_trial_id] = trial
      next_trial_id += 1

  def AddSuggestions(
      self, suggestions: Iterable[vz.TrialSuggestion]) -> Sequence[vz.Trial]:
    """Assigns ids to suggestions and add them to the study."""
    trials = []
    for suggestion in suggestions:
      # Assign temporary ids, which will be overwritten by AddTrials() method.
      trials.append(suggestion.to_trial(0))
    self.AddTrials(trials)
    return trials

  def SuggestTrials(self, algorithm: policy.Policy,
                    count: int) -> Sequence[vz.Trial]:
    """Suggest and add new trials."""

    decisions = algorithm.suggest(
        policy.SuggestRequest(
            study_descriptor=self.study_descriptor(), count=count))
    self._UpdateMetadata(decisions.metadata)
    return self.AddSuggestions([
        vz.TrialSuggestion(d.parameters, metadata=d.metadata)
        for d in decisions.suggestions
    ])


--- vizier/_src/pythia/local_policy_supporters_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pythia.base.local_policy_supporters."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.pythia import local_policy_supporters
from absl.testing import absltest
from absl.testing import parameterized

InRamPolicySupporter = local_policy_supporters.InRamPolicySupporter
_GUID = '31'


def _runner_with_10trials():
  runner = InRamPolicySupporter(vz.ProblemStatement(), study_guid=_GUID)
  runner.AddTrials([vz.Trial() for _ in range(10)])
  return runner


class LocalPolicySupportersTest(parameterized.TestCase):

  def test_time_remaining(self):
    runner = _runner_with_10trials()
    runner.TimeRemaining()

  def test_add_and_get_trials(self):
    runner = _runner_with_10trials()
    trials = runner.GetTrials()
    self.assertLen(trials, 10)
    # The 10 trials are assigned ids 1 through 10 automatically.
    self.assertSequenceEqual([t.id for t in trials], range(1, 11))

    # Add 5 more Trials.
    runner = _runner_with_10trials()
    runner.AddTrials([vz.Trial(id=-1) for _ in range(5)])
    trials = runner.GetTrials()
    self.assertLen(trials, 15)
    self.assertSequenceEqual([t.id for t in trials], range(1, 16))

  def test_add_completed_trials(self):
    runner = _runner_with_10trials()
    trials = runner.GetTrials()

    trial = trials[0]
    trial.complete(vz.Measurement(), infeasibility_reason='test')
    runner.AddTrials([trial])
    # Second call should be a no-op.
    runner.AddTrials([trial])

    trials = runner.GetTrials()
    self.assertLen(trials, 10)
    # The 10 trials are assigned ids 1 through 10 automatically.
    self.assertSequenceEqual([t.id for t in trials], range(1, 11))
    self.assertEqual(trials[0].infeasibility_reason, 'test')

    trials = runner.GetTrials(status_matches=vz.TrialStatus.ACTIVE)
    self.assertLen(trials, 9)

  def test_prior_studies(self):
    runner = _runner_with_10trials()

    # Add a PriorStudy with study guid 1 and 10 completed Trials.
    prior_runner = _runner_with_10trials()
    completed_trials = []
    for t in prior_runner.GetTrials():
      t.complete(vz.Measurement())
      completed_trials.append(t)
    study = vz.ProblemAndTrials(
        problem=vz.ProblemStatement(), trials=completed_trials
    )
    study_guid = runner.SetPriorStudy(study, study_guid='1')
    self.assertEqual(study_guid, '1')

    # Add another PriorStudy with generated study guid and it has
    # 5 active and completed Trials
    prior_runner = _runner_with_10trials()
    trials = []
    for idx, t in enumerate(prior_runner.GetTrials()):
      if idx > 4:
        t.complete(vz.Measurement())
      trials.append(t)
    study = vz.ProblemAndTrials(problem=vz.ProblemStatement(), trials=trials)
    generated_study_guid = runner.SetPriorStudy(study)

    active_trials = runner.GetTrials(
        study_guid=study_guid, status_matches=vz.TrialStatus.ACTIVE
    )
    self.assertEmpty(active_trials)
    completed_trials = runner.GetTrials(
        study_guid=study_guid, status_matches=vz.TrialStatus.COMPLETED
    )
    self.assertLen(completed_trials, 10)

    active_trials = runner.GetTrials(
        study_guid=generated_study_guid, status_matches=vz.TrialStatus.ACTIVE
    )
    self.assertLen(active_trials, 5)
    completed_trials = runner.GetTrials(
        study_guid=generated_study_guid, status_matches=vz.TrialStatus.COMPLETED
    )
    self.assertLen(completed_trials, 5)

    with self.assertRaises(KeyError):
      runner.GetTrials(study_guid='2')

  def test_push_metadata(self):
    runner = _runner_with_10trials()
    trial1 = runner.GetTrials(min_trial_id=1, max_trial_id=1)[0]

    mu = vz.MetadataDelta()
    mu.assign('ns', 'key', 'value')
    mu.assign('ns', 'key', 'value', trial_id=1)
    # Metadata update is not immediate.
    self.assertEmpty(runner.GetStudyConfig(study_guid=_GUID).metadata.ns('ns'))
    self.assertEmpty(trial1.metadata)
    runner._UpdateMetadata(mu)

    self.assertEqual(
        runner.GetStudyConfig(study_guid=_GUID).metadata.ns('ns').get('key'),
        'value')
    trial0 = runner.GetTrials(min_trial_id=1, max_trial_id=1)[0]
    self.assertCountEqual(trial0.metadata.ns('ns'), {'key': 'value'})

  # TODO: Need a test for LocalPolicySupporter.SuggestTrials().


class LocalPolicySupportersGetBestTrialsTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(goal=vz.ObjectiveMetricGoal.MAXIMIZE, count=2, best_values=[9, 8]),
      dict(goal=vz.ObjectiveMetricGoal.MINIMIZE, count=2, best_values=[0, 1]),
      dict(goal=vz.ObjectiveMetricGoal.MINIMIZE, count=None, best_values=[
          0,
      ]))
  def test_get_best_trials_single_objective(self, goal, count, best_values):
    runner = InRamPolicySupporter(
        vz.ProblemStatement(
            vz.SearchSpace(),
            metric_information=vz.MetricsConfig(
                [vz.MetricInformation(name='objective', goal=goal)])))

    trials = [
        vz.Trial().complete(vz.Measurement({'objective': i})) for i in range(10)
    ]
    np.random.shuffle(trials)
    runner.AddTrials(trials)

    objectives = np.array([
        t.final_measurement_or_die.metrics['objective'].value
        for t in runner.GetBestTrials(count=count)
    ])
    np.testing.assert_array_equal(objectives, np.array(best_values))

  @parameterized.parameters(
      dict(
          sin_goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          cos_goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          best_r=5),
      dict(
          sin_goal=vz.ObjectiveMetricGoal.MINIMIZE,
          cos_goal=vz.ObjectiveMetricGoal.MINIMIZE,
          best_r=1))
  def test_get_best_trials_multi_objective(self, sin_goal, cos_goal, best_r):
    runner = InRamPolicySupporter(
        vz.ProblemStatement(
            vz.SearchSpace(),
            metric_information=vz.MetricsConfig([
                vz.MetricInformation(name='sin', goal=sin_goal),
                vz.MetricInformation(name='cos', goal=cos_goal)
            ])))

    def build_measurement(r, theta):
      return vz.Measurement({
          'r': r,
          'theta': theta,
          'cos': r * np.cos(theta),
          'sin': r * np.sin(theta)
      })

    # Generate many trials for each radius.
    for r in range(1, 6):
      for theta in np.linspace(0, np.pi / 2, 5):
        runner.AddTrials([vz.Trial().complete(build_measurement(r, theta))])

    # Check the radius.
    rs = np.array([
        t.final_measurement_or_die.metrics['r'].value
        for t in runner.GetBestTrials()
    ])
    self.assertEqual(rs.size, 5)
    np.testing.assert_array_equal(rs, np.ones_like(rs) * best_r)

  @parameterized.parameters(
      dict(
          sin_goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          cos_goal=vz.ObjectiveMetricGoal.MAXIMIZE,
      ),
      dict(
          sin_goal=vz.ObjectiveMetricGoal.MINIMIZE,
          cos_goal=vz.ObjectiveMetricGoal.MINIMIZE,
      ),
  )
  def test_get_best_trials_safe(self, sin_goal, cos_goal):
    runner = InRamPolicySupporter(
        vz.ProblemStatement(
            vz.SearchSpace(),
            metric_information=vz.MetricsConfig([
                vz.MetricInformation(name='sin', goal=sin_goal),
                vz.MetricInformation(
                    name='cos', goal=cos_goal, safety_threshold=0.51
                ),
            ]),
        )
    )

    def build_measurement(r, theta):
      return vz.Measurement({
          'r': r,
          'theta': theta,
          'cos': float(r * np.cos(theta)),
          'sin': float(r * np.sin(theta)),
      })

    # Generate many trials for each radius.
    for r in range(1, 6):
      for theta in np.linspace(0, np.pi / 2, 5):
        runner.AddTrials([vz.Trial().complete(build_measurement(r, theta))])

    # Check the radius.
    best_trials = runner.GetBestTrials()
    self.assertLen(best_trials, 1)
    cosine = best_trials[0].final_measurement_or_die.metrics['cos'].value
    if cos_goal == vz.ObjectiveMetricGoal.MAXIMIZE:
      self.assertGreaterEqual(cosine, 0.51)
    else:
      self.assertLessEqual(cosine, 0.51)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pythia/policy.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Base class for all PythiaPolicies."""

import abc
from typing import Any, FrozenSet, Optional

import attr
from vizier._src.pyvizier.pythia import study
from vizier._src.pyvizier.shared import base_study_config
from vizier._src.pyvizier.shared import trial


def _is_positive(instance: Any, attribute: Any, value: Any):
  del instance, attribute
  if value <= 0:
    raise ValueError(f'value must be positive! given: {value}')


def _not_empty(instance: Any, attribute, value: str):
  del instance, attribute
  if not value:
    raise ValueError(f'value must be not nullable! given: {value}')


@attr.define
class EarlyStopDecision:
  """Stopping decision on a single trial.

  Attributes:
    id: Trial's id.
    reason: Explanations for how `should_stop` value was determined.
    should_stop:
    metadata:
    predicted_final_measurement: This is added to the Trial so that other
      algorithms can treat it as if it is the observed final measurement.
  """
  id: int = attr.ib(
      validator=[attr.validators.instance_of(int), _is_positive],
      on_setattr=attr.setters.validate)
  # TODO: Record this in DB even when `should_stop` == False.
  reason: str = attr.ib(
      validator=[attr.validators.instance_of(str), _not_empty],
      on_setattr=attr.setters.validate,
      converter=str)
  should_stop: bool = attr.ib(
      default=True,
      validator=[attr.validators.instance_of(bool)],
      on_setattr=attr.setters.validate)

  # TODO: Add a proper support for this in the service side.
  # NOTE: As of 2022Q3, the standard deviation field of Metrics in this value
  #   are ignored (i.e. $predicted_final_measurement.metrics[].std).
  predicted_final_measurement: Optional[trial.Measurement] = attr.ib(
      default=None,
      validator=attr.validators.optional(
          attr.validators.instance_of(trial.Measurement)
      ),
      on_setattr=attr.setters.validate,
  )


@attr.define
class EarlyStopDecisions:
  """This is the output of the Policy.early_stop() method.

  Attributes:
    decisions: For some Trials, a decision as to whether it should be stopped.
    metadata: Metadata that's associated with the Study or with already existing
      Trials.
  """

  decisions: list[EarlyStopDecision] = attr.field(
      factory=list,
      validator=attr.validators.deep_iterable(
          attr.validators.instance_of(EarlyStopDecision)),
      converter=list)

  metadata: trial.MetadataDelta = attr.field(
      default=attr.Factory(trial.MetadataDelta),
      validator=attr.validators.instance_of(trial.MetadataDelta),
  )


@attr.define
class EarlyStopRequest:
  """Early stopping request.

  Attributes:
    study_guid:
    trial_ids: Trials to be considered for stopping, or None meaning "all
      Trials". This is a hint; it is allowable to consider stopping more or
      fewer trials.
    study_config:
    checkpoint_dir: If the policy wishes to use a checkpoint, then this is the
      path to find one.
    max_trial_id: max(trial.id for all existing Trials in the Study)
  """
  _study_descriptor: study.StudyDescriptor = attr.field(
      kw_only=True, validator=attr.validators.instance_of(study.StudyDescriptor)
  )

  trial_ids: Optional[FrozenSet[int]] = attr.field(
      default=None,
      validator=lambda x, c, v: x is None or isinstance(x, FrozenSet),
      converter=lambda x: None if x is None else frozenset(x))

  checkpoint_dir: Optional[str] = attr.field(
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)))

  @property
  def study_guid(self) -> str:
    return self._study_descriptor.guid

  @property
  def study_config(self) -> base_study_config.ProblemStatement:
    return self._study_descriptor.config

  @property
  def max_trial_id(self) -> int:
    return self._study_descriptor.max_trial_id


@attr.define(init=True)
class SuggestDecision:
  """This is the output of the Policy.suggestion() method.

  Attributes:
    suggestions: Trials to be suggested to the user.
    metadata: Metadata that's associated with the Study or with already existing
      Trials.
  """

  suggestions: list[trial.TrialSuggestion] = attr.field(
      init=True,
      validator=attr.validators.deep_iterable(
          attr.validators.instance_of(trial.TrialSuggestion)
      ),
      converter=list,
  )

  metadata: trial.MetadataDelta = attr.field(
      init=True,
      default=attr.Factory(trial.MetadataDelta),
      validator=attr.validators.instance_of(trial.MetadataDelta),
  )


@attr.define
class SuggestRequest:
  """Suggestion Request.

  Attributes:
    study_descriptor: information about the Study.
    study_guid: Study id
    count: A recommendation for how many suggestions should be generated.
    study_config:
    checkpoint_dir: (If set) A system-provided directory where the policy can
      store a checkpoint.
    max_trial_id: max(trial.id for all existing Trials in the Study)
  """
  _study_descriptor: study.StudyDescriptor = attr.field(
      validator=attr.validators.instance_of(study.StudyDescriptor),
      on_setattr=attr.setters.frozen,
      kw_only=True,
  )

  count: int = attr.field(
      validator=[attr.validators.instance_of(int), _is_positive],
      on_setattr=attr.setters.validate,
      kw_only=True)

  checkpoint_dir: Optional[str] = attr.field(
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
      on_setattr=attr.setters.validate,
      kw_only=True)

  @property
  def study_config(self) -> base_study_config.ProblemStatement:
    return self._study_descriptor.config

  @property
  def study_guid(self) -> str:
    return str(self._study_descriptor.guid)

  @property
  def max_trial_id(self) -> int:
    return self._study_descriptor.max_trial_id


class Policy(abc.ABC):
  """Interface for Pythia Policy subclasses.

  Most Policy subclasses would wish to take `PolicySupporter` object in the
  `__init__` function. `PolicySupporter` provides an abstraction for how
  `Policy` reads more information about the Study in question beyond the basic
  information available in `SuggestRequest` and `EarlyStopRequest`. It allows
  the `Policy` to be compatible in multiple environments.
  """

  @abc.abstractmethod
  def suggest(self, request: SuggestRequest) -> SuggestDecision:
    """Compute suggestions that Vizier will eventually hand to the user.

    Args:
      request:

    Returns:
      A bundle of TrialSuggestions and MetadataDelta that will be passed on to
        the user.  (See caveats in the SuggestionAnswer proto.)

    Raises:
      TemporaryPythiaError:  Generic retryable error.
      InactivateStudyError:  Raise this to inactivate the Study (non-retryable
        error).
        E.g. if this Policy cannot handle this StudyConfig.
        E.g. if this StudyConfig is somehow invalid.
        E.g. if this no more suggestions will ever be generated.
    """

  @abc.abstractmethod
  def early_stop(self, request: EarlyStopRequest) -> EarlyStopDecisions:
    """Decide which Trials Vizier should stop.

    This returns a list of decisions on on-going Trials.
    Args:
      request:

    Returns:
      Decisions about stopping some Trials and metadata changes.  If no decision
      is reported for a Trial, it's treated as "do not stop".

    Raises:
      TemporaryPythiaError:  Generic retryable error.
      InactivateStudyError: If this Pythia is inappropriate for the StudyConfig.
        (Non-retryable error.)  E.g. raise this if your Policy does not
        support MakeEarlyEarlyStopDecisions().
      CachedPolicyIsStaleError: Causes the computation to be restarted with a
        freshly constructed Policy instance.  It is incorrect to raise
        this on the first use of a Policy; the Study will be inactivated.
    """

  @property
  def name(self) -> str:
    """Returns the policy name. Intended for monitoring purposes only."""
    # Derived classes should override this implementation.
    return __class__.__name__

  @property
  def should_be_cached(self) -> bool:
    """Returns True if it's safe & worthwhile to cache this Policy in RAM.

    This is called after MakeEarlyEarlyStopDecisions() and/or MakeSuggestions().
    If True, the policy may be stored in RAM (at least for a while), and state
    may be preserved for the next time that Study makes it to that Pythia
    server.
    """
    return False


--- vizier/_src/pythia/policy_factory.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Policy Factory API."""
from typing import Protocol

from vizier import pyvizier as vz
from vizier._src.pythia import policy
from vizier._src.pythia import policy_supporter as policy_supporter_lib


class PolicyFactory(Protocol):
  """Protocol (PEP-544) for a Policy Factory."""

  def __call__(
      self,
      problem_statement: vz.ProblemStatement,
      algorithm: str,
      policy_supporter: policy_supporter_lib.PolicySupporter,
      study_name: str,
  ) -> policy.Policy:
    """Creates a Pythia Policy."""


--- vizier/_src/pythia/policy_supporter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""The Policy can use these classes to communicate with Vizier."""

import abc
import datetime
from typing import Iterable, List, Optional

from vizier import pyvizier as vz


class PolicySupporter(abc.ABC):
  """Used by Policy instances to communicate with Vizier."""

  # TODO: Change to GetStudyDescriptor (Maybe: Note that
  #   StudyDescriptor includes $max_trial_id, so it causes many more database
  #   collisions than ProblemStatement, especially if there are lots of
  #   workers.)
  @abc.abstractmethod
  def GetStudyConfig(self, study_guid: str) -> vz.ProblemStatement:
    """Requests a StudyConfig from Vizier.

    This sends a PythiaToVizier.trial_selector packet and waits for the
    response(s).  You can call this multiple times, and it is thread-friendly,
    so you can even overlap calls.

    Args:
      study_guid: The GUID of the study whose StudyConfig you want. Note that
        access control applies.

    Returns:
      The requested StudyConfig proto.

    Raises:
      CancelComputeError: (Do not catch.)
      PythiaProtocolError: (Do not catch.)
      VizierDatabaseError: If the database operation raises an error, e.g. if
        $study_guid refers to a nonexistent or inaccessible study.
    """

  # TODO: Should take `TrialFilter` as input, instead of
  # its fields listed as keyword arguments.
  @abc.abstractmethod
  def GetTrials(
      self,
      *,
      study_guid: Optional[str] = None,
      trial_ids: Optional[Iterable[int]] = None,
      min_trial_id: Optional[int] = None,
      max_trial_id: Optional[int] = None,
      status_matches: Optional[vz.TrialStatus] = None,
      include_intermediate_measurements: bool = True
  ) -> List[vz.Trial]:
    """Requests Trials from Vizier.

    Args:
      study_guid: The GUID of the study to get Trials from. If None, uses the
        current Study.
      trial_ids: a list of Trial id numbers to acquire (if None, allows all
        Trials.)
      min_trial_id: Trials with Trial.id >= min_trial ID are selected, if not
        None.
      max_trial_id: Trials with Trial.id <= min_trial ID are selected, if not
        None.
      status_matches: If not None, filters for Trials where
        Trial.status==status_matches.  Selects all Trials by default.
      include_intermediate_measurements: If True (default), the returned Trials
        will include all intermediate measurements.  If False, PolicySupporter
        _may_ leave the `measurements` field empty in the returned Trials (e.g.
        to optimize speed).

    Note that the $final_measurement field will always be included when
      available, i.e. for COMPLETED Trials.

    Returns:
      Trials obtained from Vizier, in order of increasing Trial ID.
      Each argument selects a subset of the Study's Trials, and the result is
      the intersection of the subsets.

    Raises:
      CancelComputeError: (Do not catch.)
      PythiaProtocolError: (Do not catch.)
      VizierDatabaseError: If the database operation raises an error, e.g. if
        $study_guid refers to a nonexistent or inaccessible study.
    """

  @property
  @abc.abstractmethod
  def study_guid(self) -> str:
    """Default study GUID."""

  def CheckCancelled(self, note: Optional[str] = None) -> None:
    """Throws a CancelComputeError on timeout or if Vizier cancels.

    This should be called occasionally by any long-running computation.
    Raises an exception if the interaction has been cancelled by the Vizier
    side of the protocol; the exception shuts down the Pythia server.

    Args:
      note: for debugging.

    Raises:
      CancelComputeError: (Do not catch.)
    """
    pass

  def TimeRemaining(self) -> datetime.timedelta:
    """The time remaining to compute a result.

    Returns:
      The remaining time before the RPC is considered to have timed out; it
      returns datetime.timedelta.max if no deadline was specified in the RPC.

    This is an alternative to calling CheckCancelled(); both have the goal of
    terminating runaway computations.  If your computation times out,
    you should raise TemporaryPythiaError (if you want a retry) or
    InactivateStudyError (if not).
    """
    return datetime.timedelta(hours=1.0)


--- vizier/_src/pythia/pythia_errors.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Exceptions used in the PythiaServer and Pythia Policy code."""


class PythiaError(Exception):
  """Abstract superclass of Pythia-related errors."""


class TemporaryPythiaError(PythiaError):
  """Vizier should attempt this interaction with a different Pythia task.

  Policy code should raise this to terminate and retry its
  interaction with Vizier.  Maps to an ABORTED gRPC error code.

  Use case: Catch-all errors where there's some hope that a re-run will succeed
    (e.g. OOM, timeouts, snapshot/version problems, or failure of
    nondeterministic algorithms).  If a Policy raises this error, Vizier will
    re-try the computation a modest number of times.
  """


class InactivateStudyError(PythiaError):
  """Pythia cannot handle this Study as configured; inactivate it.

  This terminates the interaction and returns an ARGUMENT_INVALID gRPC error
  code.  It's intended for unrecoverable errors, or misconfigued Studies.
  There will be no re-tries.
  """


class PythiaFallbackError(PythiaError):
  """The algorithm could not handle a StudyConfig corner case: fall back.

  This is typically raised by a special-purpose algorithm.  When raised,
  Vizier should re-try once with a more generic algorithm.
  """


class LoadTooLargeError(PythiaError):
  """Raised when the Pythia server is overly busy.

  If a Policy raises this error, it will be retried a (nearly) infinite number
  of times by the Vizier server, on the assumption that there will eventually
  be a server that's not overly busy.
  """


class CancelComputeError(Exception):
  """Raised by the Pythia server to kill Policy computations.

  This is raised in the Pythia compute thread (it is raised by the
  PolicySupporter object) when Vizier cancels the interaction. Policy code
  may catch this temporarily to do clean-up actions, but should re-raise it.

  Vizier may cancel the interaction when the SuggestionWorker needs to shut down
  (i.e. because it's preempted), or when the Pythia computation runs too long.
  """


class PythiaProtocolError(Exception):
  """There was a problem with the interaction between Vizier and Pythia.

  This implies a bug in the protocol code.  Do not raise or catch this in
  Policy code; please report any occurrences to vizier-team@google.com.
  Maps to an INTERNAL gRPC error code.
  """


class VizierDatabaseError(Exception):
  """Vizier had an error on a database operation.

  This can mean an attempt to access unavailable data, access control, etc.
  """


--- vizier/_src/pythia/singleton_params.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""A helper functionality to handle singleton parameters."""

import copy
import logging
from typing import Sequence

import attrs
from vizier import pyvizier as vz


@attrs.define
class SingletonParameterHandler:
  """A helper class to handle singleton parameters.

  The class allows to remove singleton parameters (i.e. having a single value)
  from the problem statement's search space and from trial, so that designers
  won't have to handle them. In addition, it allows to re-add the singleton
  values back to the trial suggestions, so that the users can access them if
  needed.
  """

  problem: vz.ProblemStatement
  # ----------------------------------------------------------------------------
  # Internal attributes
  # ----------------------------------------------------------------------------
  _singletons: dict[str, vz.ParameterValueTypes] = attrs.field(init=False)
  _stripped_problem: vz.ProblemStatement = attrs.field(init=False)

  @property
  def stripped_problem(self) -> vz.ProblemStatement:
    """Returns the stripped problem."""
    return self._stripped_problem

  def __attrs_post_init__(self):
    logging.info("problem: %s", self.problem)
    self._singletons = self._find_singletons()
    self._stripped_problem = self._strip_problem()

  def _find_singletons(self) -> dict[str, vz.ParameterValueTypes]:
    """Finds the singleton parameters in the problem."""
    singletons = {}
    for param in self.problem.search_space.parameters:
      if param.type == vz.ParameterType.DOUBLE:
        if param.bounds[0] == param.bounds[1]:
          singletons[param.name] = param.bounds[0]
      elif param.type == vz.ParameterType.INTEGER:
        if param.bounds[0] == param.bounds[1]:
          singletons[param.name] = param.bounds[0]
      elif param.type in (
          vz.ParameterType.CATEGORICAL,
          vz.ParameterType.DISCRETE,
      ):
        if len(param.feasible_values) == 1:
          singletons[param.name] = param.feasible_values[0]
      elif param.type == vz.ParameterType.CUSTOM:
        pass
      else:
        raise ValueError("Unknown parameter type: %s" % param.type)
    return singletons

  def _strip_problem(self) -> vz.ProblemStatement:
    """Strips the problem of the singleton parameters."""
    stripped_problem = copy.deepcopy(self.problem)
    for param in self.problem.search_space.parameters:
      if param.name in self._singletons:
        stripped_problem.search_space.pop(param.name)
    return stripped_problem

  def strip_trials(
      self, trials: Sequence[vz.TrialSuggestion]
  ) -> Sequence[vz.TrialSuggestion]:
    """Strips the trials of the singleton parameters."""
    if not self._singletons:
      return trials
    new_trials = []
    for trial in trials:
      new_trial = copy.deepcopy(trial)
      for param_name in trial.parameters:
        if param_name in self._singletons:
          del new_trial.parameters[param_name]
      new_trials.append(new_trial)
    return new_trials

  def augment_trials(
      self, trials: Sequence[vz.TrialSuggestion]
  ) -> Sequence[vz.TrialSuggestion]:
    """Augments the trials with the singleton parameters."""
    if not self._singletons:
      return trials
    new_trials = []
    for trial in trials:
      new_trial = copy.deepcopy(trial)
      for singleton_name, singleton_value in self._singletons.items():
        new_trial.parameters[singleton_name] = singleton_value
      new_trials.append(new_trial)
    return new_trials


--- vizier/_src/pythia/singleton_params_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for singleton_params module which allows to handle singleton parameters."""

from vizier import pyvizier as vz
from vizier._src.pythia import singleton_params
from absl.testing import absltest
from absl.testing import parameterized


def get_problem_with_singletons() -> vz.ProblemStatement:
  """Returns a problem with singleton parameters."""
  problem = vz.ProblemStatement()
  problem.search_space.root.add_float_param('sf', min_value=1, max_value=1)
  problem.search_space.root.add_int_param('si', min_value=5, max_value=5)
  problem.search_space.root.add_categorical_param('sc', feasible_values=['a'])
  problem.search_space.root.add_discrete_param('sd', feasible_values=[3])

  problem.search_space.root.add_float_param('f', min_value=0.0, max_value=5.0)
  problem.search_space.root.add_int_param('i', min_value=0, max_value=10)
  problem.search_space.root.add_categorical_param(
      'c', feasible_values=['a', '1']
  )
  problem.search_space.root.add_discrete_param('d', feasible_values=[3, 1])
  problem.metric_information = [
      vz.MetricInformation(name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
  ]
  return problem


def get_problem_without_singletons() -> vz.ProblemStatement:
  """Returns a problem with singleton parameters."""
  problem = vz.ProblemStatement()
  problem.search_space.root.add_float_param('f', min_value=0.0, max_value=5.0)
  problem.search_space.root.add_int_param('i', min_value=0, max_value=10)
  problem.search_space.root.add_categorical_param(
      'c', feasible_values=['a', '1']
  )
  problem.search_space.root.add_discrete_param('d', feasible_values=[3, 1])
  problem.metric_information = [
      vz.MetricInformation(name='obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
  ]
  return problem


class SingletonParameterHandlerTest(parameterized.TestCase):
  """Tests for the `singletonParameterHandler`."""

  @parameterized.named_parameters(
      ('with_singletons', get_problem_with_singletons()),
      ('without_singletons', get_problem_without_singletons()),
  )
  def test_stripped_problem(self, problem):
    handler = singleton_params.SingletonParameterHandler(problem)
    expected_search_search = get_problem_without_singletons().search_space
    self.assertEqual(
        handler.stripped_problem.search_space, expected_search_search
    )

  def test_strip_trials(self):
    problem = get_problem_with_singletons()
    handler = singleton_params.SingletonParameterHandler(problem)
    trials = [
        vz.TrialSuggestion(
            parameters={
                'sf': 1.0,
                'si': 5,
                'sc': 'a',
                'sd': 3,
                'f': 1.0,
                'i': 1,
                'c': 'a',
                'd': 3,
            }
        )
    ]
    stripped_trials = handler.strip_trials(trials)
    self.assertEqual(
        stripped_trials,
        [
            vz.TrialSuggestion(
                parameters={
                    'f': 1.0,
                    'i': 1,
                    'c': 'a',
                    'd': 3,
                }
            )
        ],
    )

  def test_augment_trials(self):
    problem = get_problem_with_singletons()
    handler = singleton_params.SingletonParameterHandler(problem)
    trials = [
        vz.TrialSuggestion(
            parameters={
                'f': 1.0,
                'i': 1,
                'c': 'a',
                'd': 3,
            }
        )
    ]
    augmented_trials = handler.augment_trials(trials)
    self.assertEqual(
        augmented_trials,
        [
            vz.TrialSuggestion(
                parameters={
                    'sf': 1.0,
                    'si': 5,
                    'sc': 'a',
                    'sd': 3,
                    'f': 1.0,
                    'i': 1,
                    'c': 'a',
                    'd': 3,
                }
            )
        ],
    )

  def test_find_singletons(self):
    problem = get_problem_with_singletons()
    handler = singleton_params.SingletonParameterHandler(problem)
    self.assertEqual(
        handler._singletons,
        {
            'sf': 1.0,
            'si': 5,
            'sc': 'a',
            'sd': 3,
        },
    )

  def test_apply_strip_multiple_times(self):
    problem = get_problem_with_singletons()
    handler = singleton_params.SingletonParameterHandler(problem)
    trials = [
        vz.TrialSuggestion(
            parameters={
                'sf': 1.0,
                'si': 5,
                'sc': 'a',
                'sd': 3,
                'f': 1.0,
                'i': 1,
                'c': 'a',
                'd': 3,
            }
        )
    ]
    for _ in range(5):
      trials = handler.strip_trials(trials)
    self.assertEqual(
        trials,
        [
            vz.TrialSuggestion(
                parameters={
                    'f': 1.0,
                    'i': 1,
                    'c': 'a',
                    'd': 3,
                }
            )
        ],
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pythia/suggest_default.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Decorators for Policy.suggest."""

import functools
import types
from typing import TypeVar

import attrs
from vizier import pyvizier as vz
from vizier._src.pythia.policy import Policy
from vizier._src.pythia.policy import SuggestDecision
from vizier._src.pythia.policy import SuggestRequest


_T = TypeVar('_T')


def get_default_parameters(search_space: vz.SearchSpace) -> vz.ParameterDict:
  """Gets the default parameters for the given search space."""
  builder = vz.SequentialParameterBuilder(search_space)

  for pc in builder:
    if pc.default_value is not None:
      builder.choose_value(pc.default_value)
    elif pc.type in (
        vz.ParameterType.CATEGORICAL,
        vz.ParameterType.INTEGER,
        vz.ParameterType.DISCRETE,
    ):
      # Choose the middle value.
      builder.choose_value(pc.feasible_values[len(pc.feasible_values) // 2])
    elif pc.type == vz.ParameterType.DOUBLE:
      if pc.num_feasible_values == 1:
        builder.choose_value(pc.bounds[0])
      else:
        # TODO: Handle scaling properly.
        midpoint = (pc.bounds[0] + pc.bounds[1]) / 2
        builder.choose_value(midpoint)
  return builder.parameters


def seed_with_default(suggest_fn: _T) -> _T:
  """Decorator for Policy.suggest to always suggest the default or center.

  How to use as a decorator:

  ```
  class MyPolicy(Policy):
    @seed_with_default
    def suggest(self, ...):
      ...
  ```

  How to use as a function:
  class MyPolicy(Policy):

    def __init__(self, ..., *, use_seed_with_default: bool):
      if use_seed_with_default:
        self.suggest = seed_with_default(self.suggest)

    def suggest(self, ...):
      ...

  Args:
    suggest_fn:

  Returns:
    suggest_fn that suggest the default or center of the search space if
    the study is empty.
  """

  if hasattr(suggest_fn, '__self__'):
    unbound = seed_with_default(suggest_fn.__func__)
    return types.MethodType(unbound, suggest_fn.__self__)

  @functools.wraps(suggest_fn)
  def wrapper_fn(self: Policy, request: SuggestRequest) -> SuggestDecision:
    """If study is empty, suggests a default trial before using the policy."""
    if request.max_trial_id > 0:
      return suggest_fn(self, request)

    default_parameters = get_default_parameters(
        request.study_config.search_space
    )
    decision = SuggestDecision([vz.TrialSuggestion(default_parameters)])

    if request.count > 1:
      more_suggestions = suggest_fn(
          self, attrs.evolve(request, count=request.count - 1)
      )
      decision.suggestions.extend(more_suggestions.suggestions)
      decision.metadata = more_suggestions.metadata

    return decision

  return wrapper_fn


--- vizier/_src/pythia/suggest_default_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import unittest

import numpy as np
from vizier import pyvizier as vz
from vizier._src.pythia import policy as pythia
from vizier._src.pythia import suggest_default

from absl.testing import absltest


space = vz.SearchSpace()
space.root.add_int_param('x', 0, 1000)

empty_study_descriptor = vz.StudyDescriptor(
    vz.ProblemStatement(space),
    guid='1',
    max_trial_id=0,
)


class GetDefaultParametersTest(absltest.TestCase):

  def test_double_user_default(self):
    ss = vz.SearchSpace()
    ss.root.add_float_param('x', 0.0, 1.0, default_value=0.2)
    params = suggest_default.get_default_parameters(ss)
    self.assertEqual(params.get_value('x'), 0.2)

  @unittest.skip('TODO')
  def test_double_logscale(self):
    ss = vz.SearchSpace()
    ss.root.add_float_param(
        'x', np.exp(-2), np.exp(2), scale_type=vz.ScaleType.LOG
    )
    params = suggest_default.get_default_parameters(ss)
    self.assertEqual(params.get_value('x'), 1.0)

  def test_double_fixed(self):
    ss = vz.SearchSpace()
    ss.root.add_float_param('x', 1.0, 1.0)
    params = suggest_default.get_default_parameters(ss)
    self.assertEqual(params.get_value('x'), 1.0)

  def test_discrete(self):
    ss = vz.SearchSpace()
    ss.root.add_discrete_param('x', [1, 2, 3, 6])
    params = suggest_default.get_default_parameters(ss)
    self.assertEqual(params.get_value('x'), 3)

  def test_categorical(self):
    ss = vz.SearchSpace()
    ss.root.add_categorical_param('x', ['a', 'b', 'c', 'd'])
    params = suggest_default.get_default_parameters(ss)
    self.assertEqual(params.get_value('x'), 'c')


class SuggestDefaultTest(absltest.TestCase):

  def test_decorator(self):
    class FakeDecoratedPolicy(pythia.Policy):

      @suggest_default.seed_with_default
      def suggest(
          self, request: pythia.SuggestRequest
      ) -> pythia.SuggestDecision:
        if request.count != 2:
          raise ValueError(f'count must be 2. Was: {request.count}')
        if request._study_descriptor != empty_study_descriptor:
          raise ValueError('study_descriptor must match request')
        return pythia.SuggestDecision(
            [vz.TrialSuggestion({'x': 999}), vz.TrialSuggestion({'x': 1000})]
        )

      def early_stop(
          self, request: pythia.EarlyStopRequest
      ) -> pythia.EarlyStopDecisions:
        return pythia.EarlyStopDecisions()

    FakeDecoratedPolicy().suggest(
        pythia.SuggestRequest(count=3, study_descriptor=empty_study_descriptor)
    )

  def test_init_override(self):
    class FakeInitOverridePolicy(pythia.Policy):

      def __init__(self):
        self.suggest = suggest_default.seed_with_default(self.suggest)

      def suggest(
          self, request: pythia.SuggestRequest
      ) -> pythia.SuggestDecision:
        if request.count != 2:
          raise ValueError(f'count must be 2. Was: {request.count}')
        if request._study_descriptor != empty_study_descriptor:
          raise ValueError('study_descriptor must match request')
        return pythia.SuggestDecision(
            [vz.TrialSuggestion({'x': 999}), vz.TrialSuggestion({'x': 1000})]
        )

      def early_stop(
          self, request: pythia.EarlyStopRequest
      ) -> pythia.EarlyStopDecisions:
        return pythia.EarlyStopDecisions()

    FakeInitOverridePolicy().suggest(
        pythia.SuggestRequest(count=3, study_descriptor=empty_study_descriptor)
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/multimetric/hypervolume.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Hypervolume calculation (stochastic approximation) functions."""
import math
from typing import Optional, Callable


import numpy as np


def _cum_hypervolume_origin(points: np.ndarray,
                            vectors: np.ndarray) -> np.ndarray:
  """Returns a randomized approximation of the cumulative dominated hypervolume.

  See Section 3, Lemma 5 of https://arxiv.org/pdf/2006.04655.pdf for a fuller
  explanation of the technique. This assumes the reference point is the
  origin.

  NOTE: This returns an unnormalized hypervolume.

  Args:
    points: Any set of points with shape (num_points, dimension).
    vectors: Set of vectors with shape (num_vectors, dimension).

  Returns:
    Approximated cumulative dominated hypervolume of points[:i].

  Raises:
    ValueError: Points and vectors do not match in dimension.
  """
  if vectors.shape[1] != points.shape[1]:
    raise ValueError(f'Vectors shape {vectors.shape} do not match dimension of'
                     f' (second value in tuple of points shape {points.shape}')
  num_points, dimension = points.shape
  num_vectors = vectors.shape[0]
  temp_points = np.broadcast_to(points[np.newaxis, :, :],
                                [num_vectors, num_points, dimension])
  vectors = vectors.reshape([num_vectors, 1, dimension])
  # Here, ratios[i][j][k] is the kth coordinate of the jth point / ith vector.
  # Since points is (num_vectors, num_points, dimension) and vectors is
  # (num_vectors, 1, dimension), note that ratios is
  # (num_vectors, num_points, dimension).
  ratios = temp_points / vectors

  # These calculations are from Lemma 5 in above cited paper (dimension axis).
  coordinate_min_ratio = np.min(ratios, axis=2)

  # Maximizing across all points (num_points axis).
  point_max_ratio = np.maximum.accumulate(coordinate_min_ratio, axis=1)
  # Averaging across the vector axis.
  return np.mean(point_max_ratio**dimension, axis=0)


class ParetoFrontier:
  """Calculate hypervolume approximations for a Pareto frontier/set."""

  def __init__(
      self,
      points: np.ndarray,
      origin: np.ndarray,
      num_vectors: int = 10000,
      cum_hypervolume_base: Callable[[np.ndarray, np.ndarray],
                                     np.ndarray] = _cum_hypervolume_origin):
    """Takes a set of points and initializes approximating vectors.

    To use with XLA:
      from vizier.pyvizier.multimetric import xla_pareto

      front = ParetoFrontier(points, origin,
        cum_hypervolume_base = xla_pareto.jax_cum_hypervolume_origin)
      front.hypervolume(is_cumulative=True)

    Args:
      points: Any set of points with shape (num_points, dimension).
      origin: An point (1D or 2D array) with length = dimension from which
        hypervolume is computed.
      num_vectors: Number of random vectors used to approximate hypervolume.
      cum_hypervolume_base: The base algorithm used to calculate hypervolume
      from the origin. Parameters are [points, vectors].

    Raises:
      ValueError: When dimensions are mismatched between points and origin.
    """
    self._points = points
    if points.shape[1] != len(origin):
      raise ValueError(
          f'Dimension mismatch frontier points {self._points.shape}'
          f' and origin with length {len(origin)}')
    self._origin = origin
    self._cum_hypervolume_base = cum_hypervolume_base
    # Generating random vectors in the positive orthant for approximation.
    self._vectors = abs(np.random.normal(size=(num_vectors, len(origin))))
    self._vectors /= np.linalg.norm(self._vectors, axis=1)[..., np.newaxis]

  def hypervolume(self,
                  additional_points: Optional[np.ndarray] = None,
                  is_cumulative: bool = False,
                  num_shards: int = 10) -> np.ndarray:
    """Returns a randomized approximation of the dominated hypervolume.

    See Section 3, Lemma 5 of https://arxiv.org/pdf/2006.04655.pdf for a fuller
    explanation of the technique.

    Args:
      additional_points: Additional 2D array to add to hypervolume computation.
      is_cumulative: If true, a cumulative hypervolume is returned. A cumulative
      hypervolume is the a vector whose i-th entry is the running hypervolume of
      the first i-th points of points + additional_points.
      num_shards: Number of shards for breaking up self._vectors for reduced
      memory. A larger number means lower memory.

    Returns:
      Approximated dominated hypervolume of frontier and additional points.

    Raises:
      ValueError: Frontier and additional points do not match in dimension.
    """
    if additional_points is None:
      points = self._points
    else:
      if self._points.shape[1] != additional_points.shape[1]:
        raise ValueError(
            f'Dimension mismatch frontier points {self._points.shape}'
            f' and additional points {additional_points.shape}')
      points = np.concatenate((self._points, additional_points), axis=0)

    # Shift points by the origin and remove all dominated/negative points.
    points = points - self._origin
    non_positive_points = np.invert(np.all(points > 0, axis=1))
    points[non_positive_points] = 0
    if not points.size:
      return np.asarray([0.0])

    # We shard the vectors for better reduced memory usage.
    idx = np.linspace(0, len(self._vectors), num_shards + 1).astype(np.int32)
    idx = list(reversed(idx))
    # We average each vector's hypervolume estimate for a final estimate. More
    # vectors mean a more accurate estimate. (vector axis average)
    approx_hypervolume = 0
    _, dimension = points.shape
    unit_hypersphere_volume = math.pi**(
        dimension / 2) / math.gamma(dimension / 2 + 1) / 2**dimension
    for begin, end in zip(idx[1:], idx[:-1]):
      vectors = self._vectors[begin:end, :]
      # Apply maximization because Jax -> NP conversion can be imprecise.
      approx_hypervolume += np.maximum.accumulate(
          self._cum_hypervolume_base(points, vectors) * unit_hypersphere_volume
      )

    if is_cumulative:
      return np.array(approx_hypervolume / num_shards)
    else:
      return np.array(np.max(approx_hypervolume / num_shards))


--- vizier/_src/pyvizier/multimetric/hypervolume_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np

from vizier._src.pyvizier.multimetric import hypervolume
from absl.testing import absltest


class HypervolumeTest(absltest.TestCase):

  def testParetoHypervolume(self):
    x = np.random.normal()
    y = np.random.normal()
    points = np.array([[x, y], [y, x]])
    origin = np.array([-1, -1])
    pf = hypervolume.ParetoFrontier(points, origin)
    self.assertAlmostEqual(
        pf.hypervolume(),
        max(2 * (x + 1) * (y + 1) - min(x + 1, y + 1)**2, 0.0),
        delta=0.3)

  def testParetoHypervolumeCumulative(self):
    points = np.random.normal(size=(100, 4))
    origin = np.zeros(4)

    pf = hypervolume.ParetoFrontier(points, origin)
    cumulative_vol = pf.hypervolume(is_cumulative=True)
    self.assertLen(cumulative_vol, len(points))
    for i in range(len(points)):
      if i < len(points) - 1:
        self.assertGreaterEqual(cumulative_vol[i + 1], cumulative_vol[i])
      else:
        self.assertAlmostEqual(cumulative_vol[i], pf.hypervolume())

  def testDimensionMismatchInit(self):
    points = np.array([[2, 1], [1, 2], [0, 0]])
    origin_3d = np.array([-1, -1, 1])
    # Dimension mismatch.
    with self.assertRaisesRegex(ValueError, 'Dimension mismatch'):
      hypervolume.ParetoFrontier(points, origin_3d)

  def testDimensionMismatchHypervolume(self):
    points = np.array([[2, 1], [1, 2], [0, 0]])
    origin_3d = np.array([-1, -1, 1])
    pf = hypervolume.ParetoFrontier(points, origin=np.array([0, 0]))
    # Dimension mismatch.
    with self.assertRaisesRegex(ValueError, 'Dimension mismatch'):
      pf.hypervolume(additional_points=origin_3d[..., np.newaxis])


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/multimetric/pareto_optimal.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Fast divide-and-conquer (DC) algorithms for computing Pareto frontier."""

import abc

import numpy as np


class BaseParetoOptimalAlgorithm(metaclass=abc.ABCMeta):
  """Base metaclass for calculating pareto frontiers."""

  @abc.abstractmethod
  def is_pareto_optimal(self, points: np.ndarray) -> np.ndarray:
    """An algorithm to find the Pareto frontier.

    Args:
      points: M-by-D 2D array of points. M = number of points, D = dimension.

    Returns:
      A boolean array of length m corresponding to each point in points. True
      if pareto optimal.
    """
    pass

  @abc.abstractmethod
  def is_pareto_optimal_against(self, points: np.ndarray, against: np.ndarray,
                                *, strict: bool) -> np.ndarray:
    """An algorithm to check if the points are pareto optimal or dominated.

    NOTE: p dominates q if p_i >= q_i and strictly if p_i != q_i.

    Dominated: A point is strictly dominated by some point in dominated_points.
    Equal: A non-strictly dominated point equal to some point in
    dominated_points.
    Optimal: A point is not dominated.

    Args:
      points: M-by-D 2D array of points to be checked.  M = number of points, D
        = dimension.
      against: N-by-D 2D array of dominating points. N = number of dominating
        points, D = dimension.
      strict: If true, non-strictly dominated (i.e. equal) points are considered
        optimal.

    Returns:
      A length N boolean array corresponding to each point in points. True means
      that the point is not dominated by any point in against.
    """
    pass

  def update_pareto_optimal(
      self, current_pareto_optimal: np.ndarray, incremental_points: np.ndarray
  ) -> np.ndarray:
    """An algorithm to update the Pareto frontier.

    Args:
      current_pareto_optimal: M-by-D 2D array of current pareto optimal points.
        M = number of points, D = dimension.
      incremental_points: N-by-D 2D array of incremental points. N = number of
        points, D = dimension.

    Returns:
      List of point indices of which are in the pareto frontier.
      Example: returns [2, 4] when only ckpt_2 and ckpt_4 are on pareto.
    """
    is_optimal = self.is_pareto_optimal(
        np.append(current_pareto_optimal, incremental_points, axis=0)
    )
    return np.asarray(is_optimal).nonzero()[0]


class NaiveParetoOptimalAlgorithm(BaseParetoOptimalAlgorithm):
  """Naive implementation of Pareto frontier calculation."""

  def is_pareto_optimal_against(self, points: np.ndarray, against: np.ndarray,
                                *, strict: bool) -> np.ndarray:
    """Naive quadratic-time implementation. See base class."""
    if against.shape[1] != points.shape[1]:
      raise ValueError(f'Shape for against {against.shape}'
                       f'does not match in last dim for points {points.shape}')
    is_optimal = np.zeros(len(points), dtype=bool)
    for i, point in enumerate(points):
      strict_dominating = np.any(point > against, axis=1)
      if np.all(strict_dominating):
        # Optimal point since no points dominate it.
        is_optimal[i] = True
      # Mark equal as optimal if strict is True.
      elif strict and np.all(strict_dominating
                             | np.all(point == against, axis=1)):
        is_optimal[i] = True

    return is_optimal

  def is_pareto_optimal(self, points: np.ndarray) -> np.ndarray:
    """Naive quadratic-time implementation. See base class."""
    is_optimal = np.ones(len(points), dtype=bool)
    for i, point in enumerate(points):
      if is_optimal[i]:
        # At each step, revise all potentially optimal points.
        is_optimal[is_optimal] = (
            np.any(points[is_optimal] > point, axis=1)
            | np.all(points[is_optimal] == point, axis=1))
    return is_optimal


class FastParetoOptimalAlgorithm(BaseParetoOptimalAlgorithm):
  """Fast DC/recursive algorithm deciding if points are optimal or dominated.

  NOTE: p dominates q if p_i >= q_i and strictly if p_i != q_i.

  Citation on first publication:
  https://static.aminer.org/pdf/PDF/000/211/201/on_the_computational_complexity_of_finding_the_maxima_of_a.pdf
  """

  def __init__(
      self,
      base_algorithm: BaseParetoOptimalAlgorithm = NaiveParetoOptimalAlgorithm(
      ),
      *,
      recursive_threshold: int = 10000):
    """Init.

    To use with XLA:
      from vizier.pyvizier.multimetric import xla_pareto

      algo =
      FastParetoOptimalAlgorithm(xla_pareto.JaxParetoOptimalAlgorithm())
      algo.is_pareto_optimal(points)

    Args:
      base_algorithm: Base pareto computation algorithm used as base case.
      recursive_threshold: If points are fewer than this threshold, use base
        case naive algorithm.
    """
    self._base_algorithm = base_algorithm
    self._recursive_threshold = recursive_threshold

  def is_pareto_optimal_against(self, points: np.ndarray, against: np.ndarray,
                                *, strict: bool) -> np.ndarray:
    """Fast DC algorithm/recursive algorithm. See base class."""
    # Edge case: When either points or against are empty.
    if len(points) <= 0:
      return np.array([], dtype=bool)
    if len(against) <= 0:
      return np.array(np.ones(len(points), dtype=bool))

    # Base case.
    if len(against) < self._recursive_threshold or len(
        points) < self._recursive_threshold:
      if strict:
        return np.array(
            self._base_algorithm.is_pareto_optimal_against(
                points, against, strict=True),
            dtype=bool)
      else:
        return np.array(
            self._base_algorithm.is_pareto_optimal_against(
                points, against, strict=False),
            dtype=bool)

    # Base case: If 1D, this is a simple linear time algorithm.
    if against.shape[1] <= 1:
      max_value = np.max(against)
      if strict:
        return (points >= max_value).squeeze()
      else:
        return (points > max_value).squeeze()

    # Sort points from lowest to highest for first dimension and find split.
    ascending_indices = (points[:, 0]).argsort()
    sorted_points = points[ascending_indices]
    split_index = round(len(points) / 2)
    split_value = sorted_points[split_index][0]
    while sorted_points[split_index][0] == split_value:
      split_index += 1
      # Hard to find a clean split. Resort to simple algorithm.
      if split_index == len(sorted_points):
        if strict:
          return np.array(
              self._base_algorithm.is_pareto_optimal_against(
                  points, against, strict=True),
              dtype=bool)
        else:
          return np.array(
              self._base_algorithm.is_pareto_optimal_against(
                  points, against, strict=False),
              dtype=bool)

    # Sort dominating points from lowest to highest for first dim and split.
    sorted_dominating = against[(against[:, 0]).argsort()]
    dominating_split_index = np.searchsorted(
        sorted_dominating[:, 0], split_value, side='right')

    # Divide points, dominated_points and then recurse.
    lower_points = sorted_points[:split_index]
    upper_points = sorted_points[split_index:]
    lower_dominating = sorted_dominating[:dominating_split_index]
    upper_dominating = sorted_dominating[dominating_split_index:]

    upper_optimal = self.is_pareto_optimal_against(
        points=upper_points, against=upper_dominating, strict=strict)
    lower_optimal = self.is_pareto_optimal_against(
        points=lower_points, against=lower_dominating, strict=strict)
    # Can remove the first dimension due to sorting criteria. Also any equal
    # point is dominated because of the sorting of the first dimension.
    cross_optimal = self.is_pareto_optimal_against(
        points=lower_points[:, 1:],
        against=upper_dominating[:, 1:],
        strict=False)

    # Combine subresults and return.
    lower_optimal = lower_optimal & cross_optimal

    is_optimal = np.array(np.zeros(len(points), dtype=bool))
    is_optimal[ascending_indices[:split_index]] = lower_optimal
    is_optimal[ascending_indices[split_index:]] = upper_optimal

    return is_optimal

  def is_pareto_optimal(self, points: np.ndarray) -> np.ndarray:
    """Fast DC algorithm to find the Pareto frontier. See base class."""

    # Base case naive algorithm for find Pareto optimality.
    if len(points) <= self._recursive_threshold:
      return np.array(
          self._base_algorithm.is_pareto_optimal(points), dtype=bool)

    # Sort from lowest to highest on first dimension and split.
    ascending_indices = (points[:, 0]).argsort()
    sorted_points = points[ascending_indices]
    split_index = round(len(points) / 2)

    # Recurse on both subarrays and check for cross domination.
    lower_array = sorted_points[:split_index]
    higher_array = sorted_points[split_index:]
    higher_pareto = self.is_pareto_optimal(higher_array)
    lower_pareto = self.is_pareto_optimal(lower_array)
    cross_check = self.is_pareto_optimal_against(
        lower_array, higher_array, strict=True)
    lower_pareto = lower_pareto & cross_check

    is_optimal = np.zeros(len(points), dtype=bool)
    is_optimal[ascending_indices[:split_index]] = lower_pareto
    is_optimal[ascending_indices[split_index:]] = higher_pareto
    return is_optimal


--- vizier/_src/pyvizier/multimetric/pareto_optimal_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for pareto_optimal."""

import numpy as np

from vizier._src.pyvizier.multimetric import pareto_optimal
from absl.testing import absltest


class ParetoOptimalTest(absltest.TestCase):

  def setUp(self):
    super(ParetoOptimalTest, self).setUp()
    self.algo = pareto_optimal.FastParetoOptimalAlgorithm(
        base_algorithm=pareto_optimal.NaiveParetoOptimalAlgorithm())

  def test_is_pareto_optimal(self):
    points = np.array([[1, 2, 3], [1, 2, 3], [2, 4, 1], [1, 2, -1]])
    np.testing.assert_array_equal(
        self.algo.is_pareto_optimal(points), [True, True, True, False])

    # Adding a point that dominates the first two values.
    points = np.vstack([points, [2, 3, 4]])
    np.testing.assert_array_equal(
        self.algo.is_pareto_optimal(points), [False, False, True, False, True])

  def test_is_pareto_optimal_all_optimal(self):
    # Generate points on positive orthant sphere.
    points = abs(np.random.normal(size=(1000, 3)))
    points /= np.linalg.norm(points, axis=1)[..., np.newaxis]
    algo = pareto_optimal.FastParetoOptimalAlgorithm(
        base_algorithm=pareto_optimal.NaiveParetoOptimalAlgorithm(),
        recursive_threshold=100)
    self.assertTrue(np.all(algo.is_pareto_optimal(points)))

  def test_is_pareto_optimal_randomized(self):
    dim = 10
    points = np.random.normal(size=(1000, dim))
    # Make a copy of points to test equality case.
    points = np.vstack([points, points])

    # Make sure the fast and base algorithms match.
    simple_test = self.algo.is_pareto_optimal(points)
    fast_test = self.algo._base_algorithm.is_pareto_optimal(points)

    self.assertTrue(np.all(simple_test == fast_test))

  def test_is_pareto_optimal_against(self):
    points = np.array([[1, 2, 3], [2, 4, 1], [1, 2, -1]])
    dominating_points = np.array([[1, 2, 3], [3, 4, 0]])

    np.testing.assert_array_equal(
        self.algo.is_pareto_optimal_against(
            points, dominating_points, strict=True), [True, True, False])
    np.testing.assert_array_equal(
        self.algo.is_pareto_optimal_against(
            points, dominating_points, strict=False), [False, True, False])

  def test_is_pareto_optimal_against_randomized(self):
    dim = 4
    dominating_points = np.random.normal(size=(10000, dim))
    points = np.random.normal(size=(1000, dim))

    # Make sure the fast and base algorithms match.
    simple_check = self.algo.is_pareto_optimal_against(
        points, dominating_points, strict=True)
    fast_check = self.algo._base_algorithm.is_pareto_optimal_against(
        points, dominating_points, strict=True)

    # Results should not be affected by changing recursive threshold.
    self.assertTrue(np.all(simple_check == fast_check))

  def test_update_pareto(self):
    points = np.array([[1, 2, 3], [1, 2, 3], [2, 4, 1], [1, 2, -1]])
    np.testing.assert_array_equal(
        self.algo.is_pareto_optimal(points), [True, True, True, False]
    )

    # Adding a point that dominates the first two values.
    point = np.array([[2, 3, 4]])
    np.testing.assert_array_equal(
        self.algo.update_pareto_optimal(points, point),
        [2, 4],
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/multimetric/safety.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Multimetric utility classes for Studies/Trials."""

from typing import Iterable, Sequence
import numpy as np
from vizier import pyvizier


class SafetyChecker:
  """Returns the safety status of Measurements/Trials."""

  def __init__(self, metrics_config: pyvizier.MetricsConfig):
    """Initializes with metric information (can have non-safety info).

    Example usage from a study:

      pyconfig = pyvizier.StudyConfig.from_proto(study.study_config)
      checker = multimetric_util.SafetyChecker(pyconfig.metric_information)
      are_trials_safe = checker.are_trials_safe(
              converters.TrialConverter.from_protos(study.trials))
    Args:
      metrics_config: MetricsConfig for a Study.
    """
    self._objective_metrics = list(
        metrics_config.of_type(pyvizier.MetricType.OBJECTIVE)
    )
    self._safety_metrics = list(
        metrics_config.of_type(pyvizier.MetricType.SAFETY))

  def are_trials_safe(self, trials: Iterable[pyvizier.Trial]) -> Iterable[bool]:
    """Returns whether the final measurements of Trials are safe.

    Args:
      trials: Iterable of pyvizier Trials.

    Returns:
      Iterable of whether Trials are safe. Safety is assumed if no final
      measurement is found or safety metric is not found in final measurement.
    """
    return self.are_measurements_safe([
        t.final_measurement if t.final_measurement else pyvizier.Measurement()
        for t in trials
    ])

  def are_measurements_safe(
      self, measurements: Iterable[pyvizier.Measurement]) -> Iterable[bool]:
    """Returns whether measurements are safe.

    Args:
      measurements: Iterable of pyvizier measurements.

    Returns:
      Iterable of whether measurements are safe. Safety is assumed if safety
      metric is not found in measurement.
    """
    safety_results = []
    for measurement in measurements:
      is_safe = True
      for safety_metric in self._safety_metrics:
        # If safety metric is not in metrics, we assume it is safe.
        if safety_metric.name not in measurement.metrics:
          continue

        # Otherwise, we check if there is a safety violation.
        metric_value = measurement.metrics[safety_metric.name].value
        if safety_metric.goal == pyvizier.ObjectiveMetricGoal.MAXIMIZE:
          is_safe = is_safe and metric_value >= safety_metric.safety_threshold
        else:
          is_safe = is_safe and metric_value <= safety_metric.safety_threshold
      safety_results.append(is_safe)
    return safety_results

  def warp_unsafe_trials(
      self, trials: Sequence[pyvizier.Trial]
  ) -> Sequence[pyvizier.Trial]:
    """Sets the final measurements of unsafe Trials to worst values in place.

    Args:
      trials: Sequence of pyvizier Trials.

    Returns:
      Trials whose final measurements are warped to the worst value (inf/-inf)
      for all OBJECTIVE metrics in unsafe Trials.
    """
    are_safe = self.are_trials_safe(trials)
    objective_goals = {m.name: m.goal for m in self._objective_metrics}
    for is_safe, trial in zip(are_safe, trials):
      if is_safe:
        continue
      if trial.final_measurement is None:
        continue

      for name in trial.final_measurement.metrics.keys():
        if name in objective_goals:
          worst_value = (
              np.inf
              if objective_goals[name] == pyvizier.ObjectiveMetricGoal.MINIMIZE
              else -np.inf
          )
          trial.final_measurement.metrics[name] = pyvizier.Metric(
              value=worst_value
          )

    return trials


--- vizier/_src/pyvizier/multimetric/safety_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import numpy as np
from vizier import pyvizier
from vizier._src.pyvizier.multimetric import safety
from absl.testing import absltest


class SafeAcquisitionTest(absltest.TestCase):

  def testSafeMeasurements(self):
    info1 = pyvizier.MetricInformation(
        name='safety1',
        goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE,
        safety_threshold=0.1)
    info2 = pyvizier.MetricInformation(
        name='safety2',
        goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
        safety_threshold=0.5)
    checker = safety.SafetyChecker(pyvizier.MetricsConfig([info1, info2]))
    safe_measurement = pyvizier.Measurement(metrics={
        'safety1': pyvizier.Metric(value=3.1),
        'safety2': pyvizier.Metric(value=0.3)
    })
    unsafe1_measurement = pyvizier.Measurement(metrics={
        'safety1': pyvizier.Metric(value=-3.1),
        'safety2': pyvizier.Metric(value=0.4)
    })
    unsafe2_measurement = pyvizier.Measurement(metrics={
        'safety1': pyvizier.Metric(value=3.1),
        'safety2': pyvizier.Metric(value=0.7)
    })
    no_safety_measurement = pyvizier.Measurement(
        metrics={'safety1': pyvizier.Metric(value=3.1)})

    self.assertListEqual(
        checker.are_measurements_safe([
            safe_measurement, unsafe1_measurement, unsafe2_measurement,
            no_safety_measurement
        ]), [True, False, False, True])

  def testSafeTrials(self):
    info1 = pyvizier.MetricInformation(
        name='obj', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE)
    info2 = pyvizier.MetricInformation(
        name='safety',
        goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
        safety_threshold=0.5)
    checker = safety.SafetyChecker(pyvizier.MetricsConfig([info1, info2]))

    safe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={
            'obj': 0.2,
            'safety': 0.3
        }))
    unsafe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={
            'obj': 0.2,
            'safety': 0.7
        }))
    no_safety_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={'obj': 0.2}))

    # Empty Trial is assumed to be safe.
    self.assertListEqual(
        checker.are_trials_safe(
            [safe_trial, unsafe_trial, no_safety_trial,
             pyvizier.Trial()]), [True, False, True, True])

  def testNoSafety(self):
    info1 = pyvizier.MetricInformation(
        name='obj', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE)
    checker = safety.SafetyChecker(pyvizier.MetricsConfig([info1]))

    safe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={
            'obj': 0.2,
            'safety': 0.3
        }))
    unsafe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={
            'obj': 0.2,
            'safety': 0.7
        }))

    # With no safety metrics, all Trials are safe..
    self.assertListEqual(
        checker.are_trials_safe([safe_trial, unsafe_trial,
                                 pyvizier.Trial()]), [True, True, True])

  def testWarpUnsafeTrials(self):
    info1 = pyvizier.MetricInformation(
        name='obj', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
    )
    info2 = pyvizier.MetricInformation(
        name='safety',
        goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
        safety_threshold=0.5,
    )
    checker = safety.SafetyChecker(pyvizier.MetricsConfig([info1, info2]))

    safe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(
            metrics={'obj': 0.2, 'safety': 0.3}
        )
    )
    unsafe_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(
            metrics={'obj': 0.2, 'safety': 0.7, 'extra': 0.3}
        )
    )
    no_safety_trial = pyvizier.Trial(
        final_measurement=pyvizier.Measurement(metrics={'obj': 0.2})
    )

    # Empty safety measurement is assumed to be safe.
    warped_trials = checker.warp_unsafe_trials(
        [safe_trial, unsafe_trial, no_safety_trial]
    )

    self.assertEqual(warped_trials[0], safe_trial)
    self.assertEqual(warped_trials[2], no_safety_trial)

    # Unsafe Trial is warped.
    self.assertEqual(
        warped_trials[1].final_measurement.metrics['obj'].value, -np.inf
    )
    # Extra metrics should be untouched.
    self.assertEqual(
        warped_trials[1].final_measurement.metrics['extra'].value, 0.3
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/oss/automated_stopping.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Convenience classes for configuring Vizier Early-Stopping Configs."""
import copy

import attr
from vizier._src.service import study_pb2

# When new early stopping config protos are added, include them below
# with a Union[]
AutomatedStoppingConfigProto = study_pb2.StudySpec.DefaultEarlyStoppingSpec


@attr.s(frozen=True, init=True, slots=True, kw_only=True)
class AutomatedStoppingConfig:
  """A wrapper for study_pb2.automated_stopping_spec."""
  _proto: AutomatedStoppingConfigProto = attr.ib(init=True, kw_only=True)

  @classmethod
  def default_stopping_spec(cls) -> 'AutomatedStoppingConfig':
    """Use Vizier's default early stopping."""
    config = study_pb2.StudySpec.DefaultEarlyStoppingSpec()
    return cls(proto=config)

  @classmethod
  def from_proto(
      cls, proto: AutomatedStoppingConfigProto) -> 'AutomatedStoppingConfig':
    return cls(proto=proto)

  def to_proto(self) -> AutomatedStoppingConfigProto:
    """Returns this object as a proto."""
    return copy.deepcopy(self._proto)


--- vizier/_src/pyvizier/oss/automated_stopping_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for automated_stopping."""

from vizier._src.pyvizier.oss import automated_stopping
from vizier._src.service import study_pb2
from vizier._src.pyvizier.oss import compare
from absl.testing import absltest


class AutomatedStoppingTest(absltest.TestCase):

  def testDefaultStoppingConfig(self):
    config = automated_stopping.AutomatedStoppingConfig.default_stopping_spec()
    proto = study_pb2.StudySpec.DefaultEarlyStoppingSpec()
    compare.assertProto2Equal(self, proto, config.to_proto())


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/oss/compare.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Compatibility method comparison for protobufs.

Note that using this method can be unstable and hard to debug.
Provided here for compatibility.
"""

# TODO: Add pytypes.
# TODO: Fix this library.


# pylint: disable=invalid-name
def assertProto2Equal(self, x, y):
  self.assertEqual(x.SerializeToString(), y.SerializeToString())


def assertProto2Contains(self, x, y):
  del self
  if not isinstance(x, str):
    x = str(x)
  if x not in str(y):
    pass


def assertProto2SameElements(*args, **kwargs):
  del args, kwargs
  pass


--- vizier/_src/pyvizier/oss/metadata_util.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utility functions for handling vizier metadata."""

from typing import Dict, Iterable, Literal, Optional, Tuple, Type, TypeVar, Union
from absl import logging

from vizier._src.pyvizier.shared import common
from vizier._src.pyvizier.shared import trial
from vizier._src.service import key_value_pb2
from vizier._src.service import study_pb2
from vizier._src.service import vizier_service_pb2
from google.protobuf import any_pb2
from google.protobuf.message import Message

T = TypeVar('T')


def _assign_value(
    metadatum: key_value_pb2.KeyValue, value: Union[str, any_pb2.Any, Message]
) -> None:
  """Assigns value to $metadatum."""
  if isinstance(value, str):
    metadatum.ClearField('proto')
    metadatum.value = value
  elif isinstance(value, any_pb2.Any):
    metadatum.ClearField('value')
    metadatum.proto.CopyFrom(value)
  else:
    metadatum.ClearField('value')
    metadatum.proto.Pack(value)


def assign(
    container: Union[study_pb2.StudySpec, study_pb2.Trial],
    *,
    key: str,
    ns: str,
    value: Union[str, any_pb2.Any, Message],
    mode: Literal['insert_or_assign', 'insert_or_error', 'insert'] = 'insert',
) -> Tuple[key_value_pb2.KeyValue, bool]:
  """Insert and/or assign (key, value) to container.metadata.

  Args:
    container: container.metadata must be repeated KeyValue (protobuf) field.
    key:
    ns: A namespace for the key (defaults to '', which is the user's namespace).
    value: Behavior depends on the type. `str` is copied to KeyValue.value
      `any_pb2.Any` is copied to KeyValue.proto Other types are packed to
      any_pb2.Any proto, which is then copied to KeyValue.proto.
    mode: `insert_or_assign` overrides the value if (ns, key)-pair already
      exists and `insert_or_error` raises ValueError if duplicate (ns, key)-pair
      exists. `insert` blindly inserts. This is fastest and should be used if
      the data source can be trusted.

  Returns:
    (proto, inserted) where
    proto is the protobuf that was just inserted into the $container, and
    inserted is True if the proto was newly inserted, False if it was replaced.
  """
  inserted = True

  # Find existing metadatum, unless in `insert` mode.
  existing_metadatum = None
  if mode in ('insert_or_assign', 'insert_or_error'):
    for metadatum in container.metadata:
      if metadatum.key == key and metadatum.ns == ns:
        inserted = False
        if mode == 'insert_or_error':
          raise ValueError(
              f'Duplicate (ns, key) pair: ({metadatum.ns}, {metadatum.key})'
          )
        existing_metadatum = metadatum
        break

  # If the metadatum does not exist, then add the (ns, key) pair.
  metadatum = existing_metadatum or container.metadata.add(key=key, ns=ns)
  _assign_value(metadatum, value)

  return metadatum, inserted


def get(
    container: Union[study_pb2.StudySpec, study_pb2.Trial], *, key: str, ns: str
) -> Optional[str]:
  """Returns the metadata value associated with key, or None.

  Args:
    container: A Trial of a StudySpec in protobuf form.
    key: The key of a KeyValue protobuf.
    ns: A namespace for the key (defaults to '', which is the user's namespace).
  """

  for kv in container.metadata:
    if kv.key == key and kv.ns == ns:
      if not kv.HasField('proto'):
        return kv.value
  return None


def get_proto(
    container: Union[study_pb2.StudySpec, study_pb2.Trial],
    *,
    key: str,
    ns: str,
    cls: Type[T],
) -> Optional[T]:
  """Unpacks the proto metadata into message.

  Args:
    container: (const) StudySpec or Trial to search the metadata from.
    key: (const) Lookup key of the metadata.
    ns: A namespace for the key (defaults to '', which is the user's namespace).
    cls: Pass in a proto ***class***, not a proto object.

  Returns:
    Proto message, if the value associated with the key exists and
    can be parsed into proto; None otherwise.
  """
  for kv in container.metadata:
    if kv.key == key and kv.ns == ns:
      if kv.HasField('proto'):
        message = cls()
        success = kv.proto.Unpack(message)
        return message if success else None
  return None


def make_key_value_list(
    metadata: common.Metadata,
) -> list[key_value_pb2.KeyValue]:
  """Convert $metadata to a list of KeyValue protobufs."""
  result = []
  for ns, k, v in metadata.all_items():
    item = key_value_pb2.KeyValue(key=k, ns=ns.encode())
    _assign_value(item, v)
    result.append(item)
  return result


def from_key_value_list(
    kv_s: Iterable[key_value_pb2.KeyValue],
) -> common.Metadata:
  """Converts a list of KeyValue protos into a Metadata object."""
  metadata = common.Metadata()
  for kv in kv_s:
    metadata.abs_ns(common.Namespace.decode(kv.ns))[kv.key] = (
        kv.proto if kv.HasField('proto') else kv.value
    )
  return metadata


def trial_metadata_to_update_list(
    trial_metadata: dict[int, common.Metadata]
) -> list[vizier_service_pb2.UnitMetadataUpdate]:
  """Convert a dictionary of Trial.id:Metadata to a list of UnitMetadataUpdate.

  Args:
    trial_metadata: Typically MetadataDelta.on_trials.

  Returns:
    a list of UnitMetadataUpdate objects.
  """
  result = []
  for trial_id, md in trial_metadata.items():
    for kv in make_key_value_list(md):
      # TODO: Verify this implementation.
      # Should str(trial_id) below be "resources.StudyResource.from_name(
      # study_resource_name).trial_resource(trial_id=str(trial_id)).name"?
      result.append(
          vizier_service_pb2.UnitMetadataUpdate(
              trial_id=str(trial_id), metadatum=kv
          )
      )
  return result


def study_metadata_to_update_list(
    study_metadata: common.Metadata,
) -> list[vizier_service_pb2.UnitMetadataUpdate]:
  """Convert `on_study` metadata to list of metadata update protos."""
  unit_metadata_updates = []
  for ns, k, v in study_metadata.all_items():
    unit_metadata_update = vizier_service_pb2.UnitMetadataUpdate()
    metadatum = unit_metadata_update.metadatum
    metadatum.key = k
    metadatum.ns = ns.encode()
    _assign_value(metadatum, v)
    unit_metadata_updates.append(unit_metadata_update)
  return unit_metadata_updates


def to_request_proto(
    study_resource_name: str, delta: trial.MetadataDelta
) -> vizier_service_pb2.UpdateMetadataRequest:
  """Create an UpdateMetadataRequest proto.

  Args:
    study_resource_name:
    delta:

  Returns:
  """
  request = vizier_service_pb2.UpdateMetadataRequest(name=study_resource_name)

  # Study Metadata
  request.delta.extend(study_metadata_to_update_list(delta.on_study))
  # Trial metadata
  request.delta.extend(trial_metadata_to_update_list(delta.on_trials))
  return request


def merge_study_metadata(
    study_spec: study_pb2.StudySpec,
    new_metadata: Iterable[key_value_pb2.KeyValue],
) -> None:
  """Merges $new_metadata into a Study's existing metadata."""
  metadata_dict: Dict[Tuple[str, str], key_value_pb2.KeyValue] = {}
  for kv in study_spec.metadata:
    metadata_dict[(kv.ns, kv.key)] = kv
  for kv in new_metadata:
    metadata_dict[(kv.ns, kv.key)] = kv
  study_spec.ClearField('metadata')
  study_spec.metadata.extend(
      sorted(metadata_dict.values(), key=lambda kv: (kv.ns, kv.key))
  )


def merge_trial_metadata(
    trial_proto: study_pb2.Trial,
    new_metadata: Iterable[vizier_service_pb2.UnitMetadataUpdate],
) -> None:
  """Merges $new_metadata into a Trial's existing metadata.

  Args:
    trial_proto: A representation of a Trial; this will be modified.
    new_metadata: Metadata that will add or update metadata in the Trial.
  NOTE: the metadata updates in $new_metadata should have the same ID as
    $trial_proto.
  """
  metadata_dict: Dict[Tuple[str, str], key_value_pb2.KeyValue] = {}
  for kv in trial_proto.metadata:
    metadata_dict[(kv.ns, kv.key)] = kv
  for md_update in new_metadata:
    if md_update.trial_id == trial_proto.id:
      metadata_dict[(md_update.metadatum.ns, md_update.metadatum.key)] = (
          md_update.metadatum
      )
    else:
      logging.warning(
          'Metadata associated with wrong trial: %s instead of %s',
          md_update.trial_id,
          trial_proto.id,
      )
  trial_proto.ClearField('metadata')
  trial_proto.metadata.extend(
      sorted(metadata_dict.values(), key=lambda kv: (kv.ns, kv.key))
  )


--- vizier/_src/pyvizier/oss/metadata_util_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.oss.metadata_util."""

from vizier._src.pyvizier.oss import metadata_util
from vizier._src.service import study_pb2

from absl.testing import absltest


class MetadataUtilTest(absltest.TestCase):

  def test_get(self):
    meta_trial = study_pb2.Trial(id='meta_trial')
    trial = study_pb2.Trial(id='trial')
    metadata_util.assign(trial, key='any', ns='', value=meta_trial)
    metadata_util.assign(trial, key='text', ns='x', value='x-value')
    metadata_util.assign(trial, key='text', ns='', value='value')
    metadata_util.assign(trial, key='text', ns='y', value='y-value')
    self.assertEqual(
        metadata_util.get_proto(trial, key='any', ns='', cls=study_pb2.Trial),
        meta_trial)
    self.assertEqual(metadata_util.get(trial, key='text', ns=''), 'value')
    self.assertEqual(metadata_util.get(trial, key='text', ns='x'), 'x-value')

    self.assertIsNone(
        metadata_util.get_proto(trial, key='any', ns='', cls=study_pb2.Study))
    self.assertIsNone(metadata_util.get(trial, key='TYPO', ns=''))
    self.assertIsNone(
        metadata_util.get_proto(trial, key='TYPO', ns='', cls=study_pb2.Trial))

  def test_assign(self):
    trial = study_pb2.Trial(id='trial')
    metadata_util.assign(trial, key='k', ns='', value='value')
    self.assertEqual(metadata_util.get(trial, key='k', ns=''), 'value')
    metadata_util.assign(
        trial, key='k', ns='', value='222', mode='insert_or_assign'
    )
    self.assertEqual(metadata_util.get(trial, key='k', ns=''), '222')


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/oss/proto_converters.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Converters for OSS Vizier's protos from/to PyVizier's classes."""

import datetime
import logging
from typing import Iterable, List, Optional, Sequence, Tuple, Union

from absl import logging
from vizier._src.pythia import policy
from vizier._src.pyvizier.oss import metadata_util
from vizier._src.pyvizier.pythia import study
from vizier._src.pyvizier.shared import base_study_config
from vizier._src.pyvizier.shared import common
from vizier._src.pyvizier.shared import parameter_config
from vizier._src.pyvizier.shared import trial
from vizier._src.service import pythia_service_pb2
from vizier._src.service import study_pb2
from vizier._src.service import vizier_service_pb2


ScaleType = parameter_config.ScaleType
_ScaleTypePb2 = study_pb2.StudySpec.ParameterSpec.ScaleType
ExternalType = parameter_config.ExternalType
_ExternalTypePb2 = study_pb2.StudySpec.ParameterSpec.ExternalType
ParameterType = parameter_config.ParameterType
MonotypeParameterSequence = parameter_config.MonotypeParameterSequence


class StudyStateConverter:
  """Proto converter for Study states."""

  _pyvizier_to_proto = {
      study.StudyState.ACTIVE: study_pb2.Study.State.ACTIVE,
      study.StudyState.ABORTED: study_pb2.Study.State.INACTIVE,
      study.StudyState.COMPLETED: study_pb2.Study.State.COMPLETED,
  }
  _proto_to_pyvizier = {v: k for k, v in _pyvizier_to_proto.items()}

  @classmethod
  def to_proto(cls, state: study.StudyState) -> study_pb2.Study.State:
    if state in cls._pyvizier_to_proto:
      return cls._pyvizier_to_proto[state]
    return study_pb2.Study.State.STATE_UNSPECIFIED

  @classmethod
  def from_proto(cls, proto: study_pb2.Study.State) -> study.StudyState:
    if proto in cls._proto_to_pyvizier:
      return cls._proto_to_pyvizier[proto]
    elif proto == study_pb2.Study.State.STATE_UNSPECIFIED:
      # OSS Vizier server treats STATE_UNSPECIFIED as ACTIVE.
      return study.StudyState.ACTIVE
    else:
      raise ValueError(
          'Proto Study state {} has no equivalent in PyVizier.'.format(
              study_pb2.Study.State.Name(proto)
          )
      )


class _ScaleTypeMap:
  """Proto converter for scale type."""

  _pyvizier_to_proto = {
      ScaleType.LINEAR: _ScaleTypePb2.UNIT_LINEAR_SCALE,
      ScaleType.LOG: _ScaleTypePb2.UNIT_LOG_SCALE,
      ScaleType.REVERSE_LOG: _ScaleTypePb2.UNIT_REVERSE_LOG_SCALE,
  }
  _proto_to_pyvizier = {v: k for k, v in _pyvizier_to_proto.items()}

  @classmethod
  def to_proto(cls, pyvizier: ScaleType) -> _ScaleTypePb2:
    return cls._pyvizier_to_proto[pyvizier]

  @classmethod
  def from_proto(cls, proto: _ScaleTypePb2) -> ScaleType:
    return cls._proto_to_pyvizier[proto]


class _ExternalTypeMap:
  """Proto converter for external type."""

  _pyvizier_to_proto = dict([
      (ExternalType.INTERNAL, _ExternalTypePb2.AS_INTERNAL),
      (ExternalType.BOOLEAN, _ExternalTypePb2.AS_BOOLEAN),
      (ExternalType.INTEGER, _ExternalTypePb2.AS_INTEGER),
      (ExternalType.FLOAT, _ExternalTypePb2.AS_FLOAT),
  ])
  _proto_to_pyvizier = {v: k for k, v in _pyvizier_to_proto.items()}

  @classmethod
  def to_proto(cls, pyvizier: ExternalType) -> _ExternalTypePb2:
    return cls._pyvizier_to_proto[pyvizier]

  @classmethod
  def from_proto(cls, proto: _ExternalTypePb2) -> ExternalType:
    return cls._proto_to_pyvizier[proto]


class ParameterConfigConverter:
  """Converter for ParameterConfig."""

  @classmethod
  def _set_bounds(
      cls,
      proto: study_pb2.StudySpec.ParameterSpec,
      lower: float,
      upper: float,
      parameter_type: ParameterType,
  ):
    """Sets the proto's min_value and max_value fields."""
    if parameter_type == ParameterType.INTEGER:
      proto.integer_value_spec.min_value = lower
      proto.integer_value_spec.max_value = upper
    elif parameter_type == ParameterType.DOUBLE:
      proto.double_value_spec.min_value = lower
      proto.double_value_spec.max_value = upper

  @classmethod
  def _set_feasible_points(
      cls,
      proto: study_pb2.StudySpec.ParameterSpec,
      feasible_points: Sequence[float],
  ):
    """Sets the proto's feasible_points field."""
    feasible_points = sorted(feasible_points)
    proto.discrete_value_spec.ClearField('values')
    proto.discrete_value_spec.values.extend(feasible_points)

  @classmethod
  def _set_categories(
      cls, proto: study_pb2.StudySpec.ParameterSpec, categories: Sequence[str]
  ):
    """Sets the protos' categories field."""
    proto.categorical_value_spec.ClearField('values')
    proto.categorical_value_spec.values.extend(categories)

  @classmethod
  def _set_default_value(
      cls,
      proto: study_pb2.StudySpec.ParameterSpec,
      default_value: Union[float, int, str],
  ):
    """Sets the protos' default_value field."""
    which_pv_spec = proto.WhichOneof('parameter_value_spec')
    getattr(proto, which_pv_spec).default_value.value = default_value

  @classmethod
  def _matching_parent_values(
      cls, proto: study_pb2.StudySpec.ParameterSpec.ConditionalParameterSpec
  ) -> MonotypeParameterSequence:
    """Returns the matching parent values, if set."""
    oneof_name = proto.WhichOneof('parent_value_condition')
    if not oneof_name:
      return []
    if oneof_name in (
        'parent_discrete_values',
        'parent_int_values',
        'parent_categorical_values',
    ):
      return list(getattr(getattr(proto, oneof_name), 'values'))
    raise ValueError('Unknown matching_parent_vals: {}'.format(oneof_name))

  @classmethod
  def from_proto(
      cls,
      proto: study_pb2.StudySpec.ParameterSpec,
      *,
      strict_validation: bool = False,
  ) -> parameter_config.ParameterConfig:
    """Creates a ParameterConfig.

    Args:
      proto:
      strict_validation: If True, raise ValueError to enforce that
        from_proto(proto).to_proto == proto.

    Returns:
      ParameterConfig object

    Raises:
      ValueError: See the "strict_validtion" arg documentation.
    """
    bounds = None
    feasible_values = None
    oneof_name = proto.WhichOneof('parameter_value_spec')
    if oneof_name == 'integer_value_spec':
      bounds = (
          int(proto.integer_value_spec.min_value),
          int(proto.integer_value_spec.max_value),
      )
    elif oneof_name == 'double_value_spec':
      bounds = (
          proto.double_value_spec.min_value,
          proto.double_value_spec.max_value,
      )
    elif oneof_name == 'discrete_value_spec':
      bounds = None
      feasible_values = proto.discrete_value_spec.values
    elif oneof_name == 'categorical_value_spec':
      bounds = None
      feasible_values = proto.categorical_value_spec.values

    default_value = None
    if getattr(proto, oneof_name).default_value.value:
      default_value = getattr(proto, oneof_name).default_value.value

    if proto.conditional_parameter_specs:
      children = []
      for conditional_ps in proto.conditional_parameter_specs:
        parent_values = cls._matching_parent_values(conditional_ps)
        children.append(
            (parent_values, cls.from_proto(conditional_ps.parameter_spec))
        )
    else:
      children = None

    scale_type = None
    if proto.scale_type:
      scale_type = _ScaleTypeMap.from_proto(proto.scale_type)

    external_type = None
    if proto.external_type:
      external_type = _ExternalTypeMap.from_proto(proto.external_type)

    try:
      config = parameter_config.ParameterConfig.factory(
          name=proto.parameter_id,
          feasible_values=feasible_values,
          bounds=bounds,
          children=children,
          scale_type=scale_type,
          default_value=default_value,
          external_type=external_type,
      )
    except ValueError as e:
      raise ValueError(
          'The provided proto was misconfigured. {}'.format(proto)
      ) from e

    if strict_validation and cls.to_proto(config) != proto:
      raise ValueError(
          'The provided proto was misconfigured. Expected: {} Given: {}'.format(
              cls.to_proto(config), proto
          )
      )
    return config

  @classmethod
  def _set_child_parameter_configs(
      cls,
      parent_proto: study_pb2.StudySpec.ParameterSpec,
      pc: parameter_config.ParameterConfig,
  ):
    """Sets the parent_proto's conditional_parameter_specs field.

    Args:
      parent_proto: Modified in place.
      pc: Parent ParameterConfig to copy children from.

    Raises:
      ValueError: If the child configs are invalid
    """
    children: List[
        Tuple[MonotypeParameterSequence, parameter_config.ParameterConfig]
    ] = []
    for child in pc.child_parameter_configs:
      children.append((child.matching_parent_values, child))
    if not children:
      return

    parent_proto.ClearField('conditional_parameter_specs')
    for child_pair in children:
      if len(child_pair) != 2:
        raise ValueError("""Each element in children must be a tuple of
            (Sequence of valid parent values,  ParameterConfig)""")

    logging.debug(
        '_set_child_parameter_configs: parent_proto=%s, children=%s',
        parent_proto,
        children,
    )
    for unsorted_parent_values, child in children:
      parent_values = sorted(unsorted_parent_values)
      child_proto = cls.to_proto(child.clone_without_children)
      conditional_parameter_spec = (
          study_pb2.StudySpec.ParameterSpec.ConditionalParameterSpec(
              parameter_spec=child_proto
          )
      )

      if parent_proto.HasField('discrete_value_spec'):
        conditional_parameter_spec.parent_discrete_values.values[:] = (
            parent_values
        )
      elif parent_proto.HasField('categorical_value_spec'):
        conditional_parameter_spec.parent_categorical_values.values[:] = (
            parent_values
        )
      elif parent_proto.HasField('integer_value_spec'):
        conditional_parameter_spec.parent_int_values.values[:] = parent_values
      else:
        raise ValueError('DOUBLE type cannot have child parameters')
      if child.child_parameter_configs:
        cls._set_child_parameter_configs(child_proto, child)
      parent_proto.conditional_parameter_specs.extend(
          [conditional_parameter_spec]
      )

  @classmethod
  def to_proto(
      cls, pc: parameter_config.ParameterConfig
  ) -> study_pb2.StudySpec.ParameterSpec:
    """Returns a ParameterConfig Proto."""
    proto = study_pb2.StudySpec.ParameterSpec(parameter_id=pc.name)
    if pc.type == ParameterType.DISCRETE:
      cls._set_feasible_points(proto, [float(v) for v in pc.feasible_values])
    elif pc.type == ParameterType.CATEGORICAL:
      cls._set_categories(proto, pc.feasible_values)
    elif pc.type in (ParameterType.INTEGER, ParameterType.DOUBLE):
      cls._set_bounds(proto, pc.bounds[0], pc.bounds[1], pc.type)
    else:
      raise ValueError('Invalid ParameterConfig: {}'.format(pc))
    if (
        pc.scale_type is not None
        and pc.scale_type != ScaleType.UNIFORM_DISCRETE
    ):
      proto.scale_type = _ScaleTypeMap.to_proto(pc.scale_type)
    if pc.default_value is not None:
      cls._set_default_value(proto, pc.default_value)
    if pc.external_type is not None:
      proto.external_type = _ExternalTypeMap.to_proto(pc.external_type)

    cls._set_child_parameter_configs(proto, pc)
    return proto


class ParameterValueConverter:
  """Converter for vz.ParameterValue."""

  @classmethod
  def from_proto(
      cls, proto: study_pb2.Trial.Parameter
  ) -> Optional[trial.ParameterValue]:
    """Returns whichever value that is populated, or None."""
    value_proto = proto.value
    oneof_name = value_proto.WhichOneof('kind')
    potential_value = getattr(value_proto, oneof_name)
    if (
        isinstance(potential_value, float)
        or isinstance(potential_value, str)
        or isinstance(potential_value, bool)
    ):
      return trial.ParameterValue(potential_value)
    else:
      return None

  @classmethod
  def to_proto(
      cls, parameter_value: trial.ParameterValue, name: str
  ) -> study_pb2.Trial.Parameter:
    """Returns Parameter Proto."""
    proto = study_pb2.Trial.Parameter(parameter_id=name)

    if isinstance(parameter_value.value, int):
      proto.value.number_value = parameter_value.value
    elif isinstance(parameter_value.value, bool):
      proto.value.bool_value = parameter_value.value
    elif isinstance(parameter_value.value, float):
      proto.value.number_value = parameter_value.value
    elif isinstance(parameter_value.value, str):
      proto.value.string_value = parameter_value.value

    return proto


class MeasurementConverter:
  """Converter for vz.Measurement."""

  @classmethod
  def from_proto(cls, proto: study_pb2.Measurement) -> trial.Measurement:
    """Creates a valid instance from proto.

    Args:
      proto: Measurement proto.

    Returns:
      A valid instance of Measurement object. Metrics with invalid values
      are automatically filtered out.
    """

    metrics = dict()

    for metric in proto.metrics:
      if (
          metric.metric_id in metrics
          and metrics[metric.metric_id].value != metric.value
      ):
        logging.log_first_n(
            logging.ERROR,
            (
                'Duplicate metric of name "%s".'
                'The newly found value %s will be used and '
                'the previously found value %s will be discarded.'
                'This always happens if the proto has an empty-named metric.'
            ),
            5,
            metric.metric_id,
            metric.value,
            metrics[metric.metric_id].value,
        )
      try:
        metrics[metric.metric_id] = trial.Metric(value=metric.value)
      except ValueError:
        pass
    return trial.Measurement(
        metrics=metrics,
        elapsed_secs=proto.elapsed_duration.seconds,
        steps=proto.step_count,
    )

  @classmethod
  def to_proto(cls, measurement: trial.Measurement) -> study_pb2.Measurement:
    """Converts to Measurement proto."""
    proto = study_pb2.Measurement()
    for name, metric in measurement.metrics.items():
      proto.metrics.add(metric_id=name, value=metric.value)

    proto.step_count = measurement.steps
    int_seconds = int(measurement.elapsed_secs)
    proto.elapsed_duration.seconds = int_seconds
    proto.elapsed_duration.nanos = int(
        1e9 * (measurement.elapsed_secs - int_seconds)
    )
    return proto


class MetricInformationConverter:
  """A converter to/from study_pb2.StudySpec.MetricInformation."""

  @classmethod
  def from_proto(
      cls, proto: study_pb2.StudySpec.MetricSpec
  ) -> base_study_config.MetricInformation:
    """Converts a MetricInformation proto to a MetricInformation object."""
    if proto.goal not in list(base_study_config.ObjectiveMetricGoal):
      raise ValueError('Unknown MetricInformation.goal: {}'.format(proto.goal))

    safety_threshold = None
    desired_min_safe_trials_fraction = None

    if proto.HasField('safety_config'):
      safety_threshold = proto.safety_config.safety_threshold
    if proto.safety_config.HasField('desired_min_safe_trials_fraction'):
      desired_min_safe_trials_fraction = (
          proto.safety_config.desired_min_safe_trials_fraction
      )

    return base_study_config.MetricInformation(
        name=proto.metric_id,
        goal=proto.goal,
        safety_threshold=safety_threshold,
        desired_min_safe_trials_fraction=desired_min_safe_trials_fraction,
        min_value=None,
        max_value=None,
    )

  @classmethod
  def to_proto(
      cls, obj: base_study_config.MetricInformation
  ) -> study_pb2.StudySpec.MetricSpec:
    """Returns this object as a proto."""

    proto = study_pb2.StudySpec.MetricSpec(
        metric_id=obj.name, goal=obj.goal.value
    )

    if obj.type == base_study_config.MetricType.SAFETY:
      proto.safety_config.safety_threshold = obj.safety_threshold
      if obj.desired_min_safe_trials_fraction is not None:
        proto.safety_config.desired_min_safe_trials_fraction = (
            obj.desired_min_safe_trials_fraction
        )
    return proto


class SearchSpaceConverter:
  """A wrapper for study_pb2.StudySpec."""

  @classmethod
  def from_proto(
      cls, proto: study_pb2.StudySpec
  ) -> parameter_config.SearchSpace:
    """Extracts a SearchSpace object from a StudyConfig proto."""
    space = parameter_config.SearchSpace()
    for pc in proto.parameters:
      space.add(ParameterConfigConverter.from_proto(pc))
    return space

  @classmethod
  def parameter_protos(
      cls, obj: parameter_config.SearchSpace
  ) -> List[study_pb2.StudySpec.ParameterSpec]:
    """Returns the search space as a List of ParameterConfig protos."""
    return [ParameterConfigConverter.to_proto(pc) for pc in obj.parameters]


class MetricsConfigConverter:
  """A wrapper for study_pb2.StudySpec.MetricSpec's."""

  @classmethod
  def from_protos(
      cls, protos: Iterable[study_pb2.StudySpec.MetricSpec]
  ) -> base_study_config.MetricsConfig:
    return base_study_config.MetricsConfig(
        [MetricInformationConverter.from_proto(m) for m in protos]
    )

  @classmethod
  def to_protos(
      cls, obj: base_study_config.MetricsConfig
  ) -> List[study_pb2.StudySpec.MetricSpec]:
    return [MetricInformationConverter.to_proto(metric) for metric in obj]


def _to_pyvizier_trial_status(
    proto_state: study_pb2.Trial.State,
) -> trial.TrialStatus:
  """from_proto conversion for Trial statuses."""
  if proto_state == study_pb2.Trial.State.REQUESTED:
    return trial.TrialStatus.REQUESTED
  elif proto_state == study_pb2.Trial.State.ACTIVE:
    return trial.TrialStatus.ACTIVE
  if proto_state == study_pb2.Trial.State.STOPPING:
    return trial.TrialStatus.STOPPING
  if proto_state == study_pb2.Trial.State.SUCCEEDED:
    return trial.TrialStatus.COMPLETED
  elif proto_state == study_pb2.Trial.State.INFEASIBLE:
    return trial.TrialStatus.COMPLETED
  else:
    return trial.TrialStatus.UNKNOWN


def _from_pyvizier_trial_status(
    status: trial.TrialStatus, infeasible: bool
) -> study_pb2.Trial.State:
  """to_proto conversion for Trial states."""
  if status == trial.TrialStatus.REQUESTED:
    return study_pb2.Trial.State.REQUESTED
  elif status == trial.TrialStatus.ACTIVE:
    return study_pb2.Trial.State.ACTIVE
  elif status == trial.TrialStatus.STOPPING:
    return study_pb2.Trial.State.STOPPING
  elif status == trial.TrialStatus.COMPLETED:
    if infeasible:
      return study_pb2.Trial.State.INFEASIBLE
    else:
      return study_pb2.Trial.State.SUCCEEDED
  else:
    return study_pb2.Trial.State.STATE_UNSPECIFIED


class TrialConverter:
  """Converter for vz.Trial."""

  @classmethod
  def from_proto(cls, proto: study_pb2.Trial) -> trial.Trial:
    """Converts from Trial proto to object.

    Args:
      proto: Trial proto.

    Returns:
      A Trial object.
    """
    parameters = {}
    for parameter in proto.parameters:
      value = ParameterValueConverter.from_proto(parameter)
      if value is not None:
        if parameter.parameter_id in parameters:
          raise ValueError(
              'Invalid trial proto contains duplicate parameter {}: {}'.format(
                  parameter.parameter_id, proto
              )
          )
        parameters[parameter.parameter_id] = value
      else:
        logging.warning(
            'A parameter without a value will be dropped: %s', parameter
        )

    final_measurement = None
    if proto.HasField('final_measurement'):
      final_measurement = MeasurementConverter.from_proto(
          proto.final_measurement
      )

    completion_time = None
    infeasibility_reason = None
    if proto.state == study_pb2.Trial.State.SUCCEEDED:
      if proto.HasField('end_time'):
        completion_ts = proto.end_time.seconds + 1e-9 * proto.end_time.nanos
        completion_time = datetime.datetime.fromtimestamp(completion_ts)
    elif proto.state == study_pb2.Trial.State.INFEASIBLE:
      infeasibility_reason = proto.infeasible_reason

    metadata = common.Metadata()
    for kv in proto.metadata:
      metadata.abs_ns(common.Namespace.decode(kv.ns))[kv.key] = (
          kv.proto if kv.HasField('proto') else kv.value
      )

    measurements = []
    for measure in proto.measurements:
      measurements.append(MeasurementConverter.from_proto(measure))

    creation_time = None
    if proto.HasField('start_time'):
      creation_ts = proto.start_time.seconds + 1e-9 * proto.start_time.nanos
      creation_time = datetime.datetime.fromtimestamp(creation_ts)
    return trial.Trial(
        id=int(proto.id),
        description=proto.name,
        assigned_worker=proto.client_id or None,
        is_requested=proto.state == proto.REQUESTED,
        stopping_reason=(
            'stopping reason not supported yet'
            if proto.state == proto.STOPPING
            else None
        ),
        parameters=parameters,
        creation_time=creation_time,
        completion_time=completion_time,
        infeasibility_reason=infeasibility_reason,
        final_measurement=final_measurement,
        measurements=measurements,
        metadata=metadata,
    )  # pytype: disable=wrong-arg-types

  @classmethod
  def from_protos(cls, protos: Iterable[study_pb2.Trial]) -> List[trial.Trial]:
    """Convenience wrapper for from_proto."""
    return [TrialConverter.from_proto(proto) for proto in protos]

  @classmethod
  def to_protos(cls, pytrials: Iterable[trial.Trial]) -> List[study_pb2.Trial]:
    return [TrialConverter.to_proto(pytrial) for pytrial in pytrials]

  @classmethod
  def to_proto(cls, pytrial: trial.Trial) -> study_pb2.Trial:
    """Converts a pyvizier Trial to a Trial proto."""
    proto = study_pb2.Trial()
    if pytrial.description is not None:
      proto.name = pytrial.description
    proto.id = str(pytrial.id)
    proto.state = _from_pyvizier_trial_status(
        pytrial.status, pytrial.infeasible
    )
    proto.client_id = pytrial.assigned_worker or ''

    for name, value in pytrial.parameters.items():
      proto.parameters.append(ParameterValueConverter.to_proto(value, name))

    # pytrial always adds an empty metric. Ideally, we should remove it if the
    # metric does not exist in the study config.
    if pytrial.final_measurement is not None:
      proto.final_measurement.CopyFrom(
          MeasurementConverter.to_proto(pytrial.final_measurement)
      )

    for measurement in pytrial.measurements:
      proto.measurements.append(MeasurementConverter.to_proto(measurement))

    if pytrial.creation_time is not None:
      creation_secs = datetime.datetime.timestamp(pytrial.creation_time)
      proto.start_time.seconds = int(creation_secs)
      proto.start_time.nanos = int(1e9 * (creation_secs - int(creation_secs)))
    if pytrial.completion_time is not None:
      completion_secs = datetime.datetime.timestamp(pytrial.completion_time)
      proto.end_time.seconds = int(completion_secs)
      proto.end_time.nanos = int(1e9 * (completion_secs - int(completion_secs)))
    if pytrial.infeasibility_reason is not None:
      proto.infeasible_reason = pytrial.infeasibility_reason
    if pytrial.metadata is not None:
      for ns in pytrial.metadata.namespaces():
        ns_string = ns.encode()
        ns_layer = pytrial.metadata.abs_ns(ns)
        for key, value in ns_layer.items():
          metadata_util.assign(proto, key=key, ns=ns_string, value=value)
    return proto


class TrialSuggestionConverter:
  """Converts vz.TrialSuggestion <--> Pythia TrialSuggestion proto."""

  @classmethod
  def from_proto(
      cls, proto: pythia_service_pb2.TrialSuggestion
  ) -> trial.TrialSuggestion:
    """Converts from TrialSuggestion proto to PyVizier TrialSuggestion."""
    parameters = {}
    for parameter in proto.parameters:
      value = ParameterValueConverter.from_proto(parameter)
      if value is None:
        raise RuntimeError('Parameter %s exists without a value.' % parameter)
      if parameter.parameter_id in parameters:
        raise ValueError(
            'Invalid trial proto contains duplicate parameter {}: {}'.format(
                parameter.parameter_id, proto
            )
        )
      parameters[parameter.parameter_id] = value

    metadata = common.Metadata()
    for kv in proto.metadata:
      metadata.abs_ns(common.Namespace.decode(kv.ns))[kv.key] = (
          kv.proto if kv.HasField('proto') else kv.value
      )

    return trial.TrialSuggestion(parameters=parameters, metadata=metadata)

  @classmethod
  def from_protos(
      cls, protos: Iterable[pythia_service_pb2.TrialSuggestion]
  ) -> List[trial.TrialSuggestion]:
    """Convenience wrapper for from_proto."""
    return [cls.from_proto(proto) for proto in protos]

  @classmethod
  def to_protos(
      cls, pytrials: Iterable[trial.TrialSuggestion]
  ) -> List[pythia_service_pb2.TrialSuggestion]:
    return [cls.to_proto(pytrial) for pytrial in pytrials]

  @classmethod
  def to_proto(
      cls, suggestion: trial.TrialSuggestion
  ) -> pythia_service_pb2.TrialSuggestion:
    """Converts a pyvizier TrialSuggestion to the corresponding proto."""
    proto = pythia_service_pb2.TrialSuggestion()

    for name, value in suggestion.parameters.items():
      proto.parameters.append(ParameterValueConverter.to_proto(value, name))

    proto.metadata.extend(
        metadata_util.make_key_value_list(suggestion.metadata)
    )
    return proto


class MetadataDeltaConverter:
  """Converts pyvizier.MetadataDelta <--> List of UnitMetadataUpdate protos."""

  @classmethod
  def to_protos(
      cls, delta: trial.MetadataDelta
  ) -> List[vizier_service_pb2.UnitMetadataUpdate]:
    """Converts pyvizier.MetadataDelta to a List of UnitMetadataUpdate protos."""
    unit_metadata_updates = metadata_util.study_metadata_to_update_list(
        delta.on_study
    )
    unit_metadata_updates.extend(
        metadata_util.trial_metadata_to_update_list(delta.on_trials)
    )
    return unit_metadata_updates

  @classmethod
  def from_protos(
      cls, protos: Iterable[vizier_service_pb2.UnitMetadataUpdate]
  ) -> trial.MetadataDelta:
    """Converts a list of UnitMetadataUpdate protos to pyvizier.MetadataDelta."""
    mdd = trial.MetadataDelta()
    for u_m_u in protos:
      key_value = u_m_u.metadatum
      namespace = common.Namespace.decode(key_value.ns)
      value = (
          key_value.proto if key_value.HasField('proto') else key_value.value
      )
      if u_m_u.HasField('trial_id'):
        mdd.on_trials[int(u_m_u.trial_id)].abs_ns(namespace)[
            key_value.key
        ] = value
      else:
        mdd.on_study.abs_ns(namespace)[key_value.key] = value
    return mdd


class ProblemStatementConverter:
  """Converts pyvizier.ProblemStatement <-> Pythia ProblemStatement proto."""

  @classmethod
  def to_proto(
      cls,
      problem_statement: base_study_config.ProblemStatement,
  ) -> pythia_service_pb2.ProblemStatement:
    """Converts PyVizier ProblemStatement to Proto version."""
    parameter_spec_protos = SearchSpaceConverter.parameter_protos(
        problem_statement.search_space
    )
    metric_information_protos = MetricsConfigConverter.to_protos(
        problem_statement.metric_information
    )
    keyvalue_protos = metadata_util.make_key_value_list(
        problem_statement.metadata
    )
    return pythia_service_pb2.ProblemStatement(
        search_space=parameter_spec_protos,
        metric_information=metric_information_protos,
        metadata=keyvalue_protos,
    )

  @classmethod
  def from_proto(
      cls, proto: pythia_service_pb2.ProblemStatement
  ) -> base_study_config.ProblemStatement:
    """Converts ProblemStatement Proto to PyVizier version."""
    study_spec = study_pb2.StudySpec(parameters=proto.search_space)
    search_space = SearchSpaceConverter.from_proto(study_spec)
    metric_information = MetricsConfigConverter.from_protos(
        proto.metric_information
    )
    metadata = metadata_util.from_key_value_list(proto.metadata)
    return base_study_config.ProblemStatement(
        search_space=search_space,
        metric_information=metric_information,
        metadata=metadata,
    )


class StudyDescriptorConverter:
  """Converts Pythia StudyDescriptorConverter <-> Pythia StudyDescriptor proto."""

  @classmethod
  def to_proto(
      cls,
      study_descriptor: study.StudyDescriptor,
  ) -> pythia_service_pb2.StudyDescriptor:
    return pythia_service_pb2.StudyDescriptor(
        config=ProblemStatementConverter.to_proto(study_descriptor.config),
        guid=study_descriptor.guid,
        max_trial_id=study_descriptor.max_trial_id,
    )

  @classmethod
  def from_proto(
      cls, proto: pythia_service_pb2.StudyDescriptor
  ) -> study.StudyDescriptor:
    return study.StudyDescriptor(
        config=ProblemStatementConverter.from_proto(proto.config),
        guid=proto.guid,
        max_trial_id=proto.max_trial_id,
    )


class SuggestConverter:
  """Converts a SuggestRequest class <--> a SuggestRequest proto."""

  @classmethod
  def to_request_proto(
      cls,
      request: policy.SuggestRequest,
  ) -> pythia_service_pb2.SuggestRequest:
    """Conversion from PyVizier to proto."""
    study_descriptor_proto = StudyDescriptorConverter.to_proto(
        request._study_descriptor  # pylint:disable=protected-access
    )
    return pythia_service_pb2.SuggestRequest(
        study_descriptor=study_descriptor_proto,
        count=request.count,
        checkpoint_dir=request.checkpoint_dir,
    )

  @classmethod
  def from_request_proto(
      cls,
      proto: pythia_service_pb2.SuggestRequest,
  ) -> policy.SuggestRequest:
    """Conversion from proto to PyVizier."""
    study_descriptor = StudyDescriptorConverter.from_proto(
        proto.study_descriptor
    )
    return policy.SuggestRequest(
        study_descriptor=study_descriptor,
        count=proto.count,
        checkpoint_dir=proto.checkpoint_dir,
    )

  @classmethod
  def to_decision_proto(
      cls,
      decision: policy.SuggestDecision,
  ) -> pythia_service_pb2.SuggestDecision:
    """Conversion from PyVizier to proto."""
    trial_suggestion_protos = TrialSuggestionConverter.to_protos(
        decision.suggestions
    )
    metadelta_protos = MetadataDeltaConverter.to_protos(decision.metadata)
    return pythia_service_pb2.SuggestDecision(
        suggestions=trial_suggestion_protos, metadata=metadelta_protos
    )

  @classmethod
  def from_decision_proto(
      cls,
      proto: pythia_service_pb2.SuggestDecision,
  ) -> policy.SuggestDecision:
    """Conversion from proto to PyVizier."""
    suggestions = TrialSuggestionConverter.from_protos(proto.suggestions)
    metadata = MetadataDeltaConverter.from_protos(proto.metadata)
    return policy.SuggestDecision(suggestions=suggestions, metadata=metadata)


class EarlyStopConverter:
  """Converts Pythia EarlyStopping <-> Pythia EarlyStopping protos."""

  @classmethod
  def to_request_proto(
      cls,
      request: policy.EarlyStopRequest,
  ) -> pythia_service_pb2.EarlyStopRequest:
    """Conversion from PyVizier to proto."""
    return pythia_service_pb2.EarlyStopRequest(
        study_descriptor=StudyDescriptorConverter.to_proto(
            request._study_descriptor  # pylint:disable=protected-access
        ),
        trial_ids=request.trial_ids,
        checkpoint_dir=request.checkpoint_dir,
    )

  @classmethod
  def from_request_proto(
      cls,
      proto: pythia_service_pb2.EarlyStopRequest,
  ) -> policy.EarlyStopRequest:
    """Conversion from proto to PyVizier."""
    study_descriptor = StudyDescriptorConverter.from_proto(
        proto.study_descriptor
    )
    return policy.EarlyStopRequest(
        study_descriptor=study_descriptor,
        trial_ids=proto.trial_ids,
        checkpoint_dir=proto.checkpoint_dir,
    )

  @classmethod
  def to_decisions_proto(
      cls,
      decisions: policy.EarlyStopDecisions,
  ) -> pythia_service_pb2.EarlyStopDecisions:
    """Conversion from PyVizier to proto."""
    decision_protos = []
    for decision in decisions.decisions:
      predicted_final_measurement_proto = study_pb2.Measurement()
      if decision.predicted_final_measurement:
        predicted_final_measurement_proto = MeasurementConverter.to_proto(
            decision.predicted_final_measurement
        )
      decision_proto = pythia_service_pb2.EarlyStopDecision(
          id=decision.id,
          reason=decision.reason,
          should_stop=decision.should_stop,
          predicted_final_measurement=predicted_final_measurement_proto,
      )
      decision_protos.append(decision_proto)
    key_value_protos = MetadataDeltaConverter.to_protos(decisions.metadata)
    return pythia_service_pb2.EarlyStopDecisions(
        decisions=decision_protos, metadata=key_value_protos
    )

  @classmethod
  def from_decisions_proto(
      cls,
      proto: pythia_service_pb2.EarlyStopDecisions,
  ) -> policy.EarlyStopDecisions:
    """Conversion from proto to PyVizier."""
    decisions = []
    for decision_proto in proto.decisions:
      decision = policy.EarlyStopDecision(
          id=decision_proto.id,
          reason=decision_proto.reason,
          should_stop=decision_proto.should_stop,
          predicted_final_measurement=MeasurementConverter.from_proto(
              decision_proto.predicted_final_measurement
          ),
      )
      decisions.append(decision)
    metadata = MetadataDeltaConverter.from_protos(proto.metadata)
    return policy.EarlyStopDecisions(decisions=decisions, metadata=metadata)


--- vizier/_src/pyvizier/oss/proto_converters_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for proto_converters."""

from absl import logging
import attr
from vizier._src.pyvizier.oss import proto_converters
from vizier._src.pyvizier.pythia import study
from vizier._src.pyvizier.shared import parameter_config as pc
from vizier._src.pyvizier.shared import trial
from vizier._src.service import study_pb2

from google.protobuf import struct_pb2
from google.protobuf import wrappers_pb2
from vizier._src.pyvizier.oss import compare
from absl.testing import absltest
from absl.testing import parameterized

Metric = trial.Metric
Measurement = trial.Measurement


class StudyStateConverterTest(absltest.TestCase):

  def testReversibility(self):
    for original_pyvizier_state in study.StudyState:
      proto_state = proto_converters.StudyStateConverter.to_proto(
          original_pyvizier_state
      )
      py_state = proto_converters.StudyStateConverter.from_proto(proto_state)
      self.assertEqual(original_pyvizier_state, py_state)

  def testStudyStateNotSet(self):
    self.assertEqual(
        proto_converters.StudyStateConverter.from_proto(
            study_pb2.Study.State.STATE_UNSPECIFIED
        ),
        study.StudyState.ACTIVE,
    )


class MeasurementConverterTest(absltest.TestCase):

  def testMeasurementProtoWithEmptyNamedMetric(self):
    proto = study_pb2.Measurement()
    proto.metrics.add(metric_id='', value=0.8)
    measurement = proto_converters.MeasurementConverter.from_proto(proto)
    self.assertEqual(measurement.metrics[''], Metric(value=0.8))

  def testMeasurementCreation(self):
    measurement = Measurement(
        metrics={
            '': Metric(
                value=0
            ),  # The empty metric always exists in Measurement.
            'pr-auc': Metric(value=0.8),
            'latency': Metric(value=32),
        },
        elapsed_secs=12,
        steps=12,
    )
    proto = proto_converters.MeasurementConverter.to_proto(measurement)
    self.assertEqual(
        attr.asdict(proto_converters.MeasurementConverter.from_proto(proto)),
        attr.asdict(measurement),
    )


ParameterValue = trial.ParameterValue


class ParameterValueConverterTest(parameterized.TestCase):

  def testToDoubleProto(self):
    value = ParameterValue(True)
    compare.assertProto2Equal(
        self,
        proto_converters.ParameterValueConverter.to_proto(value, 'aa'),
        study_pb2.Trial.Parameter(
            parameter_id='aa', value=struct_pb2.Value(number_value=1.0)
        ),
    )

  def testToDiscreteProto(self):
    value = ParameterValue(True)
    compare.assertProto2Equal(
        self,
        proto_converters.ParameterValueConverter.to_proto(value, 'aa'),
        study_pb2.Trial.Parameter(
            parameter_id='aa', value=struct_pb2.Value(number_value=1.0)
        ),
    )

  def testToStringProto(self):
    value = ParameterValue('category')
    compare.assertProto2Equal(
        self,
        proto_converters.ParameterValueConverter.to_proto(value, 'aa'),
        study_pb2.Trial.Parameter(
            parameter_id='aa', value=struct_pb2.Value(string_value='category')
        ),
    )

  def testToIntegerProto(self):
    value = ParameterValue(True)
    compare.assertProto2Equal(
        self,
        proto_converters.ParameterValueConverter.to_proto(value, 'aa'),
        study_pb2.Trial.Parameter(
            parameter_id='aa', value=struct_pb2.Value(number_value=1.0)
        ),
    )


class TrialConverterTest(absltest.TestCase):

  def testFromProtoCompleted(self):
    proto = study_pb2.Trial(id=str(1))
    proto.state = study_pb2.Trial.State.SUCCEEDED
    proto.parameters.add(
        parameter_id='float', value=struct_pb2.Value(number_value=1.0)
    )
    proto.parameters.add(
        parameter_id='int', value=struct_pb2.Value(number_value=2)
    )
    proto.parameters.add(
        parameter_id='str', value=struct_pb2.Value(string_value='3')
    )
    proto.final_measurement.metrics.add(metric_id='pr-auc', value=0.8)
    proto.final_measurement.metrics.add(metric_id='latency', value=32)

    proto.start_time.seconds = 1586649600
    proto.end_time.seconds = 1586649600 + 10

    proto.measurements.add(step_count=10)
    proto.measurements[-1].elapsed_duration.seconds = 15
    proto.measurements[-1].metrics.add(metric_id='pr-auc', value=0.7)
    proto.measurements[-1].metrics.add(metric_id='latency', value=42)

    proto.measurements.add(step_count=20)
    proto.measurements[-1].elapsed_duration.seconds = 30
    proto.measurements[-1].metrics.add(metric_id='pr-auc', value=0.75)
    proto.measurements[-1].metrics.add(metric_id='latency', value=37)

    test = proto_converters.TrialConverter.from_proto(proto=proto)
    self.assertEqual(test.id, 1)
    self.assertEqual(test.status, trial.TrialStatus.COMPLETED)
    self.assertTrue(test.is_completed)
    self.assertFalse(test.infeasible)
    self.assertIsNone(test.infeasibility_reason)
    self.assertLen(test.parameters, 3)
    self.assertEqual(test.parameters['float'].value, 1.0)
    self.assertEqual(test.parameters['int'].value, 2)
    self.assertEqual(test.parameters['str'].value, '3')

    # Final measurement
    assert test.final_measurement is not None
    self.assertLen(test.final_measurement.metrics, 2)
    self.assertEqual(test.final_measurement.metrics['pr-auc'].value, 0.8)
    self.assertEqual(test.final_measurement.metrics['latency'].value, 32)

    # Intermediate measurement
    self.assertEqual(
        test.measurements[0],
        trial.Measurement(
            metrics={'pr-auc': 0.7, 'latency': 42}, steps=10, elapsed_secs=15
        ),
    )
    self.assertEqual(
        test.measurements[1],
        trial.Measurement(
            metrics={'pr-auc': 0.75, 'latency': 37}, steps=20, elapsed_secs=30
        ),
    )

    self.assertEqual(test.id, 1)

    self.assertIsNotNone(test.creation_time)
    self.assertIsNotNone(test.completion_time)
    assert test.duration is not None
    self.assertEqual(test.duration.total_seconds(), 10)

    self.assertFalse(test.infeasible)

  def testFromProtoPending(self):
    proto = study_pb2.Trial(id=str(2))
    proto.state = study_pb2.Trial.State.ACTIVE
    proto.start_time.seconds = 1586649600
    test = proto_converters.TrialConverter.from_proto(proto=proto)
    self.assertEqual(test.status, trial.TrialStatus.ACTIVE)
    self.assertFalse(test.is_completed)
    self.assertFalse(test.infeasible)
    self.assertIsNone(test.infeasibility_reason)
    self.assertIsNotNone(test.creation_time)
    self.assertIsNone(test.completion_time)
    self.assertIsNone(test.duration)
    self.assertEmpty(test.metadata)

  def testFromProtoInfeasible(self):
    proto = study_pb2.Trial(id=str(1))
    proto.state = study_pb2.Trial.State.INFEASIBLE
    proto.parameters.add(
        parameter_id='float', value=struct_pb2.Value(number_value=1.0)
    )
    proto.parameters.add(
        parameter_id='int', value=struct_pb2.Value(number_value=2)
    )
    proto.parameters.add(
        parameter_id='str', value=struct_pb2.Value(string_value='3')
    )
    proto.start_time.seconds = 1586649600
    proto.end_time.seconds = 1586649600 + 10
    proto.infeasible_reason = 'A reason'

    test = proto_converters.TrialConverter.from_proto(proto=proto)
    self.assertEqual(test.status, trial.TrialStatus.COMPLETED)
    self.assertTrue(test.is_completed)
    self.assertTrue(test.infeasible)
    self.assertEqual(test.infeasibility_reason, 'A reason')

  def testFromProtoInvalidTrial(self):
    proto = study_pb2.Trial(id=str(2))
    proto.parameters.add(
        parameter_id='float', value=struct_pb2.Value(number_value=1.0)
    )
    proto.parameters.add(
        parameter_id='float', value=struct_pb2.Value(number_value=2.0)
    )
    proto.state = study_pb2.Trial.State.ACTIVE
    proto.start_time.seconds = 1586649600
    with self.assertRaisesRegex(ValueError, 'Invalid trial proto'):
      proto_converters.TrialConverter.from_proto(proto=proto)

  def testFromProtoMetadata(self):
    proto = study_pb2.Trial(id=str(1))
    proto.state = study_pb2.Trial.ACTIVE
    proto.parameters.add(
        parameter_id='float', value=struct_pb2.Value(number_value=1.0)
    )
    proto.metadata.add(key='key0', ns='x', value='namespace=x0')
    proto.metadata.add(key='key1', ns='x', value='namespace=x1')
    proto.metadata.add(key='key1', ns='', value='gets overwritten')
    proto.metadata.add(key='key1', value='second value takes priority')
    logging.info('PROTO:: %s', proto)
    added1 = proto.metadata.add(key='proto')
    added1.proto.Pack(study_pb2.Trial(id=str(999)))
    added2 = proto.metadata.add(key='proto', ns='t')
    added2.proto.Pack(study_pb2.Trial(id=str(991)))
    test = proto_converters.TrialConverter.from_proto(proto=proto)
    logging.info('TEST:: %s', repr(test.metadata))
    logging.info('TEST-x:: %s', repr(test.metadata.ns('x')))
    logging.info('TEST-t:: %s', repr(test.metadata.ns('t')))
    logging.info('TEST-namespaces: %s', test.metadata.namespaces())
    logging.info('test.ns("x"):: %s', test.metadata.ns('x'))
    self.assertEqual(test.metadata['key1'], 'second value takes priority')
    self.assertEqual(test.metadata.abs_ns(['x'])['key0'], 'namespace=x0')
    self.assertEqual(test.metadata.abs_ns(['x'])['key1'], 'namespace=x1')
    self.assertEqual(
        test.metadata.get_proto('proto', cls=study_pb2.Trial),
        study_pb2.Trial(id=str(999)),
    )
    self.assertEqual(
        test.metadata.abs_ns(['t']).get_proto('proto', cls=study_pb2.Trial),
        study_pb2.Trial(id=str(991)),
    )


class TrialConverterToProtoTest(absltest.TestCase):
  """Tests for TrialConverter.to_proto()."""

  def _GetSingleObjectiveBaseTrial(self):
    proto = study_pb2.Trial(
        name='owners/my_username/studies/cifar_10',
        id=str(2),
        client_id='worker0',
    )
    proto.parameters.add(
        parameter_id='activation', value=struct_pb2.Value(string_value='relu')
    )
    proto.parameters.add(
        parameter_id='synchronus', value=struct_pb2.Value(string_value='true')
    )
    proto.parameters.add(
        parameter_id='batch_size', value=struct_pb2.Value(number_value=32)
    )
    proto.parameters.add(
        parameter_id='floating_point_param',
        value=struct_pb2.Value(number_value=32.0),
    )
    proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5)
    )
    proto.parameters.add(
        parameter_id='units', value=struct_pb2.Value(number_value=50)
    )
    proto.start_time.seconds = 1630505100
    return proto

  def testParameterBackToBackConversion(self):
    proto = self._GetSingleObjectiveBaseTrial()
    proto.state = study_pb2.Trial.State.ACTIVE
    pytrial = proto_converters.TrialConverter.from_proto(proto)
    got = proto_converters.TrialConverter.to_proto(pytrial)
    compare.assertProto2Equal(self, proto, got)

  def testFinalMeasurementBackToBackConversion(self):
    proto = study_pb2.Trial(id=str(1), state=study_pb2.Trial.State.SUCCEEDED)
    proto.start_time.seconds = 12456
    proto.end_time.seconds = 12456 + 10
    proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5)
    )
    proto.final_measurement.step_count = 101
    proto.final_measurement.elapsed_duration.seconds = 67
    proto.final_measurement.metrics.add(metric_id='loss', value=56.8)
    proto.final_measurement.metrics.add(metric_id='objective', value=77.7)
    proto.final_measurement.metrics.add(metric_id='objective2', value=-0.2)

    pytrial = proto_converters.TrialConverter.from_proto(proto)
    got = proto_converters.TrialConverter.to_proto(pytrial)
    compare.assertProto2Equal(self, proto, got)

  def testMeasurementBackToBackConversion(self):
    proto = study_pb2.Trial(
        id=str(2), state=study_pb2.Trial.State.ACTIVE, client_id='worker0'
    )
    proto.start_time.seconds = 1630505100
    proto.measurements.add(step_count=123)
    proto.measurements[-1].elapsed_duration.seconds = 22
    proto.measurements[-1].metrics.add(metric_id='objective', value=0.4321)
    proto.measurements[-1].metrics.add(metric_id='loss', value=0.001)

    proto.measurements.add(step_count=789)
    proto.measurements[-1].elapsed_duration.seconds = 55
    proto.measurements[-1].metrics.add(metric_id='objective', value=0.21)
    proto.measurements[-1].metrics.add(metric_id='loss', value=0.0001)

    pytrial = proto_converters.TrialConverter.from_proto(proto)
    got = proto_converters.TrialConverter.to_proto(pytrial)
    compare.assertProto2Equal(self, proto, got)


class ParameterConfigConverterToProtoTest(absltest.TestCase):
  """Tests for ParameterConfigConverter.to_proto()."""

  def testDiscreteConfigToProto(self):
    feasible_values = (-1, 3, 2)
    parameter_config = pc.ParameterConfig.factory(
        'name',
        feasible_values=feasible_values,
        scale_type=pc.ScaleType.LOG,
        external_type=pc.ExternalType.INTEGER,
        default_value=2,
    )

    proto = proto_converters.ParameterConfigConverter.to_proto(parameter_config)
    self.assertEqual(proto.parameter_id, 'name')
    self.assertEqual(proto.discrete_value_spec.values, [-1.0, 2.0, 3.0])
    self.assertEqual(proto.discrete_value_spec.default_value.value, 2)
    self.assertEqual(
        proto.scale_type,
        study_pb2.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE,
    )
    self.assertEqual(
        proto.external_type,
        study_pb2.StudySpec.ParameterSpec.ExternalType.AS_INTEGER,
    )


class ParameterConfigConverterFromProtoTest(absltest.TestCase):
  """Tests for ParameterConfigConverter.from_proto()."""

  def testCreatesFromGoodProto(self):
    proto = study_pb2.StudySpec.ParameterSpec(
        parameter_id='name',
        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
            values=[1.0, 2.0, 3.0],
            default_value=wrappers_pb2.DoubleValue(value=2.0),
        ),
    )
    parameter_config = proto_converters.ParameterConfigConverter.from_proto(
        proto
    )
    self.assertEqual(parameter_config.name, proto.parameter_id)
    self.assertEqual(parameter_config.type, pc.ParameterType.DISCRETE)
    self.assertEqual(parameter_config.bounds, (1.0, 3.0))
    self.assertEqual(parameter_config.feasible_values, [1.0, 2.0, 3.0])
    self.assertEqual(parameter_config.default_value, 2.0)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/oss/study_config.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Convenience classes for configuring Vizier Study Configs and Search Spaces.

This module contains several classes, used to access/build Vizier StudyConfig
protos:
  * `StudyConfig` class is the main class, which:
  1) Allows to easily build Vizier StudyConfig protos via a convenient
     Python API.
  2) Can be initialized from an existing StudyConfig proto, to enable easy
     Pythonic accessors to information contained in StudyConfig protos,
     and easy field editing capabilities.

  * `SearchSpace` and `SearchSpaceSelector` classes deals with Vizier search
    spaces. Both flat spaces and conditional parameters are supported.
"""

import collections
import copy
import enum
from typing import Dict, List, Optional, Sequence, Tuple, Union

import attr
from vizier._src.pyvizier.oss import automated_stopping
from vizier._src.pyvizier.oss import metadata_util
from vizier._src.pyvizier.oss import proto_converters
from vizier._src.pyvizier.shared import base_study_config
from vizier._src.pyvizier.shared import common
from vizier._src.pyvizier.shared import parameter_config
from vizier._src.pyvizier.shared import trial
from vizier._src.service import constants
from vizier._src.service import study_pb2


################### PyTypes ###################
# Possible types for trial parameter values after cast to external types.
# TODO: Define this in _src/shared/
ParameterValueSequence = Union[
    trial.ParameterValueTypes,
    Sequence[int],
    Sequence[float],
    Sequence[str],
    Sequence[bool],
]

################### Enums ###################


class Algorithm(enum.Enum):
  """Valid Values for StudyConfig.Algorithm."""
  # Let Vizier choose algorithm. Currently defaults to GP_UCB_PE.
  ALGORITHM_UNSPECIFIED = 'ALGORITHM_UNSPECIFIED'
  # Gaussian Process UCB with Pure Exploration.
  GP_UCB_PE = 'GP_UCB_PE'
  # Gaussian Process Bandit.
  GAUSSIAN_PROCESS_BANDIT = 'GAUSSIAN_PROCESS_BANDIT'
  # Grid search within the feasible space.
  GRID_SEARCH = 'GRID_SEARCH'
  # Grid search, but with parameters and values shuffled.
  SHUFFLED_GRID_SEARCH = 'SHUFFLED_GRID_SEARCH'
  # Random search within the feasible space.
  RANDOM_SEARCH = 'RANDOM_SEARCH'
  # Quasi-random search using Halton sequences.
  QUASI_RANDOM_SEARCH = 'QUASI_RANDOM_SEARCH'
  # NSGA2 (https://ieeexplore.ieee.org/document/996017).
  NSGA2 = 'NSGA2'
  # BOCS (https://arxiv.org/abs/1806.08838) only applicable to boolean search
  # spaces.
  BOCS = 'BOCS'
  # Harmonica (https://arxiv.org/abs/1706.00764) only applicable to boolean
  # search spaces.
  HARMONICA = 'HARMONICA'
  # CMA-ES (https://arxiv.org/abs/1604.00772) for DOUBLE search spaces only
  CMA_ES = 'CMA_ES'
  # Eagle Strategy (https://doi.org/10.1007/978-3-642-04944-6_14).
  EAGLE_STRATEGY = 'EAGLE_STRATEGY'


class ObservationNoise(enum.Enum):
  """Valid Values for StudyConfig.ObservationNoise."""

  OBSERVATION_NOISE_UNSPECIFIED = (
      study_pb2.StudySpec.ObservationNoise.OBSERVATION_NOISE_UNSPECIFIED
  )
  LOW = study_pb2.StudySpec.ObservationNoise.LOW
  HIGH = study_pb2.StudySpec.ObservationNoise.HIGH


################### Main Class ###################
#
# A StudyConfig object can be initialized:
# (1) From a StudyConfig proto using StudyConfig.from_proto():
#     study_config_proto = study_pb2.StudySpec(...)
#     study_config = pyvizier.StudyConfig.from_proto(study_config_proto)
#     # Attributes can be modified.
#     study_config.metadata['metadata_key'] = 'metadata_value'
#     new_proto = study_config.to_proto()
#
# (2) By directly calling __init__ and setting attributes:
#     study_config = pyvizier.StudyConfig(
#       metric_information=[pyvizier.MetricInformation(
#         name='accuracy', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE)],
#       search_space=vz.SearchSpace.from_proto(proto),
#     )
#     # OR:
#     study_config = pyvizier.StudyConfig()
#     study_config.metric_information.append(
#        pyvizier.MetricInformation(
#          name='accuracy', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE))
#
#     # Since building a search space is more involved, get a reference to the
#     # search space, and add parameters to it.
#     root = study_config.search_space.root
#     root.add_float_param('learning_rate', 0.001, 1.0,
#       scale_type=pyvizier.ScaleType.LOG)
#


@attr.define(frozen=False, init=True, slots=True, kw_only=True)
class StudyConfig(base_study_config.ProblemStatement):
  """A builder and wrapper for study_pb2.StudySpec proto."""

  algorithm: str = attr.field(
      init=True,
      validator=attr.validators.instance_of((Algorithm, str)),
      converter=lambda x: x.value if isinstance(x, enum.Enum) else x,
      on_setattr=[attr.setters.convert, attr.setters.validate],
      default='ALGORITHM_UNSPECIFIED',
      kw_only=True)

  pythia_endpoint: Optional[str] = attr.field(
      init=True,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
      on_setattr=[attr.setters.convert, attr.setters.validate],
      default=None,
      kw_only=True)

  observation_noise: ObservationNoise = attr.field(
      init=True,
      validator=attr.validators.instance_of(ObservationNoise),
      on_setattr=attr.setters.validate,
      default=ObservationNoise.OBSERVATION_NOISE_UNSPECIFIED,
      kw_only=True)

  automated_stopping_config: Optional[
      automated_stopping.AutomatedStoppingConfig] = attr.field(
          init=True,
          default=None,
          validator=attr.validators.optional(
              attr.validators.instance_of(
                  automated_stopping.AutomatedStoppingConfig)),
          on_setattr=attr.setters.validate,
          kw_only=True)

  # An internal representation as a StudyConfig proto.
  # If this object was created from a StudyConfig proto, a copy of the original
  # proto is kept, to make sure that unknown proto fields are preserved in
  # round trip serialization.
  # TODO: Fix the broken proto validation.
  _study_config: study_pb2.StudySpec = attr.field(
      init=True,
      factory=study_pb2.StudySpec,
      kw_only=True)

  # Public attributes, methods and properties.
  @classmethod
  def pythia_endpoint_metadata(cls, pythia_endpoint: str) -> common.Metadata:
    """Returns the MetaData for updating the pythia endpoint."""
    metadata = common.Metadata()
    metadata.ns(constants.PYTHIA_ENDPOINT_NAMESPACE)[
        constants.PYTHIA_ENDPOINT_KEY
    ] = pythia_endpoint
    return metadata

  @classmethod
  def from_proto(cls, proto: study_pb2.StudySpec) -> 'StudyConfig':
    """Converts a StudyConfig proto to a StudyConfig object.

    Args:
      proto: StudyConfig proto.

    Returns:
      A StudyConfig object.
    """
    algorithm = proto.algorithm

    metric_information = base_study_config.MetricsConfig(
        sorted(
            [
                proto_converters.MetricInformationConverter.from_proto(m)
                for m in proto.metrics
            ],
            key=lambda x: x.name,
        )
    )

    oneof_name = proto.WhichOneof('automated_stopping_spec')
    if not oneof_name:
      automated_stopping_config = None
    else:
      automated_stopping_config = (
          automated_stopping.AutomatedStoppingConfig.from_proto(
              getattr(proto, oneof_name)
          )
      )

    metadata = common.Metadata()
    for kv in proto.metadata:
      metadata.abs_ns(common.Namespace.decode(kv.ns))[kv.key] = (
          kv.proto if kv.HasField('proto') else kv.value
      )

    # Store the pythia_endpoint as a property for convenience.
    pythia_endpoint = None
    try:
      pythia_endpoint = metadata.ns(constants.PYTHIA_ENDPOINT_NAMESPACE)[
          constants.PYTHIA_ENDPOINT_KEY
      ]
    except KeyError:
      pass  # Pythia endpoint doesn't exist.

    return cls(
        search_space=proto_converters.SearchSpaceConverter.from_proto(proto),
        algorithm=algorithm,
        pythia_endpoint=pythia_endpoint,
        metric_information=metric_information,
        observation_noise=ObservationNoise(proto.observation_noise),
        automated_stopping_config=automated_stopping_config,
        study_config=copy.deepcopy(proto),
        metadata=metadata)

  def to_proto(self) -> study_pb2.StudySpec:
    """Serializes this object to a StudyConfig proto."""
    proto = copy.deepcopy(self._study_config)
    proto.algorithm = self.algorithm
    proto.observation_noise = self.observation_noise.value

    del proto.metrics[:]
    proto.metrics.extend(
        proto_converters.MetricsConfigConverter.to_protos(
            self.metric_information))

    del proto.parameters[:]
    proto.parameters.extend(
        proto_converters.SearchSpaceConverter.parameter_protos(
            self.search_space))

    if self.automated_stopping_config is not None:
      auto_stop_proto = self.automated_stopping_config.to_proto()
      if isinstance(auto_stop_proto,
                    study_pb2.StudySpec.DefaultEarlyStoppingSpec):
        proto.default_stopping_spec.CopyFrom(auto_stop_proto)

    # The internally stored proto already contains metadata.
    proto.ClearField('metadata')
    for ns in self.metadata.namespaces():
      ns_string = ns.encode()
      ns_layer = self.metadata.abs_ns(ns)
      for key, value in ns_layer.items():
        metadata_util.assign(proto, key=key, ns=ns_string, value=value)
    if self.pythia_endpoint is not None:
      ns = common.Namespace([constants.PYTHIA_ENDPOINT_NAMESPACE])
      metadata_util.assign(
          proto,
          key=constants.PYTHIA_ENDPOINT_KEY,
          ns=ns.encode(),
          value=self.pythia_endpoint,
          mode='insert_or_assign',
      )
    return proto

  def _trial_to_external_values(
      self, pytrial: trial.Trial
  ) -> Dict[str, Union[float, int, str, bool]]:
    """Returns the trial paremeter values cast to external types."""
    parameter_values: Dict[str, Union[float, int, str]] = {}
    external_values: Dict[str, Union[float, int, str, bool]] = {}
    # parameter_configs is a list of Tuple[parent_name, ParameterConfig].
    parameter_configs: List[
        Tuple[Optional[str], parameter_config.ParameterConfig]
    ] = [(None, p) for p in self.search_space.parameters]
    remaining_parameters = copy.deepcopy(pytrial.parameters)
    # Traverse the conditional tree using a BFS.
    while parameter_configs and remaining_parameters:
      parent_name, pc = parameter_configs.pop(0)
      parameter_configs.extend(
          (pc.name, child) for child in pc.child_parameter_configs
      )
      if pc.name not in remaining_parameters:
        continue
      if parent_name is not None:
        # This is a child parameter. If the parent was not seen,
        # skip this parameter config.
        if parent_name not in parameter_values:
          continue
        parent_value = parameter_values[parent_name]
        if parent_value not in pc.matching_parent_values:
          continue
      parameter_values[pc.name] = remaining_parameters[pc.name].value
      if pc.external_type is None:
        external_value = remaining_parameters[pc.name].value
      else:
        external_value = remaining_parameters[pc.name].cast(pc.external_type)  # pytype: disable=wrong-arg-types
      external_values[pc.name] = external_value
      remaining_parameters.pop(pc.name)
    return external_values

  def trial_parameters(
      self, proto: study_pb2.Trial) -> Dict[str, ParameterValueSequence]:
    """Returns the trial values, cast to external types, if they exist.

    Args:
      proto:

    Returns:
      Parameter values dict: cast to each parameter's external_type, if exists.
      NOTE that the values in the dict may be a Sequence as opposed to a single
      element.

    Raises:
      ValueError: If the trial parameters do not exist in this search space.
      ValueError: If the trial contains duplicate parameters.
    """
    pytrial = proto_converters.TrialConverter.from_proto(proto)
    return self._pytrial_parameters(pytrial)

  def _pytrial_parameters(
      self, pytrial: trial.Trial
  ) -> Dict[str, ParameterValueSequence]:
    """Returns the trial values, cast to external types, if they exist.

    Args:
      pytrial:

    Returns:
      Parameter values dict: cast to each parameter's external_type, if exists.
      NOTE that the values in the dict may be a Sequence as opposed to a single
      element.

    Raises:
      ValueError: If the trial parameters do not exist in this search space.
      ValueError: If the trial contains duplicate parameters.
    """
    trial_external_values: Dict[str, Union[float, int, str, bool]] = (
        self._trial_to_external_values(pytrial))
    if len(trial_external_values) != len(pytrial.parameters):
      raise ValueError('Invalid trial for this search space: failed to convert '
                       'all trial parameters: {}'.format(pytrial))

    # Combine multi-dimensional parameter values to a list of values.
    trial_final_values: Dict[str, ParameterValueSequence] = {}
    # multi_dim_params: Dict[str, List[Tuple[int, ParameterValueSequence]]]
    multi_dim_params = collections.defaultdict(list)
    for name in trial_external_values:
      base_index = parameter_config.SearchSpaceSelector.parse_multi_dimensional_parameter_name(
          name
      )
      if base_index is None:
        trial_final_values[name] = trial_external_values[name]
      else:
        base_name, index = base_index
        multi_dim_params[base_name].append((index, trial_external_values[name]))
    for name in multi_dim_params:
      multi_dim_params[name].sort(key=lambda x: x[0])
      trial_final_values[name] = [x[1] for x in multi_dim_params[name]]

    return trial_final_values

  def trial_metrics(self,
                    proto: study_pb2.Trial,
                    *,
                    include_all_metrics=False) -> Dict[str, float]:
    """Returns the trial's final measurement metric values.

    If the trial is not completed, or infeasible, no metrics are returned.
    By default, only metrics configured in the StudyConfig are returned
    (e.g. only objective and safety metrics).

    Args:
      proto:
      include_all_metrics: If True, all metrics in the final measurements are
        returned. If False, only metrics configured in the StudyConfig are
        returned.

    Returns:
      Dict[metric name, metric value]
    """
    pytrial = proto_converters.TrialConverter.from_proto(proto)
    return self._pytrial_metrics(
        pytrial, include_all_metrics=include_all_metrics)

  def _pytrial_metrics(
      self, pytrial: trial.Trial, *, include_all_metrics=False
  ) -> Dict[str, float]:
    """Returns the trial's final measurement metric values.

    If the trial is not completed, or infeasible, no metrics are returned.
    By default, only metrics configured in the StudyConfig are returned
    (e.g. only objective and safety metrics).

    Args:
      pytrial:
      include_all_metrics: If True, all metrics in the final measurements are
        returned. If False, only metrics configured in the StudyConfig are
        returned.

    Returns:
      Dict[metric name, metric value]
    """
    configured_metrics = [m.name for m in self.metric_information]

    metrics: Dict[str, float] = {}
    if pytrial.is_completed and not pytrial.infeasible:
      if pytrial.final_measurement is None:
        return metrics
      for name in pytrial.final_measurement.metrics:
        if (include_all_metrics or
            (not include_all_metrics and name in configured_metrics)):
          # Special case: Measurement always adds an empty metric by default.
          # If there is a named single objective in study_config, drop the empty
          # metric.
          if not name and self.single_objective_metric_name != name:
            continue
          metrics[name] = pytrial.final_measurement.metrics[name].value
    return metrics


--- vizier/_src/pyvizier/oss/study_config_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.oss.study_config."""
import datetime

from vizier._src.service import constants
from vizier._src.service import key_value_pb2
from vizier._src.service import study_pb2
from vizier.service import pyvizier as vz

from google.protobuf import struct_pb2
from vizier._src.pyvizier.oss import compare
from absl.testing import absltest
from absl.testing import parameterized


class StudyConfigTest(parameterized.TestCase):

  def setUp(self):
    super().setUp()
    self.pconfigs = [
        study_pb2.StudySpec.ParameterSpec(
            parameter_id='learning_rate',
            double_value_spec=study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(
                min_value=0.00001, max_value=1.0),
            scale_type=study_pb2.StudySpec.ParameterSpec.ScaleType
            .UNIT_LINEAR_SCALE),
        study_pb2.StudySpec.ParameterSpec(
            parameter_id='optimizer',
            categorical_value_spec=study_pb2.StudySpec.ParameterSpec
            .CategoricalValueSpec(values=['adagrad', 'adam', 'experimental'])),
    ]

  def testCreationFromAndToProtoStudy(self):
    expected_automated_stopping_config = (
        study_pb2.StudySpec.DefaultEarlyStoppingSpec())

    study_config_proto = study_pb2.StudySpec(
        algorithm='QUASI_RANDOM_SEARCH',
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE,
            )
        ],
        default_stopping_spec=expected_automated_stopping_config,
        metadata=[
            key_value_pb2.KeyValue(
                key='foo',
                ns=vz.Namespace(['ns_bar']).encode(),
                value='val',
            )
        ],
    )

    study_config_proto.parameters.extend(self.pconfigs)
    # Test all proprties.
    sc = vz.StudyConfig.from_proto(study_config_proto)
    expected = vz.MetricsConfig(
        [
            vz.MetricInformation(
                name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    self.assertEqual(sc.metric_information, expected)
    self.assertEqual(sc.algorithm, 'QUASI_RANDOM_SEARCH')
    self.assertIsNone(sc.pythia_endpoint)
    self.assertEqual(sc.single_objective_metric_name, 'pr-auc')
    self.assertTrue(sc.is_single_objective)
    assert sc.automated_stopping_config is not None
    compare.assertProto2Equal(self, expected_automated_stopping_config,
                              sc.automated_stopping_config.to_proto())
    compare.assertProto2Equal(self, study_config_proto, sc.to_proto())
    _ = vz.StudyConfig.from_problem(sc.to_problem())  # smoke test.

  @parameterized.parameters([
      ('my custom algorithm'),
      ('QUASI_RANDOM_SEARCH'),
  ])
  def testCreationFromAndToProtoStudyStringAlgorithm(self, algorithm):
    study_config_proto = study_pb2.StudySpec(
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE)
        ],
        algorithm=algorithm)
    # Test the algorithm when pythia endpoint is not specified.
    # This can be used when a pythia service is injected directly to the Vizier
    # service class.
    sc = vz.StudyConfig.from_proto(study_config_proto)
    self.assertEqual(sc.algorithm, algorithm)
    self.assertIsNone(sc.pythia_endpoint)

    compare.assertProto2Equal(self, study_config_proto, sc.to_proto())
    _ = vz.StudyConfig.from_problem(sc.to_problem())  # smoke test.

  @parameterized.parameters([
      'my custom algorithm',
      'QUASI_RANDOM_SEARCH',
  ])
  def testCreationFromAndToProtoStudyStringAlgorithmPythiaEndpoint(
      self, algorithm
  ):
    study_config_proto = study_pb2.StudySpec(
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE,
            )
        ],
        algorithm=algorithm,
        metadata=[
            key_value_pb2.KeyValue(
                key=constants.PYTHIA_ENDPOINT_KEY,
                ns=vz.Namespace([constants.PYTHIA_ENDPOINT_NAMESPACE]).encode(),
                value='localhost:8888',
            )
        ],
    )
    sc = vz.StudyConfig.from_proto(study_config_proto)
    self.assertEqual(sc.algorithm, algorithm)
    self.assertEqual(sc.pythia_endpoint, 'localhost:8888')

    compare.assertProto2Equal(self, study_config_proto, sc.to_proto())
    _ = vz.StudyConfig.from_problem(sc.to_problem())  # smoke test.

  def testCreationFromAndToProtoMultiObjectiveStudy(self):
    study_config_proto = study_pb2.StudySpec(
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE),
            study_pb2.StudySpec.MetricSpec(
                metric_id='loss',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MINIMIZE)
        ],)
    study_config_proto.parameters.extend(self.pconfigs)
    # Test all proprties.
    sc = vz.StudyConfig.from_proto(study_config_proto)

    expected = vz.MetricsConfig([
        vz.MetricInformation(name='loss', goal=vz.ObjectiveMetricGoal.MINIMIZE),
        vz.MetricInformation(
            name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
    ])
    self.assertEqual(sc.metric_information, expected)
    self.assertIsNone(sc.single_objective_metric_name)
    self.assertFalse(sc.is_single_objective)

    round_trip_proto = sc.to_proto()
    compare.assertProto2SameElements(self, study_config_proto, round_trip_proto)

    _ = vz.StudyConfig.from_problem(sc.to_problem())  # smoke test.

  def testCreationFromAndToProtoSafeStudy(self):
    expected_automated_stopping_config = (
        study_pb2.StudySpec.DefaultEarlyStoppingSpec())

    study_config_proto = study_pb2.StudySpec(
        algorithm='QUASI_RANDOM_SEARCH',
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE),
            study_pb2.StudySpec.MetricSpec(
                metric_id='privacy-safety',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MINIMIZE,
                safety_config=study_pb2.StudySpec.MetricSpec.SafetyMetricConfig(
                    safety_threshold=0.2, desired_min_safe_trials_fraction=0.8))
        ],
        default_stopping_spec=expected_automated_stopping_config)

    study_config_proto.parameters.extend(self.pconfigs)
    # Test all proprties.
    sc = vz.StudyConfig.from_proto(study_config_proto)
    expected = vz.MetricsConfig([
        vz.MetricInformation(
            name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        ),
        vz.MetricInformation(
            name='privacy-safety',
            goal=vz.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=0.2,
            desired_min_safe_trials_fraction=0.8,
        ),
    ])
    self.assertEqual(sc.metric_information, expected)
    self.assertEqual(sc.algorithm, 'QUASI_RANDOM_SEARCH')
    self.assertIsNone(sc.pythia_endpoint)
    self.assertEqual(sc.single_objective_metric_name, 'pr-auc')
    self.assertTrue(sc.is_single_objective)

    assert sc.automated_stopping_config is not None
    compare.assertProto2Equal(self, expected_automated_stopping_config,
                              sc.automated_stopping_config.to_proto())
    compare.assertProto2Equal(self, study_config_proto, sc.to_proto())
    _ = vz.StudyConfig.from_problem(sc.to_problem())  # smoke test.

  def testCreationFromProtoNoGoalRaises(self):
    study_config_proto = study_pb2.StudySpec()

    sc = vz.StudyConfig.from_proto(study_config_proto)
    self.assertEmpty(sc.metric_information)

  def testMetadata(self):
    empty_trial = study_pb2.Trial(id=str(1))
    sc = vz.StudyConfig()
    sc.metric_information.append(
        vz.MetricInformation(
            name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    sc.metadata.abs_ns(['ns'])['key'] = 'ns-value'
    sc.metadata.abs_ns()['key'] = 'value'
    sc.metadata['proto'] = empty_trial

    proto = sc.to_proto()
    compare.assertProto2Contains(
        self, """metadata {
        key: "key"
        ns: ":ns"
        value: "ns-value"
      }
      metadata {
        key: "key"
        value: "value"
      }
      metadata {
        key: "proto"
        proto {
          [type.googleapis.com/vizier.Trial] {
            id: '1'
          }
        }
      }
    """, proto)
    from_proto = sc.from_proto(proto).metadata
    self.assertCountEqual(sc.metadata, from_proto)

  def testCreation(self):
    sc = vz.StudyConfig()
    sc.algorithm = vz.Algorithm.RANDOM_SEARCH
    sc.metric_information.append(
        vz.MetricInformation(
            name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    root = sc.search_space.root
    root.add_float_param(
        'learning_rate', 0.00001, 1.0, scale_type=vz.ScaleType.LINEAR
    )
    root.add_categorical_param('optimizer', ['adagrad', 'adam', 'experimental'])

    sc.automated_stopping_config = (
        vz.AutomatedStoppingConfig.default_stopping_spec()
    )

    # Test all proprties.
    self.assertEqual(sc.algorithm, 'RANDOM_SEARCH')
    expected = vz.MetricsConfig(
        [
            vz.MetricInformation(
                name='pr-auc', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    self.assertEqual(sc.metric_information, expected)
    self.assertEqual(sc.single_objective_metric_name, 'pr-auc')
    self.assertTrue(sc.is_single_objective)

    expected = study_pb2.StudySpec(
        algorithm='RANDOM_SEARCH',
        metrics=[
            study_pb2.StudySpec.MetricSpec(
                metric_id='pr-auc',
                goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE)
        ],
        default_stopping_spec=study_pb2.StudySpec.DefaultEarlyStoppingSpec(),
        observation_noise=study_pb2.StudySpec.ObservationNoise
        .OBSERVATION_NOISE_UNSPECIFIED,
    )
    expected.parameters.extend(self.pconfigs)
    compare.assertProto2Equal(self, expected, sc.to_proto())

  @absltest.skip('???')
  def testTrialToDict(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)
    root.add_int_param('units', 10, 1000, scale_type=vz.ScaleType.LOG)
    root.add_discrete_param('batch_size', [8, 16, 32])
    root.add_discrete_param(
        'floating_point_param', [8., 16., 32.], auto_cast=False)
    root.add_categorical_param('activation', ['tanh', 'relu'])
    root.add_bool_param('synchronous')

    trial_proto = study_pb2.Trial(id=str(1))
    trial_proto.parameters.add(
        parameter_id='activation', value=struct_pb2.Value(string_value='relu'))
    trial_proto.parameters.add(
        parameter_id='synchronus', value=struct_pb2.Value(string_value='true'))
    trial_proto.parameters.add(
        parameter_id='batch_size', value=struct_pb2.Value(number_value=32))
    trial_proto.parameters.add(
        parameter_id='floating_point_param',
        value=struct_pb2.Value(number_value=32))
    trial_proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))
    trial_proto.parameters.add(
        parameter_id='units', value=struct_pb2.Value(number_value=50))

    parameters = py_study_config.trial_parameters(trial_proto)
    expected = {
        'learning_rate': 0.5,
        'units': 50,
        'activation': 'relu',
        'batch_size': 32,
        'floating_point_param': 32.,
        'synchronous': True
    }
    self.assertEqual(expected, parameters)
    self.assertIsInstance(parameters['batch_size'], int)
    self.assertIsInstance(parameters['floating_point_param'], float)

  def testPyTrialToDict(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)
    root.add_int_param('units', 10, 1000, scale_type=vz.ScaleType.LOG)
    root.add_discrete_param('batch_size', [8, 16, 32])
    root.add_discrete_param(
        'floating_point_param', [8., 16., 32.], auto_cast=False)
    root.add_categorical_param('activation', ['tanh', 'relu'])
    root.add_bool_param('synchronous')

    pytrial = vz.Trial(id=1)
    pytrial.parameters = {
        'activation': vz.ParameterValue(value='relu'),
        'synchronous': vz.ParameterValue(value=True),
        'batch_size': vz.ParameterValue(value=32),
        'floating_point_param': vz.ParameterValue(value=32.0),
        'learning_rate': vz.ParameterValue(value=0.5),
        'units': vz.ParameterValue(value=50),
    }
    parameters = py_study_config._pytrial_parameters(pytrial)
    expected = {
        'learning_rate': 0.5,
        'units': 50,
        'activation': 'relu',
        'batch_size': 32,
        'floating_point_param': 32.,
        'synchronous': True
    }
    self.assertEqual(expected, parameters)
    self.assertIsInstance(parameters['batch_size'], int)
    self.assertIsInstance(parameters['floating_point_param'], float)

  def testTrialToDictWithExternalType(self):
    """Test conversion when external types are not specified."""
    proto = study_pb2.StudySpec()
    proto.parameters.add(
        parameter_id='learning_rate',
        double_value_spec=study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(
            min_value=1e-4, max_value=0.1),
        scale_type=study_pb2.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE)
    proto.parameters.add(
        parameter_id='batch_size',
        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
            values=[1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0]),
        external_type=study_pb2.StudySpec.ParameterSpec.ExternalType.AS_INTEGER)
    proto.parameters.add(
        parameter_id='training_steps',
        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
            values=[1000.0, 10000.0]),
        external_type=study_pb2.StudySpec.ParameterSpec.ExternalType.AS_INTEGER)
    proto.observation_noise = study_pb2.StudySpec.ObservationNoise.HIGH
    proto.metrics.add(
        metric_id='loss', goal=study_pb2.StudySpec.MetricSpec.MINIMIZE)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.parameters.add(
        parameter_id='batch_size', value=struct_pb2.Value(number_value=128.0))
    trial_proto.parameters.add(
        parameter_id='learning_rate',
        value=struct_pb2.Value(number_value=1.2137854406366652E-4))
    trial_proto.parameters.add(
        parameter_id='training_steps',
        value=struct_pb2.Value(number_value=10000.0))

    py_study_config = vz.StudyConfig.from_proto(proto)
    self.assertEqual(
        py_study_config.observation_noise, vz.ObservationNoise.HIGH
    )
    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual(
        py_study_config.observation_noise, vz.ObservationNoise.HIGH
    )
    expected = {
        'batch_size': 128,
        'learning_rate': 1.2137854406366652E-4,
        'training_steps': 10000.0
    }
    self.assertEqual(expected, parameters)
    self.assertIsInstance(parameters['learning_rate'], float)
    self.assertIsInstance(parameters['batch_size'], int)
    self.assertIsInstance(parameters['training_steps'], int)

  def testTrialToDictWithoutExternalType(self):
    """Test conversion when external types are not specified."""
    proto = study_pb2.StudySpec()
    proto.parameters.add(
        parameter_id='learning_rate',
        double_value_spec=study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(
            min_value=1e-4, max_value=0.1),
        scale_type=study_pb2.StudySpec.ParameterSpec.ScaleType.UNIT_LOG_SCALE)
    proto.parameters.add(
        parameter_id='batch_size',
        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
            values=[1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0]))
    proto.parameters.add(
        parameter_id='training_steps',
        discrete_value_spec=study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
            values=[1000.0, 10000.0]))
    proto.observation_noise = study_pb2.StudySpec.ObservationNoise.HIGH
    proto.metrics.add(
        metric_id='loss', goal=study_pb2.StudySpec.MetricSpec.MINIMIZE)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.parameters.add(
        parameter_id='batch_size', value=struct_pb2.Value(number_value=128.0))
    trial_proto.parameters.add(
        parameter_id='learning_rate',
        value=struct_pb2.Value(number_value=1.2137854406366652E-4))
    trial_proto.parameters.add(
        parameter_id='training_steps',
        value=struct_pb2.Value(number_value=10000.0))

    py_study_config = vz.StudyConfig.from_proto(proto)
    self.assertEqual(
        py_study_config.observation_noise, vz.ObservationNoise.HIGH
    )
    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual(
        py_study_config.observation_noise, vz.ObservationNoise.HIGH
    )
    expected = {
        'batch_size': 128,
        'learning_rate': 1.2137854406366652E-4,
        'training_steps': 10000.0
    }
    self.assertEqual(expected, parameters)
    self.assertIsInstance(parameters['learning_rate'], float)
    self.assertIsInstance(parameters['batch_size'], float)
    self.assertIsInstance(parameters['training_steps'], float)

  @absltest.skip('???')
  def testTrialToDictMultidimensional(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    for index in (0, 1):
      root.add_float_param('learning_rate', 0.01, 3.0, index=index)
      root.add_int_param(
          'units', 10, 1000, scale_type=vz.ScaleType.LOG, index=index
      )
      root.add_categorical_param('activation', ['tanh', 'relu'], index=index)
      root.add_bool_param('synchronous', index=index)
      root.add_discrete_param('batch_size', [8, 16, 32], index=index)
    root.add_discrete_param(
        'floating_point_param', [8., 16., 32.], auto_cast=False)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(2)
    trial_proto.parameters.add(
        parameter_id='learning_rate[0]',
        value=struct_pb2.Value(number_value=0.5))
    trial_proto.parameters.add(
        parameter_id='learning_rate[1]',
        value=struct_pb2.Value(number_value=0.1))
    trial_proto.parameters.add(
        parameter_id='units[0]', value=struct_pb2.Value(number_value=50))
    trial_proto.parameters.add(
        parameter_id='units[1]', value=struct_pb2.Value(number_value=200))
    trial_proto.parameters.add(
        parameter_id='activation[0]',
        value=struct_pb2.Value(string_value='relu'))
    trial_proto.parameters.add(
        parameter_id='activation[1]',
        value=struct_pb2.Value(string_value='relu'))
    trial_proto.parameters.add(
        parameter_id='synchronus[0]',
        value=struct_pb2.Value(string_value='true'))
    trial_proto.parameters.add(
        parameter_id='synchronus[1]',
        value=struct_pb2.Value(string_value='false'))
    trial_proto.parameters.add(
        parameter_id='batch_size[0]', value=struct_pb2.Value(number_value=32.0))
    trial_proto.parameters.add(
        parameter_id='batch_size[1]', value=struct_pb2.Value(number_value=8.0))
    trial_proto.parameters.add(
        parameter_id='floating_point_param',
        value=struct_pb2.Value(number_value=16.0))
    parameters = py_study_config.trial_parameters(trial_proto)
    expected = {
        'learning_rate': [0.5, 0.1],
        'units': [50, 200],
        'activation': ['relu', 'relu'],
        'batch_size': [32, 8],
        'synchronous': [True, False],
        'floating_point_param': 16.,
    }
    self.assertEqual(expected, parameters)

  def testPyTrialToDictMultidimensional(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    for index in (0, 1):
      root.add_float_param('learning_rate', 0.01, 3.0, index=index)
      root.add_int_param(
          'units', 10, 1000, scale_type=vz.ScaleType.LOG, index=index
      )
      root.add_categorical_param('activation', ['tanh', 'relu'], index=index)
      root.add_bool_param('synchronous', index=index)
      root.add_discrete_param('batch_size', [8, 16, 32], index=index)
    root.add_discrete_param(
        'floating_point_param', [8., 16., 32.], auto_cast=False)

    pytrial = vz.Trial(id=2)
    pytrial.parameters = {
        'learning_rate[0]': vz.ParameterValue(value=0.5),
        'learning_rate[1]': vz.ParameterValue(value=0.1),
        'units[0]': vz.ParameterValue(value=50),
        'units[1]': vz.ParameterValue(value=200),
        'activation[0]': vz.ParameterValue(value='relu'),
        'activation[1]': vz.ParameterValue(value='relu'),
        'synchronous[0]': vz.ParameterValue(value=True),
        'synchronous[1]': vz.ParameterValue(value=False),
        'batch_size[0]': vz.ParameterValue(value=32.0),
        'batch_size[1]': vz.ParameterValue(value=8.0),
        'floating_point_param': vz.ParameterValue(value=16.0),
    }
    parameters = py_study_config._pytrial_parameters(pytrial)
    expected = {
        'learning_rate': [0.5, 0.1],
        'units': [50, 200],
        'activation': ['relu', 'relu'],
        'batch_size': [32, 8],
        'synchronous': [True, False],
        'floating_point_param': 16.,
    }
    self.assertEqual(expected, parameters)

  def testGinConfigMultiDimensional(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    block_categories = [
        'block_3x3', 'block_4x4', 'block_1x3_3x1', 'block_1x3_3x1_dw',
        'block_identity'
    ]
    for index in range(5):
      root.add_categorical_param(
          '_gin.ambient_net_exp_from_vec.block_type',
          feasible_values=block_categories,
          index=index)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(2)
    trial_proto.parameters.add(
        parameter_id='_gin.ambient_net_exp_from_vec.block_type[0]',
        value=struct_pb2.Value(string_value='block_3x3'))
    trial_proto.parameters.add(
        parameter_id='_gin.ambient_net_exp_from_vec.block_type[1]',
        value=struct_pb2.Value(string_value='block_4x4'))
    trial_proto.parameters.add(
        parameter_id='_gin.ambient_net_exp_from_vec.block_type[2]',
        value=struct_pb2.Value(string_value='block_1x3_3x1_dw'))
    trial_proto.parameters.add(
        parameter_id='_gin.ambient_net_exp_from_vec.block_type[3]',
        value=struct_pb2.Value(string_value='block_identity'))
    trial_proto.parameters.add(
        parameter_id='_gin.ambient_net_exp_from_vec.block_type[4]',
        value=struct_pb2.Value(string_value='block_1x3_3x1'))

    parameters = py_study_config.trial_parameters(trial_proto)
    expected = {
        '_gin.ambient_net_exp_from_vec.block_type': [
            'block_3x3', 'block_4x4', 'block_1x3_3x1_dw', 'block_identity',
            'block_1x3_3x1'
        ],
    }
    self.assertEqual(expected, parameters)

  @absltest.skip('???')
  def testTrialToDictConditional(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root

    model_type = root.add_categorical_param('model_type', ['dnn', 'linear'])
    dnn = model_type.select_values(['dnn'])
    dnn.add_float_param('learning_rate', 0.01, 3.0)
    dnn.add_int_param('units', 1, 50, index=0)
    dnn.add_int_param('units', 1, 80, index=1)
    dnn.add_categorical_param('activation', ['tanh', 'relu'])
    model_type.select_values(['linear'
                             ]).add_float_param('learning_rate', 0.01, 1.0)

    dnn_trial = study_pb2.Trial()
    dnn_trial.id = str(1)
    dnn_trial.parameters.add(
        parameter_id='model_type', value=struct_pb2.Value(string_value='dnn'))
    dnn_trial.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=2.1))
    dnn_trial.parameters.add(
        parameter_id='unts[0]', value=struct_pb2.Value(number_value=49))
    dnn_trial.parameters.add(
        parameter_id='unts[1]', value=struct_pb2.Value(number_value=79))
    dnn_trial.parameters.add(
        parameter_id='activation', value=struct_pb2.Value(string_value='relu'))

    parameters = py_study_config.trial_parameters(dnn_trial)
    expected = {
        'model_type': 'dnn',
        'learning_rate': 2.1,
        'units': [49, 79],
        'activation': 'relu',
    }
    self.assertEqual(expected, parameters)

  def testPyTrialToDictConditional(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root

    model_type = root.add_categorical_param('model_type', ['dnn', 'linear'])
    dnn = model_type.select_values(['dnn'])
    dnn.add_float_param('learning_rate', 0.01, 3.0)
    dnn.add_int_param('units', 1, 50, index=0)
    dnn.add_int_param('units', 1, 80, index=1)
    dnn.add_categorical_param('activation', ['tanh', 'relu'])
    model_type.select_values(['linear'
                             ]).add_float_param('learning_rate', 0.01, 1.0)

    pytrial = vz.Trial(
        id=1,
        parameters={
            'model_type': vz.ParameterValue(value='dnn'),
            'learning_rate': vz.ParameterValue(value=2.1),
            'units[0]': vz.ParameterValue(value=49),
            'units[1]': vz.ParameterValue(value=79),
            'activation': vz.ParameterValue(value='relu'),
        },
    )
    parameters = py_study_config._pytrial_parameters(pytrial)
    expected = {
        'model_type': 'dnn',
        'learning_rate': 2.1,
        'units': [49, 79],
        'activation': 'relu',
    }
    self.assertEqual(expected, parameters)

  def testTrialToDictRaisesDuplicateParameters(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.parameters.add(
        parameter_id='activation', value=struct_pb2.Value(string_value='relu'))
    trial_proto.parameters.add(
        parameter_id='activation', value=struct_pb2.Value(string_value='tanh'))
    trial_proto.parameters.add(
        parameter_id='units', value=struct_pb2.Value(number_value=50))

    with self.assertRaisesRegex(ValueError, 'Invalid trial proto'):
      py_study_config.trial_parameters(trial_proto)

  def testTrialToDictRaisesInvalidTrial(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.state = study_pb2.Trial.State.ACTIVE
    trial_proto.parameters.add(
        parameter_id='foo', value=struct_pb2.Value(number_value=0.5))
    with self.assertRaisesRegex(ValueError,
                                'Invalid trial for this search space'):
      py_study_config.trial_parameters(trial_proto)

  def testTrialToDictWithFinalMetricsSingleObjective(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.state = study_pb2.Trial.State.SUCCEEDED
    trial_proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))
    trial_proto.final_measurement.step_count = 101
    trial_proto.final_measurement.elapsed_duration.seconds = 67
    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)
    trial_proto.final_measurement.metrics.add(metric_id='objective', value=77.7)

    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    metrics = py_study_config.trial_metrics(trial_proto)
    self.assertEqual({'objective': 77.7}, metrics)
    metrics = py_study_config.trial_metrics(
        trial_proto, include_all_metrics=True)
    self.assertEqual({'objective': 77.7, 'loss': 56.8}, metrics)

  def testPyTrialToDictWithFinalMetricsSingleObjective(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    pytrial = vz.Trial(
        id=1,
        completion_time=datetime.datetime(
            year=2021, month=12, day=2, hour=7, minute=31
        ),
        parameters={'learning_rate': vz.ParameterValue(0.5)},
        final_measurement=vz.Measurement(
            metrics={
                'loss': vz.Metric(value=56.8),
                'objective': vz.Metric(value=77.7),
            },
            elapsed_secs=67,
            steps=101,
        ),
    )
    parameters = py_study_config._pytrial_parameters(pytrial)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    metrics = py_study_config._pytrial_metrics(pytrial)
    self.assertEqual({'objective': 77.7}, metrics)
    metrics = py_study_config._pytrial_metrics(
        pytrial, include_all_metrics=True)
    self.assertEqual({'objective': 77.7, 'loss': 56.8}, metrics)

  def testTrialToDictWithFinalMetricsNotCompleted(self):
    # Throw a Trial that has inconsistent field values.
    # (ACTIVE but has final measurement).
    # Pyvizier fixes the state.
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.state = study_pb2.Trial.State.ACTIVE
    trial_proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))
    trial_proto.final_measurement.step_count = 101
    trial_proto.final_measurement.elapsed_duration.seconds = 67
    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)
    trial_proto.final_measurement.metrics.add(metric_id='objective', value=77.7)

    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    self.assertLen(
        py_study_config.trial_metrics(trial_proto, include_all_metrics=True), 2)

  def testTrialToDictWithFinalMetricsInfeasible(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    trial_proto = study_pb2.Trial()
    trial_proto.id = str(1)
    trial_proto.state = study_pb2.Trial.State.INFEASIBLE
    trial_proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))
    trial_proto.final_measurement.step_count = 101
    trial_proto.final_measurement.elapsed_duration.seconds = 67
    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)
    trial_proto.final_measurement.metrics.add(metric_id='objective', value=77.7)

    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    self.assertEmpty(py_study_config.trial_metrics(trial_proto))
    self.assertEmpty(
        py_study_config.trial_metrics(trial_proto, include_all_metrics=True))

  def testPyTrialToDictWithFinalMetricsInfeasible(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    pytrial = vz.Trial(
        id=1,
        infeasibility_reason='just because',
        completion_time=datetime.datetime(
            year=2021, month=12, day=2, hour=7, minute=31
        ),
        parameters={'learning_rate': vz.ParameterValue(0.5)},
        final_measurement=vz.Measurement(
            metrics={
                'loss': vz.Metric(value=56.8),
                'other': vz.Metric(value=77.7),
            },
            elapsed_secs=67,
            steps=101,
        ),
    )
    parameters = py_study_config._pytrial_parameters(pytrial)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    self.assertEmpty(py_study_config._pytrial_metrics(pytrial))
    self.assertEmpty(
        py_study_config._pytrial_metrics(pytrial, include_all_metrics=True))

  def testTrialToDictWithFinalMetricsMultiObjective(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            ),
            vz.MetricInformation(
                name='objective2', goal=vz.ObjectiveMetricGoal.MINIMIZE
            ),
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    trial_proto = study_pb2.Trial(id=str(1))
    trial_proto.state = study_pb2.Trial.State.SUCCEEDED

    trial_proto.parameters.add(
        parameter_id='learning_rate', value=struct_pb2.Value(number_value=0.5))
    trial_proto.final_measurement.step_count = 101
    trial_proto.final_measurement.elapsed_duration.seconds = 67
    trial_proto.final_measurement.metrics.add(metric_id='loss', value=56.8)
    trial_proto.final_measurement.metrics.add(metric_id='objective', value=77.7)
    trial_proto.final_measurement.metrics.add(
        metric_id='objective2', value=-0.2)

    parameters = py_study_config.trial_parameters(trial_proto)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    metrics = py_study_config.trial_metrics(trial_proto)
    self.assertEqual({'objective': 77.7, 'objective2': -0.2}, metrics)
    metrics = py_study_config.trial_metrics(
        trial_proto, include_all_metrics=True)
    self.assertEqual({
        'objective': 77.7,
        'objective2': -0.2,
        'loss': 56.8
    }, metrics)

  def testPyTrialToDictWithFinalMetricsMultiObjective(self):
    py_study_config = vz.StudyConfig(
        metric_information=[
            vz.MetricInformation(
                name='objective', goal=vz.ObjectiveMetricGoal.MAXIMIZE
            ),
            vz.MetricInformation(
                name='objective2', goal=vz.ObjectiveMetricGoal.MINIMIZE
            ),
        ]
    )
    root = py_study_config.search_space.root
    root.add_float_param('learning_rate', 0.01, 3.0)

    pytrial = vz.Trial(
        id=1,
        completion_time=datetime.datetime(
            year=2021, month=12, day=2, hour=7, minute=31
        ),
        parameters={'learning_rate': vz.ParameterValue(0.5)},
        final_measurement=vz.Measurement(
            metrics={
                'loss': vz.Metric(value=56.8),
                'objective': vz.Metric(value=77.7),
                'objective2': vz.Metric(value=-0.2),
            },
            elapsed_secs=67,
            steps=101,
        ),
    )
    parameters = py_study_config._pytrial_parameters(pytrial)
    self.assertEqual({'learning_rate': 0.5}, parameters)
    metrics = py_study_config._pytrial_metrics(pytrial)
    self.assertEqual({'objective': 77.7, 'objective2': -0.2}, metrics)
    metrics = py_study_config._pytrial_metrics(
        pytrial, include_all_metrics=True)
    self.assertEqual({
        'objective': 77.7,
        'objective2': -0.2,
        'loss': 56.8
    }, metrics)

  def testSearchSpacesNotShared(self):
    sc1 = vz.StudyConfig()
    sc1.search_space.root.add_float_param('x', 1, 2)
    sc2 = vz.StudyConfig()
    sc2.search_space.root.add_float_param('x', 1, 2)
    self.assertLen(sc1.search_space.parameters, 1)
    self.assertLen(sc2.search_space.parameters, 1)

  def testHasConditionalParametersFlatSpace(self):
    sc = vz.StudyConfig()
    sc.search_space.root.add_float_param('x', 1, 2)
    self.assertFalse(sc.search_space.is_conditional)

  def testHasConditionalParameters(self):
    sc = vz.StudyConfig()
    root = sc.search_space.root
    model_type = root.add_categorical_param('model_type', ['linear', 'dnn'])
    _ = model_type.select_values(['dnn']).add_float_param(
        'learning_rate',
        0.1,
        1.0,
        default_value=0.001,
        scale_type=vz.ScaleType.LOG,
    )
    self.assertTrue(sc.search_space.is_conditional)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/pythia/study.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""StudyConfig used by pythia policies."""

import enum
import attr

from vizier._src.pyvizier.shared import base_study_config


class StudyState(enum.Enum):
  ACTIVE = 'ACTIVE'
  ABORTED = 'ABORTED'
  COMPLETED = 'COMPLETED'


@attr.define
class StudyStateInfo:
  state: StudyState = attr.field(
      converter=StudyState, validator=attr.validators.instance_of(StudyState))
  explanation: str = attr.field(default='')


@attr.define(frozen=True, init=True)
class StudyDescriptor:
  """Light-weight, cross-platform summary of Study."""

  config: base_study_config.ProblemStatement = attr.ib(
      init=True,
      validator=[
          attr.validators.optional(
              attr.validators.instance_of(base_study_config.ProblemStatement))
      ])

  guid: str = attr.ib(
      init=True,
      validator=[attr.validators.optional(attr.validators.instance_of(str))],
      kw_only=True)

  max_trial_id: int = attr.ib(
      init=True,
      validator=[attr.validators.optional(attr.validators.instance_of(int))],
      kw_only=True)


--- vizier/_src/pyvizier/shared/base_study_config.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Essential classes for defining a blackbox optimization problem.

Contains `ProblemStatement` and its components that are cross platform
compatible.
"""
import collections
from typing import Collection
import enum
from typing import Callable, Iterable, Iterator, List, Optional, Type, TypeVar, Union
import attr
import numpy as np
from vizier._src.pyvizier.shared import common
from vizier._src.pyvizier.shared import parameter_config

################### PyTypes ###################
ScaleType = parameter_config.ScaleType
ExternalType = parameter_config.ExternalType
# A sequence of possible internal parameter values.
MonotypeParameterSequence = parameter_config.MonotypeParameterSequence
_T = TypeVar('_T')


################### Helper Classes ###################
def _min_leq_max(instance: 'MetricInformation', _, value: float):
  if value > instance.max_value:
    raise ValueError(
        f'min_value={value} cannot exceed max_value={instance.max_value}.'
    )


def _max_geq_min(instance: 'MetricInformation', _, value: float):
  if value < instance.min_value:
    raise ValueError(
        f'min_value={instance.min_value} cannot exceed max_value={value}.'
    )


# Values should NEVER be removed from ObjectiveMetricGoal, only added.
class ObjectiveMetricGoal(enum.IntEnum):
  """Valid Values for MetricInformation.Goal."""

  MAXIMIZE = 1
  MINIMIZE = 2

  # pylint: disable=comparison-with-callable
  @property
  def is_maximize(self) -> bool:
    return self == self.MAXIMIZE

  @property
  def is_minimize(self) -> bool:
    return self == self.MINIMIZE


class MetricType(enum.Enum):
  """Type of the metric.

  OBJECTIVE: Objective to be maximized / minimized.
  SAFETY: Objective to be kept above / below a certain threshold.
  """

  OBJECTIVE = 'OBJECTIVE'
  SAFETY = 'SAFETY'  # Soft constraint

  # pylint: disable=comparison-with-callable
  @property
  def is_safety(self) -> bool:
    return self == MetricType.SAFETY

  @property
  def is_objective(self) -> bool:
    return self == MetricType.OBJECTIVE


@attr.define(frozen=False, init=True, slots=True)
class MetricInformation:
  """MetricInformation provides optimization metrics configuration."""

  # The name of this metric. An empty string is allowed for single-metric
  # optimizations.
  name: str = attr.field(
      init=True, default='', validator=attr.validators.instance_of(str)
  )

  goal: ObjectiveMetricGoal = attr.field(
      init=True,
      # pylint: disable=g-long-lambda
      converter=ObjectiveMetricGoal,
      validator=attr.validators.instance_of(ObjectiveMetricGoal),
      on_setattr=[attr.setters.convert, attr.setters.validate],
      kw_only=True,
  )

  # The following should be used to configure this as a safety metric.
  # safety_threshold must always be set (to a float) for safety metrics.
  safety_threshold: Optional[float] = attr.field(
      init=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(float)),
      kw_only=True,
  )
  # Safety_std_threshold is DEPRECATED and is here for backward compatibility.
  # To configure how cautious the optimization should be, please defer to
  # desired_min_safe_trials_fraction. This corresponds to the allowed
  # probability threshold (as a function of the z-score) of
  # violating the safety_threshold.
  safety_std_threshold: Optional[float] = attr.field(
      init=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(float)),
      kw_only=True,
  )
  # Desired minimum fraction of safe trials (over total number of trials)
  # that should be targeted by the algorithm during the entire duration of the
  # study (best effort). A value of 0.0 is the same as None. If unset, or set
  # to 0.0, then Vizier has no constraint on the number/fraction of unsafe
  # trials it can suggest.
  desired_min_safe_trials_fraction: Optional[float] = attr.field(
      init=True,
      default=None,
      validator=[
          attr.validators.optional(attr.validators.instance_of(float)),
          attr.validators.optional(attr.validators.le(1.0)),
          attr.validators.optional(attr.validators.ge(0.0)),
      ],
      kw_only=True,
  )

  # Minimum value of this metric can be optionally specified.
  min_value: float = attr.field(
      init=True,
      default=None,
      # FYI: Converter is applied before validator.
      converter=lambda x: float(x) if x is not None else -np.inf,
      validator=[attr.validators.instance_of(float), _min_leq_max],
      kw_only=True,
  )

  # Maximum value of this metric can be optionally specified.
  max_value: float = attr.field(
      init=True,
      default=None,
      # FYI: Converter is applied before validator.
      converter=lambda x: float(x) if x is not None else np.inf,
      validator=[attr.validators.instance_of(float), _max_geq_min],
      on_setattr=attr.setters.validate,
      kw_only=True,
  )

  def min_value_or(self, default_value_fn: Callable[[], float]) -> float:
    """Returns the minimum value if finite, or default_value_fn().

    Avoids the common pitfalls of using
      `metric.min_value or default_value`
    which would incorrectly use the default_value when min_value == 0, and
    requires default_value to have been computed.

    Args:
      default_value_fn: Default value if min_value is not finite. This function
        does not run at all if min_value is finite.
    """
    if np.isfinite(self.min_value):
      return self.min_value
    else:
      return default_value_fn()

  def max_value_or(self, default_value_fn: Callable[[], float]) -> float:
    """Returns the minimum value if finite, or default_value_fn().

    Avoids the common pitfalls of using
      `metric.max_value or default_value`
    which would incorrectly use the default_value when max_value == 0, and
    requires default_value to have been computed.

    Args:
      default_value_fn: Default value if max_value is not finite. This function
        does not run at all if max_value is configured.
    """
    if np.isfinite(self.max_value):
      return self.max_value
    else:
      return default_value_fn()

  @property
  def range(self) -> float:
    """Range of the metric. Can be infinite."""
    return self.max_value - self.min_value

  @property
  def type(self) -> MetricType:
    if self.safety_threshold is not None:
      return MetricType.SAFETY
    else:
      return MetricType.OBJECTIVE

  def flip_goal(self) -> 'MetricInformation':
    """Flips the goal in-place and returns the reference to self."""
    if self.goal == ObjectiveMetricGoal.MAXIMIZE:
      self.goal = ObjectiveMetricGoal.MINIMIZE
    else:
      self.goal = ObjectiveMetricGoal.MAXIMIZE
    return self


@attr.define(frozen=False, init=True, slots=True)
class MetricsConfig(Collection[MetricInformation]):
  """Container for metrics.

  Metric names should be unique.
  """

  _metrics: List[MetricInformation] = attr.ib(
      init=True,
      factory=list,
      converter=list,
      validator=attr.validators.deep_iterable(
          member_validator=attr.validators.instance_of(MetricInformation),
          iterable_validator=attr.validators.instance_of(Iterable),
      ),
  )

  def item(self) -> MetricInformation:
    if len(self._metrics) != 1:
      raise ValueError(
          'item() may only be called when there is exactly one '
          'metric (there are %d).'
          % len(self._metrics)
      )
    return self._metrics[0]

  def _assert_names_are_unique(self) -> None:
    counts = collections.Counter(metric.name for metric in self._metrics)
    if len(counts) != len(self._metrics):
      for name, count in counts.items():
        if count > 1:
          raise ValueError(f'Duplicate metric name: {name} in {self._metrics}')

  def __attrs_post_init__(self):
    self._assert_names_are_unique()

  def __iter__(self) -> Iterator[MetricInformation]:
    return iter(self._metrics)

  def __contains__(self, x: object) -> bool:
    return x in self._metrics

  def __len__(self) -> int:
    return len(self._metrics)

  def __add__(self, metrics: Iterable[MetricInformation]) -> 'MetricsConfig':
    return MetricsConfig(self._metrics + list(metrics))

  def of_type(
      self, include: Union[MetricType, Iterable[MetricType]]
  ) -> 'MetricsConfig':
    """Filters the Metrics by type."""
    if isinstance(include, MetricType):
      include = (include,)
    return MetricsConfig(m for m in self._metrics if m.type in include)

  def exclude_type(
      self, exclude: Union[MetricType, Iterable[MetricType]]
  ) -> 'MetricsConfig':
    """Filters out the Metrics by type."""
    if isinstance(exclude, MetricType):
      exclude = (exclude,)
    return MetricsConfig(m for m in self._metrics if m.type not in exclude)

  def append(self, metric: MetricInformation):
    self._metrics.append(metric)
    self._assert_names_are_unique()

  def extend(self, metrics: Iterable[MetricInformation]):
    for metric in metrics:
      self.append(metric)

  @property
  def is_single_objective(self) -> bool:
    """Returns True if only one objective metric is configured."""
    return len(self.of_type(MetricType.OBJECTIVE)) == 1

  @property
  def is_safety_metric(self) -> bool:
    """Returns True if at least one safety metric is configured."""
    return True if self.of_type(MetricType.SAFETY) else False


################### Main Class ###################
@attr.define(frozen=False, init=True, slots=True)
class ProblemStatement:
  """Defines a blackbox optimization problem.

  `ProblemStatement` contains the minimal information that defines the search
  problem. It is inherited by platform-specific classes that carry additional
  platform-specific configurations including which algorithm to use.

  Each of OSS, Vertex, and Google Vizier has their own implementations of
  `StudyConfig` that inherit from `ProblemStatement`.

  Pythia `Policy` interface uses `ProblemStatement` as opposed to `StudyConfig`
  so that the same algorithm code can be used across platforms.
  """

  search_space: parameter_config.SearchSpace = attr.ib(
      init=True,
      factory=parameter_config.SearchSpace,
      validator=attr.validators.instance_of(parameter_config.SearchSpace),
      on_setattr=[attr.setters.convert, attr.setters.validate],
  )
  # TODO: This name/type combo is confusing.
  metric_information: MetricsConfig = attr.ib(
      init=True,
      factory=MetricsConfig,
      converter=MetricsConfig,
      validator=attr.validators.instance_of(MetricsConfig),
      on_setattr=[attr.setters.convert, attr.setters.validate],
      kw_only=True,
  )

  metadata: common.Metadata = attr.field(
      init=True,
      kw_only=True,
      factory=common.Metadata,
      validator=attr.validators.instance_of(common.Metadata),
      on_setattr=[attr.setters.convert, attr.setters.validate],
  )

  @property
  def debug_info(self) -> str:
    return ''

  @classmethod
  def from_problem(cls: Type[_T], problem: 'ProblemStatement') -> _T:
    """Converts a ProblemStatement to a subclass instance.

    Note that this method is useful in subclasses but not so much in
    `ProblemStatement` itself. `ProblemStatement.from_problem` simply generates
    a (shallow) copy of `problem`.

    Args:
      problem:

    Returns:
      A subclass instance filled with shallow copies of `ProblemStatement`
      fields.
    """
    return cls(
        search_space=problem.search_space,
        metric_information=problem.metric_information,
        metadata=problem.metadata,
    )

  def to_problem(self) -> 'ProblemStatement':
    """Converts to a ProblemStatement which is the parent class of `self`.

    Note that this method is useful in subclasses but not so much in
    `ProblemStatement` itself. `ProblemStatement.to_problem` simply generates
    a (shallow) copy of `problem`.

    Returns:
      `ProblemStatement` filled with shallow copies of `self.
    """
    return ProblemStatement(
        search_space=self.search_space,
        metric_information=self.metric_information,
        metadata=self.metadata,
    )

  @property
  def is_single_objective(self) -> bool:
    """Returns True if only one objective metric is configured."""
    return self.metric_information.is_single_objective

  @property
  def single_objective_metric_name(self) -> Optional[str]:
    """Returns the name of the single-objective metric, if set.

    Returns:
      String: name of the single-objective metric.
      None: if this is not a single-objective study.
    """
    if self.is_single_objective:
      return self.metric_information.of_type(MetricType.OBJECTIVE).item().name
    return None

  @property
  def is_safety_metric(self) -> bool:
    """Returns True if at least one safety metric is configured."""
    return self.metric_information.is_safety_metric


--- vizier/_src/pyvizier/shared/base_study_config_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.shared.base_study_config."""

import numpy as np
from vizier._src.pyvizier.shared import base_study_config
from absl.testing import absltest
from absl.testing import parameterized


class ObjectiveMetricGoalTest(absltest.TestCase):

  def test_basics(self):
    self.assertTrue(base_study_config.ObjectiveMetricGoal.MAXIMIZE.is_maximize)
    self.assertFalse(base_study_config.ObjectiveMetricGoal.MAXIMIZE.is_minimize)
    self.assertTrue(base_study_config.ObjectiveMetricGoal.MINIMIZE.is_minimize)
    self.assertFalse(base_study_config.ObjectiveMetricGoal.MINIMIZE.is_maximize)


class MetricTypeTest(absltest.TestCase):

  def test_basics(self):
    self.assertTrue(base_study_config.MetricType.SAFETY.is_safety)
    self.assertTrue(base_study_config.MetricType.OBJECTIVE.is_objective)


class MetricInformationTest(absltest.TestCase):

  def testMinMaxValueDefault(self):
    info = base_study_config.MetricInformation(
        goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE)
    self.assertEqual(info.min_value, -np.inf)
    self.assertEqual(info.max_value, np.inf)

  def testMinMaxValueSet(self):
    info = base_study_config.MetricInformation(
        goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE,
        min_value=-1.,
        max_value=1.)
    self.assertEqual(info.min_value, -1.)
    self.assertEqual(info.max_value, 1.)

  def testMinMaxBadValueInit(self):
    with self.assertRaises(ValueError):
      base_study_config.MetricInformation(
          goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE,
          min_value=1.,
          max_value=-1.)

  def testMinMaxBadValueSet(self):
    info = base_study_config.MetricInformation(
        goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE,
        min_value=-1.,
        max_value=1.)
    with self.assertRaises(ValueError):
      info.min_value = 2.
    with self.assertRaises(ValueError):
      info.max_value = -2.


class MetricsConfigTest(parameterized.TestCase):

  def testBasics(self):
    config = base_study_config.MetricsConfig()
    config.append(
        base_study_config.MetricInformation(
            name='max1', goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE))
    config.extend([
        base_study_config.MetricInformation(
            name='max_safe1',
            goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE,
            safety_threshold=0.0),
        base_study_config.MetricInformation(
            name='max2', goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE),
        base_study_config.MetricInformation(
            name='min1', goal=base_study_config.ObjectiveMetricGoal.MINIMIZE),
        base_study_config.MetricInformation(
            name='min_safe2',
            goal=base_study_config.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=0.0,
            desired_min_safe_trials_fraction=0.1)
    ])
    self.assertLen(config, 5)
    self.assertLen(config.of_type(base_study_config.MetricType.OBJECTIVE), 3)
    self.assertLen(config.of_type(base_study_config.MetricType.SAFETY), 2)
    self.assertLen(
        config.exclude_type(base_study_config.MetricType.OBJECTIVE), 2
    )

  def testDuplicateNames(self):
    config = base_study_config.MetricsConfig()
    config.append(
        base_study_config.MetricInformation(
            name='max1', goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE))
    with self.assertRaises(ValueError):
      config.append(
          base_study_config.MetricInformation(
              name='max1', goal=base_study_config.ObjectiveMetricGoal.MAXIMIZE))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/shared/common.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Common classes shared between Study and Trial."""

import collections
from collections import abc
from typing import DefaultDict, Dict, overload, Iterator
from typing import Iterable, List, Optional, Tuple, TypeVar, Union, Type

from absl import logging
import attr

from google.protobuf import any_pb2
from google.protobuf.message import Message

_M = TypeVar('_M', bound=Message)
T = TypeVar('T')
T1 = TypeVar('T1')
T2 = TypeVar('T2')
MetadataValue = Union[str, any_pb2.Any, Message]
_V = TypeVar('_V', bound=MetadataValue)

# Namespace Encoding.
#
# By definition, ∀ ns ∈ Namespace, Namespace.decode(ns.encode()) == ns.
# The tricky part of that definition is handling namespaces with components
# that are empty strings.  Notably, we want to make sure that
# Namespace(()).encode() != Namespace(('',)).encode().
# So, we set up the mapping:
# Namespace(()).encode() -> ''
# Namespace((s,)).encode() -> ':s'
# Namespace((s, s)).encode() -> ':s:s',
# et cetera, and note that every tuple gets a unique encoding, even if $s is the
# empty string.  (As long as we escape colons properly.)
#
# So, ns.encode() is a bijection, therefore it has an inverse which we call
# Namespace.decode(s).


def _parse(arg: str) -> Tuple[str, ...]:
  """Parses an encoded namespace string into a namespace tuple."""
  # The tricky part here is that arg.split('') has a length of 1, so it can't
  # generate a zero-length tuple; we handle that corner case manually.
  if not arg:
    return ()
  # And, then, once we've handled the case of _parse(''), we note that all the
  # other encoded strings begin with a colon.  It thus contains no information
  # and we can remove it.
  # TODO: Once we're on Python 3.9, use: arg = arg.removeprefix(':')
  if arg.startswith(':'):
    arg = arg[1:]
  # The rest of the algorithm is that we split on all colons, both
  # escaped and unescaped.  Then, we walk through the list of fragments and
  # join back together the colons that were preceeded by an escape character,
  # dropping the escape character as we go.
  fragments = arg.split(':')
  output = []
  join = False
  for frag in fragments:
    if join and frag and frag[-1] == '\\':
      output[-1] += ':' + frag[:-1]
      join = True
    elif join:  # Doesn't end in an escape character.
      output[-1] += ':' + frag
      join = False
    elif frag and frag[-1] == '\\':  # Don't join to previous.
      output.append(frag[:-1])
      join = True
    else:  # Don't join to previous and doesn't end in an escape.
      output.append(frag)
      join = False
  return tuple(output)


@attr.frozen(eq=True, order=True, hash=True, auto_attribs=True, init=False)
class Namespace(abc.Sequence):
  r"""A namespace for the Metadata class.

  Namespaces represent a tree of Metadata; each namespace object can be thought
  of as a tuple of components obtained by walking the tree from the root.
  This makes it easy to give each part of your algorithm its own namespace,
  to avoid name collisions.  E.g. if your algorithm A uses sub-algorithms B and
  C, you might have namespaces ":A", ":A:B", and ":A:C".

  NOTE: The empty namespace is writeable by users via a RPC in Vizier's
    user-facing API; other namespaces are writeable only by Pythia algorithms.
    (Users can read all namespaces.) So, to minimize collisions, please avoid
    the empty namespace unless your algorithm needs to read user data.

  You can create a Namespace from a tuple of strings, e.g.
  Namespace(('a', 'b')).  Or, you can create a Namespace from a single string
  with Namespace.decode(s); this parses the string into components, splitting at
  colons.  For instance Namespace.decode(':a:b') gives you a two-component
  namespace, equivalent to Namespace(('a', 'b')).
  (Note that in the tuple case, the strings are not parsed and colons are
  treated as ordinary characters.)

  TLDR: If you decode() a namespace from a string, then ":" is a
    reserved character, but when constructing from a tuple, there are no
    reserved characters.

  Decoding the string form:
  * Initial colons don't matter: Namespace.decode(':a') == Namespace('a');
    this is a single-component namespace.
  * Colons separate components:
    Namespace.decode('a:b') == Namespace(('a', 'b')).
    (This is a two-component namespace.)
  * Colons are encoded as r'\:':
    Namespace.decode('a\\:b') == Namespace(('a:b',)).
    (This is a single-component namespace.)

  Conversions: For a Namespace x,
  * Namespace.decode(x.encode()) == x; here, x.encode() will be a string with
    colons separating the components.
  * Namespaces act as a Sequence[str], so Namespace(tuple(x)) == x and
    Namespace(x) == x.
  """

  _as_tuple: Tuple[str, ...] = attr.field(hash=True, eq=True, order=True)

  def __init__(self, arg: Iterable[str] = ()):
    """Generates a Namespace from its component strings.

    Args:
      arg: typically, a tuple of strings.
    """
    arg = tuple(arg)
    self.__attrs_init__(as_tuple=arg)

  _ns_repr_table = str.maketrans({':': r'\:'})

  @classmethod
  def decode(cls, s: str) -> 'Namespace':
    r"""Decode a string into a Namespace.

    For a Namespace x, Namespace.decode(x.encode()) == x.

    Args:
      s: A string where ':' separates namespace components, and colon is escaped
        as r'\:'.

    Returns:
      A namespace.
    """
    return Namespace(_parse(s))

  def encode(self) -> str:
    """Encodes a Namespace into a string.

    Given a Namespace x, Namespace.decode(x.encode()) == x.

    Returns:
      Colons are escaped, then Namespace components are joined by colons.
    """
    return ''.join(
        [':' + c.translate(self._ns_repr_table) for c in self._as_tuple]
    )

  def __len__(self) -> int:
    """Number of components (elements of the tuple form)."""
    return len(self._as_tuple)

  def __add__(self, other: Iterable[str]) -> 'Namespace':
    """Appends components onto the namespace."""
    return Namespace(self._as_tuple + tuple(other))

  @overload
  def __getitem__(self, key: int) -> str:
    ...

  @overload
  def __getitem__(self, key: slice) -> 'Namespace':
    ...

  def __getitem__(self, key):
    """Retrieves item by the specified key."""
    if isinstance(key, int):
      return self._as_tuple[key]
    return Namespace(self._as_tuple[key])

  def __str__(self) -> str:
    """Shows the namespace, fully escaped."""
    return self.encode()

  def __repr__(self) -> str:
    """Shows the namespace, fully escaped."""
    return f'Namespace({self.encode()})'

  def startswith(self, prefix: Iterable[str]) -> bool:
    """Returns True if this namespace starts with $prefix.

    So, if the current namespace is "a:b:c", then startswith() will return True
    when called with prefix=(), ('a',), ('a', 'b'), and ('a', 'b', 'c');
    otherwise False.

    Args:
      prefix: namespace components or a Namespace object.

    Returns:
    """
    ns_prefix = Namespace(prefix)
    return self[: len(ns_prefix)] == ns_prefix


class _MetadataSingleNameSpace(Dict[str, MetadataValue]):
  """Stores metadata associated with one namespace."""

  pass


class Metadata(abc.MutableMapping):
  """Metadata class: a key-value dict-like mapping, with namespaces.

  This is the main interface for reading metadata from a Trial or StudyConfig,
  or adding metadata to a Trial/StudyConfig.  Loosely speaking, within each
  namespace, Metadata acts like a dictionary mapping from a string key to a
  string or protobuf. (See more about namespaces below.)

  Metadata can be initialized from a dictionary:
    mm = Metadata({'foo': 'Foo'})
  And items can be retrieved with:
    mm.get('foo')  # Returns 'Foo'
    mm['foo']      # Returns 'Foo'

  More items can be added with:
    mm = Metadata({'foo': 'Foo'})
    mm['bar'] = 'Bar'  # Add a single item
    mm.update({'a': 'A'}, gleep='Gleep')  # Add two items

  By default, items are added to the root/empty namespace.
  Vizier users can only add metadata to the empty namespace (the Vizier service
  will reject attempts by users to add metadata elsewhere); Pythia algorithms
  can add metadata to any namespace, but should normally work in a single unique
  namespace, and should avoid the root namespace, unless they intend to
  pass data to/from Vizier users.

  1. Keys are namespaced. Each Metadata object only interacts with one
    Namespace.

    Namespaces form a tree, and you can walk down the tree.

    NOTE ns(s: str) takes one step down the namespace tree. For nearly all
    practical purposes, ignore abs_ns() which is only used for conversions
    to and from protobufs.

    mm = Metadata({'foo': 'foofoo'})
    # $mm is created with its current namespace equal to the root/empty
    # namespace.
    mm.ns('NewName')['bar'] = 'Bar'
    # We've added an item in the ":NewName" namespace, but $mm's current
    # namespace is unchanged.

  2. Values can be protobufs. If `metadata['foo']` is an instance of `MyProto`
    proto message or an `Any` proto that packs a `MyProto` message, then the
    proto can be recovered by calling:

    my_proto = metadata.get('foo', cls=MyProto)
    isinstance(my_proto, MyProto) # Returns `True`

    NOTE that the bracket operator doesn't work well for protobufs:
      metadata['foo'] will return an `Any` protobuf instead of a MyProto.
      For protos, you may wish to use
      `metadata.get_or_error('foo', cls=MyProto)` instead of the bracket form.

  3. An iteration over a Metadata object only shows you the data in the current
    namespace.  So,

    mm = Metadata({'foo': 'foofoo'})
    for k, v in mm.ns('gleep'):
      ...

    will not yield anything because there are no keys in the 'gleep' namespace.
    WARNING: Because of this behavior, if you iterate over Metadata(mm), you
      will quietly drop metadata from all but mm's current namespace.
    NOTE also that the type of $v is MetadataValue, which can carry strings and
      protos; you may want to use mm.get_or_error(key, cls=proto_class) to
      unpack the contained proto.

  4. To iterate over all the keys in all the namespaces use

    mm = Metadata()
    mm.ns('gleep')['x'] = 'X'
    for ns, k, v in mm.all_items():
      # iteration will include ('gleep', 'x', 'X')
      # Be aware that type(v) is MetadataValue, which can carry either strings
      # or protos.
  """

  def __init__(
      self,
      *args: Union[
          Dict[str, MetadataValue], Iterable[Tuple[str, MetadataValue]]
      ],
      **kwargs: MetadataValue,
  ):
    """Construct; this follows dict(), and puts data in the root namespace.

    You can pass it a dict, or an object that yields (key, value)
    pairs, and those pairs will be put in the root namespace.

    Args:
      *args: A dict or an iterable the yields key-value pairs.
      **kwargs: key=value pairs to be added to the specified namespace.
    """
    self._stores: DefaultDict[Namespace, _MetadataSingleNameSpace] = (
        collections.defaultdict(_MetadataSingleNameSpace)
    )
    self._namespace = Namespace()
    self._store = self._stores[self._namespace]
    self._store.update(*args, **kwargs)

  def ns(self, component: str) -> 'Metadata':
    r"""Switches to a deeper namespace by appending one component.

    The entire tree of metadata is shared between $self and the returned value,
    but the returned value will have a deeper current namespace.  ($self is not
    modified.)

    Args:
      component: one component to be appended to the current namespace.

    Returns:
      A new Metadata object in the specified namespace; the new object shares
      metadata with $self.
    """
    new_ns: Namespace = self._namespace + (component,)
    return self._copy_core(new_ns)

  def __repr__(self) -> str:
    """Prints items in all namespaces."""
    itemlist: List[str] = []
    for namespace, store in self._stores.items():
      item_string = f'(namespace:{namespace}, items: {store})'
      itemlist.append(item_string)
    return 'Metadata({}, current_namespace={})'.format(
        ', '.join(itemlist), self._namespace.encode()
    )

  def __str__(self) -> str:
    """Prints items in the current namespace."""
    return 'namespace: {} items: {}'.format(str(self._namespace), self._store)

  def get_proto(self, key: str, *, cls: Type[_M]) -> Optional[_M]:
    """Deprecated: use get() instead."""
    logging.warning(
        'Metadata.get_proto() is deprecated, prefer Metadata.get().'
    )
    value = self._store.get(key, None)
    if value is None:
      return None

    if isinstance(value, cls):
      # Starting from 3.10, pytype supports typeguard, which obsoletes
      # the need for the `pytype:disable` clause.
      return value  # pytype: disable=bad-return-type
    if isinstance(value, any_pb2.Any):
      # `value` is an Any proto potentially packing `cls`.
      message = cls()
      success = value.Unpack(message)
      return message if success else None
    return None

  def get_or_error(self, key: str, *, cls: Type[T] = str) -> T:
    """Gets the metadata as type `cls`, or raises a KeyError.

    This acts like the square bracket operator, except that
    it lets you specify a class; it gets the metadata from the current
    namespace.

    Examples with string metadata:
      metadata = common.Metadata({'key': 'value'})
      assert metadata.get_or_error('key') == 'value'
      metadata.get_or_error('badkey')  # raises KeyError

    Examples with numeric values:
      metadata = common.Metadata({'float': '1.2', 'int': '60'})
      assert metadata.get_or_error('int', cls=int) == 60
      assert metadata.get_or_error('float', cls=float) == 1.2
      metadata.get_or_error('badkey', cls=float)      # raises KeyError

    Example with `Duration` and `Any` proto values:
      duration = Duration(seconds=60)
      anyproto = Any()
      anyproto.Pack(duration)
      metadata = common.Metadata({'duration': duration, 'any': anyproto})
      assert metadata.get_or_error('duration', cls=Duration) == duration
      assert metadata.get_or_error('any', cls=Duration).seconds == 60

    Args:
      key:
      cls: Desired type of the value.

    Returns:
      The matching metadata value is parsed into type `cls`. For proto messages,
      it involves unpacking an Any proto.

    Raises:
      KeyError if the metadata item is not present.
      TypeError or other errors if the string can't be converted to $cls.
    """
    value = self._store[key]
    if isinstance(value, cls):
      # Starting from 3.10, pytype supports typeguard, which obsoletes
      # the need for the `pytype:disable` clause.
      return value  # pytype: disable=bad-return-type
    elif isinstance(value, any_pb2.Any):
      # `value` is an Any proto potentially packing `cls`.
      message = cls()
      if not value.Unpack(message):
        logging.warning(
            'Cannot unpack message to %s: %s', cls, str(value)[:100]
        )
        raise TypeError('Cannot unpack to %s' % cls)
      return message
    else:
      return cls(value)

  def get(
      self, key: str, default: T1 = None, *, cls: Type[T2] = str
  ) -> Union[T1, T2]:
    """Gets the metadata as type `cls`, or $default if not present.

    This returns $default if the specified metadata item is not found.
    Note that there's always a default value, and the $default defaults to None.
    This gets the data from the current namespace.

    For string values, this function behaves exactly like a
    regular string-to-string dict (within its namespace).
      metadata = common.Metadata({'key': 'value'})
      metadata.get('key')  # returns 'value'
      metadata.get('badkey')  # returns None
      assert metadata.get('badkey', 'badvalue') == 'badvalue'

    Examples with numeric values:
      metadata = common.Metadata({'float': '1.2', 'int': '60'})
      value = metadata.get('int', cls=int)
      if value is not None:
        assert value == 60
      #
      metadata.get('float', cls=float)       # returns 1.2
      metadata.get('badkey', cls=float)      # returns None
      metadata.get('int', cls=int)           # returns 60
      assert metadata.get('float', 0.0, cls=float) == 1.2
      assert metadata.get('badkey', 1, cls=int) == 1
      assert metadata.get('badkey', 0.2, cls=float) == 0.2

    Example with `Duration` and `Any` proto values:
      duration = Duration(seconds=60)
      anyproto = Any()
      anyproto.Pack(duration)
      metadata = common.Metadata({'duration': duration, 'any': anyproto})
      duration_out =  metadata.get('duration', cls=Duration)
      if duration_out is not None:
        assert duration_out == duration
      any_out =  metadata.get('any', cls=Duration)
      if any_out is not None:
        assert any_out == duration

    Args:
      key:
      default: Default value.
      cls: Desired type of the value.

    Returns:
      $default if the key does not exist. Otherwise, the matching value is
      parsed into type `cls`. For proto messages, it involves unpacking an
      Any proto.

    Raises:
      TypeError or other errors if the string can't be converted to $cls.
    """
    try:
      return self.get_or_error(key, cls=cls)
    except KeyError:
      return default

  # TODO: Rename to `abs_namespaces`
  def namespaces(self) -> List[Namespace]:
    """List all namespaces for which there is at least one key."""
    return [ns for ns, store in self._stores.items() if store]

  # TODO: Rename to `namespaces`
  def subnamespaces(self) -> Tuple[Namespace, ...]:
    """Returns relative namespaces that are at or below the current namespace.

    For all `ns` in the returned value, `self.abs_ns(md.current_ns() + ns)` is
    not empty.
    # Examples:
    md = Metadata()
    md.ns('foo').ns('bar')['A'] = 'b'
    md.subnamespaces() == (Namespace(['foo', 'bar']),)
    md.ns('foo').subnamespaces() == (Namespace(['bar']),)

    Returns:
      For all namespaces that begin with the current namespace and are
      non-empty, this returns a namespace object that contains the relative
      path from the current namespace.
    """
    return tuple(
        [
            Namespace(ns[len(self._namespace) :])
            for ns, store in self._stores.items()
            if store and ns.startswith(self._namespace)
        ]
    )

  def current_ns(self) -> Namespace:
    """Displays the object's current Namespace."""
    return self._namespace

  def all_items(self) -> Iterator[Tuple[Namespace, str, MetadataValue]]:
    """Yields an iterator that walks through all metadata items.

    This iterates through all the metadata items in all namespaces, vs.
    __iter__() which just iterates through all the items in the current
    namespace.

    Yields:
      Tuple of (namespace, key, value).
    """
    for ns in self.namespaces():
      for k, v in self.abs_ns(ns).items():
        yield (ns, k, v)

  def items_by_cls(self, *, cls: Type[_V]) -> Iterator[Tuple[str, _V]]:
    """Yields an iterator over items whose type=$cls in the current namespace.

    This iterates through the metadata items in the current namespace, like
    __iter__(), except that it only returns items of the specified type.

    Args:
      cls: What type of objects to filter for?

    Yields:
      Tuple of (key, value).
    """
    for k_v in self.items():
      if isinstance(k_v[1], cls):
        yield k_v

  # START OF abstract methods inherited from `MutableMapping` base class.
  def __getitem__(self, key: str) -> MetadataValue:
    return self._store.__getitem__(key)

  def __setitem__(self, key: str, value: MetadataValue):
    self._store[key] = value

  def __delitem__(self, key: str):
    del self._store[key]

  def __iter__(self):
    return iter(self._store)

  def __len__(self):
    """The number of elements in the current namespace."""
    return len(self._store)

  def __bool__(self):
    """True if this instance contains any metadata in _any_ namespace."""
    for s in self._stores.values():
      if s:
        return True
    return False

  def __copy__(self) -> 'Metadata':
    """Shallow copy -- metadata continues to be shared.

    Returns:
      A copy of the object.
    """
    return self._copy_core(self._namespace)

  # END OF Abstract methods inherited from `MutableMapping` base class.

  def abs_ns(self, namespace: Iterable[str] = ()) -> 'Metadata':
    """Returns a metadata object set to the specified absolute namespace.

    NOTE Prefer using ns() instead in most cases.

    abs_ns() jumps to the root namespace and
    abs_ns(ns) jumps to the specified Namespace.

    (NOTE: ns() and abs_ns() take different argument types!)
    (NOTE: Neither ns() nor abs_ns() modify the Metadata object they are called
     on: they return a shallow copy that shares all metadata items, but
     which displays a different namespace.)

    # Use of abs_ns().
    mm.abs_ns(['NewName'])  # returns 'Bar'
    mmx = mm.ns('x')
    mmx.abs_ns(['NewName'])  # returns 'Bar2'
    mmx.abs_ns().get('foo')  # returns 'foofoo'

    # Multi-component namespaces.
    mm = Metadata()
    mm.ns('a').ns('b')['foo'] = 'AB-foo'
    mm.ns('a')['foo'] = 'A-foo'
    mm['foo']          # Throws a KeyError
    mm.ns('a')['foo']  # returns 'A-foo'
    mm.ns('a').ns('b')['foo']  # returns 'AB-foo'
    # abs_ns() can be also used:
    mm.abs_ns(['a', 'b']).get('foo')  # Returns 'ab-foo'
    mm.abs_ns(Namespace.decode('a:b')).get('foo')  # Returns 'ab-foo'

    All the Metadata object's data is shared between $self and the returned
    object, but the new Metadata object will have a different current
    namespace.  (Note that $self is not modified, and the current namespace of
    $self doesn't matter.)

    NOTE: $namespace can be a Namespace object, because you can iterate over
      a Namespace to get strings.

    Args:
      namespace: a list of Namespace components.  (Defaults to the root, empty
        Namespace.)

    Returns:
      A new Metadata object that shares data with $self, but the current
      namespace is one level deeper.
    """
    if isinstance(namespace, str):
      raise ValueError(
          'Passing str to abs_ns() is rarely intended and therefore '
          'considered an error. Carefully read the class doc and prefer '
          'using ns(). If you do decide abs_ns() is the right method, '
          'expclitily pass abs_ns([namespace]).'
      )
    return self._copy_core(Namespace(namespace))

  def _copy_core(self, ns: Namespace) -> 'Metadata':
    """Shallow copy: metadata is shared, default namespace changes.

    Args:
      ns: the namespace to use for the new object.

    Returns:
      A copy of the object.
    """
    md = Metadata()
    md._namespace = ns  # pylint: disable='protected-access'
    md._stores = self._stores  # pylint: disable='protected-access'
    md._store = md._stores[md._namespace]  # pylint: disable='protected-access'
    return md

  def update(
      self,
      *args: Union[
          Dict[str, MetadataValue], Iterable[Tuple[str, MetadataValue]]
      ],
      **kwargs: MetadataValue,
  ) -> None:
    self._store.update(*args, **kwargs)

  def attach(self, other: 'Metadata') -> None:
    """Attach the $other metadata as a descendent of this metadata.

    More precisely, it takes the part of `other`'s namespace that is at or
    below `other`'s current namespace, and attaches it to `self`'s current
    namespace.
    * Tree structure is preserved and nothing is flattened.
    * Attached data overwrites existing data, item-by-item, not
      namepace-by-namespace.

    So, if we have
    other = Metadata()
    other.abs_ns(('x', 'y', 'z'))['foo'] = 'bar'
    m = Metadata()
    m.ns('w').attach(other.ns('x'))
    then
    m.abs_ns(('w', 'y', 'z'))['foo'] will contain 'bar'.

    Args:
      other: a Metadata object to copy from.
    """
    for ns in other.subnamespaces():
      self._stores[self._namespace + ns].update(
          other.abs_ns(other.current_ns() + ns)
      )


--- vizier/_src/pyvizier/shared/common_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.shared.common."""

import copy
import logging

from vizier._src.pyvizier.shared import common
from google.protobuf import any_pb2
from google.protobuf import duration_pb2
from absl.testing import absltest


class MetadataGetClsTest(absltest.TestCase):

  def test_get_proto(self):
    duration = duration_pb2.Duration(seconds=60)
    anyproto = any_pb2.Any()
    anyproto.Pack(duration)
    metadata = common.Metadata(duration=duration, any=anyproto)

    self.assertEqual(
        metadata.get_proto('duration', cls=duration_pb2.Duration), duration)
    self.assertEqual(
        metadata.get_proto('any', cls=duration_pb2.Duration), duration)
    self.assertEqual(
        metadata.get('duration', cls=duration_pb2.Duration), duration)
    self.assertEqual(metadata.get('any', cls=duration_pb2.Duration), duration)

  def test_get_int(self):
    metadata = common.Metadata({'string': '30', 'int': '60'})
    self.assertEqual(metadata.get('string', cls=int), 30)
    self.assertEqual(metadata.get('int', cls=int), 60)
    self.assertEqual(metadata.get('badkey', 1, cls=int), 1)


class MetadataNamespaceTest(absltest.TestCase):

  def test_basic(self):
    ns0 = common.Namespace()
    self.assertEmpty(ns0)
    self.assertEqual(str(ns0), '')
    self.assertEqual(ns0.encode(), '')
    self.assertEqual(ns0, common.Namespace.decode(''))
    n1t = common.Namespace(('aerer',))
    self.assertLen(n1t, 1)
    n1 = common.Namespace.decode('a78')
    self.assertLen(n1, 1)
    self.assertEqual(str(n1), ':a78')
    n2 = common.Namespace(('a78', 'bfe'))
    self.assertLen(n2, 2)
    n2s1 = common.Namespace.decode('a78:bfe')
    self.assertLen(n2s1, 2)
    self.assertEqual(n2.encode(), n2s1.encode())
    n2s2 = common.Namespace.decode(':a78:bfe')
    self.assertLen(n2s2, 2)
    self.assertEqual(n2.encode(), n2s2.encode())
    self.assertEqual(n2, n2s2)
    self.assertEqual(n2s1, n2s2)
    ns = common.Namespace(('a', 'b'))
    self.assertLen(ns, 2)
    self.assertEqual(tuple(ns), ('a', 'b'))
    self.assertEqual(str(ns), ':a:b')
    self.assertEqual(ns.encode(), ':a:b')

  def test_escape(self):
    s1 = 'a\\:A'
    ns1 = common.Namespace.decode(s1)
    self.assertLen(ns1, 1)
    self.assertEqual(str(ns1), ':a\\:A')
    self.assertEqual(ns1.encode(), ':' + s1)
    self.assertEqual(common.Namespace.decode(ns1.encode()), ns1)
    #
    s2 = 'b:B'
    ns2 = common.Namespace.decode(s2)
    self.assertLen(ns2, 2)
    self.assertEqual(str(ns2), ':' + s2)
    self.assertEqual(ns2.encode(), ':' + s2)
    self.assertEqual(common.Namespace.decode(ns2.encode()), ns2)
    #
    s1e1 = ':b\\B'
    ns1e1 = common.Namespace.decode(s1e1)
    self.assertLen(ns1e1, 1)
    self.assertEqual(ns1e1.encode(), s1e1)
    self.assertEqual(common.Namespace.decode(ns1e1.encode()), ns1e1)
    ns1e2 = common.Namespace((s1e1.lstrip(':'),))
    self.assertLen(ns1e2, 1)
    self.assertEqual(ns1e2.encode(), s1e1)
    self.assertEqual(ns1e2, ns1e1)
    self.assertEqual(common.Namespace.decode(ns1e2.encode()), ns1e2)
    #
    s1c = r':b\:B'
    ns1c = common.Namespace.decode(s1c)
    self.assertLen(ns1c, 1)
    # Initial colon is harmlessly removed.
    self.assertEqual(ns1c.encode(), s1c)
    self.assertEqual(common.Namespace.decode(ns1c.encode()), ns1c)
    self.assertEqual(common.Namespace(('b:B',)), ns1c)


class MetadataTest(absltest.TestCase):

  def create_test_metadata(self):
    md = common.Metadata({'bar': 'bar_v'}, foo='foo_v')
    md.ns('Name').update(foo='Name_foo_v', baz='Name_baz_v')
    return md

  def test_empty_namespaces(self):
    md = common.Metadata()
    self.assertEmpty(list(md.namespaces()))
    md = common.Metadata().ns('ns')
    self.assertEmpty(list(md.namespaces()))

  def test_nonempty_namespaces(self):
    mm = self.create_test_metadata()
    self.assertLen(mm.namespaces(), 2)

  def test_getters_are_consistent_when_item_is_in_dict(self):
    mm = self.create_test_metadata()
    self.assertEqual(mm['foo'], 'foo_v')
    self.assertEqual(mm.get('foo'), 'foo_v')

  def test_getters_are_consistent_when_item_is_not_in_dict(self):
    mm = self.create_test_metadata()
    self.assertIsNone(mm.get('baz'))
    with self.assertRaises(KeyError):
      _ = mm['baz']

  def test_separator_is_not_allowed_as_keys_after_init(self):
    mm = self.create_test_metadata()
    with self.assertRaises(KeyError):
      _ = mm['Name_foo']

  def test_namespace_works_as_intended(self):
    mm = self.create_test_metadata()
    self.assertEqual(mm.ns('Name')['foo'], 'Name_foo_v')
    self.assertIsNone(mm.ns('Name').get('bar'))

    mm_name = mm.ns('Name')
    self.assertEqual(mm_name['foo'], 'Name_foo_v')
    self.assertIsNone(mm_name.get('bar'))
    self.assertEqual(mm.ns('Name')['foo'], 'Name_foo_v')

  def test_create_new_namespace(self):
    # Calling ns() with an unexisting namespace should work fine.
    mm = self.create_test_metadata()
    mm.ns('NewName')['foo'] = 'NewName_foo_v'
    self.assertEqual(mm.ns('NewName')['foo'], 'NewName_foo_v')
    self.assertIsNone(mm.ns('NewName').get('bar'))

  def test_changing_namespace_copies_reference(self):
    mm = self.create_test_metadata()
    # Calling ns() copies by reference so any changes to the returned Metadata
    # object is reflected in the original object.
    mm_in_namespace = mm.ns('Name')
    mm_in_namespace['foofoo'] = 'Name_foofoo_v'
    self.assertEqual(mm.ns('Name')['foofoo'], 'Name_foofoo_v')

  def test_iterator(self):
    mm = self.create_test_metadata()
    self.assertSequenceEqual(list(mm.keys()), ['bar', 'foo'])
    self.assertSequenceEqual(
        list(mm.ns('Name').values()), ['Name_foo_v', 'Name_baz_v'])
    self.assertLen(list(mm.items()), 2)

  def test_all_items(self):
    mm = self.create_test_metadata()
    sorted_tuples = list(sorted(mm.all_items()))
    logging.info('sorted_tuples: %s', sorted_tuples)
    self.assertLen(sorted_tuples, 4)
    self.assertSequenceEqual(
        sorted_tuples, [(common.Namespace([]), 'bar', 'bar_v'),
                        (common.Namespace([]), 'foo', 'foo_v'),
                        (common.Namespace(['Name']), 'baz', 'Name_baz_v'),
                        (common.Namespace(['Name']), 'foo', 'Name_foo_v')])

  def test_repr_str(self):
    mm = self.create_test_metadata()
    self.assertNotEmpty(str(mm), '')
    self.assertNotEmpty(repr(mm), repr(''))

  def test_update(self):
    md = common.Metadata(foo='foo_v')
    md.ns('Name').update(foo='Name_foo_v', baz='Name_baz_v')

    md2 = common.Metadata()
    md2.ns('Name').update(foo='Name_foo_v2', bar='Name_bar_v2')

    md.ns('Name').update(md2.ns('Name'))

    self.assertLen(md.ns('Name'), 3)
    self.assertIn('bar', md.ns('Name'))

  def test_copy(self):
    # There's no useful distinction to be made between copy.copy() and
    # copy.deepcopy().
    mm = common.Metadata().ns('ns1')
    mm.update(foo='bar')
    mm_copy = copy.copy(mm)
    mm_deepcopy = copy.deepcopy(mm)
    # Check that copies match.
    self.assertEqual(mm['foo'], 'bar')
    self.assertEqual(mm_copy['foo'], 'bar')
    self.assertEqual(mm_deepcopy['foo'], 'bar')
    self.assertEqual(mm_deepcopy.namespaces(), mm.namespaces())
    self.assertEqual(mm_copy.namespaces(), mm.namespaces())
    # Check that the  deep copy is disconnected.
    mm_deepcopy['nerf'] = 'gleep'
    with self.assertRaises(KeyError):
      mm['nerf']  # pylint: disable=pointless-statement
    with self.assertRaises(KeyError):
      mm_copy['nerf']  # pylint: disable=pointless-statement
    # Check that the shallow copy shares the metadata store with the original.
    mm_copy['blip'] = 'tonk'
    self.assertEqual(mm['blip'], mm_copy['blip'])
    # ... but no sharing with the deep copy.
    with self.assertRaises(KeyError):
      mm_deepcopy['blip']  # pylint: disable=pointless-statement
    # Here's a test for a specific bug, where Metadata._store is improperly
    # disconnected from Metadata._stores.
    mx = common.Metadata()
    copy.copy(mx).ns('A')['a'] = 'Aa'
    self.assertEqual(mx.ns('A')['a'], 'Aa')

  def test_construction(self):
    # Test with iterables.
    m0i = common.Namespace([])
    self.assertEmpty(m0i)
    m0d = common.Namespace.decode('')
    self.assertEmpty(m0d)
    self.assertEqual(m0d, m0i)
    m1i = common.Namespace(['abc'])
    self.assertLen(m1i, 1)
    self.assertEqual(m1i, common.Namespace(tuple(m1i)))
    self.assertEqual(m1i, common.Namespace.decode(m1i.encode()))
    m2i = common.Namespace(['abc', 'def'])
    self.assertLen(m2i, 2)
    self.assertEqual(m2i, common.Namespace(tuple(m2i)))
    self.assertEqual(m2i, common.Namespace.decode(m2i.encode()))
    m3i = common.Namespace(['abc', 'de:f'])
    self.assertLen(m3i, 2)
    self.assertEqual(m3i, common.Namespace(tuple(m3i)))
    self.assertEqual(m3i, common.Namespace.decode(m3i.encode()))
    # Test with strings.
    m1sc = common.Namespace.decode(':abc')
    self.assertLen(m1sc, 1)
    self.assertEqual(m1sc, common.Namespace(tuple(m1sc)))
    self.assertEqual(m1sc, common.Namespace.decode(m1sc.encode()))
    m1s = common.Namespace.decode('abc')
    self.assertLen(m1s, 1)
    self.assertEqual(m1s, common.Namespace(tuple(m1s)))
    self.assertEqual(m1s, common.Namespace.decode(m1s.encode()))
    m2s = common.Namespace.decode('abc:def')
    self.assertLen(m2s, 2)
    self.assertEqual(m2s, common.Namespace(tuple(m2s)))
    self.assertEqual(m2s, common.Namespace.decode(m2s.encode()))
    m3s = common.Namespace.decode('abc:de\\f')
    self.assertLen(m3s, 2)
    self.assertEqual(m3s, common.Namespace(tuple(m3s)))
    self.assertEqual(m3s, common.Namespace.decode(m3s.encode()))

  def test_assign_proto(self):
    m0 = common.Metadata()
    m0['foo'] = duration_pb2.Duration(seconds=60)
    dur_out = m0.get('foo', cls=duration_pb2.Duration)
    logging.info('dur_out= %s', dur_out)
    self.assertIsNotNone(dur_out)
    self.assertIsInstance(dur_out, duration_pb2.Duration)
    if dur_out is not None:
      self.assertEqual(dur_out.seconds, 60)

  def test_get_or_error(self):
    m0 = common.Metadata({'foo': 33, 'bar': 'Z'})
    m0['gleep'] = duration_pb2.Duration(seconds=60)
    self.assertEqual(m0.get_or_error('foo', cls=int), 33)
    self.assertEqual(m0.get_or_error('bar'), 'Z')
    dur_out = m0.get_or_error('gleep', cls=duration_pb2.Duration)
    self.assertIsInstance(dur_out, duration_pb2.Duration)
    self.assertEqual(dur_out.seconds, 60)

  def test_startswith(self):
    m1 = common.Namespace(['aa', 'bb'])
    self.assertTrue(m1.startswith(common.Namespace(['aa'])))
    self.assertTrue(m1.startswith(common.Namespace(['aa', 'bb'])))
    self.assertTrue(m1.startswith(m1))
    self.assertTrue(m1.startswith(common.Namespace(tuple(m1))))
    self.assertFalse(m1.startswith(common.Namespace(['bb'])))
    self.assertFalse(m1.startswith(common.Namespace(['aa', 'bb', 'cc'])))
    self.assertFalse(m1.startswith(common.Namespace(['bb', 'bb'])))
    self.assertFalse(m1.startswith(common.Namespace(['aa', 'aa'])))

  def test_subnamespace(self):
    mm = common.Metadata()
    mm.ns('ns1')['foo'] = 'bar'
    mm.ns('ns2')['foo'] = 'bar'
    mm.ns('ns1').ns('ns11')['foo'] = 'bar'
    mm.ns('ns1').ns('ns:11')['gleep'] = 'nerf'

    self.assertSequenceEqual(mm.subnamespaces(), [
        common.Namespace(['ns1']),
        common.Namespace(['ns2']),
        common.Namespace(['ns1', 'ns11']),
        common.Namespace(['ns1', 'ns:11']),
    ])
    self.assertSequenceEqual(
        mm.ns('ns1').subnamespaces(), [
            common.Namespace([]),
            common.Namespace(['ns11']),
            common.Namespace(['ns:11'])
        ])
    self.assertSequenceEqual(mm.ns('ns2').subnamespaces(), [common.Namespace()])
    self.assertSequenceEqual(mm.ns('ns3').subnamespaces(), [])

  def test_namespace_add(self):
    n0 = common.Namespace()
    self.assertEmpty(n0)
    self.assertEqual(n0 + (), common.Namespace([]))
    self.assertEqual(n0 + ('ab',), common.Namespace([
        'ab',
    ]))
    self.assertEqual(n0 + ('a:b',), common.Namespace(['a:b']))
    self.assertEqual(n0 + ('a:b',), common.Namespace(['a:b']))
    self.assertEqual(n0 + ('ab', 'cd'), common.Namespace(['ab', 'cd']))
    n1 = common.Namespace(['xy'])
    self.assertLen(n1, 1)
    self.assertEqual(n1 + ('ab',), common.Namespace(['xy', 'ab']))
    self.assertEqual(n1 + ('a:b',), common.Namespace(['xy', 'a:b']))
    self.assertEqual(n1 + ('a:b',), common.Namespace(['xy', 'a:b']))
    n2 = common.Namespace(['xy', 'zw'])
    self.assertLen(n2, 2)
    self.assertLen(n2 + ('ab',), 3)
    self.assertEqual(n2 + ('ab',), common.Namespace(['xy', 'zw', 'ab']))
    self.assertLen(n2 + ('ab', 'cd'), 4)
    self.assertEqual(n2 + ('ab', 'cd'), common.Namespace.decode('xy:zw:ab:cd'))

  def test_metadata_attach(self):
    # Set up a metadata tree.
    mm = common.Metadata()
    mm.ns('ns1').ns('ns:11').update(foo='bar')
    mm.ns('ns1').ns('ns12').update(foo='gleep')
    mm.ns('ns1').update(foo='nerf')
    mm.ns('ns|').update(foo='pag')
    # Attach that metadata tree to a branch of an empty tree.
    m1 = common.Metadata()
    m1.ns('ns0').ns('ns00').attach(mm)
    self.assertEmpty(m1.abs_ns())
    self.assertEqual(m1.ns('ns0').ns('ns00'), mm)
    self.assertEqual(m1.abs_ns(['ns0', 'ns00', 'ns1', 'ns:11'])['foo'], 'bar')
    self.assertEqual(m1.abs_ns(['ns0', 'ns00', 'ns1', 'ns12'])['foo'], 'gleep')
    self.assertEqual(m1.abs_ns(['ns0', 'ns00', 'ns1'])['foo'], 'nerf')
    self.assertEqual(m1.abs_ns(['ns0', 'ns00', 'ns|'])['foo'], 'pag')
    # Attach just part of $mm to a branch of a new, empty tree.
    m2 = common.Metadata()
    m2.ns('nsX').attach(mm.ns('ns1'))
    self.assertEqual(m2.abs_ns(['nsX', 'ns:11'])['foo'], 'bar')
    self.assertEqual(m2.abs_ns(['nsX', 'ns12'])['foo'], 'gleep')
    self.assertEqual(m2.abs_ns(['nsX'])['foo'], 'nerf')
    # Check that attach() overwrites key collisions, but preserves other data.
    m3 = common.Metadata()
    m3['foo'] = 'Y'  # This will be overwritten.
    m3['z'] = 'Z'  # This will not be overwritten.
    m3.attach(mm.ns('ns1').ns('ns:11'))
    self.assertEqual(m3['z'], 'Z')
    self.assertEqual(m3['foo'], 'bar')

  def test_metadata_items_by_cls(self):
    mm = common.Metadata(foo='bar', nerf='gleep')
    mm['aaa'] = duration_pb2.Duration(seconds=5)
    mm.ns('ns1')['foo'] = 'bar1'
    test1 = list(sorted(mm.items_by_cls(cls=str)))
    self.assertLen(test1, 2)
    self.assertEqual(test1[0], ('foo', 'bar'))
    self.assertEqual(test1[1], ('nerf', 'gleep'))
    test2 = list(sorted(mm.items_by_cls(cls=duration_pb2.Duration)))
    self.assertLen(test2, 1)
    self.assertEqual(test2[0][1].seconds, 5)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/shared/context.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Wrapper classes for Context protos and other messages in them."""
from typing import Dict, Optional

import attr
from vizier._src.pyvizier.shared import common
from vizier._src.pyvizier.shared import trial

Metadata = common.Metadata
ParameterValue = trial.ParameterValue


@attr.s(auto_attribs=True, frozen=False, init=True, slots=True)
class Context:
  """Wrapper for Context proto."""
  description: Optional[str] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
      on_setattr=attr.setters.validate)

  parameters: Dict[str, ParameterValue] = attr.ib(
      init=True,
      kw_only=True,
      factory=dict,
      validator=attr.validators.deep_mapping(
          key_validator=attr.validators.instance_of(str),
          value_validator=attr.validators.instance_of(ParameterValue),
          mapping_validator=attr.validators.instance_of(dict)),
      on_setattr=attr.setters.validate)  # pytype: disable=wrong-arg-types

  metadata: Metadata = attr.ib(
      init=True,
      kw_only=True,
      default=Metadata(),
      validator=attr.validators.instance_of(Metadata),
      on_setattr=attr.setters.validate)

  related_links: Dict[str, str] = attr.ib(
      init=True,
      kw_only=True,
      factory=dict,
      validator=attr.validators.deep_mapping(
          key_validator=attr.validators.instance_of(str),
          value_validator=attr.validators.instance_of(str),
          mapping_validator=attr.validators.instance_of(dict)),
      on_setattr=attr.setters.validate)  # pytype: disable=wrong-arg-types


--- vizier/_src/pyvizier/shared/context_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.shared.context."""

from vizier._src.pyvizier.shared import context
from absl.testing import absltest


class ContextTest(absltest.TestCase):

  def testDefaultsNotShared(self):
    """Make sure default parameters are not shared between instances."""
    context1 = context.Context()
    context2 = context.Context()
    context1.parameters['x1'] = context.ParameterValue(5)
    self.assertEmpty(context2.parameters)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/shared/parameter_config.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""ParameterConfig wraps ParameterConfig and ParameterSpec protos."""

import collections
from typing import Iterable, Set as AbstractSet, Sized
import copy
import enum
import json
import math
import re
from typing import Generator, Iterator, List, Optional, Sequence, Tuple, Union, overload

from absl import logging
import attr
from vizier._src.pyvizier.shared import trial


ExternalType = trial.ExternalType
ParameterType = trial.ParameterType


class ScaleType(enum.Enum):
  """Valid Values for ParameterConfig.scale_type."""

  LINEAR = 'LINEAR'
  LOG = 'LOG'
  REVERSE_LOG = 'REVERSE_LOG'
  UNIFORM_DISCRETE = 'UNIFORM_DISCRETE'

  def is_nonlinear(self) -> bool:
    return self in [self.LOG, self.REVERSE_LOG]


# A sequence of possible internal parameter values.
ParameterValueTypes = trial.ParameterValueTypes
MonotypeParameterSequence = Union[Sequence[Union[int, float]], Sequence[str]]
MonotypeParameterList = Union[List[Union[int, float]], List[str]]


def _validate_bounds(bounds: Union[Tuple[int, int], Tuple[float, float]]):
  """Validates the bounds."""
  if len(bounds) != 2:
    raise ValueError(f'Bounds must have length 2. Given: {bounds}')
  lower = bounds[0]
  upper = bounds[1]
  if not all([math.isfinite(v) for v in (lower, upper)]):
    raise ValueError(
        f'Both "lower" and "upper" must be finite. Given: ({lower}, {upper})'
    )
  if lower > upper:
    raise ValueError(
        f'Lower cannot be greater than upper: given lower={lower} upper={upper}'
    )


def _get_feasible_points_and_bounds(
    feasible_values: Sequence[float],
) -> Tuple[List[float], Union[Tuple[int, int], Tuple[float, float]]]:
  """Validates and converts feasible values to floats."""
  if not all([math.isfinite(p) for p in feasible_values]):
    raise ValueError(
        f'Feasible values must all be finite. Given: {feasible_values}'
    )

  feasible_points = list(sorted(feasible_values))
  bounds = (feasible_points[0], feasible_points[-1])
  return feasible_points, bounds


def _get_categories(categories: Sequence[str]) -> List[str]:
  """Returns the categories."""
  return sorted(list(categories))


def _get_default_value(
    param_type: ParameterType, default_value: Union[float, int, str]
) -> Union[float, int, str]:
  """Validates and converts the default_value to the right type."""
  if param_type in (ParameterType.DOUBLE, ParameterType.DISCRETE) and (
      isinstance(default_value, float) or isinstance(default_value, int)
  ):
    return float(default_value)
  elif param_type == ParameterType.INTEGER and (
      isinstance(default_value, float) or isinstance(default_value, int)
  ):
    if isinstance(default_value, int):
      return default_value
    else:
      # Check if the float rounds nicely.
      default_int_value = round(default_value)
      if not math.isclose(default_value, default_int_value):
        raise ValueError(
            'default_value for an INTEGER parameter should be an '
            f'integer, got float: [{default_value}]'
        )
      return default_int_value
  elif param_type == ParameterType.CATEGORICAL and isinstance(
      default_value, str
  ):
    return default_value
  elif param_type == ParameterType.CUSTOM:
    return default_value
  raise ValueError(
      'default_value has an incorrect type. '
      f'ParameterType has type {param_type.name}, '
      f'but default_value has type {type(default_value)}'
  )


#######################
# Experimental features
#######################
class FidelityMode(enum.Enum):
  """Decides how the fidelity config should be interpreated.

  SEQUENTIAL: A high fidelity measurement can be "warm-started" from a lower
    fidelity measurement. Currently, no algorithms can take advatange of it, and
    Vizier behaves exactly like NON_SEQUENTIAL case. This is for tracking
    purposes only.

  NOT_SEQUENTIAL: Each fidelity is separately measured. Example: Fidelity
    is the fraction of dataset to train on.

  STEPS: Fidelity determines the maximum value for Measurement.steps reported
    to Vizier. There is one-to-one correspondence between steps and fidelity.
    A high fideltiy Trial's measurements contain lower fidelity evaluations.
    When this is enabled, suggestion models do not use
    Trials' final_measurement. Instead, it reads the measurements whose
    "steps" exactly match one of the fidelities, and treats them as if they
    were separate Trials. Example: Fidelity is the number of total epochs
    to train on.
  """

  SEQUENTIAL = 'SEQUENTIAL'
  NOT_SEQUENTIAL = 'NOT_SEQUENTIAL'
  STEPS = 'STEPS'


@attr.define
class FidelityConfig:
  mode: FidelityMode = attr.field(converter=FidelityMode)
  cost_ratio: Sequence[float] = attr.field(
      converter=tuple, default=tuple(), kw_only=True
  )


########################
# Experimental features end here
########################


@attr.s(auto_attribs=True, frozen=False, init=True, slots=True, eq=True)
class ParameterConfig:
  """A Vizier ParameterConfig.

  Please use ParameterConfig.factory() to create an instance instead of calling
  the constructor directly.
  """

  _name: str = attr.ib(
      init=True, validator=attr.validators.instance_of(str), kw_only=True
  )
  _type: ParameterType = attr.ib(
      init=True,
      validator=attr.validators.instance_of(ParameterType),
      repr=lambda v: v.name if v is not None else 'None',
      kw_only=True,
  )
  # Only one of _feasible_values, _bounds will be set at any given time.
  _bounds: Optional[Union[Tuple[int, int], Tuple[float, float]]] = attr.ib(
      init=True,
      validator=attr.validators.optional(
          attr.validators.deep_iterable(
              member_validator=attr.validators.instance_of((int, float)),
              iterable_validator=attr.validators.instance_of(tuple),
          )
      ),
      kw_only=True,
  )
  _feasible_values: Optional[MonotypeParameterList] = attr.ib(
      init=True,
      validator=attr.validators.optional(
          attr.validators.deep_iterable(
              member_validator=attr.validators.instance_of((int, float, str)),
              iterable_validator=attr.validators.instance_of((list, tuple)),
          )
      ),
      kw_only=True,
  )
  _scale_type: Optional[ScaleType] = attr.ib(
      init=True,
      validator=attr.validators.optional(
          attr.validators.instance_of(ScaleType)
      ),
      repr=lambda v: v.name if v is not None else 'None',
      kw_only=True,
  )
  _default_value: Optional[Union[float, int, str]] = attr.ib(
      init=True,
      validator=attr.validators.optional(
          attr.validators.instance_of((float, int, str))
      ),
      kw_only=True,
  )
  _external_type: ExternalType = attr.ib(
      init=True,
      converter=lambda v: v or ExternalType.INTERNAL,
      validator=attr.validators.optional(
          attr.validators.instance_of(ExternalType)
      ),
      repr=lambda v: v.name if v is not None else 'None',
      kw_only=True,
  )

  # TODO: Make this a defaultdict and public.
  _children: dict[Union[float, int, str, bool], 'SearchSpace'] = attr.ib(
      init=True,
      factory=dict,
      # For equality checks, drop any empty search spaces.
      eq=lambda d: {k: v for k, v in d.items() if v.parameters},
      repr=lambda d: json.dumps(d, indent=2, default=repr),
  )

  # TODO: Deprecate this field.
  _matching_parent_values: MonotypeParameterSequence = attr.ib(
      init=True, default=tuple(), kw_only=True, eq=False
  )

  # Experimental feature.
  fidelity_config: Optional[FidelityConfig] = attr.ib(
      init=True,
      default=None,
      kw_only=True,
  )

  # Pytype treats instances of EnumTypeWrapper as types, but they can't be
  # evaluated at runtime, so a Union[] of proto enums has to be a forward
  # reference below.
  @classmethod
  def factory(
      cls,
      name: str,
      *,
      bounds: Optional[Union[Tuple[int, int], Tuple[float, float]]] = None,
      feasible_values: Optional[MonotypeParameterSequence] = None,
      children: Optional[
          Sequence[Tuple[MonotypeParameterSequence, 'ParameterConfig']]
      ] = None,
      fidelity_config: Optional[FidelityConfig] = None,
      scale_type: Optional[ScaleType] = None,
      default_value: Optional[Union[float, int, str]] = None,
      external_type: Optional[ExternalType] = ExternalType.INTERNAL,
  ) -> 'ParameterConfig':
    """Factory method.

    Args:
      name: The parameter's name. Cannot be empty.
      bounds: REQUIRED for INTEGER or DOUBLE type. Specifies (min, max). The
        type of (min, max) determines the created ParameterConfig's type.
      feasible_values: REQUIRED for DISCRETE or CATEGORICAL type. The elements'
        type determines the created ParameterConfig's type.
      children: sequence of tuples formatted as: (matching_parent_values,
        ParameterConfig). ONLY THE TYPES ARE VALIDATED. If the child
        ParameterConfig protos already have parent values set, they will be
        overridden by the provided matching_parent_values.
      fidelity_config: Fidelity config.  NOT VALIDATED.
      scale_type: Scaling to be applied. NOT VALIDATED.
      default_value: A default value for the Parameter.
      external_type: An annotation indicating the type this parameter should be
        cast to.

    Returns:
      A ParameterConfig object which wraps a partially validated proto.

    Raises:
      ValueError: Exactly one of feasible_values and bounds must be convertible
        to Boolean true. Bounds and numeric feasible_values must be finite.
        Bounds and feasible_values, if provided, must consist of
        elements of the same type.
      TypeError: If children's matching_parent_values are not compatible with
        the ParameterConfig being created.
    """
    if not name:
      raise ValueError('Parameter name cannot be empty.')

    if bool(feasible_values) and bool(bounds):
      raise ValueError(
          'While creating Parameter with name={}: one or none of '
          '"feasible_values" or "bounds" must be provided, but given '
          'feasible_values={} and bounds={}.'.format(
              name, feasible_values, bounds
          )
      )
    if feasible_values:
      if len(set(feasible_values)) != len(feasible_values):
        counter = collections.Counter(feasible_values)
        duplicate_dict = {k: v for k, v in counter.items() if v > 1}
        raise ValueError(
            'Feasible values cannot have duplicates: {}'.format(duplicate_dict)
        )
      if all(isinstance(v, (float, int)) for v in feasible_values):
        inferred_type = ParameterType.DISCRETE
        feasible_values, bounds = _get_feasible_points_and_bounds(
            feasible_values
        )
      elif all(isinstance(v, str) for v in feasible_values):
        inferred_type = ParameterType.CATEGORICAL
        feasible_values = _get_categories(feasible_values)
      else:
        raise ValueError(
            'Feasible values must all be numeric or strings. Given {}'.format(
                feasible_values
            )
        )
    elif bounds:  # bounds were specified.
      if isinstance(bounds[0], int) and isinstance(bounds[1], int):
        inferred_type = ParameterType.INTEGER
        _validate_bounds(bounds)
      elif isinstance(bounds[0], float) and isinstance(bounds[1], float):
        inferred_type = ParameterType.DOUBLE
        _validate_bounds(bounds)
      else:
        raise ValueError(
            'Bounds must both be integers or doubles. Given: {}'.format(bounds)
        )
    else:
      inferred_type = ParameterType.CUSTOM

    if default_value is not None:
      default_value = _get_default_value(inferred_type, default_value)

    pc = cls(
        name=name,
        type=inferred_type,
        bounds=bounds,
        feasible_values=feasible_values,
        scale_type=scale_type,
        default_value=default_value,
        fidelity_config=fidelity_config,
        external_type=external_type,
    )
    if children:
      pc = pc._add_children(children)
    return pc

  @property
  def name(self) -> str:
    return self._name

  @property
  def type(self) -> ParameterType:
    return self._type

  @property
  def external_type(self) -> ExternalType:
    return self._external_type

  @property
  def scale_type(self) -> Optional[ScaleType]:
    return self._scale_type

  @property
  def bounds(self) -> Union[Tuple[float, float], Tuple[int, int]]:
    """Returns the bounds, if set, or raises a ValueError."""
    if self.type == ParameterType.CATEGORICAL:
      raise ValueError(
          'Accessing bounds of a categorical parameter: %s' % self.name
      )
    if self._bounds is None:
      raise ValueError(f'Accessing bounds when not set for {self.name}')
    return self._bounds

  @property
  def _child_parameter_configs(self) -> Iterator['ParameterConfig']:
    for subspace in self._children.values():
      for param in subspace.parameters:
        yield param

  # TODO: TO BE DEPRECATED. If we want to continue supporting multiple
  # matching parent values, expose "def compact_subspaces(self)" that returns
  # Iterator[tuple[MonotypeValueSequence, ParameterConfig]]
  @property
  def matching_parent_values(self) -> MonotypeParameterList:
    """Returns the matching parent values, if this is a child parameter."""
    if not self._matching_parent_values:
      return []
    return list(self._matching_parent_values)

  # TODO: TO BE DEPRECATED. Replace with
  # def subspaces() -> Iterator[Value, 'SearchSpace'] which lets users
  # iterate over all search spaces.
  @property
  def child_parameter_configs(self) -> List['ParameterConfig']:
    return copy.deepcopy(list(self._child_parameter_configs))

  def subspaces(
      self,
  ) -> Iterable[Tuple[ParameterValueTypes, 'SearchSpace']]:
    return self._children.items()

  # TODO: TO BE DEPRECATED.
  def _del_child_parameter_configs(self):
    """Deletes the current child ParameterConfigs."""
    self._children.clear()

  # TODO: Equivalent code should look like:
  # copied = copy.deepcopy(config)
  # for feasible_value in copied.feasible_values():
  #   copied.subspace(feasible_value).clear()
  @property
  def clone_without_children(self) -> 'ParameterConfig':
    """Returns the clone of self, without child_parameter_configs."""
    clone = copy.deepcopy(self)
    clone._del_child_parameter_configs()  # pylint: disable='protected-access'
    return clone

  @property
  def feasible_values(self) -> Union[List[int], List[float], List[str]]:
    """Sorted feasible values, or a ValueError if config is continuous."""
    if self.type in (ParameterType.DISCRETE, ParameterType.CATEGORICAL):
      if not self._feasible_values:
        return []
      return copy.copy(self._feasible_values)
    elif self.type == ParameterType.INTEGER:
      return list(range(self.bounds[0], self.bounds[1] + 1))
    raise ValueError('feasible_values is invalid for type: %s' % self.type)

  @property
  def default_value(self) -> Optional[Union[int, float, str]]:
    """Returns the default value, or None if not set."""
    return self._default_value

  @property
  def deterministic_value(self) -> Optional[Union[int, float, str]]:
    """Returns the value if ParameterConfig only allows one value."""
    if self.type in [ParameterType.DOUBLE, ParameterType.INTEGER]:
      min_val, max_val = self.bounds
      if min_val == max_val:
        return trial.ParameterValue(min_val).cast_as_internal(self.type)
    else:
      feasible_values = self.feasible_values
      if len(feasible_values) == 1:
        return trial.ParameterValue(self.feasible_values[0]).cast_as_internal(
            self.type
        )
    return None

  # TODO: TO BE DEPRECATED. Used by factory() only.
  def _add_children(
      self,
      new_children: Sequence[
          Tuple[MonotypeParameterSequence, 'ParameterConfig']
      ],
  ) -> 'ParameterConfig':
    """Clones the ParameterConfig and adds new children to it.

    Args:
      new_children: A sequence of tuples formatted as: (matching_parent_values,
        ParameterConfig). If the child ParameterConfig have pre-existing parent
        values, they will be overridden.

    Returns:
      A parent parameter config, with children set.

    Raises:
      ValueError: If the child configs are invalid
      TypeError: If matching parent values are invalid
    """
    parent = copy.deepcopy(self)
    if not new_children:
      return parent

    for child_pair in new_children:
      if len(child_pair) != 2:
        raise ValueError(
            'Each element in new_children must be a tuple of '
            '(Sequence of valid parent values,  ParameterConfig),'
            ' given: {}'.format(child_pair)
        )

    logging.debug('_add_children: new_children=%s', new_children)
    for unsorted_parent_values, child in new_children:
      parent_values = sorted(unsorted_parent_values)
      for parent_value in parent_values:
        parent.subspace(parent_value).add(copy.deepcopy(child))
    return parent

  def continuify(self) -> 'ParameterConfig':
    """Returns a newly created DOUBLE parameter with the same range."""
    if self.type == ParameterType.DOUBLE:
      return copy.deepcopy(self)
    elif not self.type.is_numeric():
      raise ValueError(
          'Cannot convert a non-numeric parameter to DOUBLE: {}'.format(self)
      )
    elif list(self._child_parameter_configs):
      raise ValueError(
          'Cannot convert a parent parameter to DOUBLE: {}'.format(self)
      )

    scale_type = self.scale_type
    if scale_type == ScaleType.UNIFORM_DISCRETE:
      logging.log_every_n(
          logging.WARNING,
          (
              'Converting a UNIFORM_DISCRETE scaled discrete parameter '
              'to DOUBLE: %s'
          ),
          10,
          self,
      )
      scale_type = None

    default_value = self.default_value
    if default_value is not None:
      default_value = float(default_value)
    return ParameterConfig.factory(
        self.name,
        bounds=(float(self.bounds[0]), float(self.bounds[1])),
        scale_type=scale_type,
        default_value=default_value,
    )

  @classmethod
  def merge(
      cls, one: 'ParameterConfig', other: 'ParameterConfig'
  ) -> 'ParameterConfig':
    """Merge two ParameterConfigs.

    Args:
      one: ParameterConfig with no child parameters.
      other: Must have the same type as one, and may not have child parameters.

    Returns:
      For Categorical, Discrete or Integer ParameterConfigs, the resulting
      config will be the union of all feasible values.
      For Double ParameterConfigs, the resulting config will have [min_value,
      max_value] set to the smallest and largest bounds.

    Raises:
      ValueError: If any of the input configs has child parameters, or if
        the two parameters have different types.
    """
    if one.child_parameter_configs or other.child_parameter_configs:
      raise ValueError(
          'Cannot merge parameters with child_parameter_configs: %s and %s'
          % one,
          other,
      )
    if one.type != other.type:
      raise ValueError(
          'Type conflicts between {} and {}'.format(
              one.type.name, other.type.name
          )
      )
    if one.scale_type != other.scale_type:
      logging.warning(
          'Scale type conflicts while merging %s and %s', one, other
      )

    if one.type in (ParameterType.CATEGORICAL, ParameterType.DISCRETE):
      new_feasible_values = list(
          set(one.feasible_values + other.feasible_values)
      )
      return ParameterConfig.factory(
          name=one.name,
          feasible_values=new_feasible_values,
          scale_type=one.scale_type,
      )
    elif one.type in (ParameterType.INTEGER, ParameterType.DOUBLE):
      original_min, original_max = one.bounds
      other_min, other_max = other.bounds
      new_bounds = (min(original_min, other_min), max(original_max, other_max))
      return ParameterConfig.factory(
          name=one.name, bounds=new_bounds, scale_type=one.scale_type
      )
    raise ValueError(
        'Unknown type {}. This is currentlyan unreachable code.'.format(
            one.type
        )
    )

  def traverse(
      self, show_children: bool = False
  ) -> Generator['ParameterConfig', None, None]:
    """DFS Generator for parameter configs.

    Args:
      show_children: If True, every generated ParameterConfig has
        child_parameter_configs. For example, if 'foo' has two child configs
        'bar1' and 'bar2', then traversing 'foo' with show_children=True
        generates (foo, with bar1,bar2 as children), (bar1), and (bar2). If
        show_children=False, it generates (foo, without children), (bar1), and
        (bar2).

    Yields:
      DFS on all parameter configs.
    """
    if show_children:
      yield self
    else:
      yield self.clone_without_children
    for child in self.child_parameter_configs:
      yield from child.traverse(show_children)

  # TODO: Rename to `validate_value or is_feasible`
  def contains(
      self, value: Union[trial.ParameterValueTypes, trial.ParameterValue]
  ) -> bool:
    """Check if the `value` is a valid value for this parameter config."""
    if isinstance(value, trial.ParameterValue):
      # TODO: Extract the raw value.
      value = value.value
    try:
      self._assert_feasible(value)
    except (TypeError, ValueError):
      return False
    return True

  @property
  def num_feasible_values(self) -> Union[float, int]:
    if self.type == ParameterType.DOUBLE:
      return float('inf')
    elif self.type == ParameterType.INTEGER:
      return self.bounds[1] - self.bounds[0] + 1
    elif self.type == ParameterType.CUSTOM:
      return float('inf')
    else:
      return len(self.feasible_values)

  def _assert_bounds(self, value: trial.ParameterValueTypes) -> None:
    if not self.bounds[0] <= value <= self.bounds[1]:
      raise ValueError(
          f'Parameter {self.name} has bounds: {self.bounds}. Given: {value}'
      )

  def _assert_in_feasible_values(
      self, value: trial.ParameterValueTypes
  ) -> None:
    if value not in self._feasible_values:
      raise ValueError(
          f'Parameter {self.name} has feasible values: '
          f'{self.feasible_values}. '
          f'Given: {value}'
      )

  def _assert_feasible(self, value: trial.ParameterValueTypes) -> None:
    """Asserts that the value is feasible for this parameter config.

    Args:
      value:

    Raises:
      TypeError: Value does not match the config's type
      ValueError: Value is not feasible.
      RuntimeError: Other errors.
    """
    try:
      self.type.assert_correct_type(value)
    except TypeError as e:
      raise TypeError(
          f'Parameter {self.name} is not compatible with value: {value}'
      ) from e

    # TODO: We should be able to directly use "value" without
    # casting to the internal type.
    value = trial.ParameterValue(value)
    if self.type == ParameterType.DOUBLE:
      self._assert_bounds(value.as_float)
    elif self.type == ParameterType.INTEGER:
      self._assert_bounds(value.as_int)
    elif self.type == ParameterType.DISCRETE:
      self._assert_in_feasible_values(value.as_float)
    elif self.type == ParameterType.CATEGORICAL:
      self._assert_in_feasible_values(value.as_str)
    else:
      raise RuntimeError(
          f'Parameter {self.name} has unknown parameter type: {self.type}'
      )

  def get_subspace_deepcopy(self, value: ParameterValueTypes) -> 'SearchSpace':
    """Get a deep copy of the subspace.

    Validates the feasibility of value.

    Args:
      value: Must be a feasible value per this parameter config.

    Returns:
      Subspace conditioned on the value. Note that an empty search space is
      returned if the parameter config is continuous and thus cannot have
      a subspace.
    """
    if not math.isfinite(self.num_feasible_values):
      return SearchSpace()
    value = trial.ParameterValue(value).cast_as_internal(self.type)
    self._assert_feasible(value)
    return copy.deepcopy(self._children.get(value, SearchSpace()))

  def subspace(self, value: ParameterValueTypes) -> 'SearchSpace':
    """Selects the subspace for a specified parent value."""
    if not math.isfinite(self.num_feasible_values):
      raise TypeError('DOUBLE type cannot have child parameters')

    # TODO: We should be able to directly use "value".
    value = trial.ParameterValue(value).cast_as_internal(self.type)
    self._assert_feasible(value)
    if value not in self._children:
      self._children[value] = SearchSpace(parent_values=[value])
    return self._children[value]


@attr.define(init=False)
class ParameterConfigSelector(Iterable[ParameterConfig], Sized):
  """Holds a reference to ParameterConfigs."""

  # Selected configs.
  _selected: tuple[ParameterConfig] = attr.field(init=True, converter=tuple)

  def __iter__(self) -> Iterator[ParameterConfig]:
    return iter(self._selected)

  def __len__(self) -> int:
    return len(self._selected)

  def __init__(
      self, selected: Union[ParameterConfig, Iterable[ParameterConfig]], /
  ):
    if isinstance(selected, ParameterConfig):
      self.__attrs_init__(tuple([selected]))
    else:
      self.__attrs_init__(tuple(selected))

  def select_values(
      self, values: MonotypeParameterSequence
  ) -> 'SearchSpaceSelector':
    """Select values."""
    values = tuple(values)

    for value in values:
      for config in self._selected:
        if not config.contains(value):
          # Validate first so we don't create a lot of unnecessary empty
          # search space upon failure.
          raise ValueError(f'{value} is not feasible in {self}')

    spaces = []
    for value in values:
      for config in self._selected:
        spaces.append(config.subspace(value))
    return SearchSpaceSelector(spaces)

  def merge(self) -> 'ParameterConfigSelector':
    """Merge by taking the union of the parameter configs with the same name.

    Returns:
      The returned ParameterConfigSelector does not contain parameters with
      duplicate names. Their feasible set (either as a range or discrete set) is
      the union of all feasible sets under the same parameter name.
    """
    merged_configs = {}
    for parameter_config in self:
      name = parameter_config.name  # Alias
      existing_config = merged_configs.setdefault(name, parameter_config)
      merged_configs[name] = ParameterConfig.merge(
          existing_config, parameter_config
      )
    return ParameterConfigSelector(merged_configs.values())


class InvalidParameterError(ValueError):
  """Error thrown when parameter values are invalid."""


################### Main Classes ###################


@attr.define(init=False)
class SearchSpaceSelector:
  """Holds a reference to (sub) spaces."""

  # Selected (sub)-spaces.
  # TODO: Consider switching the order of SearchSpaceSelector and
  # SearchSpace.
  _selected: tuple['SearchSpace'] = attr.field(init=True)

  def __len__(self) -> int:
    return len(self._selected)

  def __init__(
      self, selected: Union['SearchSpace', Iterable['SearchSpace']], /
  ):
    if isinstance(selected, SearchSpace):
      self.__attrs_init__(tuple([selected]))
    else:
      self.__attrs_init__(tuple(selected))

  def add_float_param(
      self,
      name: str,
      min_value: float,
      max_value: float,
      *,
      default_value: Optional[float] = None,
      scale_type: Optional[ScaleType] = None,
      index: Optional[int] = None,
  ) -> 'ParameterConfigSelector':
    """Adds floating point parameter config(s) to the selected search space(s).

    Args:
      name: The parameter's name. Cannot be empty.
      min_value: Inclusive lower bound for the parameter.
      max_value: Inclusive upper bound for the parameter.
      default_value: A default value for the Parameter.
      scale_type: Scaling to be applied. NOT VALIDATED.
      index: Specifies the multi-dimensional index for this parameter. E.g. if
        name='rate' and index=0, then a single ParameterConfig with name
        'rate[0]' is added. `index` should be >= 0.

    Returns:
      SearchSpaceSelector(s) for the newly added parameter(s):
      One SearchSpaceSelector if one parameter was added, or a list of
      SearchSpaceSelector if multiple parameters were added.

    Raises:
      ValueError: If `index` is invalid (e.g. negative).
    """
    if scale_type is None:
      scale_type = ScaleType.LINEAR
    bounds = (float(min_value), float(max_value))
    param_names = self._get_parameter_names_to_create(name=name, index=index)

    new_params = []
    for param_name in param_names:
      new_pc = ParameterConfig.factory(
          name=param_name,
          bounds=bounds,
          scale_type=scale_type,
          default_value=default_value,
      )
      new_params.append(new_pc)
    return self._add_parameters(new_params)

  def add_int_param(
      self,
      name: str,
      min_value: int,
      max_value: int,
      *,
      default_value: Optional[int] = None,
      scale_type: Optional[ScaleType] = None,
      index: Optional[int] = None,
      experimental_fidelity_config: Optional[FidelityConfig] = None,
  ) -> 'ParameterConfigSelector':
    """Adds integer parameter config(s) to the selected search space(s).

    Args:
      name: The parameter's name. Cannot be empty.
      min_value: Inclusive lower bound for the parameter.
      max_value: Inclusive upper bound for the parameter.
      default_value: A default value for the Parameter.
      scale_type: Scaling to be applied. NOT VALIDATED.
      index: Specifies the multi-dimensional index for this parameter. E.g. if
        name='hidden_units' and index=0, then a single ParameterConfig with name
        'hidden_units[0]' is added. `index` should be >= 0.
      experimental_fidelity_config: EXPERIMENTAL. See FidelityConfig doc.

    Returns:
      ParameterConfigSelector for the newly added parameter(s).

    Raises:
      ValueError: If min_value or max_value are not integers.
      ValueError: If `index` is invalid (e.g. negative).
    """
    int_min_value = int(min_value)
    if not math.isclose(min_value, int_min_value):
      raise ValueError(
          'min_value for an INTEGER parameter should be an integer'
          ', got: [{}]'.format(min_value)
      )
    int_max_value = int(max_value)
    if not math.isclose(max_value, int_max_value):
      raise ValueError(
          'max_value for an INTEGER parameter should be an integer'
          ', got: [{}]'.format(min_value)
      )
    bounds = (int_min_value, int_max_value)

    param_names = self._get_parameter_names_to_create(name=name, index=index)

    new_params = []
    for param_name in param_names:
      new_pc = ParameterConfig.factory(
          name=param_name,
          bounds=bounds,
          scale_type=scale_type,
          fidelity_config=experimental_fidelity_config,
          default_value=default_value,
      )
      new_params.append(new_pc)
    return self._add_parameters(new_params)

  def add_discrete_param(
      self,
      name: str,
      feasible_values: Union[Sequence[float], Sequence[int]],
      *,
      default_value: Optional[Union[float, int]] = None,
      scale_type: Optional[ScaleType] = ScaleType.LINEAR,
      index: Optional[int] = None,
      auto_cast: Optional[bool] = True,
      experimental_fidelity_config: Optional[FidelityConfig] = None,
  ) -> 'ParameterConfigSelector':
    """Adds ordered numeric parameter config(s) with a finite set of values.

    IMPORTANT: If a parameter is discrete, its values are assumed to have
    ordered semantics. Thus, you should not use discrete parameters for
    unordered values such as ids. In this case, see add_categorical_param()
    below.

    Args:
      name: The parameter's name. Cannot be empty.
      feasible_values: The set of feasible values for this parameter.
      default_value: A default value for the Parameter.
      scale_type: Scaling to be applied. NOT VALIDATED.
      index: Specifies the multi-dimensional index for this parameter. E.g. if
        name='batch_size' and index=0, then a single ParameterConfig with name
        'batch_size[0]' is added. `index` should be >= 0.
      auto_cast: If True, the external type will be set to INTEGER if all values
        are castable to an integer without losing precision. If False, the
        external type will be set to float.
      experimental_fidelity_config: EXPERIMENTAL. See FidelityConfig doc.

    Returns:
      ParameterConfigSelector for the newly added parameter(s).

    Raises:
      ValueError: If `index` is invalid (e.g. negative).
    """
    param_names = self._get_parameter_names_to_create(name=name, index=index)

    external_type = ExternalType.FLOAT
    if auto_cast:
      # If all feasible values are convertible to ints without loss of
      # precision, annotate the external type as INTEGER. This will cast
      # [0., 1., 2.] into [0, 1, 2] when parameter values are returned in
      # clients.
      if all([v == round(v) for v in feasible_values]):
        external_type = ExternalType.INTEGER

    new_params = []
    for param_name in param_names:
      new_pc = ParameterConfig.factory(
          name=param_name,
          feasible_values=sorted(feasible_values),
          scale_type=scale_type,
          fidelity_config=experimental_fidelity_config,
          default_value=default_value,
          external_type=external_type,
      )
      new_params.append(new_pc)
    return self._add_parameters(new_params)

  def add_categorical_param(
      self,
      name: str,
      feasible_values: Sequence[str],
      *,
      default_value: Optional[str] = None,
      scale_type: Optional[ScaleType] = None,
      index: Optional[int] = None,
  ) -> 'ParameterConfigSelector':
    """Adds string-valued parameter config(s) to the selected search space(s).

    IMPORTANT: If a parameter is categorical, its values are assumed to be
    unordered. If the `feasible_values` have ordering, use add_discrete_param()
    above, since it will improve Vizier's model quality.

    Args:
      name: The parameter's name. Cannot be empty.
      feasible_values: The set of feasible values for this parameter.
      default_value: A default value for the Parameter.
      scale_type: Scaling to be applied. NOT VALIDATED.
      index: Specifies the multi-dimensional index for this parameter. E.g. if
        name='id' and index=0, then a single ParameterConfig with name 'id[0]'
        is added. `index` should be >= 0.

    Returns:
      ParameterConfigSelector for the newly added parameter(s).

    Raises:
      ValueError: If `index` is invalid (e.g. negative), or `feasible_values`
        are invalid (not strings).
    """
    for value in feasible_values:
      if not isinstance(value, str):
        raise ValueError(f'feasible_values must be strings; got: {value}')

    param_names = self._get_parameter_names_to_create(name=name, index=index)

    new_params = []
    for param_name in param_names:
      new_pc = ParameterConfig.factory(
          name=param_name,
          feasible_values=sorted(feasible_values),
          scale_type=scale_type,
          default_value=default_value,
      )
      new_params.append(new_pc)
    return self._add_parameters(new_params)

  def add_custom_param(
      self,
      name: str,
      *,
      default_value: Optional[ParameterValueTypes] = None,
  ) -> 'ParameterConfigSelector':
    """Adds custom parameter config(s) to the selected search space(s).

    Args:
      name: The parameter's name. Cannot be empty.
      default_value: A default value for the Parameter. Generally should be set.

    Returns:
      ParameterConfigSelector for the newly added parameter(s).

    Raises:
      ValueError: If `index` is invalid (e.g. negative)
    """
    new_pc = ParameterConfig.factory(
        name=name,
        default_value=default_value,
    )
    return self._add_parameters([new_pc])

  def add_bool_param(
      self,
      name: str,
      feasible_values: Optional[Sequence[bool]] = None,
      *,
      default_value: Optional[bool] = None,
      scale_type: Optional[ScaleType] = None,
      index: Optional[int] = None,
  ) -> 'ParameterConfigSelector':
    """Adds boolean-valued parameter config(s) to the selected search space(s).

    Args:
      name: The parameter's name. Cannot be empty.
      feasible_values: An optional list of feasible boolean values, i.e. one of
        the following: [True], [False], [True, False], [False, True].
      default_value: A default value for the Parameter.
      scale_type: Scaling to be applied. NOT VALIDATED.
      index: Specifies the multi-dimensional index for this parameter. E.g. if
        name='match' and index=0, then a single ParameterConfig with name
        'match[0]' is added. `index` should be >= 0.

    Returns:
      ParameterConfigSelector for the newly added parameter(s).

    Raises:
      ValueError: If `feasible_values` has invalid values.
      ValueError: If `index` is invalid (e.g. negative).
    """
    allowed_values = (None, (True, False), (False, True), (True,), (False,))
    if (
        feasible_values is not None
        and tuple(feasible_values) not in allowed_values
    ):
      raise ValueError(
          'feasible_values must be one of %s; got: %s.'
          % (allowed_values, feasible_values)
      )
    # Boolean parameters are represented as categorical parameters internally.
    bool_to_string = lambda x: trial.TRUE_VALUE if x else trial.FALSE_VALUE
    if feasible_values is None:
      categories = (trial.TRUE_VALUE, trial.FALSE_VALUE)
    else:
      categories = [bool_to_string(x) for x in feasible_values]
    feasible_values = sorted(categories, reverse=True)

    if default_value is not None:
      default_value = bool_to_string(default_value)

    param_names = self._get_parameter_names_to_create(name=name, index=index)

    new_params = []
    for param_name in param_names:
      new_pc = ParameterConfig.factory(
          name=param_name,
          feasible_values=sorted(feasible_values),
          scale_type=scale_type,
          default_value=default_value,
          external_type=ExternalType.BOOLEAN,
      )
      new_params.append(new_pc)
    return self._add_parameters(new_params)

  @overload
  def select(
      self,
      parameter_name: str,
      parameter_values: None,
  ) -> ParameterConfigSelector:
    ...

  @overload
  def select(
      self, parameter_name: str, parameter_values: MonotypeParameterSequence
  ) -> 'SearchSpaceSelector':
    ...

  def select(
      self,
      parameter_name,
      parameter_values: Optional[MonotypeParameterSequence] = None,
  ):
    """Selects a parameter config or its subspace.

    This method is for constructing a _conditional_ search space.

    EXAMPLE: Suppose we have a selector to the root of the search space with one
    categorical parameter.
    root = pyvizier.SearchSpace().root
    root.add_categorical_param('model_type', ['dnn', 'linear'])

    1) Select a `ParameterConfig`:
      model = root.select('model_type')

    2) Select a subspace conditioned on `model_type == 'dnn'` and add
    a child parameter `hidden_units`:
      dnn_subspace = root.select('model_type', ['dnn'])
      dnn_subspace.add_int_param('hidden_layers', ...)

    or equivalently,
      dnn_subspace = root.select('model_type').select_values(['dnn'])
      dnn_subspace.add_int_param('hidden_layers', ...)

    3) Traverse your search space by chaining select() calls:
      root.select('model_type', ['dnn']).select('hidden_layers', [1, 2])

    4) Select more than one search space simultaneously:
      selected = root.select('model_type', ['dnn', 'linear'])
        .add_categorical_param('optimizer', ['adam', 'adagrad'])
      assert len(selected) == 4  # {dnn, linear} x {adam, adagard}

    Args:
      parameter_name:
      parameter_values: Optional parameter values for this selector, which will
        be used to add child parameters, or traverse a conditional tree.

    Returns:
      ParameterConfigSelector for `ParameterConfig`(s) if the values are not
        specified.
      SearchSpaceSelector for subspace(s) if parameter_values are specified.
    """
    if parameter_values is None:
      selected_configs = []
      for space in self._selected:
        selected_configs.append(space.get(parameter_name))
      return ParameterConfigSelector(selected_configs)
    else:
      selected_spaces = []
      for space in self._selected:
        selected_parameter = space.get(parameter_name)
        for value in parameter_values:
          selected_spaces.append(selected_parameter.subspace(value))
      return SearchSpaceSelector(selected_spaces)

  @classmethod
  def _get_parameter_names_to_create(
      cls,
      *,
      name: str,
      length: Optional[int] = None,
      index: Optional[int] = None,
  ) -> List[str]:
    """Returns the names of all parameters which should be created.

    Args:
      name: The base parameter name.
      length: Specifies the length of a multi-dimensional parameters. If larger
        than 1, then multiple ParameterConfigs are added. E.g. if name='rate'
        and length=2, then two ParameterConfigs with names 'rate[0]', 'rate[1]'
        are added. Cannot be specified together with `index`.
      index: Specifies the multi-dimensional index for this parameter. Cannot be
        specified together with `length`. E.g. if name='rate' and index=1, then
        a single ParameterConfig with name 'rate[1]' is added.

    Returns:
      List of parameter names to create.

    Raises:
      ValueError: If `length` or `index` are invalid.
    """
    if length is not None and index is not None:
      raise ValueError(
          'Only one of `length` and `index` can be specified. Got'
          ' length={}, index={}'.format(length, index)
      )
    if length is not None and length < 1:
      raise ValueError('length must be >= 1. Got length={}'.format(length))
    if index is not None and index < 0:
      raise ValueError('index must be >= 0. Got index={}'.format(index))

    param_names = []
    if length is None and index is None:
      # Add one parameter with no multi-dimensional index.
      param_names.append(name)
    elif index is not None:
      # Add one parameter with a multi-dimensional index.
      param_names.append(cls._multi_dimensional_parameter_name(name, index))
    elif length is not None:
      # `length > 0' is synthatic sugar for multi multi-dimensional parameter.
      # Each multi-dimensional parameter is encoded as a list of separate
      # parameters with names equal to `name[index]` (index is zero based).
      for i in range(length):
        param_names.append(cls._multi_dimensional_parameter_name(name, i))
    return param_names

  @classmethod
  def _multi_dimensional_parameter_name(cls, name: str, index: int) -> str:
    """Returns the indexed parameter name."""
    return '{}[{}]'.format(name, index)

  @classmethod
  def parse_multi_dimensional_parameter_name(
      cls, name: str
  ) -> Optional[Tuple[str, int]]:
    """Returns the base name for a multi-dimensional parameter name.

    Args:
      name: A parameter name.

    Returns:
      (base_name, index): if name='hidden_units[10]', base_name='hidden_units'
        and index=10.
      Returns None if name is not in the format 'base_name[idx]'.
    """
    regex = r'(?P<name>[^()]*)\[(?P<index>\d+)\]$'
    pattern = re.compile(regex)
    matches = pattern.match(name)
    if matches is None:
      return None
    return (matches.groupdict()['name'], int(matches.groupdict()['index']))

  # TODO: Add def extend(space: SearchSpace)
  def _add_parameters(
      self, parameters: Iterable[ParameterConfig]
  ) -> ParameterConfigSelector:
    """Adds deepcopy of the ParameterConfigs.

    Args:
      parameters: The parameters to add to the search space.

    Returns:
      A list of SearchSpaceSelectors, one for each parameters added.
    """
    parameters = list(parameters)
    logging.info(
        'Adding child parameters %s to %s subspaces ',
        set(p.name for p in parameters),
        len(self._selected),
    )
    added = []
    for parameter in parameters:
      for selected in self._selected:
        # Adds a deepcopy so that every ParameterConfig object is unique.
        added.append(selected.add(copy.deepcopy(parameter)))

    return ParameterConfigSelector(added)

  def select_all(self) -> ParameterConfigSelector:
    """Select all parameters at all levels."""
    all_parameter_configs = []
    for space in self._selected:
      for top_level_config in space.parameters:
        all_parameter_configs.extend(list(top_level_config.traverse()))

    return ParameterConfigSelector(all_parameter_configs)


@attr.define(frozen=False, init=True, slots=True, kw_only=True)
class SearchSpace:
  """[Cross-platform] Collection of ParameterConfigs.

  Vizier search space can be *conditional*.
  Parameter names are guaranteed to be unique in any subspace.

  Attribute:
    _parameter_configs: Maps parameter names to configs.
  """

  _parameter_configs: dict[str, ParameterConfig] = attr.field(
      init=False, factory=dict
  )

  # TODO: To be deprecated.
  _parent_values: MonotypeParameterSequence = attr.field(
      default=tuple(), converter=tuple, kw_only=True
  )

  @property
  def parameter_names(self) -> AbstractSet[str]:
    return self._parameter_configs.keys()

  def get(self, name: str) -> ParameterConfig:
    if name not in self._parameter_configs:
      raise KeyError(f'{name} is not in the search space.')
    return self._parameter_configs[name]

  def pop(self, name: str) -> ParameterConfig:
    return self._parameter_configs.pop(name)

  def add(
      self, parameter_config: ParameterConfig, *, replace: bool = False
  ) -> ParameterConfig:
    """Adds the ParameterConfig.

    For advanced users only. Takes a reference to Parameterconfig.
    Future edits will change the search space.

    Args:
      parameter_config:
      replace: Determines the behavior when there already exists a
        ParameterConfig with the same name. If set to True, replaces it. If set
        to False, raises ValueError.

    Returns:
      Reference to the ParameterConfig that was added to the search space.
    """
    name = parameter_config.name
    parameter_config._matching_parent_values = tuple(self._parent_values)  # pylint: disable=protected-access
    if (name in self._parameter_configs) and (not replace):
      raise ValueError(
          f'Duplicate name: {parameter_config.name} already exists.\n'
          f'Existing config: {parameter_config}\n'
          f'New config:{parameter_config}'
      )

    self._parameter_configs[name] = parameter_config
    return parameter_config

  # TODO: Change the return type to Iterator.
  @property
  def parameters(self) -> list[ParameterConfig]:
    """Returns the parameter configs in this search space."""
    return list(self._parameter_configs.values())

  def select_root(self) -> SearchSpaceSelector:
    # Deprecated function.
    # TODO: Remove this from downstream user code.
    return SearchSpaceSelector(self)

  @property
  def root(self) -> SearchSpaceSelector:
    """Returns a selector for the root of the search space.

    Parameters can be added to the search space using the returned
    SearchSpaceSelector.
    """
    return SearchSpaceSelector(self)

  @property
  def is_conditional(self) -> bool:
    """Returns True if search_space contains any conditional parameters."""
    return any([p.child_parameter_configs for p in self.parameters])

  def contains(self, parameters: trial.ParameterDict) -> bool:
    try:
      self.assert_contains(parameters)
      return True
    except InvalidParameterError:
      return False

  def assert_contains(self, parameters: trial.ParameterDict) -> bool:
    """Throws an error if parameters is not a valid point in the space.

    Args:
      parameters:

    Returns:
      Always returns True unless an exception is Raised.

    Raises:
      InvalidParameterError: If parameters are invalid.
      NotImplementedError: If parameter type is unknown
    """
    if self.is_conditional:
      raise NotImplementedError('Not implemented for conditional space.')
    if len(parameters) != len(self._parameter_configs.values()):
      set1 = set(pc.name for pc in self._parameter_configs.values())
      set2 = set(parameters)
      raise InvalidParameterError(
          f'Search space has {len(self._parameter_configs.values())} parameters'
          f' but only {len(parameters)} were given. Missing in search space:'
          f' {set2 - set1}. Missing in parameters: {set1 - set2}.'
      )
    for pc in self._parameter_configs.values():
      if pc.name not in parameters:
        raise InvalidParameterError(f'{pc.name} is missing in {parameters}.')
      elif not pc.contains(parameters[pc.name]):
        raise InvalidParameterError(
            f'{parameters[pc.name]} is not feasible in {pc}'
        )
    return True

  def num_parameters(self, param_type: Optional[ParameterType] = None) -> int:
    """Counts number of parameters with the param_type (if given)."""
    if param_type is None:
      return len(self.parameters)
    return [pc.type for pc in self.parameters].count(param_type)


--- vizier/_src/pyvizier/shared/parameter_config_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.shared.parameter_config."""

from typing import Any

from absl import logging
from vizier._src.pyvizier.shared import parameter_config as pc
from vizier._src.pyvizier.shared import trial
from vizier.testing import test_studies

from absl.testing import absltest
from absl.testing import parameterized


class ParameterConfigFactoryTest(parameterized.TestCase):

  def testCreatesDoubleConfig(self):
    parameter_config = pc.ParameterConfig.factory(
        'name',
        bounds=(-1.0, 1.0),
        scale_type=pc.ScaleType.LINEAR,
        default_value=0.1)
    self.assertEqual(parameter_config.name, 'name')
    self.assertEqual(parameter_config.type, pc.ParameterType.DOUBLE)
    self.assertEqual(parameter_config.bounds, (-1, 1))
    self.assertEqual(parameter_config.scale_type, pc.ScaleType.LINEAR)
    self.assertEqual(parameter_config.default_value, 0.1)
    self.assertIsInstance(parameter_config.default_value, float)
    with self.assertRaises(ValueError):
      _ = parameter_config.feasible_values

    self.assertEqual(parameter_config.continuify(), parameter_config)

  def testCreatesIntegerConfig(self):
    parameter_config = pc.ParameterConfig.factory(
        'name', bounds=(1, 3), scale_type=pc.ScaleType.LOG, default_value=1)
    self.assertEqual(parameter_config.name, 'name')
    self.assertEqual(parameter_config.type, pc.ParameterType.INTEGER)
    self.assertEqual(parameter_config.feasible_values, [1, 2, 3])
    self.assertEqual(parameter_config.bounds, (1, 3))
    self.assertEqual(parameter_config.scale_type, pc.ScaleType.LOG)
    self.assertEqual(parameter_config.default_value, 1)
    self.assertIsInstance(parameter_config.default_value, int)

    self.assertEqual(
        parameter_config.continuify(),
        pc.ParameterConfig.factory(
            'name',
            bounds=(1.0, 3.0),
            scale_type=pc.ScaleType.LOG,
            default_value=1.0))

  def testCreatesDiscreteConfig(self):
    feasible_values = (-1, 3, 2)
    parameter_config = pc.ParameterConfig.factory(
        'name',
        feasible_values=feasible_values,
        scale_type=pc.ScaleType.UNIFORM_DISCRETE,
        default_value=2,
        external_type=pc.ExternalType.INTEGER)
    self.assertEqual(parameter_config.name, 'name')
    self.assertEqual(parameter_config.type, pc.ParameterType.DISCRETE)
    self.assertEqual(parameter_config.feasible_values, [-1, 2, 3])
    self.assertEqual(parameter_config.bounds, (-1, 3))
    self.assertEqual(parameter_config.scale_type, pc.ScaleType.UNIFORM_DISCRETE)
    self.assertEqual(parameter_config.default_value, 2)
    self.assertIsInstance(parameter_config.default_value, float)
    self.assertEqual(parameter_config.external_type, pc.ExternalType.INTEGER)

    self.assertEqual(
        parameter_config.continuify(),
        pc.ParameterConfig.factory(
            'name', bounds=(-1.0, 3.0), default_value=2.0))

  def testCreatesCategoricalConfig(self):
    feasible_values = ('b', 'a', 'c')
    parameter_config = pc.ParameterConfig.factory(
        'name', feasible_values=feasible_values, default_value='c')
    self.assertEqual(parameter_config.name, 'name')
    self.assertEqual(parameter_config.feasible_values, ['a', 'b', 'c'])
    self.assertEqual(parameter_config.default_value, 'c')
    with self.assertRaises(ValueError):
      _ = parameter_config.bounds

  def testCreatesDoubleConfigIntDefault(self):
    parameter_config = pc.ParameterConfig.factory(
        'name',
        bounds=(-1.0, 1.0),
        scale_type=pc.ScaleType.LINEAR,
        default_value=1)
    self.assertEqual(parameter_config.default_value, 1.0)
    self.assertIsInstance(parameter_config.default_value, float)

  def testCreatesDiscreteConfigDoubleDefault(self):
    feasible_values = (-1, 3, 2)
    parameter_config = pc.ParameterConfig.factory(
        'name',
        feasible_values=feasible_values,
        scale_type=pc.ScaleType.UNIFORM_DISCRETE,
        default_value=2.0)
    self.assertEqual(parameter_config.default_value, 2.0)
    self.assertIsInstance(parameter_config.default_value, float)

  def testCreatesIntegerConfigDoubleDefault(self):
    parameter_config = pc.ParameterConfig.factory(
        'name', bounds=(1, 3), scale_type=pc.ScaleType.LOG, default_value=2.0)
    self.assertEqual(parameter_config.default_value, 2.0)
    self.assertIsInstance(parameter_config.default_value, int)

  def testCreatesIntegerConfigInvalidDoubleDefault(self):
    with self.assertRaisesRegex(ValueError, 'default_value for an.*'):
      pc.ParameterConfig.factory(
          'name',
          bounds=(1, 3),
          scale_type=pc.ScaleType.LOG,
          default_value=2.0001)

  def testCreatesCategoricalConfigNoDefault(self):
    feasible_values = ('b', 'a', 'c')
    parameter_config = pc.ParameterConfig.factory(
        'name', feasible_values=feasible_values)
    self.assertIsNone(parameter_config.default_value)

  def testCreatesCategoricalConfigBadDefault(self):
    feasible_values = ('b', 'a', 'c')
    with self.assertRaisesRegex(ValueError,
                                'default_value has an incorrect type.*'):
      pc.ParameterConfig.factory(
          'name', feasible_values=feasible_values, default_value=0.1)

  def testRaisesErrorWhenNameIsEmpty(self):
    with self.assertRaises(ValueError):
      _ = pc.ParameterConfig.factory('', bounds=(-1.0, 1.0))

  def testRaisesErrorWhenOverSpecified(self):
    with self.assertRaises(ValueError):
      _ = pc.ParameterConfig.factory(
          'name', bounds=(-1.0, 1.0), feasible_values=['a', 'b', 'c'])

  @parameterized.named_parameters(
      ('HaveInfinity', (-float('inf'), 1)), ('HaveNan', (1, float('nan'))),
      ('HaveMixedTypes', (1, float(1))), ('AreWronglyOrdered', (1, -1)))
  def testRaisesErrorWhenBounds(self, bounds):
    with self.assertRaises(ValueError):
      _ = pc.ParameterConfig.factory('name', bounds=bounds)

  @parameterized.named_parameters(('HaveDuplicateCategories', ['a', 'a', 'b']),
                                  ('HaveDuplicateNumbers', [1.0, 2.0, 2.0]),
                                  ('HaveMixedTypes', ['a', 1, 2]))
  def testRaisesErrorWhenFeasibleValues(self, feasible_values):
    with self.assertRaises(ValueError):
      _ = pc.ParameterConfig.factory('name', feasible_values=feasible_values)


_child1 = pc.ParameterConfig.factory('double_child', bounds=(0.0, 1.0))
_child2 = pc.ParameterConfig.factory('integer_child', bounds=(0, 1))


class ParameterConfigFactoryTestWithChildren(parameterized.TestCase):

  @parameterized.named_parameters(
      ('IntParentValues', [([0], _child1), ([0, 1], _child2)]),
      ('FloatParentValues', [([0.0], _child1), ([0.0, 1.0], _child2)]))
  def testIntegerWithValid(self, children):
    p = pc.ParameterConfig.factory('parent', bounds=(0, 1), children=children)
    self.assertCountEqual(p.subspace(0.0).parameters, [_child1, _child2])
    self.assertCountEqual(p.subspace(1.0).parameters, [_child2])

  @parameterized.named_parameters(
      ('FloatParentValues', [([0.5], _child1)]),
      ('StringParentValues', [(['0'], _child1), (['0.0', '1.0'], _child2)]))
  def testIntegerWithInvalid(self, children):
    with self.assertRaises(TypeError):
      _ = pc.ParameterConfig.factory('parent', bounds=(0, 1), children=children)

  @parameterized.named_parameters(
      ('IntParentValues', [([0], _child1), ([0, 1], _child2)]),
      ('FloatParentValues', [([0.0], _child1), ([0.0, 1.0], _child2)]))
  def testDiscreteWithValid(self, children):
    p = pc.ParameterConfig.factory(
        'parent', feasible_values=[0.0, 1.0], children=children)
    self.assertCountEqual(p.subspace(0.0).parameters, [_child1, _child2])
    self.assertCountEqual(p.subspace(1.0).parameters, [_child2])

  @parameterized.named_parameters(('StringParentValues', [(['0.0'], _child1),
                                                          (['0.0',
                                                            '1.0'], _child2)]))
  def testDiscreteWithInvalid(self, children):
    with self.assertRaises(TypeError):
      _ = pc.ParameterConfig.factory(
          'parent', feasible_values=[0.0, 1.0], children=children)

  @parameterized.named_parameters(  # pyformat: disable
      ('StringParentValues', [(['a'], _child1), (['a', 'b'], _child2)]))
  def testCategoricalWithValid(self, children):
    p = pc.ParameterConfig.factory(
        'parent', feasible_values=['a', 'b'], children=children)
    self.assertCountEqual(p.subspace('a').parameters, [_child1, _child2])
    self.assertCountEqual(p.subspace('b').parameters, [_child2])

  @parameterized.named_parameters(('StringParentValues', [(['0.0'], _child1),
                                                          (['1.0'], _child2)]))
  def testCategoricalWithInvalid(self, children):
    with self.assertRaises(TypeError):
      _ = pc.ParameterConfig.factory(
          'parent', feasible_values=[0.0, 1.0], children=children)


class MergeTest(parameterized.TestCase):

  def test_merge_bounds(self):
    pc1 = pc.ParameterConfig.factory('pc1', bounds=(0.0, 2.0))
    pc2 = pc.ParameterConfig.factory('pc2', bounds=(-1.0, 1.0))
    self.assertEqual(
        pc.ParameterConfig.merge(pc1, pc2),
        pc.ParameterConfig.factory('pc1', bounds=(-1.0, 2.0)))

  def test_merge_discrete(self):
    pc1 = pc.ParameterConfig.factory(
        'pc1', feasible_values=[0.0, 2.0], scale_type=pc.ScaleType.LINEAR)
    pc2 = pc.ParameterConfig.factory('pc2', feasible_values=[-1.0, 0.0])
    self.assertEqual(
        pc.ParameterConfig.merge(pc1, pc2),
        pc.ParameterConfig.factory(
            'pc1',
            feasible_values=[-1.0, 0.0, 2.0],
            scale_type=pc.ScaleType.LINEAR))

  def test_merge_categorical(self):
    pc1 = pc.ParameterConfig.factory('pc1', feasible_values=['a', 'b'])
    pc2 = pc.ParameterConfig.factory('pc2', feasible_values=['a', 'c'])
    self.assertEqual(
        pc.ParameterConfig.merge(pc1, pc2),
        pc.ParameterConfig.factory('pc1', feasible_values=['a', 'b', 'c']))


class ParameterConfigContainsTest(parameterized.TestCase):

  @parameterized.parameters((1.0, True), (-2.0, False), (3.0, False))
  def testFloat(self, value: Any, expected: bool):
    config = pc.ParameterConfig.factory('pc1', bounds=(-1., 2.))
    self.assertEqual(config.contains(value), expected)

  @parameterized.parameters((1, True), (-2, False), (3, False), (1.5, False))
  def testInt(self, value: Any, expected: bool):
    config = pc.ParameterConfig.factory('pc1', bounds=(-1, 2))
    self.assertEqual(config.contains(value), expected)

  @parameterized.parameters((1.0, False), (2, True), (-1, True))
  def testDiscrete(self, value: Any, expected: bool):
    config = pc.ParameterConfig.factory('pc1', feasible_values=[-1., 0., 2.])
    self.assertEqual(config.contains(value), expected)

  @parameterized.parameters(('a', True), ('b', False), ('c', False))
  def testCategorical(self, value: Any, expected: bool):
    config = pc.ParameterConfig.factory(
        'pc1', feasible_values=['a', 'aa', 'aaa'])
    self.assertEqual(config.contains(value), expected)

  @parameterized.parameters((True, True), ('a', False), (0, False))
  def testBoolean(self, value: Any, expected: bool):
    config = pc.ParameterConfig.factory(
        'pc1', feasible_values=[trial.TRUE_VALUE, trial.FALSE_VALUE])
    self.assertEqual(config.contains(value), expected)


class ParameterConfigPropertyTest(parameterized.TestCase):

  @parameterized.parameters(
      ((-1.0, 1.0), None), ((-1, 1), None), ((-2.0, -2.0), -2.0), ((-2, -2), -2)
  )
  def testFloatandInt(self, bounds: Any, expected: Any):
    config = pc.ParameterConfig.factory('pc1', bounds=bounds)
    value = config.deterministic_value
    if expected is None:
      self.assertIsNone(value)
    else:
      self.assertEqual(value, expected)

  @parameterized.parameters(
      ([-1.0, 2.0], None), ([-1.0], -1.0), (['a', 'b'], None), (['a'], 'a')
  )
  def testDiscreteandCategorical(self, feasible_values: Any, expected: Any):
    config = pc.ParameterConfig.factory('pc1', feasible_values=feasible_values)
    value = config.deterministic_value
    if expected is None:
      self.assertIsNone(value)
    else:
      self.assertEqual(value, expected)


class TraverseTest(parameterized.TestCase):

  @parameterized.named_parameters(('ShowChildrenTrue', True),
                                  ('ShowChildrenFalse', False))
  def testTraverse(self, show_children):
    grandchild1 = pc.ParameterConfig.factory('grandchild1', bounds=(-1.0, 1.0))
    grandchildren = [(['a'], grandchild1), (['b'], grandchild1)]
    child1 = pc.ParameterConfig.factory(
        'child1', feasible_values=['a', 'b'], children=grandchildren)

    child2 = pc.ParameterConfig.factory('child2', bounds=(0.0, 1.0))
    children = [([0], child1), ([1], child1), ([0, 1], child2)]
    parent = pc.ParameterConfig.factory(
        'parent', bounds=(0, 1), children=children)
    traversed_names = [
        pc.name for pc in parent.traverse(show_children=show_children)
    ]
    # Some parameter names are reused for separate child nodes, so they
    # will appear multiple times.
    self.assertEqual(traversed_names, [
        'parent', 'child1', 'grandchild1', 'grandchild1', 'child2', 'child1',
        'grandchild1', 'grandchild1', 'child2'
    ])


class SearchSpaceTest(parameterized.TestCase):
  """Check basic functionalities."""

  def testRootAndSelectRootEqual(self):
    space = pc.SearchSpace()
    select_root = space.select_root()
    shortcut_root = space.root
    self.assertIs(select_root._selected[0], shortcut_root._selected[0])

  def testAddFloatParamMinimal(self):
    # Remove this test once we deprecate select_root().
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_float_param('f1', 1.0, 15.0)
    _ = space.select_root().add_float_param('f2', 2.0, 16.0)

    self.assertLen(space.parameters, 2)
    self.assertCountEqual(space.parameter_names, ['f1', 'f2'])

  def testMultidimensionalParameters(self):
    space = pc.SearchSpace()
    _ = space.select_root().add_float_param(
        'f', 1.0, 15.0, default_value=3.0, scale_type=pc.ScaleType.LOG, index=0)
    _ = space.select_root().add_float_param(
        'f',
        2.0,
        10.0,
        default_value=4.0,
        scale_type=pc.ScaleType.LINEAR,
        index=1)
    self.assertCountEqual(space.parameter_names, ['f[0]', 'f[1]'])


class SearchSpaceAddParamtest(parameterized.TestCase):
  """Check `add_xx_param` methods relay the arguments correctly."""

  def testAddFloatParam(self):
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_float_param(
        'f1', 1.0, 15.0, default_value=3.0, scale_type=pc.ScaleType.LOG)
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].name, 'f1')
    self.assertEqual(space.parameters[0].type, pc.ParameterType.DOUBLE)
    self.assertEqual(space.parameters[0].bounds, (1.0, 15.0))
    self.assertEqual(space.parameters[0].scale_type, pc.ScaleType.LOG)
    self.assertEmpty(space.parameters[0].matching_parent_values)
    self.assertEmpty(space.parameters[0].child_parameter_configs)
    with self.assertRaisesRegex(ValueError, 'feasible_values is invalid.*'):
      _ = space.parameters[0].feasible_values
    self.assertEqual(space.parameters[0].default_value, 3.0)

  def testAddDiscreteParamIntegerFeasibleValues(self):
    """Test a Discrete parameter with integer feasible values."""
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_discrete_param(
        'd1', [101, 15.0, 21.0], default_value=15.0)
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].name, 'd1')
    self.assertEqual(space.parameters[0].type, pc.ParameterType.DISCRETE)
    self.assertEqual(space.parameters[0].bounds, (15.0, 101.0))
    self.assertEqual(space.parameters[0].scale_type, pc.ScaleType.LINEAR)
    self.assertEmpty(space.parameters[0].matching_parent_values)
    self.assertEmpty(space.parameters[0].child_parameter_configs)
    self.assertEqual(space.parameters[0].feasible_values, [15.0, 21.0, 101])
    self.assertEqual(space.parameters[0].default_value, 15.0)
    self.assertEqual(space.parameters[0].external_type, pc.ExternalType.INTEGER)

  def testAddDiscreteParamFloatFeasibleValues(self):
    """Test a Discrete parameter with float feasible values."""
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_discrete_param(
        'd1', [15.1, 21.0, 101], default_value=15.1)
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].external_type, pc.ExternalType.FLOAT)

  def testAddBooleanParam(self):
    """Test a Boolean parameter."""
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_bool_param('b1', default_value=True)
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].name, 'b1')
    self.assertEqual(space.parameters[0].type, pc.ParameterType.CATEGORICAL)
    with self.assertRaisesRegex(ValueError,
                                'Accessing bounds of a categorical.*'):
      _ = space.parameters[0].bounds
    self.assertIsNone(space.parameters[0].scale_type)
    self.assertEmpty(space.parameters[0].matching_parent_values)
    self.assertEmpty(space.parameters[0].child_parameter_configs)
    self.assertEqual(space.parameters[0].feasible_values, ['False', 'True'])
    self.assertEqual(space.parameters[0].default_value, 'True')
    self.assertEqual(space.parameters[0].external_type, pc.ExternalType.BOOLEAN)

  def testAddBooleanParamWithFalseDefault(self):
    """Test a Boolean parameter."""
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_bool_param('b1', default_value=False)
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].default_value, 'False')

  def testAddCustomParam(self):
    """Test a Boolean parameter."""
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    _ = space.select_root().add_custom_param('c1', default_value='default')
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].name, 'c1')
    self.assertEqual(space.parameters[0].default_value, 'default')

  def testConditionalParameters(self):
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    root = space.select_root()
    root.add_categorical_param(
        'model_type', ['linear', 'dnn'], default_value='dnn')

    # Test the search space.
    self.assertLen(space.parameters, 1)
    self.assertEqual(space.parameters[0].name, 'model_type')
    self.assertEqual(space.parameters[0].type, pc.ParameterType.CATEGORICAL)
    with self.assertRaisesRegex(ValueError,
                                'Accessing bounds of a categorical.*'):
      _ = space.parameters[0].bounds
    self.assertIsNone(space.parameters[0].scale_type)
    self.assertEmpty(space.parameters[0].matching_parent_values)
    self.assertEmpty(space.parameters[0].child_parameter_configs)
    self.assertEqual(space.parameters[0].feasible_values, ['dnn', 'linear'])
    self.assertEqual(space.parameters[0].default_value, 'dnn')

    dnn = root.select('model_type', ['dnn'])
    dnn.add_float_param(
        'learning_rate',
        0.0001,
        1.0,
        default_value=0.001,
        scale_type=pc.ScaleType.LOG)
    # Test the search space.
    self.assertLen(space.parameters, 1)
    self.assertLen(space.get('model_type').subspaces(), 1)

    linear = root.select('model_type', ['linear'])
    linear.add_float_param(
        'learning_rate',
        0.1,
        1.0,
        default_value=0.1,
        scale_type=pc.ScaleType.LOG)
    # Test the search space.
    self.assertLen(space.parameters, 1)
    self.assertLen(space.get('model_type').subspaces(), 2)

    _ = dnn.add_categorical_param('optimizer_type', ['adam', 'adagrad'])
    # Test the search space.
    self.assertLen(space.parameters, 1)

    # Chained select() calls, path length of 1.
    selected = root.select('model_type',
                           ['dnn']).select('optimizer_type',
                                           ['adam']).add_float_param(
                                               'learning_rate',
                                               0.1,
                                               1.0,
                                               default_value=0.1,
                                               scale_type=pc.ScaleType.LOG)
    self.assertLen(selected, 1)

    # Test the search space.
    self.assertLen(space.parameters, 1)

    # Chained select() calls, path length of 2.
    ko = root.select('model_type', ['dnn']).select('optimizer_type',
                                                   ['adam']).add_bool_param(
                                                       'use_keras_optimizer',
                                                       default_value=False)
    self.assertLen(ko, 1)
    # Test the search space.
    self.assertLen(space.parameters, 1)

    ko2 = ko.select_values(['True'])
    _ = ko2.add_float_param('keras specific', 1.3, 2.4, default_value=2.1)
    # Test the search space.
    self.assertLen(space.parameters, 1)

  def testConditionalParametersWithReturnedSelectors(self):
    space = pc.SearchSpace()
    self.assertEmpty(space.parameters)
    root = space.select_root()
    model_type = root.add_categorical_param('model_type', ['linear', 'dnn'])
    _ = model_type.select_values(['dnn']).add_float_param(
        'learning_rate',
        0.1,
        1.0,
        default_value=0.001,
        scale_type=pc.ScaleType.LOG)
    # Test the search space.
    self.assertLen(space.parameters, 1)

    # It is possible to select different values for the same selector.
    self.assertLen(
        model_type.select_values(['linear', 'dnn']).add_categorical_param(
            'optimizer_type', ['adam', 'adagrad']), 2)
    # Test the search space.
    self.assertLen(space.parameters, 1)

  @parameterized.named_parameters(
      ('Multi', 'units[0]', ('units', 0)),
      ('Multi2', 'with_underscore[1]', ('with_underscore', 1)),
      ('NotMulti', 'units', None),
      ('NotMulti2', 'with space', None),
      ('NotMulti3', 'with[8]space', None),
      ('NotMulti4', 'units[0][4]', ('units[0]', 4)),
      ('GinStyle', '_gin.ambient_net_exp_from_vec.block_type[3]',
       ('_gin.ambient_net_exp_from_vec.block_type', 3)),
  )
  def testParseMultiDimensionalParameterName(self, name, expected):
    base_name_index = pc.SearchSpaceSelector.parse_multi_dimensional_parameter_name(
        name)
    self.assertEqual(base_name_index, expected)

  def testValidateCategoricalInput(self):
    space = pc.SearchSpace()
    root = space.select_root()
    with self.assertRaises(ValueError):
      root.add_categorical_param('categorical', ['3.2', '2', 5])


class FlattenAndMergeTest(absltest.TestCase):

  def testFlattenAndMerge(self):
    space = test_studies.conditional_automl_space()
    parameters = space.root.select_all().merge()
    logging.info('Merged: %s', parameters)
    self.assertCountEqual(
        [p.name for p in parameters],
        [
            'model_type',
            'learning_rate',
            'optimizer_type',
            'use_special_logic',
            'special_logic_parameter',
        ],
    )


class SearchSpaceContainsTest(absltest.TestCase):

  def _space(self):
    space = pc.SearchSpace()
    root = space.select_root()
    root.add_float_param('learning-rate', 1e-4, 1e-2)
    root.add_categorical_param('optimizer', ['adagrad', 'adam', 'experimental'])
    return space

  def testFloatCat1(self):
    self._space().assert_contains(
        trial.ParameterDict({
            'optimizer': 'adagrad',
            'learning-rate': 1e-2
        }))

  def testFloatCat2(self):
    self.assertFalse(self._space().contains(
        trial.ParameterDict({
            'optimizer': 'adagrad',
            'BADPARAM': 1e-2
        })))

  def testFloatCat3(self):
    self.assertFalse(self._space().contains(
        trial.ParameterDict({
            'optimizer': 'adagrad',
            'learning-rate': 1e-2,
            'BADPARAM': 1e-2
        })))

  def testFloatCat4(self):
    self.assertFalse(self._space().contains(
        trial.ParameterDict({
            'optimizer': 'adagrad',
            'learning-rate': 1e2
        })))


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/shared/parameter_iterators.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tools for iterating through a collection of Parameter(config)s."""

from typing import Iterator
import copy
from typing import Generator, Literal, Union

from vizier._src.pyvizier.shared.parameter_config import ParameterConfig
from vizier._src.pyvizier.shared.parameter_config import SearchSpace
from vizier._src.pyvizier.shared.trial import ParameterDict
from vizier._src.pyvizier.shared.trial import ParameterValueTypes


class SequentialParameterBuilder(Iterator[ParameterConfig]):
  """Builds a ParameterDict by choosing one parameter value at a time.

  Example usage:
    from vizier import pyvizier as vz

    def decide_value(pc: vz.ParameterConfig) -> vz.ParameterValueTypes:
      ...

    search_space : vz.SearchSpace
    builder = SequentialParameterBuilder(search_space)

    for pc in builder:
      builder.choose_value(decide_value(pc))

    assert isinstance(builder.parameters, vz.ParameterDict)
  """

  def __init__(self,
               search_space: SearchSpace,
              
               *,
               traverse_order: Literal['dfs', 'bfs'] = 'dfs'):
    """Init.

    See the class pydoc for more details.

    Args:
      search_space: Search space to iterate over.
      traverse_order: 'dfs' or 'bfs'.
    """
    self._parameters = ParameterDict()
    self._traverse_order = traverse_order
    self._gen = self._coroutine(search_space)
    self._next = next(self._gen)
    self._stop_iteration = None

  def _coroutine(
      self, search_space: SearchSpace
  ) -> Generator[ParameterConfig, Union[ParameterValueTypes, None], None]:
    search_space = copy.deepcopy(search_space)
    while search_space.parameters:
      parameter_config = search_space.parameters[0]
      value = yield parameter_config
      if value is None:
        search_space.pop(parameter_config.name)
        continue
      # Note: get_subspace also validates the value.
      subspace = search_space.get(parameter_config.name).get_subspace_deepcopy(
          value
      )
      search_space.pop(parameter_config.name)
      self._parameters[parameter_config.name] = value

      if self._traverse_order == 'bfs':
        # For BFS: append the subspace to the current search space.
        for child_parameter in subspace.parameters:
          search_space.add(child_parameter)
      else:
        # For DFS: append the current search space to the subspace.
        for parameter in search_space.parameters:
          subspace.add(parameter)
        search_space = subspace

  def __next__(self) -> ParameterConfig:
    if self._stop_iteration is not None:
      raise self._stop_iteration
    return self._next

  def choose_value(self, value: ParameterValueTypes) -> None:
    """Choose the value for the last ParameterConfig."""
    try:
      self._next = self._gen.send(value)
    except StopIteration as e:
      self._stop_iteration = e

  def skip(self) -> None:
    """Skip the value for the last ParameterConfig."""
    try:
      self._next = self._gen.send(None)
    except StopIteration as e:
      self._stop_iteration = e

  @property
  def parameters(self) -> ParameterDict:
    """Parameters chosen so far.

    WARNING: Do not mutate the dict until this Iterator is exhausted.
    """
    return self._parameters


--- vizier/_src/pyvizier/shared/parameter_iterators_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for parameter iterators."""

from typing import Sequence
from typing import Literal

from vizier import pyvizier as vz
from vizier._src.pyvizier.shared import parameter_iterators as pi

from absl.testing import absltest
from absl.testing import parameterized


class ParameterIteratorsTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(
          traverse_order='bfs',
          expected_order=[
              'model', 'apply_preprocessing', 'num_layers', 'preprocessor'
          ]),
      dict(
          traverse_order='dfs',
          expected_order=[
              'model', 'num_layers', 'apply_preprocessing', 'preprocessor'
          ]))
  def test_e2e(self, traverse_order: Literal['bfs', 'dfs'],
               expected_order: Sequence[str]):
    valid_params = {
        'model': 'dnn',
        'apply_preprocessing': True,
        'preprocessor': 'augment',
        'num_layers': 1,  # child parameter comes last.
    }
    valid_params = {k: valid_params[k] for k in expected_order}

    space = vz.SearchSpace()
    model = space.root.add_categorical_param('model', ['dnn', 'gbdt'])
    model.select_values(['dnn']).add_int_param('num_layers', 1, 4)
    model.select_values(['gbdt']).add_discrete_param('num_estimators',
                                                     [100, 200])
    preprocessing = space.root.add_bool_param('apply_preprocessing')
    preprocessing.select_values([True]).add_categorical_param(
        'preprocessor', ['normalize', 'augment'])

    builder = pi.SequentialParameterBuilder(
        space, traverse_order=traverse_order)
    for parameter_config in builder:
      builder.choose_value(valid_params[parameter_config.name])

    self.assertEqual(builder.parameters.as_dict(), valid_params)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/pyvizier/shared/study.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Shared classes for representing studies."""

from typing import List
import attr
from vizier._src.pyvizier.shared import base_study_config
from vizier._src.pyvizier.shared import trial


@attr.define(frozen=True, init=True, slots=True, kw_only=False)
class ProblemAndTrials:
  """Container for problem statement and trials."""
  problem: base_study_config.ProblemStatement = attr.ib(init=True)
  trials: List[trial.Trial] = attr.ib(
      init=True,
      # TODO: Remove the pylint.
      converter=lambda x: list(x),  # pylint: disable=unnecessary-lambda
      default=attr.Factory(list))


--- vizier/_src/pyvizier/shared/trial.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Wrapper classes for Trial protos and other messages in them."""

import collections
from collections import abc
import copy
import dataclasses
import datetime
import enum
from typing import Any, Dict, List, Mapping, MutableMapping, Optional, Union, FrozenSet

from absl import logging
import attr
import numpy as np

from vizier._src.pyvizier.shared import common

ParameterValueTypes = Union[str, int, float, bool]

# TODO: These constants should be deleted.
TRUE_VALUE = 'True'
FALSE_VALUE = 'False'


class ParameterType(enum.Enum):
  """Valid Values for ParameterConfig.type."""
  DOUBLE = 'DOUBLE'
  INTEGER = 'INTEGER'
  CATEGORICAL = 'CATEGORICAL'
  DISCRETE = 'DISCRETE'
  CUSTOM = 'CUSTOM'

  def is_numeric(self) -> bool:
    return self in [self.DOUBLE, self.INTEGER, self.DISCRETE]

  def is_continuous(self) -> bool:
    return self == self.DOUBLE

  def _raise_type_error(self, value: ParameterValueTypes) -> None:
    raise TypeError(f'Type {self} is not compatible with value: {value}')

  def assert_correct_type(self, value: ParameterValueTypes) -> None:
    if self.is_numeric() and float(value) != value:
      self._raise_type_error(value)

    # TODO: Accepting boolean into categorical is unintuitive.
    elif (self
          == ParameterType.CATEGORICAL) and (not isinstance(value,
                                                            (str, bool))):
      self._raise_type_error(value)

    if self == self.INTEGER and int(value) != value:
      self._raise_type_error(value)


# TODO: Trial class should not depend on these.
class ExternalType(enum.Enum):
  """Valid Values for ParameterConfig.external_type."""
  INTERNAL = 'INTERNAL'
  BOOLEAN = 'BOOLEAN'
  INTEGER = 'INTEGER'
  FLOAT = 'FLOAT'


# Values should NEVER be removed from the enums below, only added.
class TrialStatus(enum.Enum):
  """Values for Trial.Status."""
  UNKNOWN = 'UNKNOWN'
  REQUESTED = 'REQUESTED'
  ACTIVE = 'ACTIVE'
  COMPLETED = 'COMPLETED'
  STOPPING = 'STOPPING'


@attr.s(frozen=True, init=True, slots=True, kw_only=False)
class Metric:
  """Enhanced immutable wrapper for vizier_pb2.Metric proto.

  It has an optional field "std" for internal usage. This field gets lost
  when the object is converted to proto.
  """

  def _std_not_negative(self, _, stddev: Optional[float]) -> bool:
    if (stddev is not None) and (not stddev >= 0):
      raise ValueError(
          'Standard deviation must be a non-negative finite number.'
      )  # pytype: disable=bad-return-type

  value: float = attr.ib(
      converter=float,
      init=True,
      validator=[attr.validators.instance_of(float)],
      kw_only=False)
  std: Optional[float] = attr.ib(
      converter=lambda x: float(x) if x is not None else None,
      validator=[
          attr.validators.optional(attr.validators.instance_of(float)),
          _std_not_negative
      ],
      init=True,
      default=None,
      kw_only=True)


# Use when you want to preserve the shapes or reduce if-else statements.
# e.g. `metrics.get('metric_name', NaNMetric).value` to get NaN or the actual
# value.
NaNMetric = Metric(value=np.nan)


# TODO: This class should be deleted in the future.
@attr.s(auto_attribs=True, frozen=True, init=True, slots=True, repr=False)
class ParameterValue:
  """Immutable wrapper for vizier_pb2.Parameter.value, which is a oneof field.

  Has accessors (properties) that cast the value into the type according
  to StudyConfiguration class behavior. In particular, 'true' and FALSE_VALUE
  are
  treated as special strings that are cast to a numeric value of 1 and 0,
  respectively, and boolean value of True and False, repectively.
  """

  value: ParameterValueTypes = attr.ib(
      init=True,
      validator=[
          attr.validators.instance_of((str, int, float, bool)),
      ])

  def cast_as_internal(self,
                       internal_type: ParameterType) -> ParameterValueTypes:
    """Cast to the internal type."""
    internal_type.assert_correct_type(self.value)

    if internal_type in (ParameterType.DOUBLE, ParameterType.DISCRETE):
      return self.as_float  # pytype: disable=bad-return-type
    elif internal_type == ParameterType.INTEGER:
      return self.as_int  # pytype: disable=bad-return-type
    elif internal_type == ParameterType.CATEGORICAL:
      return self.as_str  # pytype: disable=bad-return-type
    else:
      raise RuntimeError(f'Unknown type {internal_type}')

  def cast(
      self,
      external_type: ExternalType,
  ) -> ParameterValueTypes:
    """Returns ParameterValue cast to external_type.

    Args:
      external_type:

    Returns:
      self.value if external_type is INTERNAL.
      self.as_bool if external_type is BOOLEAN.
      self.as_int if external_type is INTEGER.
      self.as_float if external_type is FLOAT.

    Raises:
      ValueError: If external_type is not valid.
    """
    if external_type == ExternalType.INTERNAL:
      return self.value
    elif external_type == ExternalType.BOOLEAN:
      return self.as_bool  # pytype: disable=bad-return-type
    elif external_type == ExternalType.INTEGER:
      return self.as_int  # pytype: disable=bad-return-type
    elif external_type == ExternalType.FLOAT:
      return self.as_float  # pytype: disable=bad-return-type
    else:
      raise ValueError(
          'Unknown external type enum value: {}.'.format(external_type))

  @property
  def as_float(self) -> Optional[float]:
    """Returns the value cast to float."""
    if self.value == TRUE_VALUE:
      return 1.0
    elif self.value == FALSE_VALUE:
      return 0.0
    try:
      # Note str -> float conversion exists for benchmark use.
      return float(self.value)
    except ValueError:
      return None

  @property
  def as_int(self) -> Optional[int]:
    """Returns the value cast to int."""
    if self.value == TRUE_VALUE:
      return 1
    elif self.value == FALSE_VALUE:
      return 0
    try:
      # Note str -> int conversion exists for benchmark use.
      return int(self.value)
    except ValueError:
      return None

  @property
  def as_str(self) -> Optional[str]:
    """Returns str-typed value or 'True'/'False' if value is bool."""
    if isinstance(self.value, bool):
      if self.value:
        return TRUE_VALUE
      else:
        return FALSE_VALUE
    elif isinstance(self.value, str):
      return self.value
    # Note str conversion exists for benchmark use. Use __str__ instead.
    return str(self.value)

  @property
  def as_bool(self) -> Optional[bool]:
    """Returns the value as bool following StudyConfiguration's behavior.

    Returns: True if value is TRUE_VALUE or 1. False if value is
      FALSE_VALUE or 0. For all other cases, returns None.
      For string type, this behavior is consistent with how
      StudyConfiguration.AddBooleanParameter's. For other types, this
      guarantees that self.value == self.as_bool
    """
    if isinstance(self.value, str):
      if self.value == TRUE_VALUE:
        return True
      elif self.value == FALSE_VALUE:
        return False
    else:
      if self.value == 1.0:
        return True
      elif self.value == 0.0:
        return False
    return None

  def __str__(self) -> str:
    return str(self.value)

  def __repr__(self) -> str:
    return repr(self.value)


class _MetricDict(collections.UserDict, Mapping[str, Metric]):
  """Dictionary of string to metrics."""

  def get_value(self, key: str, default: float | None) -> float | None:
    if key in self.data:
      return self.data[key].value
    else:
      return default

  def __setitem__(self, key: str, value: Union[float, Metric]):
    if isinstance(value, Metric):
      self.data.__setitem__(key, value)
    else:
      self.data.__setitem__(key, Metric(value=value))

  def as_float_dict(self) -> dict[str, float]:
    return {k: m.value for k, m in self.data.items()}


@attr.s(auto_attribs=True, frozen=False, init=True, slots=True)
class Measurement:
  """A collection of metrics with a timestamp & checkpoint.

  metrics: Named, floating-point metrics.  A typical example would be the
    accuracy of a machine-learning model.  Typically, all the metrics mentioned
    in the MetricInformation class would be listed here; other metrics may be
    listed but would not normally be used by Vizier.

  elapsed_secs: (optional) The length of time it took to evaluate the Trial to
    reach this Measurement, in seconds.  This may be used by some Vizier
    algorithms.

  steps: (optional)  A positive integer roughly proportional to the amount of
    work spent.  When training a ML system, often this is a count of training
    steps/epochs.  When supplied, $steps should be consistent with the order of
    Measurements, but they need not be consecutive values.

  checkpoint_path: (optional) A slash-separated pathname.  Typically used when
    training a ML model; it would normally be a pathname for the checkpoint that
    produced the Measurement.  Implementations may limit the length of this
    string, but at least 2048 bytes will be allowed.
  """

  def _value_is_finite(self, _, value):
    if not (np.isfinite(value) and value >= 0):
      raise ValueError('Must be finite and non-negative.')

  # Should be used as a regular Dict.
  metrics: _MetricDict = attr.ib(
      init=True,
      converter=lambda d: _MetricDict(**d),
      default=_MetricDict(),
      validator=attr.validators.instance_of(_MetricDict),
      on_setattr=[attr.setters.convert, attr.setters.validate])

  elapsed_secs: float = attr.ib(
      converter=float,
      init=True,
      default=0,
      validator=[attr.validators.instance_of(float), _value_is_finite],
      on_setattr=[attr.setters.convert, attr.setters.validate],
      kw_only=True)

  # TODO: Change type annotation to int.
  steps: float = attr.ib(
      converter=int,
      init=True,
      default=0,
      validator=[attr.validators.instance_of(int), _value_is_finite],
      on_setattr=[attr.setters.convert, attr.setters.validate],
      kw_only=True)

  checkpoint_path: str = attr.ib(
      init=True,
      default='',
      validator=[attr.validators.instance_of(str)],
      kw_only=True,
  )


def _to_local_time(
    dt: Optional[datetime.datetime]) -> Optional[datetime.datetime]:
  """Converter for initializing timestamps in Trial class."""
  return dt.astimezone() if dt else None


# TODO: This class should have group() method that
# groups list parameters under the same key.
@attr.define(init=False, frozen=True, eq=True)
class ParameterDict(abc.MutableMapping):
  """Parameter dictionary.

  Maps the parameter names to their values. Works like a regular
  dict[str, ParameterValue] for the most part, except one can directly assign
  values of type `ParameterValueType`. So,
    ParameterDict(a=3) and
    ParameterDict(a=ParameterValue(3)) are equivalent.


  To access the raw value directly, use get_value() or as_dict():
    d.get_value('a') == d.get('a').value
    d.as_dict()['a'] == d.get_value('a')
  """

  _items: MutableMapping[str, ParameterValue] = attr.field(
      init=False, factory=dict)

  def as_dict(self) -> Dict[str, ParameterValueTypes]:
    """Returns the dict of parameter names to raw values."""
    return {k: self.get_value(k) for k in self._items}  # pytype: disable=bad-return-type

  def __init__(self, iterable: Any = tuple(), **kwargs):
    self.__attrs_init__()
    self.update(iterable, **kwargs)

  def __setitem__(self, key: str, value: Union[ParameterValue,
                                               ParameterValueTypes]):
    if isinstance(value, ParameterValue):
      self._items[key] = value
    else:
      self._items[key] = ParameterValue(value)

  def __delitem__(self, key: str):
    del self._items[key]

  def __getitem__(self, key: str) -> ParameterValue:
    return self._items[key]

  def __len__(self) -> int:
    return len(self._items)

  def __iter__(self):
    return iter(self._items)

  def get_value(
      self,
      key: str,
      default: Optional[ParameterValueTypes] = None
  ) -> Optional[ParameterValueTypes]:
    """Returns the raw value of the given parameter name."""
    pv = self.get(key, default)
    if isinstance(pv, ParameterValue):
      return pv.value
    else:
      return pv


@attr.define(auto_attribs=True, frozen=False, init=True, slots=True)
class TrialSuggestion:
  """Freshly suggested trial.

  Suggestion can be converted to Trial object which has more functionalities.
  """

  parameters: ParameterDict = attr.field(
      init=True,
      factory=ParameterDict,
      converter=ParameterDict,
      validator=attr.validators.instance_of(ParameterDict))  # pytype: disable=wrong-arg-types

  metadata: common.Metadata = attr.field(
      init=True,
      kw_only=True,
      factory=common.Metadata,
      validator=attr.validators.instance_of(common.Metadata))

  def to_trial(self, uid: int = 0) -> 'Trial':
    """Assign an id and make it a Trial object.

    Usually SuggetedTrial objects are shorted-lived and not exposed to end
    users. This method is for non-service usage of trial suggestions in
    benchmarks, tests, colabs, etc.

    Args:
      uid: Trial id.

    Returns:
      Trial object.
    """
    return Trial(id=uid, parameters=self.parameters, metadata=self.metadata)


@attr.define(auto_attribs=True, frozen=False, init=True, slots=True)
class Trial(TrialSuggestion):
  """A Vizier Trial."""
  id: int = attr.ib(
      init=True,
      kw_only=True,
      default=0,
      validator=attr.validators.instance_of(int),
  )

  is_requested: bool = attr.ib(
      init=True,
      kw_only=True,
      default=False,
      validator=attr.validators.instance_of(bool))

  assigned_worker: Optional[str] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
  )

  stopping_reason: Optional[str] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
  )

  _infeasibility_reason: Optional[str] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
  )

  description: Optional[str] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(attr.validators.instance_of(str)),
  )

  related_links: Dict[str, str] = attr.ib(
      init=True,
      kw_only=True,
      factory=dict,
      validator=attr.validators.deep_mapping(
          key_validator=attr.validators.instance_of(str),
          value_validator=attr.validators.instance_of(str),
          mapping_validator=attr.validators.instance_of(dict)),
  )  # pytype: disable=wrong-arg-types

  final_measurement: Optional[Measurement] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      validator=attr.validators.optional(
          attr.validators.instance_of(Measurement)),
  )

  measurements: List[Measurement] = attr.ib(
      init=True,
      kw_only=True,
      factory=list,
      validator=attr.validators.deep_iterable(
          member_validator=attr.validators.instance_of(Measurement),
          iterable_validator=attr.validators.instance_of(list)),
  )

  creation_time: Optional[datetime.datetime] = attr.ib(
      init=True,
      factory=datetime.datetime.now,
      converter=_to_local_time,
      kw_only=True,
      repr=lambda v: v.strftime('%x %X') if v is not None else 'None',
      validator=attr.validators.optional(
          attr.validators.instance_of(datetime.datetime)
      ),
  )

  completion_time: Optional[datetime.datetime] = attr.ib(
      init=True,
      kw_only=True,
      default=None,
      eq=False,
      repr=lambda v: v.strftime('%x %X') if v is not None else 'None',
      converter=_to_local_time,
      validator=attr.validators.optional(
          attr.validators.instance_of(datetime.datetime)
      ),
  )

  def __attrs_post_init__(self):
    if self.completion_time is None and (
        self.final_measurement is not None or self.infeasibility_reason
    ):
      self.completion_time = self.creation_time

  @property
  def duration(self) -> Optional[datetime.timedelta]:
    """Returns the duration of this Trial if it is completed, or None."""
    if self.completion_time:
      return self.completion_time - self.creation_time
    else:
      return None

  @property
  def status(self) -> TrialStatus:
    """Status.

    COMPLETED: Trial has final measurement or is declared infeasible.
    ACTIVE: Trial is being evaluated.
    STOPPING: Trial is being evaluated, but was decided to be not worth further
      evaluating.
    REQUESTED: Trial is queued for future suggestions.
    """
    if self.final_measurement is not None or self.infeasible:
      return TrialStatus.COMPLETED
    elif self.stopping_reason is not None:
      return TrialStatus.STOPPING
    elif self.is_requested:
      return TrialStatus.REQUESTED
    else:
      return TrialStatus.ACTIVE

  @property
  def is_completed(self) -> bool:
    """Returns True if this Trial is completed."""
    if self.status == TrialStatus.COMPLETED:
      if self.completion_time is None:
        logging.warning('Invalid Trial state: status is COMPLETED, but a '
                        ' completion_time was not set')
      return True
    elif self.completion_time is not None:
      if self.status is None:
        logging.warning('Invalid Trial state: status is not set to COMPLETED, '
                        'but a completion_time is set')
      return True
    return False

  @property
  def infeasible(self) -> bool:
    """Returns True if this Trial is infeasible."""
    return self._infeasibility_reason is not None

  @property
  def infeasibility_reason(self) -> Optional[str]:
    """Returns this Trial's infeasibility reason, if set."""
    return self._infeasibility_reason

  def complete(self,
               measurement: Measurement,
               *,
               infeasibility_reason: Optional[str] = None,
               inplace: bool = True) -> 'Trial':
    """Completes the trial and returns it.

    Args:
      measurement: Measurement to complete the trial with.
      infeasibility_reason: If set, completes the trial as infeasible. If the
        trial was already infeasible and infeasibility_reason is not set, the
        trial remains infeasible.
      inplace: If True, Trial is modified in place. If False, which is the
        default, then the operation is performed and it returns a copy of the
        object.

    Returns:
      Completed Trial.
    """
    if inplace:
      # Use setattr. If we assign to self.final_measurement, then hyperref
      # mechanisms think this line is where `final_measurement` property
      # is defined, instead of where we declare attr.ib.
      self.__setattr__('final_measurement', copy.deepcopy(measurement))
      if infeasibility_reason is not None:
        self.__setattr__('_infeasibility_reason', infeasibility_reason)
      self.completion_time = _to_local_time(datetime.datetime.now())
      return self
    else:
      clone = copy.deepcopy(self)
      return clone.complete(
          measurement, inplace=True, infeasibility_reason=infeasibility_reason)

  @property
  def final_measurement_or_die(self) -> Measurement:
    if self.final_measurement is None:
      raise ValueError(f'Trial is missing final_measurement: {self}')
    return self.final_measurement


# Define aliases.
CompletedTrial = Trial
PendingTrial = Trial
CompletedTrialWithMeasurements = Trial
PendingTrialWithMeasurements = Trial


@attr.define(kw_only=True)
class TrialFilter:
  """Trial filter.

  All filters are by default 'AND' conditions.

  Attributes:
    ids: If set, requires the trial's id to be in the set.
    min_id: If set, requires the trial's id to be at least this number.
    max_id: If set, requires the trial's id to be at most this number.
    status: If set, requires the trial's status to be in the set.
  """
  ids: Optional[FrozenSet[int]] = attr.field(
      default=None,
      converter=lambda x: frozenset(x) if x is not None else None,
      validator=attr.validators.optional(
          attr.validators.deep_iterable(
              attr.validators.instance_of(int),
              attr.validators.instance_of(frozenset))))
  min_id: Optional[int] = attr.field(default=None)
  max_id: Optional[int] = attr.field(default=None)
  status: Optional[FrozenSet[TrialStatus]] = attr.field(
      default=None,
      converter=lambda x: frozenset(x) if x is not None else None,
      validator=attr.validators.optional(
          attr.validators.deep_iterable(
              attr.validators.instance_of(TrialStatus),
              attr.validators.instance_of(frozenset))))

  # TODO: Add "search_space" argument

  def __call__(self, trial: Trial) -> bool:
    if self.ids is not None:
      if trial.id not in self.ids:
        return False
    if self.min_id is not None:
      if trial.id < self.min_id:
        return False
    if self.max_id is not None:
      if trial.id > self.max_id:
        return False
    if self.status is not None:
      if trial.status not in self.status:
        return False
    return True


@dataclasses.dataclass(frozen=True)
class MetadataDelta:
  """Carries cumulative delta for a batch metadata update.

  Attributes:
    on_study: Updates to be made on study-level metadata.
    on_trials: Maps trial id to updates.
  """

  on_study: common.Metadata = dataclasses.field(default_factory=common.Metadata)

  on_trials: Dict[int, common.Metadata] = dataclasses.field(
      default_factory=lambda: collections.defaultdict(common.Metadata))

  def __bool__(self):
    """Returns True if this carries any Metadata items."""
    if self.on_study:
      return True
    for v in self.on_trials.values():
      if v:
        return True
    return False

  def on_trial(self, trial_id: int) -> common.Metadata:
    """Enables easy assignment to a single Trial."""
    return self.on_trials[trial_id]

  def assign(self,
             namespace: str,
             key: str,
             value: common.MetadataValue,
             *,
             trial: Optional[Trial] = None,
             trial_id: Optional[int] = None):
    """Assigns metadata.

    Args:
      namespace: Namespace of the metadata. See common.Metadata doc for more
        details.
      key:
      value:
      trial: If specified, `trial_id` must be None. It behaves the same as when
        `trial_id=trial.id`, and additionally, the metadata is added to `trial`.
      trial_id: If specified, `trial` must be None. If both `trial` and
        `trial_id` are None, then the key-value pair will be assigned to the
        study.

    Raises:
      ValueError:
    """
    if trial is None and trial_id is None:
      self.on_study.ns(namespace)[key] = value
    elif trial is not None and trial_id is not None:
      raise ValueError(
          'At most one of `trial` and `trial_id` can be specified.')
    elif trial is not None:
      self.on_trials[trial.id].ns(namespace)[key] = value
      trial.metadata.ns(namespace)[key] = value
    elif trial_id is not None:
      self.on_trials[trial_id].ns(namespace)[key] = value


--- vizier/_src/pyvizier/shared/trial_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.pyvizier.shared.trial."""
import copy
import datetime

from typing import Sequence

import numpy as np

from vizier._src.pyvizier.shared import trial
from absl.testing import absltest
from absl.testing import parameterized

Metric = trial.Metric
Measurement = trial.Measurement


class MetricTest(absltest.TestCase):

  def testMetricCreation(self):
    _ = Metric(value=0, std=0.5)

  def testMetricCannotHaveNaN(self):
    with self.assertRaises(ValueError):
      _ = Metric(value=np.nan, std=-np.nan)

  def testMetricCannotHaveNegativeStd(self):
    with self.assertRaises(ValueError):
      _ = Metric(value=0, std=-0.5)


class MeasurementTest(absltest.TestCase):

  def testMetricsInitializedFromFloats(self):
    m = Measurement()
    m.metrics = dict(a=0.3)
    self.assertEqual(m.metrics['a'], Metric(0.3))
    m.metrics['b'] = 0.5
    self.assertEqual(m.metrics, {'a': Metric(0.3), 'b': Metric(0.5)})

  def testMetrics(self):
    m = Measurement()
    m.metrics = dict(a=Metric(0.3))
    self.assertEqual(m.metrics['a'], Metric(0.3))

  def testTimeStampsAreNotFrozen(self):
    m = Measurement()
    m.elapsed_secs = 1.0
    m.steps = 5


ParameterValue = trial.ParameterValue


class ParameterValueTest(parameterized.TestCase):

  @parameterized.parameters((True,), (False,))
  def testBool(self, bool_value):
    value = ParameterValue(bool_value)
    self.assertEqual(value.as_float, float(bool_value))
    self.assertEqual(value.as_int, int(bool_value))
    self.assertEqual(value.as_str, str(bool_value))

  def testIntegralFloat0(self):
    value = ParameterValue(0.0)
    self.assertEqual(value.as_float, 0.0)
    self.assertEqual(value.as_int, 0)
    self.assertEqual(value.as_bool, False)
    self.assertEqual(value.as_str, '0.0')

  def testIntegralFloat1(self):
    value = ParameterValue(1.0)
    self.assertEqual(value.as_float, 1.0)
    self.assertEqual(value.as_int, 1)
    self.assertEqual(value.as_bool, True)
    self.assertEqual(value.as_str, '1.0')

  def testIntegralFloat2(self):
    value = ParameterValue(2.0)
    self.assertEqual(value.as_float, 2.0)
    self.assertEqual(value.as_int, 2)
    self.assertIsNone(value.as_bool)
    self.assertEqual(value.as_str, '2.0')

  def testInteger0(self):
    value = ParameterValue(0)
    self.assertEqual(value.as_float, 0)
    self.assertEqual(value.as_int, 0)
    self.assertEqual(value.as_bool, False)
    self.assertEqual(value.as_str, '0')

  def testInteger1(self):
    value = ParameterValue(1)
    self.assertEqual(value.as_float, 1)
    self.assertEqual(value.as_int, 1)
    self.assertEqual(value.as_bool, True)
    self.assertEqual(value.as_str, '1')

  def testInteger2(self):
    value = ParameterValue(2)
    self.assertEqual(value.as_float, 2)
    self.assertEqual(value.as_int, 2)
    self.assertIsNone(value.as_bool)
    self.assertEqual(value.as_str, '2')

  def testStringTrue(self):
    value = ParameterValue(trial.TRUE_VALUE)
    self.assertEqual(value.as_bool, True)
    self.assertEqual(value.as_str, trial.TRUE_VALUE)

  def testStringFalse(self):
    value = ParameterValue(trial.FALSE_VALUE)
    self.assertEqual(value.as_bool, False)
    self.assertEqual(value.as_str, trial.FALSE_VALUE)

  def testStringFloat1(self):
    value = ParameterValue('1.0')
    self.assertEqual(value.as_float, 1.0)
    self.assertIsNone(value.as_int)
    self.assertIsNone(value.as_bool)
    self.assertEqual(value.as_str, '1.0')

  def testStringInt1(self):
    value = ParameterValue('1')
    self.assertEqual(value.as_float, 1.0)
    self.assertEqual(value.as_int, 1)
    self.assertIsNone(value.as_bool)
    self.assertEqual(value.as_str, '1')

  def testParameterCanHaveNonFiniteValues(self):
    ParameterValue(float('nan'))
    ParameterValue(value=float('inf'))
    ParameterValue(value=float('inf'))


class TrialTest(absltest.TestCase):

  def testCompleteInplace(self):
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(measurement, inplace=True)

    # The trial was completed in place.
    self.assertEqual(test.final_measurement, measurement)
    self.assertLessEqual(test.completion_time,
                         datetime.datetime.now().astimezone())
    self.assertGreaterEqual(test.completion_time, test.creation_time)
    assert test.duration is not None
    self.assertGreaterEqual(test.duration.total_seconds(), 0)

    self.assertEqual(completed.final_measurement, measurement)
    self.assertLessEqual(completed.completion_time,
                         datetime.datetime.now().astimezone())
    self.assertGreaterEqual(completed.completion_time, completed.creation_time)
    assert completed.duration is not None
    self.assertGreaterEqual(completed.duration.total_seconds(), 0)

    # completed is the same reference as test.
    self.assertEqual(test, completed)

  def testCompleteNotInplace(self):
    """Complete with inplace=False."""
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })

    test_copy = copy.deepcopy(test)

    completed = test.complete(measurement, inplace=False)

    # The returned Trial is completed.
    self.assertEqual(completed.final_measurement, measurement)
    self.assertGreaterEqual(completed.completion_time, completed.creation_time)
    self.assertLessEqual(completed.completion_time,
                         datetime.datetime.now().astimezone())
    assert completed.duration is not None
    self.assertGreaterEqual(completed.duration.total_seconds(), 0)
    self.assertEqual(completed.status, trial.TrialStatus.COMPLETED)
    self.assertTrue(completed.is_completed)

    # The original Trial is unchanged.
    self.assertEqual(test_copy, test)
    self.assertIsNone(test.final_measurement)
    self.assertIsNone(test.completion_time)
    self.assertIsNone(test.duration)
    self.assertEqual(test.status, trial.TrialStatus.ACTIVE)
    self.assertFalse(test.is_completed)

  def testCompleteInfeasible(self):
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(
        measurement, inplace=False, infeasibility_reason='reason')
    # Test infeasibility.
    self.assertTrue(completed.infeasible)
    self.assertEqual(completed.infeasibility_reason, 'reason')

  def testCompleteInfeasible2(self):
    test = trial.Trial(infeasibility_reason='reason')
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(measurement, inplace=False)
    # When infeasibility not provided, the trial shoud remain infeasible.
    self.assertTrue(completed.infeasible)
    self.assertEqual(completed.infeasibility_reason, 'reason')

  def testCompleteInfeasible3(self):
    test = trial.Trial(infeasibility_reason='reason')
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(
        measurement, inplace=False, infeasibility_reason='other')
    # Infeasibility reason should be updated.
    self.assertTrue(completed.infeasible)
    self.assertEqual(completed.infeasibility_reason, 'other')
    # The original trial should retain the original reason.
    self.assertTrue(test.infeasible)
    self.assertEqual(test.infeasibility_reason, 'reason')

  def testCompleteEmptyInfeasible(self):
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(
        measurement, inplace=False, infeasibility_reason='')
    # Test infeasibility.
    self.assertTrue(completed.infeasible)
    self.assertEqual(completed.infeasibility_reason, '')

  def testCompleteInfeasibleInplace(self):
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    test.complete(measurement, inplace=True, infeasibility_reason='reason')
    # Test infeasibility.
    self.assertTrue(test.infeasible)
    self.assertEqual(test.infeasibility_reason, 'reason')

  def testCompleteInfeasibleInplace2(self):
    test = trial.Trial(infeasibility_reason='reason')
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    test.complete(measurement, inplace=True)
    # When infeasibility not provided, the trial shoud remain infeasible.
    self.assertTrue(test.infeasible)
    self.assertEqual(test.infeasibility_reason, 'reason')

  def testCompleteInfeasibleInplace3(self):
    test = trial.Trial(infeasibility_reason='reason')
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    test.complete(measurement, inplace=True, infeasibility_reason='other')
    # Infeasibility reason should be updated.
    self.assertTrue(test.infeasible)
    self.assertEqual(test.infeasibility_reason, 'other')

  def testCompleteEmptyInfeasibleInplace(self):
    test = trial.Trial()
    measurement = Measurement(metrics={
        'pr-auc': Metric(value=0.8),
        'latency': Metric(value=32)
    })
    completed = test.complete(
        measurement, inplace=True, infeasibility_reason='')
    # Test infeasibility.
    self.assertTrue(completed.infeasible)
    self.assertEqual(completed.infeasibility_reason, '')

  def testDefaultsNotShared(self):
    """Make sure default parameters are not shared between instances."""
    trial1 = trial.Trial()
    trial2 = trial.Trial()
    trial1.parameters['x1'] = trial.ParameterValue(5)
    self.assertEmpty(trial2.parameters)

  def testCreationTime(self):
    trial1 = trial.Trial()
    trial2 = trial.Trial()
    self.assertGreater(trial2.creation_time, trial1.creation_time)

  def testCompletionTime(self):
    trial1 = trial.Trial(
        final_measurement=trial.Measurement(
            metrics={'pr-auc': Metric(value=0.8)}
        )
    )
    self.assertEqual(trial1.creation_time, trial1.completion_time)
    trial2 = trial.Trial(infeasibility_reason='reasons')
    self.assertEqual(trial2.creation_time, trial2.completion_time)


class ParameterDictTest(parameterized.TestCase):

  @parameterized.parameters((True,), (3,), (1.,), ('aa',))
  def testAssignRawValue(self, v):
    d = trial.ParameterDict()
    d['p1'] = v
    self.assertEqual(d.get('p1'), trial.ParameterValue(v))
    self.assertEqual(d.get_value('p1'), v)
    self.assertEqual(d.get_value('p2', 'default'), 'default')
    self.assertLen(d, 1)
    self.assertLen(d.items(), 1)

  @parameterized.parameters((True,), (3,), (1.,), ('aa',))
  def testAssignWrappedValue(self, v):
    d = trial.ParameterDict()
    v = trial.ParameterValue(v)
    d['p1'] = v
    self.assertEqual(d.get('p1'), v)
    self.assertEqual(d.get_value('p1'), v.value)
    self.assertEqual(d.get_value('p2', 'default'), 'default')
    self.assertLen(d, 1)
    self.assertLen(d.items(), 1)


class SuggestionTestI(absltest.TestCase):

  def testToTrial(self):
    suggestion = trial.TrialSuggestion({'a': 3, 'b': True})
    suggestion.metadata['key'] = 'value'

    t = suggestion.to_trial(1)
    self.assertEqual(t.id, 1)
    self.assertEqual(t.parameters, suggestion.parameters)
    self.assertEqual(t.metadata, suggestion.metadata)


class TrialFilterTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(filtr=trial.TrialFilter(), answers=[True, True, True, True]),
      dict(
          filtr=trial.TrialFilter(ids=(2, 3)),
          answers=[False, True, True, False]),
      dict(
          filtr=trial.TrialFilter(min_id=3, ids=(2, 3)),
          answers=[False, False, True, False]),
      dict(
          filtr=trial.TrialFilter(
              min_id=2,
              max_id=3,
              ids=(1, 2, 3, 4),
              status=[trial.TrialStatus.REQUESTED]),
          answers=[False, True, False, False]))
  def test_filter(self, filtr: trial.TrialFilter, answers: Sequence[bool]):
    trials = (
        trial.Trial(id=1),  # ACTIVE
        trial.Trial(id=2, is_requested=True),  #  REQUESTED
        trial.Trial(id=3, stopping_reason='stopping'),  # STOPPING
        trial.Trial(id=4).complete(trial.Measurement()),  # COMPLETED
    )
    self.assertSequenceEqual([filtr(t) for t in trials], answers)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/raytune/converters.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Converters for PyVizier with RayTune."""

from typing import Any, Callable, Dict, Union

from ray import tune
from ray.tune.search import sample
from vizier import pyvizier as vz
from vizier.benchmarks import experimenters


class SearchSpaceConverter:
  """Converts pyvizier.SearchSpace <-> RayTune Search Space."""

  @classmethod
  def to_dict(
      cls,
      search_space: vz.SearchSpace,
  ) -> Dict[str, Union[sample.Domain, sample.Sampler]]:
    """Converts PyVizier ProblemStatement to Ray search space."""
    param_space = {}

    for param in search_space.parameters:
      if param.type == vz.ParameterType.DOUBLE:
        lower, upper = param.bounds
        if param.scale_type == vz.ScaleType.LINEAR:
          param_space[param.name] = tune.uniform(lower, upper)
        elif param.scale_type == vz.ScaleType.LOG:
          param_space[param.name] = tune.loguniform(lower, upper)
        else:
          raise ValueError(f'DOUBLE scale {param.scale_type} not supported.')
      elif param.type == vz.ParameterType.INTEGER:
        lower, upper = param.bounds
        param_space[param.name] = tune.randint(lower, upper)
      else:
        feasible_values = param.feasible_values
        param_space[param.name] = tune.choice(feasible_values)
    return param_space

  @classmethod
  def to_vizier(
      cls,
      param_space: Dict[str, Any],
  ) -> vz.SearchSpace:
    """Converts from a Ray to a Vizier SearchSpace."""
    space = vz.SearchSpace()

    for param_name, param_config in param_space.items():
      # Find out if the parameter should be scaled.
      scale_type = None
      if isinstance(param_config, sample.Float):
        if isinstance(param_config.sampler, (sample.Grid, sample.Uniform)):
          scale_type = vz.ScaleType.LINEAR
        elif isinstance(param_config.sampler, sample.LogUniform):
          scale_type = vz.ScaleType.LOG
        elif isinstance(param_config.sampler, sample.Normal):
          raise ValueError(
              f'Normal sampler is not supported: {param_name}: {param_config}'
          )
        else:
          raise ValueError(
              f'Unknown sampler type encountered: {param_name}: {param_config}'
          )

      # Add the parameter to the search space.
      if isinstance(param_config, sample.Function):
        raise ValueError('Must use tune defined types. Functions not supported')
      elif isinstance(param_config, sample.Float):
        space.root.add_float_param(
            param_name,
            min_value=param_config.lower,
            max_value=param_config.upper,
            scale_type=scale_type,
        )
      elif isinstance(param_config, sample.Integer):
        space.root.add_int_param(
            param_name,
            min_value=param_config.lower,
            max_value=param_config.upper - 1,
        )
      elif isinstance(param_config, sample.Categorical):
        if not all([isinstance(c, str) for c in param_config.categories]):
          raise ValueError('Only string values are supported for categories')
        space.root.add_categorical_param(
            param_name, feasible_values=list(map(str, param_config.categories))
        )
      else:
        raise ValueError(
            f'Unsupported config encountered: {param_name}: {param_config}'
        )
    return space


class ExperimenterConverter:
  """Converts Experimenters to Ray Trainables."""

  @classmethod
  def to_callable(
      cls,
      experimenter: experimenters.Experimenter,
  ) -> Callable[[vz.ParameterDict], Dict[str, float]]:
    """Returns a callable that takes Parameters and returns metric values."""

    def trainable(config: vz.ParameterDict) -> Dict[str, float]:
      trial = vz.Trial(parameters=config)
      experimenter.evaluate([trial])
      if trial.final_measurement is None:
        raise ValueError(f'No final_measurement on trial{trial.id}')
      return {k: v.value for k, v in trial.final_measurement.metrics.items()}

    return trainable


--- vizier/_src/raytune/converters_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Test for converters. Cannot be tested internally but can be on GitHub."""

# pytype: skip-file
from ray import tune
from vizier import pyvizier as vz
from vizier._src.raytune import converters
from vizier.benchmarks import experimenters

from absl.testing import absltest


class ConvertersTest(absltest.TestCase):

  def test_to_dict(self):
    param_space = {
        'float': tune.uniform(1, 2),
        'float_log': tune.loguniform(2, 3),
        'int': tune.randint(4, 6),
        'choice': tune.choice(['a', 'b', 'c']),
    }
    search_space = converters.SearchSpaceConverter.to_vizier(param_space)
    self.assertIsInstance(search_space, vz.SearchSpace)
    self.assertLen(search_space.parameters, 4)

    config = search_space.get('float')
    self.assertEqual(config.type, vz.ParameterType.DOUBLE)
    self.assertEqual(config.bounds, (1, 2))
    self.assertEqual(config.scale_type, vz.ScaleType.LINEAR)

    config = search_space.get('float_log')
    self.assertEqual(config.type, vz.ParameterType.DOUBLE)
    self.assertEqual(config.bounds, (2, 3))
    self.assertEqual(config.scale_type, vz.ScaleType.LOG)

    config = search_space.get('int')
    self.assertEqual(config.type, vz.ParameterType.INTEGER)
    self.assertEqual(config.bounds, (4, 5))

    config = search_space.get('choice')
    self.assertEqual(config.type, vz.ParameterType.CATEGORICAL)
    self.assertEqual(config.feasible_values, ['a', 'b', 'c'])

  def test_run_study_with_search_space(self):
    space = vz.SearchSpace()
    root = space.select_root()
    root.add_float_param('uniform', 0.5, 1.5)
    root.add_float_param('loguniform', 0.1, 2.5, scale_type=vz.ScaleType.LOG)
    root.add_int_param('int_uniform', 1, 10)
    root.add_discrete_param('discrete', [1.0, 2.3, 0.5])
    root.add_categorical_param('categorical', ['a', 'b', 'c'])

    def trainable(config):  # Pass a "config" dictionary into your trainable.
      score = config['uniform'] * config['loguniform'] + config['int_uniform']
      if config['categorical'] == 'a':
        score += config['discrete']
      else:
        score -= config['discrete']

      return {'score': score}

    param_space = converters.SearchSpaceConverter.to_dict(space)
    tuner = tune.Tuner(
        trainable,
        param_space=param_space,
        tune_config=tune.TuneConfig(num_samples=10),
    )
    tuner.fit()
    self.assertLen(tuner.get_results(), 10)

  def test_conversion(self):
    dim = 2
    bbob_factory = experimenters.BBOBExperimenterFactory(name='Sphere', dim=dim)
    exptr = bbob_factory()
    search_space = exptr.problem_statement().search_space
    param_space = converters.SearchSpaceConverter.to_dict(search_space)
    trainable = converters.ExperimenterConverter.to_callable(exptr)

    config = {k: 1.5 for k in param_space.keys()}
    output_dict = trainable(config)
    self.assertEqual(type(output_dict['bbob_eval']), float)

  def test_run_study_with_experimenter(self):
    dim = 4
    bbob_factory = experimenters.BBOBExperimenterFactory(name='Sphere', dim=dim)
    exptr = bbob_factory()

    search_space = exptr.problem_statement().search_space
    self.assertLen(search_space.parameters, dim)

    param_space = converters.SearchSpaceConverter.to_dict(search_space)
    trainable = converters.ExperimenterConverter.to_callable(exptr)
    tuner = tune.Tuner(
        trainable,
        param_space=param_space,
        tune_config=tune.TuneConfig(num_samples=10),
    )
    tuner.fit()
    self.assertLen(tuner.get_results(), 10)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/raytune/run_tune.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Running Ray Tuners: https://docs.ray.io/en/latest/ray-air/tuner.html."""

from typing import Any, Callable, List, Optional, Tuple

import numpy as np
from ray import air
from ray import data
from ray import tune
from ray.air import session
from vizier import pyvizier as vz
from vizier._src.raytune import converters
from vizier.benchmarks import experimenters


# See https://docs.ray.io/en/latest/data/dataset.html
def run_tune_distributed(
    run_tune_args_list: List[Tuple[Any]],
    run_tune: Callable[[Any], tune.result_grid.ResultGrid],
) -> List[tune.result_grid.ResultGrid]:
  """Distributes tuning via Ray datasets API for MapReduce purposes.

  NOTE: There are no datasets processed. However, all MapReduce operations
  are now done in the Datasets API under Ray.

  Args:
    run_tune_args_list: List of Tuples that are to be passed into run_tune.
    run_tune: Callable that accepts args from previous list.

  Returns:
    List of results.
  """
  ds = data.from_items([{'args_tuple': args} for args in run_tune_args_list])
  ds = ds.map(lambda x: {'result': run_tune(*x['args_tuple'])})
  return ds.take_all()


def run_tune_bbob(
    function_name: str,
    dimension: int,
    shift: Optional[np.ndarray] = None,
    tune_config: Optional[tune.TuneConfig] = None,
    run_config: Optional[air.RunConfig] = None,
) -> tune.result_grid.ResultGrid:
  """Runs Ray Tuners for BBOB problems.

  See https://docs.ray.io/en/latest/tune/key-concepts.html
  For more information on Tune and Run configs, see
  https://docs.ray.io/en/latest/ray-air/tuner.html

  Args:
    function_name: BBOB function name.
    dimension: Dimension of BBOB function.
    shift: Shift of BBOB function.
    tune_config: Ray Tune Config.
    run_config: Ray Run Config.

  Returns:
  """
  experimenter_factory = experimenters.BBOBExperimenterFactory(
      name=function_name, dim=dimension
  )
  if shift is not None:
    experimenter_factory = experimenters.SingleObjectiveExperimenterFactory(
        base_factory=experimenter_factory, shift=shift
    )
  return run_tune_from_factory(experimenter_factory, tune_config, run_config)


def run_tune_from_factory(
    experimenter_factory: experimenters.ExperimenterFactory,
    tune_config: Optional[tune.TuneConfig] = None,
    run_config: Optional[air.RunConfig] = None,
) -> tune.result_grid.ResultGrid:
  """Runs Ray Tuners from an Experimenter Factory.

  See https://docs.ray.io/en/latest/tune/key-concepts.html
  For more information on Tune and Run configs, see
  https://docs.ray.io/en/latest/ray-air/tuner.html

  Args:
    experimenter_factory: Experimenter Factory.
    tune_config: Ray Tune Config.
    run_config: Ray Run Config.

  Returns:
  """
  experimenter = experimenter_factory()
  problem = experimenter.problem_statement()

  param_space = converters.SearchSpaceConverter.to_dict(problem.search_space)
  objective = converters.ExperimenterConverter.to_callable(experimenter)

  metric_info = problem.metric_information.item()
  if tune_config is None:
    tune_config = tune.TuneConfig()
  tune_config.metric = metric_info.name
  if metric_info.goal == vz.ObjectiveMetricGoal.MINIMIZE:
    tune_config.mode = 'min'
  else:
    tune_config.mode = 'max'

  def objective_fn(config: vz.ParameterDict) -> None:
    # Config contains parameter names to values and is autopopulated for each
    # Trial. Evaluation is static for BBOB so we simply loop.
    for _ in range(tune_config.num_samples):
      result_dict = objective(config)
      session.report(result_dict)

  tuner = tune.Tuner(
      objective_fn,
      param_space=param_space,
      run_config=run_config,
      tune_config=tune_config,
  )
  return tuner.fit()


--- vizier/_src/raytune/run_tune_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Github test for run_tune."""
# pytype: skip-file
import itertools

import numpy as np
from ray import tune
from vizier._src.raytune import run_tune

from absl.testing import absltest


class RunTuneTest(absltest.TestCase):

  def test_simple_fit(self):
    # Uses random search by default.
    tune_config = tune.TuneConfig(num_samples=7, max_concurrent_trials=1)
    results = run_tune.run_tune_bbob(
        function_name='Sphere',
        dimension=5,
        shift=np.ones(5),
        tune_config=tune_config,
    )
    self.assertLen(results, 7)

  # TODO: Remove this when able to dedicate multi-threading.
  @absltest.skip('Parallelized fit fails due to multiple worker crashing.')
  def test_parallelized_fit(self):
    tune_config = tune.TuneConfig(num_samples=3)
    function_names = ['Sphere']
    dimensions = [4]
    product_list = list(itertools.product(function_names, dimensions))
    args_list = []
    for product in product_list:
      # Add args for shift, tune_config, run_config.
      args = product + (None, tune_config, None)
      args_list.append(args)
    results_list = run_tune.run_tune_distributed(
        args_list, run_tune.run_tune_bbob
    )

    # There should be 1 studies in the product, each with 3 Trials.
    self.assertLen(results_list, 1)
    for result in results_list:
      self.assertLen(result['result'], 3)

if __name__ == '__main__':
  absltest.main()


--- vizier/_src/raytune/vizier_search.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""A Vizier Ray Searcher."""

import datetime
import json
from typing import Dict, Optional
import uuid

from ray import tune
from ray.tune import search
from vizier._src.raytune import converters
from vizier.service import clients
from vizier.service import pyvizier as svz


class VizierSearch(search.Searcher):
  """An OSS Vizier Searcher for Ray."""

  def __init__(
      self,
      study_id: Optional[str] = None,
      problem: Optional[svz.StudyConfig] = None,
      algorithm: Optional[str] = 'DEFAULT',
      **kwargs,
  ):
    """Initialize a Searcher via ProblemStatement.

    To initialize VizierSearch via set_search_properties, do not set problem.

    Args:
      study_id: The study id in the Vizier service.
      problem: The study config to optimize over. `problem.algorithm` is
        overwritten by `algorithm`.
      algorithm: The Vizier algorithm to use. Overrides the algorithm in
        problem.
      **kwargs:
    """
    super().__init__(**kwargs)

    if study_id:
      self.study_id = study_id
    else:
      self.study_id = f'ray_vizier_{uuid.uuid1()}'

    self.algorithm = algorithm

    # Mapping from Ray trial id to Vizier Trial client.
    self._active_trials: Dict[str, clients.Trial] = {}

    # The name of the metric being optimized, for single objective studies.
    self._metric = None

    # Vizier service client.
    self.study_client: Optional[clients.Study] = None
    if problem:
      if not problem.is_single_objective:
        raise ValueError(
            f'Only single objective studies are supported: {problem}'
        )
      # We can't store StudyConfig since it contains a proto, and it's not
      # pickleable, so we store problem statement instead.
      # TODO: store a proto string instead.
      self._problem = problem.to_problem()
      self._metric = problem.metric_information.item().name
      if self.algorithm is None:
        self.algorithm = problem.algorithm

  def set_search_properties(
      self, metric: Optional[str], mode: Optional[str], config: Dict, **spec  # pylint: disable=g-bare-generic
  ) -> bool:
    """Pass search properties to searcher.

    This method acts as an alternative to instantiating search algorithms
    with their own specific search spaces. Instead they can accept a
    Tune config through this method. A searcher should return ``True``
    if setting the config was successful, or ``False`` if it was
    unsuccessful, e.g. when the search space has already been set.

    Args:
        metric: Metric to optimize
        mode: One of ["min", "max"]. Direction to optimize.
        config: Tune config dict.
        **spec: Any kwargs for forward compatiblity. Info like
          Experiment.PUBLIC_KEYS is provided through here.

    Returns:
      True on success, False on failure.
    """
    if self.study_client:
      # The study is already configured.
      return False

    if mode not in ['min', 'max']:
      raise ValueError("'mode' must be one of ['min', 'max']")

    self._metric = metric or tune.result.DEFAULT_METRIC

    search_space = converters.SearchSpaceConverter.to_vizier(config)
    vizier_goal = (
        svz.ObjectiveMetricGoal.MAXIMIZE
        if mode == 'max'
        else svz.ObjectiveMetricGoal.MINIMIZE
    )
    study_config = svz.StudyConfig(
        search_space=search_space,
        algorithm=self.algorithm,
        metric_information=[
            svz.MetricInformation(self._metric, goal=vizier_goal)
        ],
    )
    self.study_client = clients.Study.from_study_config(
        study_config, owner='raytune', study_id=self.study_id
    )
    return True

  def on_trial_result(self, trial_id: str, result: Dict) -> None:  # pylint: disable=g-bare-generic
    if trial_id not in self._active_trials:
      raise RuntimeError(f'No active trial for {trial_id}')
    if self.study_client is None:
      raise RuntimeError(
          'VizierSearch not initialized! Set a search space first.'
      )
    trial_client = self._active_trials[trial_id]
    elapsed_secs = (
        datetime.datetime.now().astimezone()
        - trial_client.materialize().creation_time
    )
    trial_client.add_measurement(
        svz.Measurement(
            {k: v for k, v in result.items() if isinstance(v, float)},
            elapsed_secs=elapsed_secs.total_seconds(),
        )
    )

  def on_trial_complete(
      self, trial_id: str, result: Optional[Dict] = None, error: bool = False  # pylint: disable=g-bare-generic
  ) -> None:
    if trial_id not in self._active_trials:
      raise RuntimeError(f'No active trial for {trial_id}')
    if self.study_client is None:
      raise RuntimeError(
          'VizierSearch not initialized! Set a search space first.'
      )
    trial_client = self._active_trials[trial_id]

    if error:
      # Mark the trial as infeasible.
      trial_client.complete(
          infeasible_reason=f'Trial {trial_id} failed: {result}'
      )
    else:
      measurement = None
      if result:
        elapsed_secs = (
            datetime.datetime.now().astimezone()
            - trial_client.materialize().creation_time
        )
        measurement = svz.Measurement(
            {k: v for k, v in result.items() if isinstance(v, float)},
            elapsed_secs=elapsed_secs.total_seconds(),
        )
      trial_client.complete(measurement=measurement)
    self._active_trials.pop(trial_id)

  def suggest(self, trial_id):
    if self.study_client is None:
      study_config = svz.StudyConfig.from_problem(self._problem)
      study_config.algorithm = self.algorithm
      self.study_client = clients.Study.from_study_config(
          study_config, owner='raytune', study_id=self.study_id
      )
    suggestions = self.study_client.suggest(count=1, client_id=trial_id)
    if not suggestions:
      return search.Searcher.FINISHED

    self._active_trials[trial_id] = suggestions[0]
    return self._active_trials[trial_id].parameters

  # TODO: Test save and restore.
  def save(self, checkpoint_path):
    # We assume that the Vizier service continues running, so the only
    # information needed to restore this searcher is the mapping from the Ray
    # to Vizier trial ids. All other information can become stale and is best
    # restored from the Vizier service in restore().
    ray_to_vizier_trial_ids = {}
    for trial_id, trial_client in self._active_trials.items():
      ray_to_vizier_trial_ids[trial_id] = trial_client.id
    with open(checkpoint_path, 'w') as f:
      json.dump(
          {
              'study_id': self.study_id,
              'ray_to_vizier_trial_ids': ray_to_vizier_trial_ids,
          },
          f,
      )

  def restore(self, checkpoint_path):
    with open(checkpoint_path, 'r') as f:
      obj = json.load(f)

    self.study_id = obj['study_id']
    self.study_client = clients.Study.from_owner_and_id(
        'raytune', self.study_id
    )
    self._metric = (
        self.study_client.materialize_study_config().metric_information.item()
    )
    self._active_trials = {}
    for ray_id, vizier_trial_id in obj['ray_to_vizier_trial_ids'].items():
      self._active_trials[ray_id] = self.study_client.get_trial(vizier_trial_id)


--- vizier/_src/raytune/vizier_search_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Test for VizierSearch. Cannot be tested internally but can be on GitHub."""
from ray import tune
from vizier._src.raytune import converters
from vizier._src.raytune import run_tune
from vizier._src.raytune import vizier_search
from vizier.benchmarks import experimenters
from vizier.service import clients
from vizier.service import pyvizier as vz

from absl.testing import absltest


# Required to use local Vizier service.
clients.environment_variables.servicer_use_sql_ram()


class VizierSearchTest(absltest.TestCase):

  def test_search_with_study_config(self):
    dim = 4
    bbob_factory = experimenters.BBOBExperimenterFactory(name='Sphere', dim=dim)
    exptr = bbob_factory()

    study_config = vz.StudyConfig.from_problem(exptr.problem_statement())
    self.assertLen(study_config.search_space.parameters, dim)
    searcher = vizier_search.VizierSearch(
        'test study', study_config, algorithm='RANDOM_SEARCH'
    )

    trainable = converters.ExperimenterConverter.to_callable(exptr)
    tuner = tune.Tuner(
        trainable,
        param_space=None,
        tune_config=tune.TuneConfig(num_samples=5, search_alg=searcher),
    )
    tuner.fit()
    self.assertLen(tuner.get_results(), 5)

  # def test_search_with_ray_search_space(self):
  #  dim = 4
  #  bbob_factory = experimenters.BBOBExperimenterFactory(name='Sphere',
  # dim=dim)
  #  exptr = bbob_factory()
  #
  #  trainable = converters.ExperimenterConverter.to_callable(exptr)

  #  tuner = tune.Tuner(
  #      trainable,
  #      param_space=None,
  #      tune_config=tune.TuneConfig(
  #          num_samples=10, search_alg=vizier_search.VizierSearch(),
  # max_concurrent_trials=1),
  #  )
  #  tuner.fit()
  #  self.assertLen(tuner.get_results(), 10)

  def test_random_search_with_run_tune(self):
    results = run_tune.run_tune_bbob(
        function_name='Sphere',
        dimension=3,
        shift=None,
        tune_config=tune.TuneConfig(
            search_alg=vizier_search.VizierSearch(algorithm='RANDOM_SEARCH'),
            num_samples=9,
            max_concurrent_trials=1,
        ),
        run_config=None,
    )
    self.assertLen(results, 9)

  def test_vizier_search_with_run_tune(self):
    # Use the default algorithm = GP Bandit.
    results = run_tune.run_tune_bbob(
        function_name='Sphere',
        dimension=3,
        shift=None,
        tune_config=tune.TuneConfig(
            search_alg=vizier_search.VizierSearch(),
            num_samples=5,
            max_concurrent_trials=1,
        ),
        run_config=None,
    )
    self.assertLen(results, 5)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/clients.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""OSS Vizier client."""

# TODO: Raise vizier-specific exceptions.

from typing import Any, Callable, Iterable, Iterator, List, Mapping, Optional, Type
import attr
from vizier._src.service import constants
from vizier._src.service import resources
from vizier._src.service import vizier_client
from vizier.client import client_abc
from vizier.service import pyvizier as vz

# Redeclared so users do not have to also import client_abc and clients.py.
ResourceNotFoundError = client_abc.ResourceNotFoundError

# Redeclared since clients.py is the user-facing client API.
environment_variables = vizier_client.environment_variables

UNUSED_CLIENT_ID = constants.UNUSED_CLIENT_ID


@attr.define
class Trial(client_abc.TrialInterface):
  """Trial class.

  This class owns a Vizier client of the Study that contains the Trial that
  it is associated with.
  """

  _client: vizier_client.VizierClient = attr.field()
  _id: int = attr.field(validator=attr.validators.instance_of(int))

  @property
  def id(self) -> int:
    return self._id

  @property
  def parameters(self) -> Mapping[str, Any]:
    trial = self.materialize(include_all_measurements=False)
    study_config = self._client.get_study_config()
    return study_config.trial_parameters(vz.TrialConverter.to_proto(trial))

  def delete(self) -> None:
    self._client.delete_trial(self._id)

  def update_metadata(self, delta: vz.Metadata) -> None:
    actual_delta = vz.MetadataDelta(on_trials={self._id: delta})
    self._client.update_metadata(actual_delta)

  def complete(
      self,
      measurement: Optional[vz.Measurement] = None,
      *,
      infeasible_reason: Optional[str] = None,
  ) -> Optional[vz.Measurement]:
    self._trial = self._client.complete_trial(
        self._id, measurement, infeasible_reason
    )
    return self._trial.final_measurement

  def check_early_stopping(self) -> bool:
    return self._client.should_trial_stop(self._id)

  def stop(self) -> None:
    return self._client.stop_trial(self._id)

  def add_measurement(self, measurement: vz.Measurement) -> None:
    self._client.report_intermediate_objective_value(
        int(measurement.steps),
        measurement.elapsed_secs,
        [{k: v.value for k, v in measurement.metrics.items()}],
        trial_id=self._id,
    )

  def materialize(
      self,
      *,
      include_all_measurements: bool = True,
  ) -> vz.Trial:
    trial = self._client.get_trial(self._id)
    if not include_all_measurements:
      trial.measurements.clear()
    return trial

  @property
  def study(self) -> 'Study':
    return Study(self._client)


@attr.define
class TrialIterable(client_abc.TrialIterable):
  """Holds a collection of materialized Trials.

  See the parent class for full pydocs.
  """

  _iterable_factory: Callable[[], Iterable[vz.Trial]] = attr.field()
  _client: vizier_client.VizierClient = attr.field()

  def __iter__(self) -> Iterator[Trial]:
    for trial in self._iterable_factory():
      yield Trial(self._client, trial.id)

  def get(self) -> Iterator[vz.Trial]:
    for trial in self._iterable_factory():
      yield trial


@attr.define
class Study(client_abc.StudyInterface):
  """Responsible for study-level operations."""

  _client: vizier_client.VizierClient = attr.field()

  @property
  def resource_name(self) -> str:
    return self._client.study_resource_name

  def _trial_client(self, trial: vz.Trial) -> Trial:
    """Returns the client for the vz.Trial object."""
    return Trial(self._client, trial.id)

  def suggest(
      self, *, count: Optional[int] = None, client_id: str = 'default_client_id'
  ) -> List[Trial]:
    suggestions = self._client.get_suggestions(
        count, client_id_override=client_id
    )
    return [self._trial_client(t) for t in suggestions]

  def delete(self) -> None:
    self._client.delete_study()

  def update_metadata(self, delta: vz.Metadata) -> None:
    actual_delta = vz.MetadataDelta(on_study=delta)
    self._client.update_metadata(actual_delta)

  def add_trial(self, trial: vz.Trial) -> Trial:
    sc = self._client.get_study_config(self.resource_name)
    sc.search_space.assert_contains(trial.parameters)
    return self._trial_client(self._client.add_trial(trial))

  def request(self, suggestion: vz.TrialSuggestion) -> Trial:
    trial = suggestion.to_trial()
    trial.is_requested = True
    return self._trial_client(self._client.add_trial(trial))

  def trials(
      self, trial_filter: Optional[vz.TrialFilter] = None
  ) -> TrialIterable:
    all_trials = self._client.list_trials()
    trial_filter = trial_filter or vz.TrialFilter()

    def iterable_factory():
      for t in filter(trial_filter, all_trials):
        yield t

    return TrialIterable(iterable_factory, self._client)

  def get_trial(self, trial_id: int) -> Trial:
    try:
      # Check if the trial actually exists.
      trial = self._client.get_trial(trial_id)
      return self._trial_client(trial)
    except KeyError as err:
      raise ResourceNotFoundError(
          f'Study {self.resource_name} does not have Trial {trial_id}.'
      ) from err

  def optimal_trials(self, count: Optional[int] = None) -> TrialIterable:
    if count is None:
      trials = self._client.list_optimal_trials()
    else:
      raise ValueError('Count not supported.')
    return TrialIterable(lambda: trials, self._client)

  def materialize_problem_statement(self) -> vz.ProblemStatement:
    """Returns a cross-platform compatible minimal StudyConfig."""
    return self._client.get_study_config().to_problem()

  def materialize_study_config(self) -> vz.StudyConfig:
    """Returns a fully specific StudyConfig."""
    return self._client.get_study_config(self.resource_name)

  def set_state(self, state: vz.StudyState) -> None:
    self._client.set_study_state(state)

  def materialize_state(self) -> vz.StudyState:
    return self._client.get_study_state()

  @classmethod
  def from_resource_name(cls: Type['Study'], name: str) -> 'Study':
    client = vizier_client.VizierClient(name, UNUSED_CLIENT_ID)
    try:
      _ = client.get_study_config()  # Make sure study exists.
    except Exception as err:
      raise ResourceNotFoundError(f'Study {name} does not exist.') from err
    return Study(client)

  @classmethod
  def from_owner_and_id(
      cls: Type['Study'], owner: str, study_id: str
  ) -> 'Study':
    """Create study from `owner` and `study_id`.

    Args:
      owner: Owner of the study.
      study_id: Unique identifier within the same owner.

    Returns:
      Study.

    Raises:
      ResourceNotFoundError.
    """
    resource = resources.StudyResource(owner, study_id)
    return cls.from_resource_name(resource.name)

  @classmethod
  def from_study_config(
      cls, config: vz.StudyConfig, *, owner: str, study_id: str
  ) -> 'Study':
    """Create study from StudyConfig.

    Args:
      config: OSS Study configuration. It is platform-specific. It is ignored if
        `owner` already has Study with `study_id`. # TODO: Instead
        of ignoring it, compare with the existing # study and return error if
        there's a change.
      owner: Owner of the study.
      study_id: Unique identifier within the same owner.

    Returns:
      Study.
    """
    # Use a dummy client_id, since for multi-worker workflows, each worker is
    # expected to provide a client_id in the suggest() call.
    client = vizier_client.create_or_load_study(
        owner_id=owner,
        client_id=UNUSED_CLIENT_ID,
        study_id=study_id,
        study_config=config,
    )
    return Study(client)


--- vizier/_src/service/clients_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for clients."""
import functools

from absl import logging
from vizier._src.service import clients
from vizier._src.service import constants
from vizier._src.service import vizier_server
from vizier.client import client_abc_testing
from vizier.service import pyvizier as vz

from absl.testing import absltest


# Affects local Vizier servicer tests only.
clients.environment_variables.servicer_use_sql_ram()


class VizierClientTest(client_abc_testing.TestCase):
  _owner: str

  def create_study(
      self, problem: vz.ProblemStatement, study_id: str
  ) -> clients.Study:
    config = vz.StudyConfig.from_problem(problem)
    config.algorithm = vz.Algorithm.RANDOM_SEARCH
    study = clients.Study.from_study_config(
        config, owner='owner', study_id=study_id
    )
    return study

  def test_e2e_tuning(self):
    self.assertPassesE2ETuning()

  def test_e2e_tuning_gp(self):
    def create_study(
        problem: vz.ProblemStatement, study_id: str
    ) -> clients.Study:
      config = vz.StudyConfig.from_problem(problem)
      config.algorithm = vz.Algorithm.GAUSSIAN_PROCESS_BANDIT
      study = clients.Study.from_study_config(
          config, owner='owner', study_id=study_id
      )
      return study

    study_factory = functools.partial(create_study, study_id=self.id())
    self.assertPassesE2ETuning(study_factory=study_factory, num_iterations=2)


class VizierClientTestOnServicer(VizierClientTest):

  @classmethod
  def setUpClass(cls):
    logging.info('Test setup started.')
    super().setUpClass()
    clients.environment_variables.server_endpoint = constants.NO_ENDPOINT
    logging.info('Test setup finished.')


class VizierClientTestOnDefaultServer(VizierClientTest):
  _server: vizier_server.DefaultVizierServer

  @classmethod
  def setUpClass(cls):
    logging.info('Test setup started.')
    super().setUpClass()
    cls._server = vizier_server.DefaultVizierServer(
        database_url=constants.SQL_MEMORY_URL
    )
    clients.environment_variables.server_endpoint = cls._server.endpoint
    logging.info('Test setup finished.')

  @classmethod
  def tearDownClass(cls):
    cls._server._server.stop(None)
    super().tearDownClass()


class VizierClientTestOnDistributedPythiaServer(VizierClientTest):
  _server: vizier_server.DistributedPythiaVizierServer

  @classmethod
  def setUpClass(cls):
    logging.info('Test setup started.')
    super().setUpClass()
    cls._server = vizier_server.DistributedPythiaVizierServer(
        database_url=constants.SQL_MEMORY_URL
    )
    clients.environment_variables.server_endpoint = cls._server.endpoint
    logging.info('Test setup finished.')

  @classmethod
  def tearDownClass(cls):
    cls._server._server.stop(None)
    cls._server._pythia_server.stop(None)
    super().tearDownClass()


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/constants.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Hard coded constants used in /service/ folder."""
import os

# The metadata namespace under which the Pythia endpoint is stored.
PYTHIA_ENDPOINT_NAMESPACE = 'service'

# The Study.metadata key where a Pythia endpoint can be stored.
PYTHIA_ENDPOINT_KEY = 'PYTHIA_ENDPOINT'

# A hard coded value which indicates that the Pythia endpoint is not set.
NO_ENDPOINT = 'NO_ENDPOINT'

# Default client ID.
UNUSED_CLIENT_ID = 'unused_client_id'

# Max int32 value.
MAX_STUDY_ID = 2147483647

# Will use RAM for SQL memory.
SQL_MEMORY_URL = 'sqlite:///:memory:'

# Will use local file path for HDD storage.
SERVICE_DIR = os.path.dirname(os.path.realpath(__file__))
VIZIER_DB_PATH = os.path.join(SERVICE_DIR, 'vizier.db')
SQL_LOCAL_URL = f'sqlite:///{VIZIER_DB_PATH}'


--- vizier/_src/service/custom_errors.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Custom Errors used in service."""


class AlreadyExistsError(ValueError):
  """The resource key already exists in the database."""

  pass


class NotFoundError(KeyError):
  """The resource key does not exist in the database."""

  pass


class ImmutableStudyError(ValueError):
  """The study is now immutable and cannot be modified."""

  pass


class ImmutableTrialError(ValueError):
  """The trial is now immutable and cannot be modified."""

  pass


--- vizier/_src/service/datastore.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Contains datastore abstraction, used for saving study + trial data.

See resources.py for naming conventions.
"""
import abc
from typing import Callable, Iterable, List, Optional

from vizier._src.service import key_value_pb2
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2
from vizier._src.service import vizier_service_pb2
from google.longrunning import operations_pb2

UnitMetadataUpdate = vizier_service_pb2.UnitMetadataUpdate


class DataStore(abc.ABC):
  """Abstract class for data storage.

  Input/Outputs should always be pass-by-value.
  """

  @abc.abstractmethod
  def create_study(self, study: study_pb2.Study) -> resources.StudyResource:
    """Creates study in database. If preexisting, raises AlreadyExistsError."""

  @abc.abstractmethod
  def load_study(self, study_name: str) -> study_pb2.Study:
    """Loads study from the database. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def update_study(self, study: study_pb2.Study) -> resources.StudyResource:
    """Updates pre-existing study. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def delete_study(self, study_name: str) -> None:
    """Deletes study from database. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def list_studies(self, owner_name: str) -> List[study_pb2.Study]:
    """Lists all studies under owner.

    Args:
      owner_name: Name of owner.

    Returns:
      List of Studies associated with owner.

    Raises:
      NotFoundError: If the owner does not exist.
    """

  @abc.abstractmethod
  def create_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    """Stores trial in database.

    Args:
      trial: Trial to be stored.

    Returns:
      The newly created trial resource name.

    Raises:
      AlreadyExistsError: If trial already exists.
    """

  @abc.abstractmethod
  def get_trial(self, trial_name: str) -> study_pb2.Trial:
    """Retrieves trial from database. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def update_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    """Updates pre-existing trial. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def list_trials(self, study_name: str) -> List[study_pb2.Trial]:
    """List all trials for study. If study nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def delete_trial(self, trial_name: str) -> None:
    """Deletes trial from database. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def max_trial_id(self, study_name: str) -> int:
    """Maximal trial ID in study (defaults to 0 if no trials exist).

    Args:
      study_name: Name of study.

    Returns:
      Maximum trial ID as an int.

    Raises:
      NotFoundError: If the study does not exist.
    """

  @abc.abstractmethod
  def create_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    """Stores suggestion operation.

    Args:
      operation:

    Returns:
      Resource for created op.

    Raises:
      AlreadyExistsError: If the suggest op already exists.
    """

  @abc.abstractmethod
  def get_suggestion_operation(
      self, operation_name: str
  ) -> operations_pb2.Operation:
    """Retrieves suggestion operation. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def update_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    """Updates pre-existing suggestion op.

    Args:
      operation: New suggestion speration.

    Returns:
      Resource to operation.

    Raises:
      NotFoundError if suggest op is nonexistent.
    """

  @abc.abstractmethod
  def list_suggestion_operations(
      self,
      study_name: str,
      client_id: str,
      filter_fn: Optional[Callable[[operations_pb2.Operation], bool]] = None,
  ) -> List[operations_pb2.Operation]:
    """Retrieve all suggestion op from client.

    Args:
      study_name: Associated study for the suggest op.
      client_id: Associated client for the suggest op.
      filter_fn: Optional function to filter out the suggest ops.

    Raises:
      NotFoundError: If study or client is nonexistent.
    """

  @abc.abstractmethod
  def max_suggestion_operation_number(
      self, study_name: str, client_id: str
  ) -> int:
    """Maximal suggestion number for given client.

    Args:
      study_name: Name of associated owners.
      client_id: ID of associated client.

    Returns:
      Maximum suggest op number as an int.

    Raises:
      NotFoundError: If owner or client is nonexistent.
    """

  @abc.abstractmethod
  def create_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    """Stores early stopping operation.

    Args:
      operation: Operation to be stored.

    Returns:
      The newly created op resource name.

    Raises:
      AlreadyExistsError: If the suggest op already exists.
    """

  @abc.abstractmethod
  def get_early_stopping_operation(
      self, operation_name: str
  ) -> vizier_oss_pb2.EarlyStoppingOperation:
    """Retrieves early stopping op. If nonexistent, raises NotFoundError."""

  @abc.abstractmethod
  def update_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    """Updates preexisting early stopping op.

    Args:
      operation: New version of EarlyStoppingOp.

    Returns:
      Resource to the EarlyStoppingOp.

    Raises:
      NotFoundError: If early stopping op is nonexistent.
    """

  # TODO: Simplify the API by taking MetadataUpdateRequest proto
  # as input.
  @abc.abstractmethod
  def update_metadata(
      self,
      study_name: str,
      study_metadata: Iterable[key_value_pb2.KeyValue],
      trial_metadata: Iterable[UnitMetadataUpdate],
  ) -> None:
    """Store the supplied metadata in the database.

    Args:
      study_name: (Typically derived from a StudyResource.)
      study_metadata: Metadata to attach to the Study as a whole.
      trial_metadata: Metadata to attach to Trials.

    Raises:
      NotFoundError: If the update fails because of an attempt to attach
        metadata to a nonexistent Trial.
    """


--- vizier/_src/service/datastore_test_lib.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Library functions for testing databases."""
import copy
import random
import string
from typing import List

from vizier._src.service import custom_errors
from vizier._src.service import datastore
from vizier._src.service import key_value_pb2
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2
from vizier._src.service import vizier_service_pb2

from google.longrunning import operations_pb2
from absl.testing import parameterized

UnitMetadataUpdate = vizier_service_pb2.UnitMetadataUpdate


def make_random_string() -> str:
  return ''.join(random.choice(string.ascii_lowercase) for _ in range(10))


class DataStoreTestCase(parameterized.TestCase):
  """Base class for testing datastores."""

  def assertStudyAPI(self, ds: datastore.DataStore, study: study_pb2.Study):
    """Tests if the datastore handles studies correctly."""
    ds.create_study(study)
    with self.assertRaises(custom_errors.AlreadyExistsError):
      ds.create_study(study)  # Can't make another study w/ same name.

    copied_study = ds.load_study(study.name)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.load_study(study.name + 'does_not_exist')  # Non-existent study.
    self.assertEqual(copied_study, study)
    self.assertIsNot(copied_study, study)  # Check pass-by-value.

    owner_name = resources.StudyResource.from_name(
        study.name
    ).owner_resource.name
    list_of_one_study = ds.list_studies(owner_name)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.list_studies(owner_name + 'does_not_exist')  # Non-existent study.
    self.assertLen(list_of_one_study, 1)
    self.assertEqual(list_of_one_study[0], study)
    self.assertIsNot(list_of_one_study[0], study)  # Check pass-by-value.

    study.state = study_pb2.Study.State.COMPLETED
    ds.update_study(study)
    self.assertEqual(ds.load_study(study.name), study)
    self.assertIsNot(ds.load_study(study.name), study)  # Check pass-by-value.
    with self.assertRaises(custom_errors.NotFoundError):
      missing_study = copy.deepcopy(study)
      missing_study.name += str('i_dont_exist')
      ds.update_study(missing_study)  # Does not exist.

    study.inactive_reason = make_random_string()
    copied_original_study = ds.load_study(study.name)
    self.assertNotEqual(study, copied_original_study)  # Check pass-by-value.

    ds.delete_study(study.name)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.delete_study(study.name)  # Deleting non-existent study.
    with self.assertRaises(custom_errors.NotFoundError):
      ds.load_study(study.name)  # Non-existent study.

    # Owner should be kept track of.
    expected_studies = ds.list_studies(owner_name)
    self.assertEmpty(expected_studies)

  def assertTrialAPI(
      self,
      ds: datastore.DataStore,
      study: study_pb2.Study,
      trials: List[study_pb2.Trial],
  ):
    """Tests if the datastore handles trials correctly."""
    num_trials = len(trials)

    ds.create_study(study)
    empty_list = ds.list_trials(study.name)
    self.assertEmpty(empty_list)

    expected_max_trial_id = ds.max_trial_id(study.name)
    self.assertEqual(expected_max_trial_id, 0)

    for trial in trials:
      ds.create_trial(trial)
      with self.assertRaises(custom_errors.AlreadyExistsError):
        ds.create_trial(trial)  # Already exists.
      copied_trial = ds.get_trial(trial.name)
      with self.assertRaises(custom_errors.NotFoundError):
        ds.get_trial(trial.name + str(num_trials))  # Does not exist.
      self.assertEqual(trial, copied_trial)
      self.assertIsNot(trial, copied_trial)  # Check pass-by-value.

    self.assertLen(trials, ds.max_trial_id(study.name))
    with self.assertRaises(custom_errors.NotFoundError):
      ds.max_trial_id(study.name + 'does_not_exist')  # Does not exist.

    list_of_trials = ds.list_trials(study.name)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.list_trials(study.name + 'does_not_exist')  # Does not exist.
    self.assertEqual(list_of_trials, trials)
    self.assertIsNot(list_of_trials, trials)  # Check pass-by-value.

    first_trial = trials[0]
    first_trial.infeasible_reason = make_random_string()
    ds.update_trial(first_trial)
    with self.assertRaises(custom_errors.NotFoundError):
      missing_trial = copy.deepcopy(first_trial)
      missing_trial.name += str(num_trials)
      ds.update_trial(missing_trial)  # Does not exist.
    new_first_trial = ds.get_trial(first_trial.name)
    self.assertEqual(first_trial, new_first_trial)
    self.assertIsNot(first_trial, new_first_trial)  # Check pass-by-value.

    ds.delete_trial(first_trial.name)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.delete_trial(first_trial.name)  # Already deleted.
      ds.delete_trial(first_trial.name + str(num_trials))  # Does not exist.
    leftover_trials = ds.list_trials(study.name)
    self.assertEqual(leftover_trials, trials[1:])
    self.assertIsNot(leftover_trials, trials[1:])  # Check pass-by-value.

  def assertSuggestOpAPI(
      self,
      ds: datastore.DataStore,
      study: study_pb2.Study,
      client_id: str,
      suggestion_ops: List[operations_pb2.Operation],
  ):
    """Tests if the datastore handles suggest ops correctly."""
    study_resource = resources.StudyResource.from_name(study.name)

    ds.create_study(study)
    for operation in suggestion_ops:
      ds.create_suggestion_operation(operation)
      with self.assertRaises(custom_errors.AlreadyExistsError):
        ds.create_suggestion_operation(operation)  # Already exists.

    self.assertLen(
        suggestion_ops,
        ds.max_suggestion_operation_number(study_resource.name, client_id),
    )

    with self.assertRaises(custom_errors.NotFoundError):
      ds.max_suggestion_operation_number(
          study_resource.name, client_id + 'does_not_exist'
      )  # Client doesn't exist.

    list_of_operations = ds.list_suggestion_operations(
        study_resource.name, client_id
    )
    self.assertEqual(list_of_operations, suggestion_ops)
    with self.assertRaises(custom_errors.NotFoundError):
      ds.list_suggestion_operations(
          study_resource.name, client_id + 'does_not_exist'
      )  # Client doesn't exist.

    output_op = ds.get_suggestion_operation(
        resources.SuggestionOperationResource(
            study_resource.owner_id,
            study_resource.study_id,
            client_id,
            operation_number=1,
        ).name
    )
    self.assertEqual(output_op, suggestion_ops[0])
    self.assertIsNot(output_op, suggestion_ops[0])  # Check pass-by-value.

    with self.assertRaises(custom_errors.NotFoundError):
      ds.get_suggestion_operation(
          resources.SuggestionOperationResource(
              study_resource.owner_id,
              study_resource.study_id,
              client_id + 'does_not_exist',  # Client doesn't exist.
              operation_number=1,
          ).name
      )

    output_op.metadata.type_url = make_random_string()
    ds.update_suggestion_operation(output_op)
    new_output_op = ds.get_suggestion_operation(output_op.name)
    self.assertEqual(output_op, new_output_op)

    wrong_output_op = copy.deepcopy(output_op)
    wrong_output_op.name = resources.SuggestionOperationResource(
        study_resource.owner_id,
        study_resource.study_id,
        client_id + 'does_not_exist',  # Client doesn't exist.
        operation_number=1,
    ).name
    with self.assertRaises(custom_errors.NotFoundError):
      ds.update_suggestion_operation(wrong_output_op)

  def assertEarlyStoppingAPI(
      self,
      ds: datastore.DataStore,
      study: study_pb2.Study,
      trials: List[study_pb2.Trial],
      early_stopping_ops: List[vizier_oss_pb2.EarlyStoppingOperation],
  ):
    """Tests if the datastore handles early stopping ops correctly."""
    study_resource = resources.StudyResource.from_name(study.name)
    ds.create_study(study)

    for trial in trials:
      ds.create_trial(trial)

    for operation in early_stopping_ops:
      ds.create_early_stopping_operation(operation)
      with self.assertRaises(custom_errors.AlreadyExistsError):
        ds.create_early_stopping_operation(operation)  # Op already exists.

    output_op = ds.get_early_stopping_operation(
        resources.EarlyStoppingOperationResource(
            study_resource.owner_id, study_resource.study_id, 1
        ).name
    )
    self.assertEqual(output_op, early_stopping_ops[0])
    self.assertIsNot(output_op, early_stopping_ops[0])  # Check pass-by-value.

    wrong_op_name = resources.EarlyStoppingOperationResource(
        study_resource.owner_id,
        study_resource.study_id + 'does_not_exist',  # Study doesn't exist.
        1,
    ).name
    with self.assertRaises(custom_errors.NotFoundError):
      ds.get_early_stopping_operation(wrong_op_name)

    output_op.failure_message = make_random_string()
    ds.update_early_stopping_operation(output_op)
    new_output_op = ds.get_early_stopping_operation(output_op.name)
    self.assertEqual(output_op, new_output_op)

    wrong_output_op = copy.deepcopy(output_op)
    wrong_output_op.name = resources.EarlyStoppingOperationResource(
        study_resource.owner_id,
        study_resource.study_id + 'does_not_exist',  # Study doesn't exist.
        1,
    ).name
    with self.assertRaises(custom_errors.NotFoundError):
      ds.update_early_stopping_operation(wrong_output_op)

  def assertUpdateMetadataAPI(
      self,
      ds: datastore.DataStore,
      study: study_pb2.Study,
      trials: List[study_pb2.Trial],
  ):
    """Tests if the datastore handles metadata updates properly."""
    #   We start with two metadata items in the Trial:  'd':'D' and 'e':'E',
    #   and we merge in two metadata items in $new_metadata:
    #   {'d':'Dnew' and 'f': 'Fnew'}.  The result should overwrite 'd' and
    #   be {'d':'Dnew', 'e':'E', 'f':'Fnew'}.
    study.study_spec.metadata.add(ns='s', key='d', value='D')
    study.study_spec.metadata.add(ns='s', key='e', value='E')
    ds.create_study(study)
    for trial in trials:
      trial.metadata.add(ns='t', key='d', value='D')
      trial.metadata.add(ns='t', key='e', value='E')
      ds.create_trial(trial)
    study_metadata = [
        key_value_pb2.KeyValue(key='d', ns='s', value='Dnew'),
        key_value_pb2.KeyValue(key='f', ns='s', value='Fnew'),
    ]
    trial_metadata = [
        UnitMetadataUpdate(
            trial_id='1',
            metadatum=key_value_pb2.KeyValue(key='d', ns='t', value='Dnew'),
        ),
        UnitMetadataUpdate(
            trial_id='1',
            metadatum=key_value_pb2.KeyValue(key='f', ns='t', value='Fnew'),
        ),
    ]
    ds.update_metadata(study.name, study_metadata, trial_metadata)
    mutated_study_config = ds.load_study(study.name).study_spec
    self.assertEqual(
        list(mutated_study_config.metadata),
        [
            key_value_pb2.KeyValue(ns='s', key='d', value='Dnew'),
            key_value_pb2.KeyValue(ns='s', key='e', value='E'),
            key_value_pb2.KeyValue(ns='s', key='f', value='Fnew'),
        ],
    )
    mutated_trial = ds.get_trial(trials[0].name)
    self.assertEqual(mutated_trial.id, str(trial_metadata[0].trial_id))
    self.assertEqual(
        list(mutated_trial.metadata),
        [
            key_value_pb2.KeyValue(ns='t', key='d', value='Dnew'),
            key_value_pb2.KeyValue(ns='t', key='e', value='E'),
            key_value_pb2.KeyValue(ns='t', key='f', value='Fnew'),
        ],
    )

    study_not_exist_name = study.name + 'i_dont_exist'
    with self.assertRaises(custom_errors.NotFoundError):
      ds.update_metadata(study_not_exist_name, [], [])


--- vizier/_src/service/grpc_util.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utilities for handling GRPC API."""

from typing import Optional
import grpc
from vizier._src.service import custom_errors


# TODO: Create a type Union[_InactiveRpcError, LocalRpcError]
class LocalRpcError(grpc.RpcError):
  """Class for imitating both a `grpc.ServiceContext` and a `grpc.RpcError`."""

  _code: grpc.StatusCode = grpc.StatusCode.UNKNOWN
  _details: Optional[str] = None

  def set_code(self, code: grpc.StatusCode) -> None:
    self._code = code

  def set_details(self, details: str) -> None:
    self._details = details

  def code(self) -> grpc.StatusCode:
    return self._code

  def details(self) -> Optional[str]:
    return self._details


# TODO: Use this for all service errors.
def handle_exception(
    e: Exception, context: Optional[grpc.ServicerContext] = None
) -> None:
  """Converts custom exception into correct context error code.

  The rules for gRPC are:

  1) In the remote case (servicer wrapped into a server), the context is
  automatically generated by gRPC. Calling `context.set_code()` will
  automatically trigger an ` _InactiveRpcError` on the client side, which can
  collect the code and details from said RPC error.

  2) In the local case (e.g. when using a `VizierServicer` only), contexts are
  not used and set to None. In order to imitate the behavior of 1), we will
  instead use `LocalRpcError` to imitate both the behavior of a
  `ServicerContext` for setting codes and details, AND as an `_InactiveRpcError`
  to be raised on the client side.

  Args:
    e: Exception to be wrapped.
    context: For collecting error code and details. Set to None in the local
      case.

  Raises:
    LocalRpcError: If in the local case (when context is None).
  """
  if context is None:
    context = LocalRpcError(e)

  if isinstance(
      e, (custom_errors.ImmutableStudyError, custom_errors.ImmutableTrialError)
  ):
    context.set_code(grpc.StatusCode.FAILED_PRECONDITION)
  elif isinstance(e, custom_errors.NotFoundError):
    context.set_code(grpc.StatusCode.NOT_FOUND)
  elif isinstance(e, custom_errors.AlreadyExistsError):
    context.set_code(grpc.StatusCode.ALREADY_EXISTS)
  else:
    context.set_code(grpc.StatusCode.UNKNOWN)

  context.set_details(str(e))

  if isinstance(context, LocalRpcError):
    raise context


--- vizier/_src/service/performance_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Large-scale stress tests (multiple clients, mulithreading, etc.) for Vizier Service."""

import multiprocessing.pool
import time

from absl import logging
from vizier._src.service import constants
from vizier._src.service import vizier_client
from vizier._src.service import vizier_server
from vizier.benchmarks import experimenters
from vizier.service import pyvizier

from absl.testing import absltest
from absl.testing import parameterized


class PerformanceTest(parameterized.TestCase):
  server: vizier_server.DefaultVizierServer

  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    cls.server = vizier_server.DefaultVizierServer(
        database_url=constants.SQL_MEMORY_URL
    )
    vizier_client.environment_variables.server_endpoint = cls.server.endpoint

  @parameterized.parameters((1, 10), (2, 10), (10, 10), (50, 5), (100, 5))
  def test_multiple_clients_basic(
      self, num_simultaneous_clients, num_trials_per_client
  ):
    def fn(client_id: int):
      experimenter = experimenters.BBOBExperimenterFactory('Sphere', 2)()
      problem_statement = experimenter.problem_statement()
      study_config = pyvizier.StudyConfig.from_problem(problem_statement)
      study_config.algorithm = pyvizier.Algorithm.RANDOM_SEARCH

      client = vizier_client.create_or_load_study(
          owner_id='my_username',
          study_id=self.id(),  # Use the testcase name.
          study_config=study_config,
          client_id=str(client_id),
      )

      for _ in range(num_trials_per_client):
        suggestions = client.get_suggestions(suggestion_count=1)
        experimenter.evaluate(suggestions)
        for completed_trial in suggestions:
          client.complete_trial(
              completed_trial.id, completed_trial.final_measurement
          )

      return client

    client_ids = range(num_simultaneous_clients)
    pool = multiprocessing.pool.ThreadPool(num_simultaneous_clients)

    start = time.time()
    clients = pool.map(fn, client_ids)
    end = time.time()
    pool.close()

    self.assertEqual(
        max({t.id for t in clients[0].list_trials()}),
        num_simultaneous_clients * num_trials_per_client,
    )

    logging.info(
        'For %d clients to evaluate %d trials each, it took %f seconds total.',
        num_simultaneous_clients,
        num_trials_per_client,
        end - start,
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/policy_factory.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Service-related policy factories."""

# pylint:disable=g-import-not-at-top
import functools
import time

from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.algorithms.policies import designer_policy as dp


class DefaultPolicyFactory(pythia.PolicyFactory):
  """Default Policy Factory used in Pythia service."""

  def __call__(
      self,
      problem_statement: vz.ProblemStatement,
      algorithm: str,
      policy_supporter: pythia.PolicySupporter,
      study_name: str,
  ) -> pythia.Policy:
    del study_name
    # TODO: Compare string to Algorithm enums.
    if algorithm in (
        'DEFAULT',
        'ALGORITHM_UNSPECIFIED',
        'GP_UCB_PE',
    ):
      from vizier._src.algorithms.designers import gp_ucb_pe

      return dp.DesignerPolicy(policy_supporter, gp_ucb_pe.VizierGPUCBPEBandit)
    elif algorithm == 'GAUSSIAN_PROCESS_BANDIT':
      from vizier._src.algorithms.designers import gp_bandit

      return dp.DesignerPolicy(
          policy_supporter, gp_bandit.VizierGPBandit.from_problem
      )
    elif algorithm == 'RANDOM_SEARCH':
      from vizier._src.algorithms.policies import random_policy

      return random_policy.RandomPolicy(policy_supporter)
    elif algorithm == 'QUASI_RANDOM_SEARCH':
      from vizier._src.algorithms.designers import quasi_random

      return dp.PartiallySerializableDesignerPolicy(
          problem_statement,
          policy_supporter,
          quasi_random.QuasiRandomDesigner.from_problem,
      )
    elif algorithm == 'GRID_SEARCH':
      from vizier._src.algorithms.designers import grid

      return dp.PartiallySerializableDesignerPolicy(
          problem_statement,
          policy_supporter,
          grid.GridSearchDesigner.from_problem,
      )
    elif algorithm == 'SHUFFLED_GRID_SEARCH':
      from vizier._src.algorithms.designers import grid

      shuffle_seed = int(time.time())
      grid_factory = functools.partial(
          grid.GridSearchDesigner.from_problem, shuffle_seed=shuffle_seed
      )
      return dp.PartiallySerializableDesignerPolicy(
          problem_statement,
          policy_supporter,
          grid_factory,
      )
    elif algorithm == 'NSGA2':
      from vizier._src.algorithms.evolution import nsga2

      return dp.PartiallySerializableDesignerPolicy(
          problem_statement, policy_supporter, nsga2.NSGA2Designer
      )
    elif algorithm == 'BOCS':
      from vizier._src.algorithms.designers import bocs

      return dp.DesignerPolicy(policy_supporter, bocs.BOCSDesigner)
    elif algorithm == 'HARMONICA':
      from vizier._src.algorithms.designers import harmonica

      return dp.DesignerPolicy(policy_supporter, harmonica.HarmonicaDesigner)
    elif algorithm == 'CMA_ES':
      from vizier._src.algorithms.designers import cmaes

      return dp.PartiallySerializableDesignerPolicy(
          problem_statement, policy_supporter, cmaes.CMAESDesigner
      )
    elif algorithm == 'EAGLE_STRATEGY':
      from vizier._src.algorithms.designers.eagle_strategy import eagle_strategy

      return dp.PartiallySerializableDesignerPolicy(
          problem_statement,
          policy_supporter,
          eagle_strategy.EagleStrategyDesigner,
      )
    else:
      raise ValueError(f'Algorithm {algorithm} is not registered.')


--- vizier/_src/service/pythia_service.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Separate Pythia service for handling algorithmic logic."""
from typing import Optional

from absl import logging
import attr
import grpc
from vizier import pythia
from vizier._src.service import policy_factory as service_policy_factory
from vizier._src.service import pythia_service_pb2
from vizier._src.service import pythia_service_pb2_grpc
from vizier._src.service import service_policy_supporter
from vizier._src.service import stubs_util
from vizier._src.service import types
from vizier.service import pyvizier as vz

from google.protobuf import empty_pb2


@attr.define
class PythiaServicer(pythia_service_pb2_grpc.PythiaServiceServicer):
  """Implements the GRPC functions outlined in pythia_service.proto."""

  # Can be either an actual VizierService object or a stub. An actual
  # VizierService should be passed only for local testing/local development.
  _vizier_service: Optional[types.VizierService] = attr.field(
      init=True, default=None
  )
  # Factory for creating policies. Defaulted to OSS Vizier-specific policies,
  # but allows use of external package (e.g. PyGlove) poliices.
  _policy_factory: pythia.PolicyFactory = attr.field(
      init=True, factory=service_policy_factory.DefaultPolicyFactory
  )

  def __attrs_post_init__(self):
    try:
      # If Jax is installed, always use float64 for all policies.
      import jax  # pylint:disable=g-import-not-at-top

      jax.config.update('jax_enable_x64', True)
    except ImportError:
      pass

  def connect_to_vizier(self, endpoint: str) -> None:
    """Only needs to be called if VizierService wasn't passed in init."""
    if self._vizier_service:
      raise ValueError('Vizier Service was already set:', self._vizier_service)
    self._vizier_service = stubs_util.create_vizier_server_stub(endpoint)

  def Suggest(
      self,
      request: pythia_service_pb2.SuggestRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> pythia_service_pb2.SuggestDecision:
    """Performs Suggest RPC call."""
    # Setup Policy Supporter.
    study_config = vz.SuggestConverter.from_request_proto(request).study_config
    study_name = request.study_descriptor.guid
    policy_supporter = service_policy_supporter.ServicePolicySupporter(
        study_name, self._vizier_service
    )
    pythia_policy = self._policy_factory(
        study_config, request.algorithm, policy_supporter, study_name
    )

    # Perform algorithmic computation.
    suggest_request = vz.SuggestConverter.from_request_proto(request)
    try:
      suggest_decision = pythia_policy.suggest(suggest_request)
    # Leaving a broad catch for now since Pythia can raise any exception.
    # TODO: Be more specific about exception raised,
    # e.g. AttributeError, ModuleNotFoundError, SyntaxError
    except Exception as e:  # pylint: disable=broad-except
      logging.error(
          'Failed to request trials from Pythia for request: %s', request
      )
      raise RuntimeError('Pythia has encountered an error: ' + str(e)) from e

    return vz.SuggestConverter.to_decision_proto(suggest_decision)

  def EarlyStop(
      self,
      request: pythia_service_pb2.EarlyStopRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> pythia_service_pb2.EarlyStopDecisions:
    """Performs EarlyStop RPC call."""
    # Setup Policy Supporter.
    study_config = vz.EarlyStopConverter.from_request_proto(
        request
    ).study_config
    study_name = request.study_descriptor.guid
    policy_supporter = service_policy_supporter.ServicePolicySupporter(
        study_name, self._vizier_service
    )
    pythia_policy = self._policy_factory(
        study_config, request.algorithm, policy_supporter, study_name
    )

    # Perform algorithmic computation.
    early_stop_request = vz.EarlyStopConverter.from_request_proto(request)
    early_stopping_decisions = pythia_policy.early_stop(early_stop_request)

    return vz.EarlyStopConverter.to_decisions_proto(early_stopping_decisions)

  def Ping(
      self,
      request: empty_pb2.Empty,
      context: Optional[grpc.ServicerContext] = None,
  ) -> empty_pb2.Empty:
    return empty_pb2.Empty()


--- vizier/_src/service/pythia_util.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utility functions for the Pythia protocol code."""

import logging
import threading
from typing import Generic, Optional, Protocol, TypeVar
from vizier import pythia


class _HasErrorDetails(Protocol):
  error_details: str


_T = TypeVar('_T', bound=_HasErrorDetails)


class ResponseWaiter(Generic[_T]):
  """A stored message with a Wait() mechanism.

  This is a bridge between two threads; it stores a message, and
  the reader thread waits until the writer thread calls Report(..., done=True).
  Once Report() is called, it unblocks WaitForRespose().
  """

  def __init__(self):
    self._lock = threading.Lock()
    self._response: Optional[_T] = None
    self._wait = threading.Event()

  def Report(self, update: _T) -> None:
    """Called by the gRPC thread with a message from Vizier.

    When called, this unblocks WaitForResponse().

    Args:
      update:

    Raises:
      PythiaProtocolError
    """
    logging.info('About to take _lock in ResponseWaiter.Report()')
    with self._lock:
      if self._wait.is_set():
        raise pythia.PythiaProtocolError(
            'ResponseWaiter.Report() called after wait is set')
      self._response = update
      self._wait.set()

  def WaitForResponse(self) -> _T:
    """Returns the result or raises an error.

    Raises:
      PythiaProtocolError: Due to a failure of the Pythia protocol.
      VizierDatabaseError: Vizier was unable to service the request.
    """
    self._wait.wait()
    logging.info('About to take _lock in ResponseWaiter.WaitForResponse()')
    with self._lock:
      if self._response is None:
        logging.info('Raise')
        raise pythia.PythiaProtocolError('No response.')
      elif self._response.error_details:
        raise pythia.VizierDatabaseError(self._response.error_details)
      logging.info('No raise -- WaitForResponse done')
      return self._response

  def CancelWait(self) -> None:
    """Cancel the wait."""
    logging.info('About to CancelWait()')
    self._wait.set()


--- vizier/_src/service/ram_datastore.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Basic datastore written using Python primitives.

For debugging/testing purposes mainly.
"""

import collections
import copy
import dataclasses
import threading
from typing import Callable, DefaultDict, Dict, Iterable, List, Optional

from absl import logging
from vizier._src.service import custom_errors
from vizier._src.service import datastore
from vizier._src.service import key_value_pb2
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2
from vizier._src.service import vizier_service_pb2
from vizier.service import pyvizier as vz

from google.longrunning import operations_pb2

UnitMetadataUpdate = vizier_service_pb2.UnitMetadataUpdate


@dataclasses.dataclass(frozen=True)
class ClientNode:
  """Only contains suggestion operations associated with this client."""

  # Keys are `operation_id`.
  suggestion_operations: Dict[str, operations_pb2.Operation] = (
      dataclasses.field(default_factory=dict)
  )


# Specific dataclasses used for NestedDictRAMDataStore.
@dataclasses.dataclass(frozen=True)
class StudyNode:
  """Contains original study and currently stored trials."""

  study_proto: study_pb2.Study

  # Keys are `trial_id`.
  trial_protos: Dict[int, study_pb2.Trial] = dataclasses.field(
      default_factory=dict
  )

  # Keys are `operation_id`.
  early_stopping_operations: Dict[
      str, vizier_oss_pb2.EarlyStoppingOperation
  ] = dataclasses.field(default_factory=dict)

  # Key is `client_id`, which distinguishes clients, so you can have several
  # clients interacting with the same Vizier server.
  clients: Dict[str, ClientNode] = dataclasses.field(default_factory=dict)


@dataclasses.dataclass(frozen=True)
class OwnerNode:
  """First level of the entire RAM Datastore."""

  # Key is a `study_id` that pick's out one of the owner's Studies.
  studies: Dict[str, StudyNode] = dataclasses.field(default_factory=dict)


class NestedDictRAMDataStore(datastore.DataStore):
  """Basic Datastore class using nested dictionaries."""

  def __init__(self):
    """Organization as follows."""
    # Key is `owner_id`, which corresponds to the (perhaps human) owner
    # of the Study.
    self._owners: Dict[str, OwnerNode] = {}
    # TODO: Use more fine-grained (study/trial/etc.) locks.
    self._lock = threading.Lock()

  def create_study(self, study: study_pb2.Study) -> resources.StudyResource:
    resource = resources.StudyResource.from_name(study.name)
    temp_dict = {resource.study_id: StudyNode(study_proto=copy.deepcopy(study))}

    with self._lock:
      if resource.owner_id not in self._owners:
        self._owners[resource.owner_id] = OwnerNode(studies=temp_dict)
      else:
        study_dict = self._owners[resource.owner_id].studies
        if resource.study_id not in study_dict:
          study_dict.update(temp_dict)
        else:
          raise custom_errors.AlreadyExistsError(
              'Study with that name already exists.', study.name
          )
    return resource

  def load_study(self, study_name: str) -> study_pb2.Study:
    resource = resources.StudyResource.from_name(study_name)

    with self._lock:
      try:
        return copy.deepcopy(
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .study_proto
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not get Study with name:', resource.name
        ) from err

  def update_study(self, study: study_pb2.Study) -> resources.StudyResource:
    resource = resources.StudyResource.from_name(study.name)
    with self._lock:
      try:
        self._owners[resource.owner_id].studies[
            resource.study_id
        ].study_proto.CopyFrom(study)
        return resource
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not update Study with name:', resource.name
        ) from err

  def delete_study(self, study_name: str) -> None:
    resource = resources.StudyResource.from_name(study_name)
    with self._lock:
      try:
        del self._owners[resource.owner_id].studies[resource.study_id]
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Study does not exist:', study_name
        ) from err

  def list_studies(self, owner_name: str) -> List[study_pb2.Study]:
    resource = resources.OwnerResource.from_name(owner_name)

    with self._lock:
      try:
        study_nodes = list(self._owners[resource.owner_id].studies.values())
        return copy.deepcopy(
            [study_node.study_proto for study_node in study_nodes]
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Owner does not exist:', owner_name
        ) from err

  def create_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    resource = resources.TrialResource.from_name(trial.name)
    with self._lock:
      trial_protos = (
          self._owners[resource.owner_id]
          .studies[resource.study_id]
          .trial_protos
      )
      if resource.trial_id in trial_protos:
        raise custom_errors.AlreadyExistsError(
            'Trial %s already exists' % trial.name
        )
      else:
        trial_protos[resource.trial_id] = copy.deepcopy(trial)
    return resource

  def get_trial(self, trial_name: str) -> study_pb2.Trial:
    resource = resources.TrialResource.from_name(trial_name)

    with self._lock:
      try:
        return copy.deepcopy(
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .trial_protos[resource.trial_id]
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not get Trial with name:', resource.name
        ) from err

  def update_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    resource = resources.TrialResource.from_name(trial.name)
    with self._lock:
      try:
        trial_protos = (
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .trial_protos
        )
        if resource.trial_id not in trial_protos:
          raise custom_errors.NotFoundError(
              'Trial %s does not exist.' % trial.name
          )
        trial_protos[resource.trial_id] = copy.deepcopy(trial)
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not update Trial with name:', resource.name
        ) from err

    return resource

  def list_trials(self, study_name: str) -> List[study_pb2.Trial]:
    resource = resources.StudyResource.from_name(study_name)
    with self._lock:
      try:
        return copy.deepcopy(
            list(
                self._owners[resource.owner_id]
                .studies[resource.study_id]
                .trial_protos.values()
            )
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Study does not exist:', study_name
        ) from err

  def delete_trial(self, trial_name: str) -> None:
    resource = resources.TrialResource.from_name(trial_name)
    with self._lock:
      try:
        del (
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .trial_protos[resource.trial_id]
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Trial does not exist:', trial_name
        ) from err

  def max_trial_id(self, study_name: str) -> int:
    resource = resources.StudyResource.from_name(study_name)
    with self._lock:
      try:
        trial_ids = copy.deepcopy(
            list(
                self._owners[resource.owner_id]
                .studies[resource.study_id]
                .trial_protos.keys()
            )
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Study does not exist:', study_name
        ) from err

    return max(trial_ids) if trial_ids else 0

  def create_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    resource = resources.SuggestionOperationResource.from_name(operation.name)
    with self._lock:
      if (
          resource.client_id
          not in self._owners[resource.owner_id]
          .studies[resource.study_id]
          .clients
      ):
        self._owners[resource.owner_id].studies[resource.study_id].clients[
            resource.client_id
        ] = ClientNode()
      suggestion_operations = (
          self._owners[resource.owner_id]
          .studies[resource.study_id]
          .clients[resource.client_id]
          .suggestion_operations
      )

      if resource.operation_id in suggestion_operations:
        raise custom_errors.AlreadyExistsError(
            'Operation already exists:', resource.operation_id
        )

      suggestion_operations[resource.operation_id] = copy.deepcopy(operation)
    return resource

  def get_suggestion_operation(
      self, operation_name: str
  ) -> operations_pb2.Operation:
    resource = resources.SuggestionOperationResource.from_name(operation_name)

    with self._lock:
      try:
        return copy.deepcopy(
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .clients[resource.client_id]
            .suggestion_operations[resource.operation_id]
        )

      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not find SuggestionOperation with name:', resource.name
        ) from err

  def update_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    resource = resources.SuggestionOperationResource.from_name(operation.name)

    with self._lock:
      try:
        self._owners[resource.owner_id].studies[resource.study_id].clients[
            resource.client_id
        ].suggestion_operations[resource.operation_id] = copy.deepcopy(
            operation
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not update SuggestionOperation with name:', resource.name
        ) from err

      return resource

  def list_suggestion_operations(
      self,
      study_name: str,
      client_id: str,
      filter_fn: Optional[Callable[[operations_pb2.Operation], bool]] = None,
  ) -> List[operations_pb2.Operation]:
    resource = resources.StudyResource.from_name(study_name)

    with self._lock:
      try:
        operations_list = copy.deepcopy(
            list(
                self._owners[resource.owner_id]
                .studies[resource.study_id]
                .clients[client_id]
                .suggestion_operations.values()
            )
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            '(study_name, client_id) does not exist:', (study_name, client_id)
        ) from err

      if filter_fn is not None:
        return copy.deepcopy([op for op in operations_list if filter_fn(op)])
      else:
        return copy.deepcopy(operations_list)

  def max_suggestion_operation_number(
      self, study_name: str, client_id: str
  ) -> int:
    resource = resources.StudyResource.from_name(study_name)

    with self._lock:
      try:
        ops = (
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .clients[client_id]
            .suggestion_operations
        )
        return len(ops)
      except KeyError as err:
        raise custom_errors.NotFoundError(
            '(study_name, client_id) does not exist:', (study_name, client_id)
        ) from err

  def create_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    resource = resources.EarlyStoppingOperationResource.from_name(
        operation.name
    )
    with self._lock:
      early_stopping_ops = (
          self._owners[resource.owner_id]
          .studies[resource.study_id]
          .early_stopping_operations
      )
      if resource.operation_id in early_stopping_ops:
        raise custom_errors.AlreadyExistsError(
            'Operation already exists:', resource.operation_id
        )

      early_stopping_ops[resource.operation_id] = copy.deepcopy(operation)
    return resource

  def get_early_stopping_operation(
      self, operation_name: str
  ) -> vizier_oss_pb2.EarlyStoppingOperation:
    resource = resources.EarlyStoppingOperationResource.from_name(
        operation_name
    )
    with self._lock:
      try:
        return copy.deepcopy(
            self._owners[resource.owner_id]
            .studies[resource.study_id]
            .early_stopping_operations[resource.operation_id]
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not find EarlyStoppingOperation with name:', resource.name
        ) from err

  def update_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    resource = resources.EarlyStoppingOperationResource.from_name(
        operation.name
    )
    with self._lock:
      try:
        self._owners[resource.owner_id].studies[
            resource.study_id
        ].early_stopping_operations[resource.operation_id] = copy.deepcopy(
            operation
        )
      except KeyError as err:
        raise custom_errors.NotFoundError(
            'Could not update EarlyStoppingOperation with name:', resource.name
        ) from err
    return resource

  def update_metadata(
      self,
      study_name: str,
      study_metadata: Iterable[key_value_pb2.KeyValue],
      trial_metadata: Iterable[UnitMetadataUpdate],
  ) -> None:
    # TODO:
    """Writes the supplied metadata to the database.

    Args:
      study_name:
      study_metadata: Metadata that's associated with the Study as a whole.
      trial_metadata: Metadata that's associated with trials.  (Note that its an
        error to attach metadata to a Trial that doesn't exist.)
    """
    s_resource = resources.StudyResource.from_name(study_name)
    logging.debug('database.update_metadata s_resource= %s', s_resource)

    with self._lock:
      try:
        study_node = self._owners[s_resource.owner_id].studies[
            s_resource.study_id
        ]
      except KeyError as e:
        raise custom_errors.NotFoundError(
            'No such study:', s_resource.name
        ) from e
      # Store Study-related metadata into the database.
      vz.metadata_util.merge_study_metadata(
          study_node.study_proto.study_spec, copy.deepcopy(study_metadata)
      )
      # Split the trial-related metadata by Trial.
      split_metadata: DefaultDict[str, List[UnitMetadataUpdate]] = (
          collections.defaultdict(list)
      )
      for md in copy.deepcopy(trial_metadata):
        split_metadata[md.trial_id].append(md)
      # Now, we update one Trial at a time:
      for trial_id, md_list in split_metadata.items():
        t_resource = s_resource.trial_resource(trial_id)
        trial_proto = study_node.trial_protos[t_resource.trial_id]
        vz.metadata_util.merge_trial_metadata(trial_proto, md_list)


--- vizier/_src/service/ram_datastore_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier._src.service import datastore_test_lib
from vizier._src.service import ram_datastore
from vizier._src.service import vizier_service_pb2
from vizier._src.service.testing import util as test_util

from absl.testing import absltest

UnitMetadataUpdate = vizier_service_pb2.UnitMetadataUpdate


class NestedDictRAMDataStoreTest(datastore_test_lib.DataStoreTestCase):

  def setUp(self):
    self.owner_id = 'my_username'
    self.study_id = '123123123'
    self.client_id = 'client_0'
    self.datastore = ram_datastore.NestedDictRAMDataStore()
    self.example_study = test_util.generate_study(self.owner_id, self.study_id)
    self.example_trials = test_util.generate_trials(
        [1, 2], owner_id=self.owner_id, study_id=self.study_id
    )
    self.example_suggestion_operations = (
        test_util.generate_suggestion_operations(
            [1, 2, 3, 4], self.owner_id, self.study_id, self.client_id
        )
    )
    self.example_early_stopping_operations = (
        test_util.generate_early_stopping_operations(
            [1, 2], self.owner_id, self.study_id
        )
    )
    super().setUp()

  def test_study_api(self):
    self.assertStudyAPI(self.datastore, self.example_study)

  def test_trial(self):
    self.assertTrialAPI(self.datastore, self.example_study, self.example_trials)

  def test_suggestion_operation(self):
    self.assertSuggestOpAPI(
        self.datastore,
        self.example_study,
        self.client_id,
        self.example_suggestion_operations,
    )

  def test_early_stopping_operation(self):
    self.assertEarlyStoppingAPI(
        self.datastore,
        self.example_study,
        self.example_trials,
        self.example_early_stopping_operations,
    )

  def test_update_metadata(self):
    self.assertUpdateMetadataAPI(
        self.datastore, self.example_study, self.example_trials
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/resources.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Contains resource utilities, such as name parsing.

Convention for variable naming for string identifiers:
  1. {}_id means single component (e.g. study_id = '1312931').
  2. {}_name means directory (e.g. study_name =
  'owners/my_username/studies/1312931').

Everything related to naming should be self-contained in this file; i.e.
dependents should only call official API functions from this file and never
explicitly write their own string processing.
"""
import re
import attr

from vizier.utils import attrs_utils

# Resource components cannot contain "/".
_resource_component_validator = [attrs_utils.assert_re_fullmatch(r'[^\/]+')]


@attr.define(init=True, frozen=True)
class OwnerResource:
  """Resource for Owners."""

  _owner_id: str = attr.ib(init=True, validator=_resource_component_validator)

  @classmethod
  def from_name(cls, resource_name: str):
    owner_match = re.match(r'^owners\/(?P<owner_id>[^\/]+)$', resource_name)

    if owner_match:
      return OwnerResource(owner_id=owner_match.group('owner_id'))
    else:
      raise ValueError(f'Incorrect resource name sent: {resource_name}')

  @property
  def owner_id(self):
    return self._owner_id

  @property
  def name(self) -> str:
    return f'owners/{self._owner_id}'


@attr.define(init=True, frozen=True)
class StudyResource:
  """Resource for Studies."""

  owner_id: str = attr.ib(init=True, validator=_resource_component_validator)
  study_id: str = attr.ib(init=True, validator=_resource_component_validator)

  @classmethod
  def from_name(cls, resource_name: str):
    """Creates StudyResource from a name."""
    study_match = re.match(
        r'^owners\/(?P<owner_id>[^\/]+)\/studies\/(?P<study_id>[^\/]+)$',
        resource_name,
    )

    if study_match:
      return StudyResource(
          study_match.group('owner_id'), study_match.group('study_id')
      )
    else:
      raise ValueError(
          f'{repr(resource_name)} is not a valid name for a Study resource.'
      )

  @property
  def owner_resource(self) -> OwnerResource:
    return OwnerResource(owner_id=self.owner_id)

  @property
  def name(self) -> str:
    return f'owners/{self.owner_id}/studies/{self.study_id}'

  def trial_resource(self, trial_id: str) -> 'TrialResource':
    """Creates a TrialResource when given a trial_id."""
    int_id = int(trial_id)
    if int_id <= 0:
      raise ValueError('Invalid trial_id: "{trial_id}"')
    return TrialResource(self.owner_id, self.study_id, int_id)


@attr.define(init=True, frozen=True)
class TrialResource:
  """Resource for Trials."""

  owner_id: str = attr.ib(init=True, validator=_resource_component_validator)
  study_id: str = attr.ib(init=True, validator=_resource_component_validator)
  trial_id: int = attr.ib(
      init=True,
      validator=[
          attr.validators.instance_of(int),
          attrs_utils.assert_not_negative,
      ],
  )

  @classmethod
  def from_name(cls, resource_name: str) -> 'TrialResource':
    """Creates TrialResource from a name."""
    trial_match = re.match(
        r'^owners\/(?P<owner_id>[^\/]+)\/studies\/(?P<study_id>[^\/]+)\/trials\/(?P<trial_id>[^\/]+)$',
        resource_name,
    )

    if trial_match:
      return TrialResource(
          trial_match.group('owner_id'),
          trial_match.group('study_id'),
          int(trial_match.group('trial_id')),
      )
    else:
      raise ValueError(
          f'{repr(resource_name)} is not a valid name for a Trial resource.'
      )

  @property
  def study_resource(self) -> StudyResource:
    return StudyResource(owner_id=self.owner_id, study_id=self.study_id)

  @property
  def early_stopping_operation_resource(
      self,
  ) -> 'EarlyStoppingOperationResource':
    return EarlyStoppingOperationResource(
        owner_id=self.owner_id, study_id=self.study_id, trial_id=self.trial_id
    )

  @property
  def name(self) -> str:
    return (
        f'owners/{self.owner_id}/studies/{self.study_id}/trials/{self.trial_id}'
    )


@attr.define(init=True, frozen=True)
class EarlyStoppingOperationResource:
  """Resource for Early Stopping Operations."""

  owner_id: str = attr.ib(init=True, validator=_resource_component_validator)
  study_id: str = attr.ib(init=True, validator=_resource_component_validator)
  trial_id: int = attr.ib(
      init=True,
      validator=[
          attr.validators.instance_of(int),
          attrs_utils.assert_not_negative,
      ],
  )

  @property
  def operation_id(self) -> str:
    return f'earlystopping/{self.study_id}/{self.trial_id}'

  @property
  def name(self) -> str:
    return f'owners/{self.owner_id}/operations/{self.operation_id}'

  @classmethod
  def from_name(cls, resource_name: str):
    """Creates EarlyStoppingOperationResource from a name."""
    operation_match = re.match(
        r'^owners\/(?P<owner_id>[^\/]+)/operations/earlystopping/(?P<study_id>[^\/]+)/(?P<trial_id>[^\/]+)$',
        resource_name,
    )
    if operation_match:
      return EarlyStoppingOperationResource(
          operation_match.group('owner_id'),
          operation_match.group('study_id'),
          int(operation_match.group('trial_id')),
      )
    else:
      raise ValueError(f'Incorrect resource name sent: {resource_name}')

  @property
  def trial_resource(self) -> TrialResource:
    return TrialResource(
        owner_id=self.owner_id, study_id=self.study_id, trial_id=self.trial_id
    )


@attr.define(init=True, frozen=True)
class SuggestionOperationResource:
  """Resource for Suggestion Operations."""

  owner_id: str = attr.ib(init=True, validator=_resource_component_validator)
  study_id: str = attr.ib(init=True, validator=_resource_component_validator)
  client_id: str = attr.ib(init=True, validator=_resource_component_validator)
  operation_number: int = attr.ib(
      init=True,
      validator=[
          attr.validators.instance_of(int),
          attrs_utils.assert_not_negative,
      ],
  )

  @property
  def operation_id(self) -> str:
    return (
        f'suggestion/{self.study_id}/{self.client_id}/{self.operation_number}'
    )

  @property
  def name(self) -> str:
    return f'owners/{self.owner_id}/operations/{self.operation_id}'

  @classmethod
  def from_name(cls, resource_name: str):
    """Creates SuggestionOperationResource from a name."""
    operation_match = re.match(
        r'^owners\/(?P<owner_id>[^\/]+)/operations/suggestion/(?P<study_id>[^\/]+)/(?P<client_id>[^\/]+)/(?P<operation_number>[^\/]+)$',
        resource_name,
    )
    if operation_match:
      return SuggestionOperationResource(
          operation_match.group('owner_id'),
          operation_match.group('study_id'),
          operation_match.group('client_id'),
          int(operation_match.group('operation_number')),
      )
    else:
      raise ValueError(f'Incorrect resource name sent: {resource_name}')


--- vizier/_src/service/resources_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.service.resources."""

from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service.testing import util as test_util

from absl.testing import absltest
from absl.testing import parameterized


class UtilTest(parameterized.TestCase):

  def setUp(self):
    self.owner_id = 'my_username'
    self.study_id = '12312312'
    self.trial_id = 1
    self.client_id = 'client_0'
    self.operation_number = 5
    super().setUp()

  def test_parsing_correct(self):
    """Tests for correct resource strings only."""
    owner_resource = resources.OwnerResource(self.owner_id)
    same_owner_resource = resources.OwnerResource.from_name(owner_resource.name)
    self.assertEqual(owner_resource, same_owner_resource)

    study_resource = resources.StudyResource(self.owner_id, self.study_id)
    same_study_resource = resources.StudyResource.from_name(study_resource.name)
    self.assertEqual(study_resource, same_study_resource)

    trial_resource = resources.TrialResource(
        self.owner_id, self.study_id, self.trial_id
    )
    same_trial_resource = resources.TrialResource.from_name(trial_resource.name)
    self.assertEqual(trial_resource, same_trial_resource)
    self.assertEqual(trial_resource.study_resource, study_resource)

    early_stopping_op_resource = resources.EarlyStoppingOperationResource(
        self.owner_id, self.study_id, self.trial_id
    )
    same_early_stopping_op_resource = (
        resources.EarlyStoppingOperationResource.from_name(
            early_stopping_op_resource.name
        )
    )
    self.assertEqual(
        early_stopping_op_resource, same_early_stopping_op_resource
    )
    self.assertEqual(early_stopping_op_resource.trial_resource, trial_resource)
    self.assertEqual(
        trial_resource.early_stopping_operation_resource,
        early_stopping_op_resource,
    )

    suggestion_op_resource = resources.SuggestionOperationResource(
        self.owner_id, self.study_id, self.client_id, self.operation_number
    )
    same_suggestion_op_resource = (
        resources.SuggestionOperationResource.from_name(
            suggestion_op_resource.name
        )
    )
    self.assertEqual(same_suggestion_op_resource, same_suggestion_op_resource)

  @parameterized.named_parameters(
      ('owner', 'owner/my_username', resources.OwnerResource),
      ('study', 'owners/my_username/study/cifar10', resources.StudyResource),
      (
          'trial',
          'owners/my_username/studies/cifar10/trials/not_an_int',
          resources.TrialResource,
      ),
  )
  def test_parsing_wrong(self, bad_name, resource_class):
    """Tests for incorrect resource strings, for input validation."""
    with self.assertRaises(ValueError):
      resource_class.from_name(bad_name)

  def test_generate_studies(self):
    study_spec = test_util.generate_all_four_parameter_specs()
    self.assertLen(study_spec.parameters, 4)

  def test_generate_trials(self):
    basic_trials = test_util.generate_trials(
        range(2), self.owner_id, self.study_id
    )
    self.assertLen(basic_trials, 2)
    all_states_trials = test_util.generate_all_states_trials(
        0, self.owner_id, self.study_id
    )
    self.assertLen(all_states_trials, len(study_pb2.Trial.State.keys()))

  def test_generate_suggestion_operations(self):
    basic_operations = test_util.generate_suggestion_operations(
        range(4), self.owner_id, self.study_id, self.client_id
    )
    self.assertLen(basic_operations, 4)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/service_policy_supporter.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Policy Supporter used for the OSS Vizier service.

The Policy can use these methods to communicate with Vizier.
"""
import datetime
from typing import Iterable, List, Optional

from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.service import types
from vizier._src.service import vizier_service_pb2
from vizier.service import pyvizier


# TODO: Consider replacing protos with Pyvizier clients.py.
class ServicePolicySupporter(pythia.PolicySupporter):
  """Service version of the PolicySupporter."""

  def __init__(self, study_guid: str, vizier_service: types.VizierService):
    """Initalization.

    Args:
      study_guid: A default study_name; the name of this study.
      vizier_service: Vizier Service, in the form of a GRPC stub or actual
        class.
    """
    self._study_guid = study_guid
    self._vizier_service = vizier_service

  @property
  def study_guid(self) -> str:
    return self._study_guid

  def GetStudyConfig(self, study_guid: str) -> vz.ProblemStatement:
    request = vizier_service_pb2.GetStudyRequest(name=study_guid)
    study = self._vizier_service.GetStudy(request)
    return pyvizier.StudyConfig.from_proto(study.study_spec).to_problem()

  # TODO: Support filters in ListTrialsRequest.
  def GetTrials(
      self,
      *,
      study_guid: Optional[str] = None,  # Equivalent to Study.name in OSS.
      trial_ids: Optional[Iterable[int]] = None,
      min_trial_id: Optional[int] = None,
      max_trial_id: Optional[int] = None,
      status_matches: Optional[vz.TrialStatus] = None,
      include_intermediate_measurements: bool = True,
  ) -> List[vz.Trial]:
    """Fetch all trials and then apply the filter."""

    if study_guid is None:
      study_guid = self._study_guid
    request = vizier_service_pb2.ListTrialsRequest(parent=study_guid)
    trials = self._vizier_service.ListTrials(request).trials
    all_pytrials = pyvizier.TrialConverter.from_protos(trials)

    trial_filter = vz.TrialFilter(
        ids=trial_ids,
        min_id=min_trial_id,
        max_id=max_trial_id,
        status=[status_matches] if status_matches else None,
    )
    filtered_pytrials = [t for t in all_pytrials if trial_filter(t)]

    # Doesn't affect datastore when measurements are deleted.
    if not include_intermediate_measurements:
      for filtered_pytrial in filtered_pytrials:
        filtered_pytrial.measurements = []

    return filtered_pytrials

  def CheckCancelled(self, note: Optional[str] = None) -> None:
    """Throws a CancelComputeError on timeout or if Vizier cancels."""
    pass  # Do nothing since it's one single process.

  def TimeRemaining(self) -> datetime.timedelta:
    """The time remaining to compute a result."""
    return datetime.timedelta.max  # RPCs don't have timeouts in OSS.


--- vizier/_src/service/service_policy_supporter_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from vizier._src.service import constants
from vizier._src.service import resources
from vizier._src.service import service_policy_supporter
from vizier._src.service import study_pb2
from vizier._src.service import vizier_service
from vizier._src.service.testing import util as test_util
from vizier.service import pyvizier

from absl.testing import absltest


class PythiaSupporterTest(absltest.TestCase):

  def setUp(self):
    self.owner_id = 'my_username'
    self.study_id = '1231223'
    self.study_name = resources.StudyResource(
        owner_id=self.owner_id, study_id=self.study_id
    ).name
    self.vs = vizier_service.VizierServicer(
        database_url=constants.SQL_MEMORY_URL
    )
    self.example_study = test_util.generate_study(self.owner_id, self.study_id)
    self.vs.datastore.create_study(self.example_study)

    self.active_trials = test_util.generate_trials(
        range(1, 7), self.owner_id, self.study_id
    )
    self.succeeded_trial = test_util.generate_trials(
        [7],
        self.owner_id,
        self.study_id,
        state=study_pb2.Trial.State.SUCCEEDED,
        final_measurement=study_pb2.Measurement(),
    )[0]

    for trial in self.active_trials + [self.succeeded_trial]:
      self.vs.datastore.create_trial(trial)

    self.policy_supporter = service_policy_supporter.ServicePolicySupporter(
        self.study_name, self.vs
    )

    super().setUp()

  def test_trial_names_filter(self):
    trials = self.policy_supporter.GetTrials(
        study_guid=self.study_name, trial_ids=[3, 4]
    )

    self.assertEqual(
        trials[0], pyvizier.TrialConverter.from_proto(self.active_trials[2])
    )
    self.assertEqual(
        trials[1], pyvizier.TrialConverter.from_proto(self.active_trials[3])
    )

  def test_min_max_filter(self):
    trials = self.policy_supporter.GetTrials(
        study_guid=self.study_name, min_trial_id=3, max_trial_id=4
    )

    self.assertEqual(
        trials[0], pyvizier.TrialConverter.from_proto(self.active_trials[2])
    )
    self.assertEqual(
        trials[1], pyvizier.TrialConverter.from_proto(self.active_trials[3])
    )

  def test_status_match_filter(self):
    trials = self.policy_supporter.GetTrials(
        study_guid=self.study_name, status_matches=pyvizier.TrialStatus.ACTIVE
    )

    self.assertLen(trials, 6)
    self.assertEqual(
        trials[0], pyvizier.TrialConverter.from_proto(self.active_trials[0])
    )

  def test_raise_value_error(self):
    def should_raise_value_error_fn():
      self.policy_supporter.GetTrials(
          study_guid=self.study_name,
          trial_ids=[1],
          status_matches=pyvizier.TrialStatus.ACTIVE,
      )

      with self.assertRaises(ValueError):
        should_raise_value_error_fn()

  def test_get_study_config(self):
    pythia_problem = self.policy_supporter.GetStudyConfig(self.study_name)
    correct_pythia_problem = pyvizier.StudyConfig.from_proto(
        self.example_study.study_spec
    ).to_problem()
    self.assertEqual(pythia_problem, correct_pythia_problem)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/sql_datastore.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Implementation of SQL Datastore."""

import collections
import threading
from typing import Callable, Iterable, List, Optional

from absl import logging
import sqlalchemy as sqla
from vizier._src.service import custom_errors
from vizier._src.service import datastore
from vizier._src.service import key_value_pb2
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2
from vizier.service import pyvizier as vz

from google.longrunning import operations_pb2

NotFoundError = custom_errors.NotFoundError
AlreadyExistsError = custom_errors.AlreadyExistsError


# TODO: Consider using ORM API (when fixed) to reduce code length.
class SQLDataStore(datastore.DataStore):
  """SQL Datastore."""

  def __init__(self, engine: sqla.engine.Engine):
    self._engine = engine
    self._connection = self._engine.connect()
    self._root_metadata = sqla.MetaData()
    self._owners_table = sqla.Table(
        'owners',
        self._root_metadata,
        sqla.Column('owner_name', sqla.String, primary_key=True),
    )
    self._studies_table = sqla.Table(
        'studies',
        self._root_metadata,
        sqla.Column('study_name', sqla.String, primary_key=True),
        sqla.Column('owner_id', sqla.String),
        sqla.Column('study_id', sqla.String),
        sqla.Column('serialized_study', sqla.String),
    )
    self._trials_table = sqla.Table(
        'trials',
        self._root_metadata,
        sqla.Column('trial_name', sqla.String, primary_key=True),
        sqla.Column('owner_id', sqla.String),
        sqla.Column('study_id', sqla.String),
        sqla.Column('trial_id', sqla.INTEGER),
        sqla.Column('serialized_trial', sqla.String),
    )
    self._suggestion_operations_table = sqla.Table(
        'suggestion_operations',
        self._root_metadata,
        sqla.Column('operation_name', sqla.String, primary_key=True),
        sqla.Column('owner_id', sqla.String),
        sqla.Column('study_id', sqla.String),
        sqla.Column('client_id', sqla.String),
        sqla.Column('operation_number', sqla.INTEGER),
        sqla.Column('serialized_op', sqla.String),
    )
    self._early_stopping_operations_table = sqla.Table(
        'early_stopping_operations',
        self._root_metadata,
        sqla.Column('operation_name', sqla.String, primary_key=True),
        sqla.Column('owner_id', sqla.String),
        sqla.Column('study_id', sqla.String),
        sqla.Column('trial_id', sqla.INTEGER),
        sqla.Column('serialized_op', sqla.String),
    )
    # This lock is meant to lock `execute()` calls for database types which
    # don't support multi-threading, like SQLite.
    self._lock = threading.Lock()
    self._root_metadata.create_all(self._engine)

  def _write_or_rollback(self, write_query: sqla.sql.Executable) -> None:
    """Wraps connection.execute() to roll back on write query failure.

    Args:
      write_query: The write query to execute.

    Raises:
      sqla.exc.DatabaseError: Generic database error.
    """
    try:
      self._connection.execute(write_query)
    except sqla.exc.DatabaseError as e:
      self._connection.rollback()
      raise e

  def create_study(self, study: study_pb2.Study) -> resources.StudyResource:
    study_resource = resources.StudyResource.from_name(study.name)
    owner_name = study_resource.owner_resource.name
    owner_query = self._owners_table.insert().values(owner_name=owner_name)
    study_query = self._studies_table.insert().values(
        study_name=study.name,
        owner_id=study_resource.owner_id,
        study_id=study_resource.study_id,
        serialized_study=study.SerializeToString(),
    )

    with self._lock:
      try:
        self._write_or_rollback(owner_query)
      except sqla.exc.IntegrityError:
        logging.info('Owner with name %s currently exists.', owner_name)

      try:
        self._write_or_rollback(study_query)
      except sqla.exc.IntegrityError as e:
        raise AlreadyExistsError(
            'Study with name %s already exists.' % study.name
        ) from e
      self._connection.commit()

    return study_resource

  def load_study(self, study_name: str) -> study_pb2.Study:
    query = sqla.select(self._studies_table)
    query = query.where(self._studies_table.c.study_name == study_name)

    with self._lock:
      row = self._connection.execute(query).fetchone()
      if not row:
        raise NotFoundError('Failed to find study name: %s' % study_name)
      study = study_pb2.Study.FromString(row.serialized_study)

    return study

  def update_study(self, study: study_pb2.Study) -> resources.StudyResource:
    study_resource = resources.StudyResource.from_name(study.name)

    # Exist query
    eq = sqla.select(self._studies_table)
    eq = eq.where(self._studies_table.c.study_name == study.name)
    eq = sqla.exists(eq).select()

    # Update query
    uq = sqla.update(self._studies_table)
    uq = uq.where(self._studies_table.c.study_name == study.name)
    uq = uq.values(
        study_name=study.name,
        owner_id=study_resource.owner_id,
        study_id=study_resource.study_id,
        serialized_study=study.SerializeToString(),
    )

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Study %s does not exist.' % study.name)
      self._write_or_rollback(uq)
      self._connection.commit()

    return study_resource

  def delete_study(self, study_name: str) -> None:
    study_resource = resources.StudyResource.from_name(study_name)

    # Exist query
    eq = sqla.select(self._studies_table)
    eq = eq.where(self._studies_table.c.study_name == study_name)
    eq = sqla.exists(eq).select()

    # Delete study query
    dsq = self._studies_table.delete()
    dsq = dsq.where(self._studies_table.c.study_name == study_name)

    # Delete trials query
    dtq = self._trials_table.delete()
    dtq = dtq.where(self._trials_table.c.owner_id == study_resource.owner_id)
    dtq = dtq.where(self._trials_table.c.study_id == study_resource.study_id)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Study %s does not exist.' % study_name)
      self._write_or_rollback(dsq)
      self._write_or_rollback(dtq)
      self._connection.commit()

  def list_studies(self, owner_name: str) -> List[study_pb2.Study]:
    owner_id = resources.OwnerResource.from_name(owner_name).owner_id

    # Exist query
    eq = sqla.select(self._owners_table)
    eq = eq.where(self._owners_table.c.owner_name == owner_name)
    eq = sqla.exists(eq).select()

    # List query
    lq = sqla.select(self._studies_table)
    lq = lq.where(self._studies_table.c.owner_id == owner_id)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Owner name %s does not exist.' % owner_name)
      result = self._connection.execute(lq).fetchall()
      studies = [
          study_pb2.Study.FromString(row.serialized_study) for row in result
      ]

    return studies

  def create_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    trial_resource = resources.TrialResource.from_name(trial.name)
    query = self._trials_table.insert().values(
        trial_name=trial.name,
        owner_id=trial_resource.owner_id,
        study_id=trial_resource.study_id,
        trial_id=trial_resource.trial_id,
        serialized_trial=trial.SerializeToString(),
    )

    with self._lock:
      try:
        self._write_or_rollback(query)
      except sqla.exc.IntegrityError as e:
        raise AlreadyExistsError(
            'Trial with name %s already exists.' % trial.name
        ) from e
      self._connection.commit()

    return trial_resource

  def get_trial(self, trial_name: str) -> study_pb2.Trial:
    query = sqla.select(self._trials_table)
    query = query.where(self._trials_table.c.trial_name == trial_name)

    with self._lock:
      result = self._connection.execute(query)
      row = result.fetchone()

      if not row:
        raise NotFoundError('Failed to find trial name: %s' % trial_name)
      trial = study_pb2.Trial.FromString(row.serialized_trial)

    return trial

  def update_trial(self, trial: study_pb2.Trial) -> resources.TrialResource:
    trial_resource = resources.TrialResource.from_name(trial.name)

    # Exist query
    eq = sqla.select(self._trials_table)
    eq = eq.where(self._trials_table.c.trial_name == trial.name)
    eq = sqla.exists(eq).select()

    # Update query
    uq = sqla.update(self._trials_table)
    uq = uq.where(self._trials_table.c.trial_name == trial.name)
    uq = uq.values(
        trial_name=trial.name,
        owner_id=trial_resource.owner_id,
        study_id=trial_resource.study_id,
        trial_id=trial_resource.trial_id,
        serialized_trial=trial.SerializeToString(),
    )

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Trial %s does not exist.' % trial.name)
      self._write_or_rollback(uq)
      self._connection.commit()

    return trial_resource

  def list_trials(self, study_name: str) -> List[study_pb2.Trial]:
    study_resource = resources.StudyResource.from_name(study_name)

    # Exist query
    eq = sqla.select(self._studies_table)
    eq = eq.where(self._studies_table.c.study_name == study_name)
    eq = sqla.exists(eq).select()

    # List query
    lq = sqla.select(self._trials_table)
    lq = lq.where(self._trials_table.c.owner_id == study_resource.owner_id)
    lq = lq.where(self._trials_table.c.study_id == study_resource.study_id)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Study name %s does not exist.' % study_name)
      result = self._connection.execute(lq)
      trials = [
          study_pb2.Trial.FromString(row.serialized_trial) for row in result
      ]

    return trials

  def delete_trial(self, trial_name: str) -> None:
    # Exist query
    eq = sqla.select(self._trials_table)
    eq = eq.where(self._trials_table.c.trial_name == trial_name)
    eq = sqla.exists(eq).select()

    # Delete query
    dq = self._trials_table.delete()
    dq = dq.where(self._trials_table.c.trial_name == trial_name)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Trial %s does not exist.' % trial_name)
      self._write_or_rollback(dq)
      self._connection.commit()

  def max_trial_id(self, study_name: str) -> int:
    study_resource = resources.StudyResource.from_name(study_name)

    # Exist query
    eq = sqla.select(self._studies_table)
    eq = eq.where(self._studies_table.c.study_name == study_name)
    eq = sqla.exists(eq).select()

    # Trial ID query
    tq = sqla.func.max(self._trials_table.c.trial_id, type_=sqla.INT)
    tq = sqla.select(tq)
    tq = tq.where(self._trials_table.c.owner_id == study_resource.owner_id)
    tq = tq.where(self._trials_table.c.study_id == study_resource.study_id)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Study %s does not exist.' % study_name)
      potential_trial_id = self._connection.execute(tq).fetchone()[0]

    return potential_trial_id if potential_trial_id is not None else 0

  def create_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    resource = resources.SuggestionOperationResource.from_name(operation.name)
    query = self._suggestion_operations_table.insert().values(
        operation_name=operation.name,
        owner_id=resource.owner_id,
        study_id=resource.study_id,
        client_id=resource.client_id,
        operation_number=resource.operation_number,
        serialized_op=operation.SerializeToString(),
    )

    with self._lock:
      try:
        self._write_or_rollback(query)
      except sqla.exc.IntegrityError as e:
        raise AlreadyExistsError(
            'Suggest Op with name %s already exists.' % operation.name
        ) from e
      self._connection.commit()

    return resource

  def get_suggestion_operation(
      self, operation_name: str
  ) -> operations_pb2.Operation:
    q = sqla.select(self._suggestion_operations_table)
    q = q.where(
        self._suggestion_operations_table.c.operation_name == operation_name
    )

    with self._lock:
      row = self._connection.execute(q).fetchone()

      if not row:
        raise NotFoundError(
            'Failed to find suggest op name: %s' % operation_name
        )
      operation = operations_pb2.Operation.FromString(row.serialized_op)

    return operation

  def update_suggestion_operation(
      self, operation: operations_pb2.Operation
  ) -> resources.SuggestionOperationResource:
    resource = resources.SuggestionOperationResource.from_name(operation.name)

    # Exist query
    eq = sqla.select(self._suggestion_operations_table)
    eq = eq.where(
        self._suggestion_operations_table.c.operation_name == operation.name
    )
    eq = sqla.exists(eq).select()

    # Update query
    uq = sqla.update(self._suggestion_operations_table)
    uq = uq.where(
        self._suggestion_operations_table.c.operation_name == operation.name
    )
    uq = uq.values(
        operation_name=operation.name,
        owner_id=resource.owner_id,
        study_id=resource.study_id,
        client_id=resource.client_id,
        operation_number=resource.operation_number,
        serialized_op=operation.SerializeToString(),
    )

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError('Suggest op %s does not exist.' % operation.name)
      self._write_or_rollback(uq)
      self._connection.commit()

    return resource

  def list_suggestion_operations(
      self,
      study_name: str,
      client_id: str,
      filter_fn: Optional[Callable[[operations_pb2.Operation], bool]] = None,
  ) -> List[operations_pb2.Operation]:
    study_resource = resources.StudyResource.from_name(study_name)
    q = sqla.select(self._suggestion_operations_table)
    q = q.where(
        self._suggestion_operations_table.c.owner_id == study_resource.owner_id
    )
    q = q.where(
        self._suggestion_operations_table.c.study_id == study_resource.study_id
    )
    q = q.where(self._suggestion_operations_table.c.client_id == client_id)

    eq = sqla.exists(q).select()
    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError(
            'Could not find (study_name, client_id):',
            (study_resource.name, client_id),
        )
      result = self._connection.execute(q)
      all_ops = [
          operations_pb2.Operation.FromString(row.serialized_op)
          for row in result
      ]

    if filter_fn is None:
      return all_ops
    return [op for op in all_ops if filter_fn(op)]

  def max_suggestion_operation_number(
      self, study_name: str, client_id: str
  ) -> int:
    resource = resources.StudyResource.from_name(study_name)

    # Exist query
    eq = sqla.select(self._suggestion_operations_table)
    eq = eq.where(
        self._suggestion_operations_table.c.owner_id == resource.owner_id
    )
    eq = eq.where(
        self._suggestion_operations_table.c.study_id == resource.study_id
    )
    eq = eq.where(self._suggestion_operations_table.c.client_id == client_id)
    eq = sqla.exists(eq).select()

    # Max query
    mq = sqla.func.max(
        self._suggestion_operations_table.c.operation_number,
        type_=sqla.INT,
    )
    mq = sqla.select(mq)
    mq = mq.where(
        self._suggestion_operations_table.c.owner_id == resource.owner_id
    )
    mq = mq.where(
        self._suggestion_operations_table.c.study_id == resource.study_id
    )
    mq = mq.where(self._suggestion_operations_table.c.client_id == client_id)

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError(
            'Could not find (study_name, client_id):', (study_name, client_id)
        )
      max_op_number = self._connection.execute(mq).fetchone()[0]

    return max_op_number

  def create_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    resource = resources.EarlyStoppingOperationResource.from_name(
        operation.name
    )
    query = self._early_stopping_operations_table.insert().values(
        operation_name=operation.name,
        owner_id=resource.owner_id,
        study_id=resource.study_id,
        trial_id=resource.trial_id,
        serialized_op=operation.SerializeToString(),
    )

    with self._lock:
      try:
        self._write_or_rollback(query)
      except sqla.exc.IntegrityError as e:
        raise AlreadyExistsError(
            'Early stopping op with name %s already exists.' % operation.name
        ) from e
      self._connection.commit()

    return resource

  def get_early_stopping_operation(
      self, operation_name: str
  ) -> vizier_oss_pb2.EarlyStoppingOperation:
    q = sqla.select(self._early_stopping_operations_table)
    q = q.where(
        self._early_stopping_operations_table.c.operation_name == operation_name
    )

    with self._lock:
      result = self._connection.execute(q)

      row = result.fetchone()
      if not row:
        raise NotFoundError(
            'Failed to find early stopping op name: %s' % operation_name
        )
      operation = vizier_oss_pb2.EarlyStoppingOperation.FromString(
          row.serialized_op
      )

    return operation

  def update_early_stopping_operation(
      self, operation: vizier_oss_pb2.EarlyStoppingOperation
  ) -> resources.EarlyStoppingOperationResource:
    resource = resources.EarlyStoppingOperationResource.from_name(
        operation.name
    )

    # Exist query
    eq = sqla.select(self._early_stopping_operations_table)
    eq = eq.where(
        self._early_stopping_operations_table.c.operation_name == operation.name
    )
    eq = sqla.exists(eq).select()

    # Update query
    uq = sqla.update(self._early_stopping_operations_table)
    uq = uq.where(
        self._early_stopping_operations_table.c.operation_name == operation.name
    )
    uq = uq.values(
        operation_name=operation.name,
        owner_id=resource.owner_id,
        study_id=resource.study_id,
        trial_id=resource.trial_id,
        serialized_op=operation.SerializeToString(),
    )

    with self._lock:
      if not self._connection.execute(eq).fetchone()[0]:
        raise NotFoundError(
            'Early stopping op %s does not exist.' % operation.name
        )
      self._connection.execute(uq)
      self._connection.commit()

    return resource

  def update_metadata(
      self,
      study_name: str,
      study_metadata: Iterable[key_value_pb2.KeyValue],
      trial_metadata: Iterable[datastore.UnitMetadataUpdate],
  ) -> None:
    """Store the supplied metadata into the SQL database."""
    s_resource = resources.StudyResource.from_name(study_name)
    logging.debug('database.update_metadata s_resource= %s', s_resource)
    # Obtain original study.
    sq = sqla.select(self._studies_table)
    sq = sq.where(self._studies_table.c.study_name == study_name)

    with self._lock:
      row = self._connection.execute(sq).fetchone()
      if not row:
        raise NotFoundError('No such study:', s_resource.name)
      original_study = study_pb2.Study.FromString(row.serialized_study)

      # Store Study-related metadata into the database.
      vz.metadata_util.merge_study_metadata(
          original_study.study_spec, study_metadata
      )

      usq = sqla.update(self._studies_table)
      usq = usq.where(self._studies_table.c.study_name == study_name)
      usq = usq.values(serialized_study=original_study.SerializeToString())
      self._write_or_rollback(usq)

      # Split the trial-related metadata by Trial.
      split_metadata = collections.defaultdict(list)
      for md in trial_metadata:
        split_metadata[md.trial_id].append(md)

      # Now, we update one Trial at a time:
      for trial_id, md_list in split_metadata.items():
        t_resource = s_resource.trial_resource(trial_id)
        trial_name = t_resource.name

        # Obtain original trial.
        otq = sqla.select(self._trials_table)
        otq = otq.where(self._trials_table.c.trial_name == trial_name)
        row = self._connection.execute(otq).fetchone()
        if not row:
          self._connection.rollback()
          raise NotFoundError('No such trial:', trial_name)
        original_trial = study_pb2.Trial.FromString(row.serialized_trial)

        # Update Trial.
        vz.metadata_util.merge_trial_metadata(original_trial, md_list)
        utq = sqla.update(self._trials_table)
        utq = utq.where(self._trials_table.c.trial_name == trial_name)
        utq = utq.values(serialized_trial=original_trial.SerializeToString())
        self._write_or_rollback(utq)

      # Commit ALL changes if everything went well.
      self._connection.commit()


--- vizier/_src/service/sql_datastore_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for sql_datastore."""

import os
import sqlalchemy as sqla
from vizier._src.service import constants
from vizier._src.service import datastore_test_lib
from vizier._src.service import sql_datastore
from vizier._src.service.testing import util as test_util
from absl.testing import absltest


class SQLDataStoreTest(datastore_test_lib.DataStoreTestCase):

  def setUp(self):
    self.owner_id = 'my_username'
    self.study_id = '123123123'
    self.client_id = 'client_0'
    self.example_study = test_util.generate_study(self.owner_id, self.study_id)
    self.example_trials = test_util.generate_trials(
        [1, 2], owner_id=self.owner_id, study_id=self.study_id
    )
    self.example_suggestion_operations = (
        test_util.generate_suggestion_operations(
            [1, 2, 3, 4], self.owner_id, self.study_id, self.client_id
        )
    )
    self.example_early_stopping_operations = (
        test_util.generate_early_stopping_operations(
            [1, 2], self.owner_id, self.study_id
        )
    )

    engine = sqla.create_engine(
        constants.SQL_MEMORY_URL, echo=True, future=True
    )
    self.datastore = sql_datastore.SQLDataStore(engine)
    super().setUp()

  def test_study_api(self):
    self.assertStudyAPI(self.datastore, self.example_study)

  def test_trial(self):
    self.assertTrialAPI(self.datastore, self.example_study, self.example_trials)

  def test_suggestion_operation(self):
    self.assertSuggestOpAPI(
        self.datastore,
        self.example_study,
        self.client_id,
        self.example_suggestion_operations,
    )

  def test_early_stopping_operation(self):
    self.assertEarlyStoppingAPI(
        self.datastore,
        self.example_study,
        self.example_trials,
        self.example_early_stopping_operations,
    )

  def test_update_metadata(self):
    self.assertUpdateMetadataAPI(
        self.datastore, self.example_study, self.example_trials
    )


class SQLDataStoreAdditionalTest(absltest.TestCase):
  """For additional tests outside of regular database functionality."""

  def setUp(self):
    super().setUp()
    self.owner_id = 'my_username'
    self.study_id = '123123123'
    self.example_study = test_util.generate_study(self.owner_id, self.study_id)

  @absltest.skip("Github workflow tests don't allow using directories.")
  def test_local_hdd_persistence(self):
    db_path = os.path.join(absltest.get_default_test_tmpdir(), 'local.db')
    sql_url = f'sqlite:///{db_path}'

    engine = sqla.create_engine(sql_url, echo=True, future=True)
    datastore = sql_datastore.SQLDataStore(engine)
    datastore.create_study(self.example_study)
    del datastore

    engine2 = sqla.create_engine(sql_url, echo=True, future=True)
    datastore2 = sql_datastore.SQLDataStore(engine2)
    study = datastore2.load_study(self.example_study.name)

    self.assertEqual(self.example_study, study)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/stubs_util.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utility functions for creating GRPC stubs."""

import functools
from typing import Optional
from absl import logging
import grpc

from vizier._src.service import pythia_service_pb2_grpc
from vizier._src.service import vizier_service_pb2_grpc


def _create_channel(
    endpoint: str, timeout: Optional[float] = None
) -> grpc.Channel:
  """Creates GRPC channel."""
  logging.info('Securing channel to %s.', endpoint)
  channel = grpc.insecure_channel(endpoint)
  grpc.channel_ready_future(channel).result(timeout=timeout)
  logging.info('Created channel to %s.', endpoint)
  return channel


@functools.lru_cache(maxsize=128)
def create_pythia_server_stub(
    endpoint: str, timeout: Optional[float] = 10.0
) -> pythia_service_pb2_grpc.PythiaServiceStub:
  """Creates the GRPC stub.

  This method uses LRU cache so we create a single stub per endpoint (which is
  effectively one per binary). Stub and channel are both thread-safe and can
  take a while to create. The LRU cache makes binaries run faster, especially
  for unit tests.

  Args:
    endpoint: Pythia server endpoint.
    timeout: Timeout in seconds. If None, no timeout will be used.

  Returns:
    Pythia Server stub at endpoint.
  """
  return pythia_service_pb2_grpc.PythiaServiceStub(
      _create_channel(endpoint, timeout)
  )


@functools.lru_cache(maxsize=128)
def create_vizier_server_stub(
    endpoint: str, timeout: Optional[float] = 10.0
) -> vizier_service_pb2_grpc.VizierServiceStub:
  """Creates the GRPC stub.

  This method uses LRU cache so we create a single stub per endpoint (which is
  effectively one per binary). Stub and channel are both thread-safe and can
  take a while to create. The LRU cache makes binaries run faster, especially
  for unit tests.

  Args:
    endpoint: Vizier server endpoint.
    timeout: Timeout in seconds. If None, no timeout will be used.

  Returns:
    Vizier Server stub at endpoint.
  """
  return vizier_service_pb2_grpc.VizierServiceStub(
      _create_channel(endpoint, timeout)
  )


--- vizier/_src/service/stubs_util_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.service.stubs_util."""

import grpc
from vizier._src.service import stubs_util
from absl.testing import absltest
from absl.testing import parameterized


class StubsUtilTest(parameterized.TestCase):

  @parameterized.parameters(
      (stubs_util.create_pythia_server_stub,),
      (stubs_util.create_vizier_server_stub,),
  )
  def test_bad_endpoint(self, stub_creator):
    # Make sure connection errors out instead of stalling forever.
    endpoint = 'i_dont_exist'
    with self.assertRaises(grpc.FutureTimeoutError):
      stub_creator(endpoint, timeout=0.1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/testing/util.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Below contain utilities for writing tests to reduce code duplication."""
from typing import List, Sequence

from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2

from google.longrunning import operations_pb2


def generate_study(owner_id: str = 'my_username',
                   study_id: str = '1234',
                   display_name: str = 'cifar10',
                   **study_kwargs) -> study_pb2.Study:
  study_name = resources.StudyResource(owner_id, study_id).name
  return study_pb2.Study(
      name=study_name, display_name=display_name, **study_kwargs)


def generate_trials(trial_id_list: Sequence[int],
                    owner_id: str = 'my_username',
                    study_id: str = '1234',
                    **trial_kwargs) -> List[study_pb2.Trial]:
  """Generates arbitrary trials."""
  trials = []
  for trial_id in trial_id_list:
    trial = study_pb2.Trial(
        name=resources.TrialResource(owner_id, study_id, trial_id).name,
        id=str(trial_id),
        **trial_kwargs)
    trials.append(trial)
  return trials


def generate_all_states_trials(start_trial_index: int,
                               owner_id: str = 'my_username',
                               study_id: str = '1234',
                               **trial_kwargs) -> List[study_pb2.Trial]:
  """Generates a trial for each possible trial state."""
  trials = []
  for i, state in enumerate(study_pb2.Trial.State.keys()):
    trial_id = start_trial_index + i
    trial = study_pb2.Trial(
        name=resources.TrialResource(owner_id, study_id, trial_id).name,
        id=str(trial_id),
        state=state,
        **trial_kwargs)
    trials.append(trial)
  return trials


def generate_suggestion_operations(
    operation_numbers: Sequence[int],
    owner_id: str = 'my_username',
    study_id: str = 'cifar10',
    client_id: str = 'client0',
    **operation_kwargs) -> List[operations_pb2.Operation]:
  """Generates arbitrary suggestion operations."""
  operations = []
  for operation_number in operation_numbers:
    operation = operations_pb2.Operation(
        name=resources.SuggestionOperationResource(owner_id, study_id,
                                                   client_id,
                                                   operation_number).name,
        **operation_kwargs)
    operations.append(operation)
  return operations


def generate_early_stopping_operations(
    trial_id_list: Sequence[int],
    owner_id: str = 'my_username',
    study_id: str = '1234',
    **operation_kwargs) -> List[vizier_oss_pb2.EarlyStoppingOperation]:
  """Generates arbitrary early stopping operations."""
  operations = []
  for trial_id in trial_id_list:
    operation = vizier_oss_pb2.EarlyStoppingOperation(
        name=resources.EarlyStoppingOperationResource(owner_id, study_id,
                                                      trial_id).name,
        **operation_kwargs)
    operations.append(operation)
  return operations


def generate_all_four_parameter_specs(**study_spec_kwargs
                                     ) -> study_pb2.StudySpec:
  """All possible primitive parameter specs for testing."""
  double_value_spec = study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(
      min_value=-1.0, max_value=1.0)
  double_parameter_spec = study_pb2.StudySpec.ParameterSpec(
      parameter_id='learning_rate', double_value_spec=double_value_spec)

  integer_value_spec = study_pb2.StudySpec.ParameterSpec.IntegerValueSpec(
      min_value=1, max_value=10)
  integer_parameter_spec = study_pb2.StudySpec.ParameterSpec(
      parameter_id='num_layers', integer_value_spec=integer_value_spec)

  categorical_value_spec = (
      study_pb2.StudySpec.ParameterSpec.CategoricalValueSpec(
          values=['relu', 'sigmoid']
      )
  )
  categorical_parameter_spec = study_pb2.StudySpec.ParameterSpec(
      parameter_id='nonlinearity',
      categorical_value_spec=categorical_value_spec)

  discrete_value_spec = study_pb2.StudySpec.ParameterSpec.DiscreteValueSpec(
      values=[1.0, 1.5, 3.0, 4.5])
  discrete_parameter_spec = study_pb2.StudySpec.ParameterSpec(
      parameter_id='discrete_unnamed', discrete_value_spec=discrete_value_spec)

  return study_pb2.StudySpec(
      parameters=[
          double_parameter_spec, integer_parameter_spec,
          categorical_parameter_spec, discrete_parameter_spec
      ],
      **study_spec_kwargs)


--- vizier/_src/service/types.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Convenient types library for the /service/ folder."""
from typing import Union

from vizier._src.service import pythia_service_pb2_grpc
from vizier._src.service import vizier_service_pb2_grpc

# PythiaService and VizierService aliases allow us to use either a gRPC stub or
# the actual gRPC servicer implementation (for local setup) interchangeably.
PythiaService = Union[
    pythia_service_pb2_grpc.PythiaServiceStub,
    pythia_service_pb2_grpc.PythiaServiceServicer,
]

VizierService = Union[
    vizier_service_pb2_grpc.VizierServiceStub,
    vizier_service_pb2_grpc.VizierServiceServicer,
]


--- vizier/_src/service/vizier_client.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Makes client calls via gRPC to an existing Vizier Service Server.

This client can be used interchangeably with the Cloud Vizier client.
"""

import datetime
import functools
import time
from typing import Any, Dict, List, Mapping, Optional, Union

from absl import logging
import attr
import grpc

from vizier._src.service import constants
from vizier._src.service import resources
from vizier._src.service import stubs_util
from vizier._src.service import study_pb2
from vizier._src.service import types
from vizier._src.service import vizier_service_pb2
from vizier._src.service import vizier_service_pb2_grpc
from vizier.service import pyvizier
from vizier.utils import attrs_utils

from google.longrunning import operations_pb2
from google.protobuf import duration_pb2
from google.protobuf import json_format


@attr.define
class _EnvironmentVariables:
  """Global environment variables.

  Attributes:
    server_endpoint: Endpoint to the Vizier server.
    servicer_kwargs:
    new_suggestion_polling_secs: The period to wait between polling for the
      status of long-running SuggestOperations. Vizier may increase this period
      if multiple polls are needed. (You may use zero for interactive demos, but
      it is only appropriate for very small Studies.)
  """

  server_endpoint: str = attr.field(
      default=constants.NO_ENDPOINT, validator=attr.validators.instance_of(str)
  )
  servicer_kwargs: Dict[str, Any] = attr.field(factory=dict)

  # TODO: Add an e2e test for this.
  new_suggestion_polling_secs: float = attr.field(default=1.0)

  def servicer_use_sql_ram(self) -> None:
    """Should be used in tests to avoid filepath issues."""
    self.servicer_kwargs['database_url'] = constants.SQL_MEMORY_URL


environment_variables = _EnvironmentVariables()


@functools.lru_cache(maxsize=None)
def _create_local_vizier_servicer() -> (
    vizier_service_pb2_grpc.VizierServiceServicer
):
  from vizier._src.service import vizier_service  # pylint:disable=g-import-not-at-top

  return vizier_service.VizierServicer(**environment_variables.servicer_kwargs)


def create_vizier_servicer_or_stub() -> types.VizierService:
  endpoint = environment_variables.server_endpoint
  if endpoint == constants.NO_ENDPOINT:
    logging.info('No endpoint given; using cached local VizierServicer.')
    logging.warning('Python 3.8+ is required in this case.')
    return _create_local_vizier_servicer()
  return stubs_util.create_vizier_server_stub(endpoint)


@attr.frozen(init=True)
class VizierClient:
  """Client for communicating with the Vizier Service via GRPC.

  It can be initialized directly with a VizierService, or
  created from endpoint. See also `create_server_stub`.
  """

  _study_resource_name: str = attr.field(
      validator=attr.validators.instance_of(str)
  )
  _client_id: str = attr.field(
      validator=[attr.validators.instance_of(str), attrs_utils.assert_not_empty]
  )
  _service: types.VizierService = attr.field(
      repr=False, factory=create_vizier_servicer_or_stub
  )

  @property
  def _study_resource(self) -> resources.StudyResource:
    return resources.StudyResource.from_name(self._study_resource_name)

  @property
  def _owner_id(self) -> str:
    return self._study_resource.owner_id

  @property
  def _study_id(self) -> str:
    return self._study_resource.study_id

  @property
  def study_resource_name(self) -> str:
    return self._study_resource_name

  def get_suggestions(
      self, suggestion_count: int, *, client_id_override: Optional[str] = None
  ) -> List[pyvizier.Trial]:
    """Gets a list of suggested Trials.

    Args:
        suggestion_count: The number of suggestions to request.
        client_id_override: If set, overrides self._client_id for this call.

    Returns:
      A list of PyVizier Trials. This may be an empty list if:
      1. A finite search space has been exhausted.
      2. If max_num_trials has been reached.
      3. Or if there are no longer any trials that match a supplied Context.

    Raises:
        RuntimeError: Indicates that a suggestion was requested
            from an inactive study. Note that this is NOT raised when a
            finite Study runs out of suggestions. In such a case, an empty
            list is returned.
    """
    if client_id_override is not None:
      client_id = client_id_override
    else:
      client_id = self._client_id
    request = vizier_service_pb2.SuggestTrialsRequest(
        parent=resources.StudyResource(self._owner_id, self._study_id).name,
        suggestion_count=suggestion_count,
        client_id=client_id,
    )
    try:
      operation = self._service.SuggestTrials(request)
    except grpc.RpcError as rpc_error:
      # If ImmutableStudyError occurs, we simply return empty suggestion list.
      # Otherwise, halt the client and raise error.
      if rpc_error.code() == grpc.StatusCode.FAILED_PRECONDITION:  # pytype:disable=attribute-error
        return []
      raise rpc_error

    num_attempts = 0
    while not operation.done:
      sleep_time = PollingDelay(
          num_attempts, environment_variables.new_suggestion_polling_secs
      )
      num_attempts += 1
      logging.info(
          'Waiting for operation with name %s to be done', operation.name
      )
      time.sleep(sleep_time.total_seconds())

      operation = self._service.GetOperation(
          operations_pb2.GetOperationRequest(name=operation.name)
      )

    if operation.HasField('error'):
      error_message = 'SuggestOperation {} failed with message: {}'.format(
          operation.name, operation.error
      )
      logging.error(error_message)
      raise RuntimeError(error_message)
    # TODO: Replace with any.Unpack().
    trials = vizier_service_pb2.SuggestTrialsResponse.FromString(
        operation.response.value
    ).trials
    return pyvizier.TrialConverter.from_protos(trials)

  def report_intermediate_objective_value(
      self,
      step: int,
      elapsed_secs: float,
      metric_list: List[Mapping[str, Union[int, float]]],
      trial_id: int,
  ) -> pyvizier.Trial:
    """Sends intermediate objective value for the trial identified by trial_id."""
    new_metric_list = []
    for metric in metric_list:
      for metric_name in metric:
        metric_pb2 = study_pb2.Measurement.Metric(
            metric_id=metric_name, value=metric[metric_name]
        )
        new_metric_list.append(metric_pb2)

    integer_seconds = int(elapsed_secs)
    nano_seconds = int((elapsed_secs - integer_seconds) * 1e9)
    measurement = study_pb2.Measurement(
        elapsed_duration=duration_pb2.Duration(
            seconds=integer_seconds, nanos=nano_seconds
        ),
        step_count=step,
        metrics=new_metric_list,
    )
    request = vizier_service_pb2.AddTrialMeasurementRequest(
        trial_name=resources.TrialResource(
            self._owner_id, self._study_id, trial_id
        ).name,
        measurement=measurement,
    )
    trial = self._service.AddTrialMeasurement(request)
    return pyvizier.TrialConverter.from_proto(trial)

  def should_trial_stop(self, trial_id: int) -> bool:
    request = vizier_service_pb2.CheckTrialEarlyStoppingStateRequest(
        trial_name=resources.TrialResource(
            self._owner_id, self._study_id, trial_id
        ).name
    )
    early_stopping_response = self._service.CheckTrialEarlyStoppingState(
        request
    )
    return early_stopping_response.should_stop

  def stop_trial(self, trial_id: int) -> None:
    request = vizier_service_pb2.StopTrialRequest(
        name=resources.TrialResource(
            self._owner_id, self._study_id, trial_id
        ).name
    )
    self._service.StopTrial(request)
    logging.info('Trial with id %s stopped.', trial_id)

  def complete_trial(
      self,
      trial_id: int,
      final_measurement: Optional[pyvizier.Measurement] = None,
      infeasibility_reason: Optional[str] = None,
  ) -> pyvizier.Trial:
    """Completes the trial, which is infeasible if given a infeasibility_reason."""
    request = vizier_service_pb2.CompleteTrialRequest(
        name=resources.TrialResource(
            self._owner_id, self._study_id, trial_id
        ).name,
        trial_infeasible=infeasibility_reason is not None,
        infeasible_reason=infeasibility_reason,
    )

    if final_measurement is not None:
      # Final measurement can still be included even for infeasible trials, for
      # other metrics, or a subset of objective + safety metrics.
      request.final_measurement.CopyFrom(
          pyvizier.MeasurementConverter.to_proto(final_measurement)
      )

    trial = self._service.CompleteTrial(request)
    return pyvizier.TrialConverter.from_proto(trial)

  def get_trial(self, trial_id: int) -> pyvizier.Trial:
    """Return the Optimizer trial for the given trial_id."""
    request = vizier_service_pb2.GetTrialRequest(
        name=resources.TrialResource(
            self._owner_id, self._study_id, trial_id
        ).name
    )
    trial = self._service.GetTrial(request)
    return pyvizier.TrialConverter.from_proto(trial)

  def list_trials(self) -> List[pyvizier.Trial]:
    """List all trials."""
    parent = resources.StudyResource(self._owner_id, self._study_id).name
    request = vizier_service_pb2.ListTrialsRequest(parent=parent)
    response = self._service.ListTrials(request)
    return pyvizier.TrialConverter.from_protos(response.trials)

  def list_optimal_trials(self) -> List[pyvizier.Trial]:
    """List only the optimal completed trials."""
    parent = resources.StudyResource(self._owner_id, self._study_id).name
    request = vizier_service_pb2.ListOptimalTrialsRequest(parent=parent)
    response = self._service.ListOptimalTrials(request)
    return pyvizier.TrialConverter.from_protos(response.optimal_trials)

  def list_studies(self) -> List[Dict[str, Any]]:
    """List all studies for the given owner."""
    request = vizier_service_pb2.ListStudiesRequest(
        parent=resources.OwnerResource(self._owner_id).name
    )
    list_studies_response = self._service.ListStudies(request)
    # TODO: Use PyVizier StudyDescriptor instead.
    return [
        json_format.MessageToJson(study)
        for study in list_studies_response.studies
    ]

  def add_trial(self, trial: pyvizier.Trial) -> pyvizier.Trial:
    """Adds a trial.

    Args:
      trial:

    Returns:
      A new trial object from the database. It will have different timestamps,
      newly assigned id, and newly assigned state (either REQUESTED
      or COMPLETED).
    """
    request = vizier_service_pb2.CreateTrialRequest(
        parent=resources.StudyResource(self._owner_id, self._study_id).name,
        trial=pyvizier.TrialConverter.to_proto(trial),
    )
    trial_proto = self._service.CreateTrial(request)
    return pyvizier.TrialConverter.from_proto(trial_proto)

  def delete_trial(self, trial_id: int) -> None:
    """Deletes trial from datastore."""
    request_trial_name = resources.TrialResource(
        self._owner_id, self._study_id, trial_id
    ).name
    request = vizier_service_pb2.DeleteTrialRequest(name=request_trial_name)
    self._service.DeleteTrial(request)
    logging.info('Trial deleted: %s', trial_id)

  def delete_study(self, study_resource_name: Optional[str] = None) -> None:
    """Deletes study from datastore."""
    study_resource_name = study_resource_name or (
        resources.StudyResource(self._owner_id, self._study_id).name
    )
    request = vizier_service_pb2.DeleteStudyRequest(name=study_resource_name)
    self._service.DeleteStudy(request)
    logging.info('Study deleted: %s', study_resource_name)

  def get_study_config(
      self, study_resource_name: Optional[str] = None
  ) -> pyvizier.StudyConfig:
    """Returns the study config."""
    study_resource_name = study_resource_name or (
        resources.StudyResource(self._owner_id, self._study_id).name
    )
    request = vizier_service_pb2.GetStudyRequest(name=study_resource_name)
    study = self._service.GetStudy(request)
    return pyvizier.StudyConfig.from_proto(study.study_spec)

  def get_study_state(
      self, study_resource_name: Optional[str] = None
  ) -> pyvizier.StudyState:
    study_resource_name = study_resource_name or (
        resources.StudyResource(self._owner_id, self._study_id).name
    )
    request = vizier_service_pb2.GetStudyRequest(name=study_resource_name)
    study = self._service.GetStudy(request)
    return pyvizier.StudyStateConverter.from_proto(study.state)

  def set_study_state(
      self,
      state: pyvizier.StudyState,
      study_resource_name: Optional[str] = None,
  ) -> None:
    """Sets the study to a given state."""
    study_resource_name = study_resource_name or (
        resources.StudyResource(self._owner_id, self._study_id).name
    )
    proto_state = pyvizier.StudyStateConverter.to_proto(state)
    request = vizier_service_pb2.SetStudyStateRequest(
        parent=study_resource_name, state=proto_state
    )
    self._service.SetStudyState(request)
    logging.info(
        'Study with resource name %s set to %s state.',
        study_resource_name,
        state,
    )

  def update_metadata(
      self,
      delta: pyvizier.MetadataDelta,
      study_resource_name: Optional[str] = None,
  ) -> None:
    """Updates metadata.

    Args:
      delta: Batch updates to Metadata, consisting of numerous (namespace, key,
        value) objects.
      study_resource_name: An identifier of the study. The full study name will
        be `owners/{owner_id}/studies/{study_id}`.

    Returns:
      None.

    Raises:
      RuntimeError: If service reported an error or if a value could not be
      pickled.
    """
    study_resource_name = study_resource_name or (
        resources.StudyResource(self._owner_id, self._study_id).name
    )
    request = pyvizier.metadata_util.to_request_proto(
        study_resource_name, delta
    )
    response = self._service.UpdateMetadata(request)

    if response.error_details:
      raise RuntimeError(response.error_details)


def create_or_load_study(
    owner_id: str,
    client_id: str,
    study_id: str,
    study_config: pyvizier.StudyConfig,
) -> VizierClient:
  """Factory method for creating or loading a VizierClient.

  This will either create or load the specified study, given
  (owner_id, study_id, study_config). It will create it if it doesn't
  already exist, and load it if someone has already created it.

  Note that once a study is created, you CANNOT modify it with this function.

  This function is designed for use in a distributed system, where many jobs
  initially call create_or_load_study() nearly simultaneously with the same
  `study_config`. In that situation, all clients will end up pointing nicely
  to the same study.

  Args:
      owner_id: An owner id.
      client_id: ID for the VizierClient. See class for notes.
      study_id: Each study is uniquely identified by the tuple (owner_id,
        study_id).
      study_config: Study configuration for Vizier service. If not supplied, it
        will be assumed that the study with the given study_id already exists,
        and will try to retrieve that study.

  Returns:
      A VizierClient object with the specified study created or loaded.

  Raises:
      RuntimeError: Indicates that study_config is supplied but CreateStudy
          failed and GetStudy did not succeed after
          constants.MAX_NUM_TRIES_FOR_STUDIES tries.
      ValueError: Indicates that study_config is not supplied and the study
          with the given study_id does not exist.
  """
  vizier_stub = create_vizier_servicer_or_stub()
  study = study_pb2.Study(
      display_name=study_id, study_spec=study_config.to_proto()
  )
  request = vizier_service_pb2.CreateStudyRequest(
      parent=resources.OwnerResource(owner_id).name, study=study
  )
  # The response study contains a service assigned `name`, and may have been
  # created by this RPC or a previous RPC from another client.
  study = vizier_stub.CreateStudy(request)
  return VizierClient(study.name, client_id, vizier_stub)


def PollingDelay(num_attempts: int, time_scale: float) -> datetime.timedelta:  # pylint:disable=invalid-name
  """Computes a delay to the next attempt to poll the Vizier service.

  This does bounded exponential backoff, starting with $time_scale.
  If $time_scale == 0, it starts with a small time interval, less than
  1 second.

  Args:
    num_attempts: The number of times have we polled and found that the desired
      result was not yet available.
    time_scale: The shortest polling interval, in seconds, or zero. Zero is
      treated as a small interval, less than 1 second.

  Returns:
    A recommended delay interval, in seconds.
  """
  small_interval = 0.3  # Seconds
  interval = max(time_scale, small_interval) * 1.41 ** min(num_attempts, 9)
  return datetime.timedelta(seconds=interval)


--- vizier/_src/service/vizier_client_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.service.vizier_client."""

# TODO: Rewrite using RPC calls and remove all direct lookups
# into the datastore.
# TODO: Cover delete_trial, delete_study, get_study_config, and
# add_trial, OR turn it into a private module

from typing import List

from absl import logging
from vizier._src.service import constants
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_client
from vizier._src.service import vizier_server
from vizier._src.service import vizier_service_pb2_grpc
from vizier.service import pyvizier as vz

from absl.testing import absltest
from absl.testing import parameterized


class VizierClientTest(parameterized.TestCase):

  def setUp(self):
    super().setUp()

    # Setup Vizier Server and some pre-stored data.
    self.local_server = vizier_server.DefaultVizierServer(
        database_url=constants.SQL_MEMORY_URL
    )
    self.servicer = self.local_server._servicer
    self.owner_id = 'my_username'
    self.study_id = '1231232'
    self.study_resource_name = resources.StudyResource(
        self.owner_id, self.study_id
    ).name

    # Setup connection to server.
    vizier_client.environment_variables.server_endpoint = (
        self.local_server.endpoint
    )
    self.client = vizier_client.VizierClient(
        study_resource_name=self.study_resource_name, client_id='my_client'
    )

    # Store initial data in the vizier service.
    double_value_spec = study_pb2.StudySpec.ParameterSpec.DoubleValueSpec(
        min_value=-1.0, max_value=1.0
    )
    double_parameter_spec = study_pb2.StudySpec.ParameterSpec(
        parameter_id='double', double_value_spec=double_value_spec
    )
    metric_spec = study_pb2.StudySpec.MetricSpec(
        metric_id='example_metric',
        goal=study_pb2.StudySpec.MetricSpec.GoalType.MAXIMIZE,
    )
    self.example_study = study_pb2.Study(
        name=self.study_resource_name,
        study_spec=study_pb2.StudySpec(
            algorithm='RANDOM_SEARCH',
            parameters=[double_parameter_spec],
            metrics=[metric_spec],
        ),
    )
    self.active_trial = study_pb2.Trial(
        name=resources.TrialResource(self.owner_id, self.study_id, 1).name,
        id='1',
        state=study_pb2.Trial.State.ACTIVE,
    )
    self.servicer.datastore.create_study(self.example_study)
    self.servicer.datastore.create_trial(self.active_trial)

  def test_create_or_load_study(self):
    study_config = vz.StudyConfig()
    study_id = 'example_display_name'

    client = vizier_client.create_or_load_study(
        owner_id=self.owner_id,
        client_id='a_client',
        study_id=study_id,
        study_config=study_config,
    )
    study = self.servicer.datastore.load_study(client.study_resource_name)
    self.assertEqual(study.study_spec, study_config.to_proto())
    self.assertIsNotNone(study.name)

    another_client = vizier_client.create_or_load_study(
        owner_id=self.owner_id,
        client_id='another_client',
        study_id=study_id,
        study_config=study_config,
    )
    self.assertEqual(
        client.study_resource_name, another_client.study_resource_name
    )

  def test_list_studies(self):
    study_list_json = self.client.list_studies()
    self.assertLen(study_list_json, 1)

  @parameterized.parameters(
      (vz.StudyState.ABORTED,),
      (vz.StudyState.ACTIVE,),
  )
  def test_set_and_get_study_state(self, state):
    self.client.set_study_state(state)
    self.assertEqual(self.client.get_study_state(), state)

  def test_delete_study(self):
    self.client.delete_study()
    empty_list_json = self.client.list_studies()
    self.assertEmpty(empty_list_json)

  def test_list_trials(self):
    trial_list = self.client.list_trials()
    self.assertLen(trial_list, 1)

  def test_list_optimal_trials(self):
    for i in range(2, 10):
      metric = study_pb2.Measurement.Metric(
          metric_id='example_metric', value=0.2 * i
      )
      completed_trial = study_pb2.Trial(
          name=resources.TrialResource(self.owner_id, self.study_id, i).name,
          id=str(i),
          state=study_pb2.Trial.State.SUCCEEDED,
          final_measurement=study_pb2.Measurement(metrics=[metric]),
      )

      self.servicer.datastore.create_trial(completed_trial)

    trial_list = self.client.list_optimal_trials()
    self.assertLen(trial_list, 1)

  def test_get_trial(self):
    active_trial = self.client.get_trial(trial_id=1)
    self.assertEqual(
        active_trial, vz.TrialConverter.from_proto(self.active_trial)
    )

  @parameterized.named_parameters(
      ('Infeasible', 'infeasible_reason'), ('Complete', None)
  )
  def test_complete_trial(self, infeasibility_reason):
    final_measurement = vz.Measurement(metrics={'metric': vz.Metric(value=0.1)})
    output_trial = self.client.complete_trial(
        trial_id=1,
        final_measurement=final_measurement,
        infeasibility_reason=infeasibility_reason,
    )

    self.assertEqual(output_trial.status, vz.TrialStatus.COMPLETED)
    self.assertEqual(output_trial.infeasibility_reason, infeasibility_reason)

    # See if the rest of the contents were maintained.
    completed_trial = vz.TrialConverter.from_proto(
        self.servicer.datastore.get_trial(self.active_trial.name)
    )
    self.assertEqual(output_trial, completed_trial)

  def test_should_trial_stop(self):
    # Only trial 1 was ACTIVE, so RandomPolicy will signal to stop it.
    should_stop = self.client.should_trial_stop(trial_id=1)
    self.assertTrue(should_stop)
    self.client.stop_trial(trial_id=1)

    # The op will become recycled after the time period and early stopping will
    # be recomputed again. But RandomPolicy will consider the trial non-ACTIVE
    # and simply return False.
    self.local_server.wait_for_early_stop_recycle_period()
    should_stop_again = self.client.should_trial_stop(trial_id=1)
    self.assertFalse(should_stop_again)

  def test_intermediate_measurement(self):
    updated_trial = self.client.report_intermediate_objective_value(
        step=5,
        elapsed_secs=3.0,
        metric_list=[{'example_metric': 5}],
        trial_id=1,
    )
    self.assertLen(updated_trial.measurements, 1)
    self.assertEqual(updated_trial.measurements[0].steps, 5)
    self.assertEqual(updated_trial.measurements[0].elapsed_secs, 3.0)
    self.assertEqual(
        updated_trial.measurements[0].metrics['example_metric'],
        vz.Metric(value=5.0),
    )
    self.assertEqual(updated_trial.id, 1)

  def test_get_suggestions(self):
    suggestion_count = 2
    suggestions_list = self.client.get_suggestions(
        suggestion_count=suggestion_count
    )
    self.assertLen(suggestions_list, suggestion_count)
    logging.info('Suggestions List: %s', suggestions_list)

  # Only test algorithms which don't depend on external libraries (except for
  # numpy).
  @parameterized.parameters(
      dict(algorithm='DEFAULT'),
      dict(algorithm=vz.Algorithm.ALGORITHM_UNSPECIFIED),
      dict(algorithm=vz.Algorithm.RANDOM_SEARCH),
      dict(algorithm=vz.Algorithm.QUASI_RANDOM_SEARCH),
      dict(algorithm=vz.Algorithm.GRID_SEARCH),
      dict(algorithm=vz.Algorithm.NSGA2, multiobj=True),
      dict(algorithm=vz.Algorithm.GAUSSIAN_PROCESS_BANDIT, multiobj=True),
      dict(algorithm=vz.Algorithm.GAUSSIAN_PROCESS_BANDIT, multiobj=False),
      dict(algorithm=vz.Algorithm.GP_UCB_PE),
  )
  def test_e2e_tuning(
      self,
      *,
      algorithm,
      num_iterations: int = 10,
      batch_size: int = 1,
      multiobj: bool = False,
  ):
    # Runs end-to-end tuning via back-and-forth communication to server.
    def learning_curve_generator(learning_rate: float) -> List[float]:
      return [learning_rate * step for step in range(10)]

    study_config = vz.StudyConfig()
    study_config.search_space.root.add_float_param(
        'learning_rate', min_value=0.0, max_value=1.0, default_value=0.5
    )
    study_config.search_space.root.add_int_param(
        'num_layers', min_value=1, max_value=5
    )
    study_config.metric_information = [
        vz.MetricInformation(
            name='accuracy', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    ]
    if multiobj:
      study_config.metric_information.append(
          vz.MetricInformation(
              name='latency', goal=vz.ObjectiveMetricGoal.MINIMIZE
          )
      )
    study_config.algorithm = algorithm

    cifar10_client = vizier_client.create_or_load_study(
        owner_id=self.owner_id,
        study_id='cifar10',
        study_config=study_config,
        client_id='cifar10_client',
    )

    for _ in range(num_iterations):
      suggestions = cifar10_client.get_suggestions(suggestion_count=batch_size)
      for trial in suggestions:
        learning_rate = trial.parameters.get_value('learning_rate')
        num_layers = trial.parameters.get_value('num_layers')
        curve = learning_curve_generator(learning_rate)
        for i in range(len(curve)):
          cifar10_client.report_intermediate_objective_value(
              step=i,
              elapsed_secs=0.1 * i,
              metric_list=[{'accuracy': curve[i], 'latency': 0.5 * num_layers}],
              trial_id=trial.id,
          )
        cifar10_client.complete_trial(trial_id=trial.id)

        # Recover the trial from database.
        study_resource = resources.StudyResource.from_name(
            cifar10_client.study_resource_name
        )
        trial_name = resources.TrialResource(
            study_resource.owner_id, study_resource.study_id, trial.id
        ).name
        stored_trial = self.servicer.datastore.get_trial(trial_name)
        stored_curve = [m.metrics[0].value for m in stored_trial.measurements]

        # See if curve was stored correctly.
        self.assertEqual(curve, stored_curve)

        # See if final_measurement is defaulted to end of curve.
        final_accuracy = stored_trial.final_measurement.metrics[0].value
        self.assertEqual(curve[-1], final_accuracy)

  def test_update_metadata(self):
    # Only a smoke test, same as in `service_policy_supporter_test.py`.
    on_study_metadata = vz.Metadata()
    on_study_metadata.ns('bar')['foo'] = '.bar.foo.1'
    on_trial1_metadata = vz.Metadata()
    on_trial1_metadata.ns('bax')['nerf'] = '1.bar.nerf.2'
    metadata_delta = vz.MetadataDelta(
        on_study=on_study_metadata, on_trials={1: on_trial1_metadata}
    )
    self.client.update_metadata(metadata_delta)

  def test_unset_endpoint_client(self):
    study_id = 'dummy_study'
    study_config = vz.StudyConfig()
    study_resource_name = resources.StudyResource(self.owner_id, study_id).name

    vizier_client.environment_variables.server_endpoint = constants.NO_ENDPOINT
    vizier_client.environment_variables.servicer_use_sql_ram()
    # Check if servicer is stored in client.
    local_client1 = vizier_client.create_or_load_study(
        owner_id=self.owner_id,
        client_id='local_client1',
        study_id=study_id,
        study_config=study_config,
    )
    self.assertIsInstance(
        local_client1._service, vizier_service_pb2_grpc.VizierServiceServicer
    )

    # Check if the local server is shared.
    local_client2 = vizier_client.VizierClient(
        study_resource_name=study_resource_name, client_id='local_client2'
    )
    self.assertEqual(local_client1._service, local_client2._service)

    # Same server still exists globally in cache after clients are deleted.
    del local_client1
    del local_client2
    local_client3 = vizier_client.VizierClient(
        study_resource_name=study_resource_name, client_id='local_client3'
    )
    self.assertLen(local_client3.list_studies(), 1)


if __name__ == '__main__':
  absltest.main()


--- vizier/_src/service/vizier_server.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Classes for starting the Vizier Server.

Extensive tests can be found in `clients_test.py`.
"""

from concurrent import futures
import datetime
import time
from typing import Optional

import attr
import grpc
import portpicker
from vizier import pythia
from vizier._src.service import constants
from vizier._src.service import datastore
from vizier._src.service import policy_factory as service_policy_factory_lib
from vizier._src.service import pythia_service
from vizier._src.service import pythia_service_pb2_grpc
from vizier._src.service import stubs_util
from vizier._src.service import vizier_service
from vizier._src.service import vizier_service_pb2_grpc


@attr.define
class DefaultVizierServer:
  """Vizier Server which runs Pythia and Vizier Servicers in the same process.

  Both servicers have access to the others' literal class instances.

  NOTE: When using this in a test, the database_url should be in-memory
  (SQL_MEMORY_URL) since tests don't easily allow arbitrarily filepaths.
  """

  _host: str = attr.field(default='localhost')
  _database_url: Optional[str] = attr.field(
      default=constants.SQL_LOCAL_URL, kw_only=True
  )
  _policy_factory: pythia.PolicyFactory = attr.field(
      factory=service_policy_factory_lib.DefaultPolicyFactory, kw_only=True
  )
  _early_stop_recycle_period: datetime.timedelta = attr.field(
      default=datetime.timedelta(seconds=0.1), kw_only=True
  )
  _port: int = attr.field(factory=portpicker.pick_unused_port, kw_only=True)

  # Fields which should not be set by users.
  _servicer: vizier_service.VizierServicer = attr.field(init=False)
  _server: grpc.Server = attr.field(init=False)
  stub: vizier_service_pb2_grpc.VizierServiceStub = attr.field(init=False)

  @property
  def datastore(self) -> datastore.DataStore:
    return self._servicer.datastore

  @property
  def endpoint(self) -> str:
    return f'{self._host}:{self._port}'

  def __attrs_post_init__(self):
    # Setup Vizier server.
    self._servicer = vizier_service.VizierServicer(
        database_url=self._database_url,
        early_stop_recycle_period=self._early_stop_recycle_period,
    )
    self._server = grpc.server(futures.ThreadPoolExecutor(max_workers=30))
    vizier_service_pb2_grpc.add_VizierServiceServicer_to_server(
        self._servicer, self._server
    )
    self._server.add_insecure_port(self.endpoint)
    self._server.start()
    self.stub = stubs_util.create_vizier_server_stub(self.endpoint)

    # Re-set the default Pythia Service to allow custom policy factories.
    default_pythia_service = pythia_service.PythiaServicer(
        self._servicer, policy_factory=self._policy_factory
    )
    self._servicer.default_pythia_service = default_pythia_service

  def wait_for_early_stop_recycle_period(self) -> None:
    time.sleep(self._early_stop_recycle_period.total_seconds())


@attr.define
class DistributedPythiaVizierServer(DefaultVizierServer):
  """Separates Pythia from Vizier via over-the-wire distributed communication.

  This is for testing / demonstration purposes only, as in normal use-cases, the
  Pythia server should actually be created in a separate process from the Vizier
  server.
  """

  _pythia_port: int = attr.field(
      factory=portpicker.pick_unused_port, kw_only=True
  )

  # Fields which should not be set by users.
  _pythia_servicer: pythia_service.PythiaServicer = attr.field(init=False)
  _pythia_server: grpc.Server = attr.field(init=False)
  pythia_stub: pythia_service_pb2_grpc.PythiaServiceStub = attr.field(
      init=False
  )

  @property
  def pythia_endpoint(self) -> str:
    return f'{self._host}:{self._pythia_port}'

  def __attrs_post_init__(self):
    super().__attrs_post_init__()
    # Setup Pythia server.
    self._pythia_servicer = pythia_service.PythiaServicer(
        policy_factory=self._policy_factory
    )
    # `max_workers=1` is used since we can only run one Pythia thread at a time.
    self._pythia_server = grpc.server(futures.ThreadPoolExecutor(max_workers=1))
    pythia_service_pb2_grpc.add_PythiaServiceServicer_to_server(
        self._pythia_servicer, self._pythia_server
    )
    self._pythia_server.add_insecure_port(self.pythia_endpoint)
    self._pythia_server.start()
    self.pythia_stub = stubs_util.create_pythia_server_stub(
        self.pythia_endpoint
    )

    # Connect Vizier and Pythia servers together.
    self._servicer.default_pythia_service = self.pythia_stub
    self._pythia_servicer.connect_to_vizier(self.endpoint)


--- vizier/_src/service/vizier_service.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""RPC functions implemented from vizier_service.proto."""

import collections
import datetime
import threading
from typing import Optional

from absl import logging
import grpc
import numpy as np
import sqlalchemy as sqla
from vizier import pythia
from vizier import pyvizier as vz
from vizier._src.service import constants
from vizier._src.service import custom_errors
from vizier._src.service import grpc_util
from vizier._src.service import pythia_service
from vizier._src.service import ram_datastore
from vizier._src.service import resources
from vizier._src.service import sql_datastore
from vizier._src.service import stubs_util
from vizier._src.service import study_pb2
from vizier._src.service import types
from vizier._src.service import vizier_oss_pb2
from vizier._src.service import vizier_service_pb2
from vizier._src.service import vizier_service_pb2_grpc
from vizier.service import pyvizier as svz

from google.longrunning import operations_pb2
from google.protobuf import empty_pb2
from google.protobuf import timestamp_pb2
from google.rpc import code_pb2
from google.rpc import status_pb2


def _get_current_time() -> timestamp_pb2.Timestamp:
  now = timestamp_pb2.Timestamp()
  now.GetCurrentTime()
  return now


StudyResource = resources.StudyResource
TrialResource = resources.TrialResource


# TODO: remove context = None
# TODO: remove context = None
class VizierServicer(vizier_service_pb2_grpc.VizierServiceServicer):
  """Implements the GRPC functions outlined in vizier_service.proto."""

  # Trial states in which a trial can be modified.
  _TRIAL_MUTABLE_STATES = (
      study_pb2.Trial.State.ACTIVE,
      study_pb2.Trial.State.STOPPING,
  )

  def __init__(
      self,
      database_url: Optional[str] = constants.SQL_LOCAL_URL,
      early_stop_recycle_period: datetime.timedelta = datetime.timedelta(
          seconds=60
      ),
      default_pythia_service: Optional[types.PythiaService] = None,
  ):
    """Initializes the service.

    Creates the datastore and relevant locks for multhreading. Note that the
    datastore input/output is assumed to always be pass-by-value.

    Args:
      database_url: URL to the SQL database. If None, it connects to our custom
        RAM Datastore.
      early_stop_recycle_period: Amount of time needed to pass before recycling
        an early stopping operation. See `CheckEarlyStoppingState` for more
        details.
      default_pythia_service: Default PythiaService to use when
        StudyConfig.pythia_endpoint is unset. If None, creates a local
        PythiaServicer.
    """
    # By default, uses a local PythiaServicer instance.
    self.default_pythia_service: types.PythiaService = (
        default_pythia_service
        or pythia_service.PythiaServicer(vizier_service=self)
    )

    if database_url is None:
      self.datastore = ram_datastore.NestedDictRAMDataStore()
    else:
      engine = sqla.create_engine(
          database_url,
          connect_args={'check_same_thread': False},
          echo=False,  # Set True to log transactions for debugging.
          future=True,  # Backward compatibility with sqlalchemy 1.4.
          poolclass=sqla.pool.StaticPool,
      )
      self.datastore = sql_datastore.SQLDataStore(engine)

    # For database edits using owner names.
    self._owner_name_to_lock = collections.defaultdict(threading.Lock)
    # For database edits using study names.
    self._study_name_to_lock = collections.defaultdict(threading.Lock)
    # For calls to Pythia (SuggestTrials and CheckTrialEarlyStoppingState).
    self._operation_lock = collections.defaultdict(threading.Lock)

    self._early_stop_recycle_period = early_stop_recycle_period

  def _select_pythia_service(
      self, endpoint: Optional[str] = None
  ) -> types.PythiaService:
    """Selects PythiaService to use."""
    # TODO: Add test for StudyConfig endpointing.
    if endpoint is None:
      logging.info('Using default PythiaServicer.')
      return self.default_pythia_service

    logging.info('Connecting to Pythia endpoint: %s', endpoint)
    pythia_stub = stubs_util.create_pythia_server_stub(endpoint)
    logging.info('Created Pythia Server stub: %s', pythia_stub)
    return pythia_stub

  def _study_is_immutable(self, study_name: str) -> bool:
    """Checks if study is immutable to block study-related mutations."""
    study = self.datastore.load_study(study_name)
    return study.state not in (
        study_pb2.Study.State.ACTIVE,
        study_pb2.Study.State.STATE_UNSPECIFIED,
    )

  def CreateStudy(
      self,
      request: vizier_service_pb2.CreateStudyRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Study:
    """Creates a study or loads an existing one.

    The initial request contains the study without the study name, only
    a user/client specified display_name for locating a potential pre-existing
    study. The returned study will then have the service-provided resource name.

    Args:
      request: A CreateStudyRequest.
      context: Optional GRPC ServicerContext.

    Returns:
      study: The Study created with generated service resource name.
    """
    study = request.study
    owner_id = resources.OwnerResource.from_name(request.parent).owner_id
    if request.study.name:
      e = ValueError(
          'Maximum number of studies reached for owner {}.'.format(owner_id)
      )
      grpc_util.handle_exception(e, context)
    if not request.study.display_name:
      grpc_util.handle_exception(
          ValueError('Study display_name must be specified.'), context
      )

    with self._owner_name_to_lock[request.parent]:
      # Database creates a new active study or loads existing study using the
      # display name.
      try:
        possible_candidate_studies = self.datastore.list_studies(request.parent)
      except custom_errors.NotFoundError:
        possible_candidate_studies = []

      # Check if all possible study_id's have been taken.
      if len(possible_candidate_studies) >= constants.MAX_STUDY_ID:
        e = ValueError(
            'Maximum number of studies reached for owner {}.'.format(owner_id)
        )
        grpc_util.handle_exception(e, context)

      for candidate_study in possible_candidate_studies:
        if candidate_study.display_name == request.study.display_name:
          logging.info(
              'Found existing study of owner=%s display_name=%s!',
              owner_id,
              candidate_study.display_name,
          )
          return candidate_study

      # No study in the database matches the resource name. Making a new one.
      # study_id must be unique among existing studies.
      study_id = study.display_name

      # Finally create study in database and return it.
      study.name = StudyResource(owner_id, study_id).name
      self.datastore.create_study(study)
    return study

  def GetStudy(
      self,
      request: vizier_service_pb2.GetStudyRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Study:
    """Gets a Study by name. If the study does not exist, return error."""
    return self.datastore.load_study(request.name)

  def ListStudies(
      self,
      request: vizier_service_pb2.ListStudiesRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> vizier_service_pb2.ListStudiesResponse:
    """Lists all the studies in a region for an associated project."""
    studies = self.datastore.list_studies(request.parent)
    return vizier_service_pb2.ListStudiesResponse(studies=studies)

  def DeleteStudy(
      self,
      request: vizier_service_pb2.DeleteStudyRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> empty_pb2.Empty:
    """Deletes a Study."""
    self.datastore.delete_study(request.name)
    return empty_pb2.Empty()

  def SetStudyState(
      self,
      request: vizier_service_pb2.SetStudyStateRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Study:
    with self._study_name_to_lock[request.parent]:
      study = self.datastore.load_study(request.parent)
      study.state = request.state
      self.datastore.update_study(study)
    return study

  def SuggestTrials(
      self,
      request: vizier_service_pb2.SuggestTrialsRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> operations_pb2.Operation:
    """Adds one or more Trials to a Study, with parameter values suggested by a Pythia policy.

    The logic is as follows:
    1. If there is already an active (not done) operation, simply return that.
    2. Else, create a new active operation.
    3. We need the requested number of ACTIVE trials to return. These will come
    from 3 sources:
      A. ACTIVE trials already assigned to the client.
      B. REQUESTED trials which not been assigned to any client.
      C. Pythia-computed suggestions.

    We first look from source A, then source B, then source C. If we ever reach
    source C and Pythia over-delivers too many suggestions (e.g. evolutionary
    algorithms sometimes do so because of batched behaviors), we put the extra
    suggestions into the REQUESTED pool (i.e. source B) for future use.

    The returned operation can either be:
    1. Done (Successfully contains the requested number of suggestions)
    2. Error-ed, which will happen if Pythia had an issue producing the right
    number of trials (via numerical crash or under-delivering).

    Args:
      request:
      context:

    Returns:
      A long-running operation associated with the generation of Trial
      suggestions. When this long-running operation succeeds, it will contain a
      [SuggestTrialsResponse].

    Raises:
      ImmutableStudyError: If study was already immutable.
    """
    # Convenient names and id's to be used below.
    study_name = request.parent
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot suggest trial.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    study_resource = StudyResource.from_name(study_name)
    study_id = study_resource.study_id
    owner_id = study_resource.owner_id

    # Don't allow simultaneous SuggestTrial or EarlyStopping calls to be
    # processed.
    with self._operation_lock[request.parent]:
      study = self.datastore.load_study(request.parent)

      # Checks for a non-done operation in the database with this name.
      active_op_filter_fn = lambda op: not op.done
      try:
        active_op_list = self.datastore.list_suggestion_operations(
            study_name, request.client_id, active_op_filter_fn
        )
      except custom_errors.NotFoundError:
        active_op_list = []
      if active_op_list:
        return active_op_list[0]  # We've found the active one!

      start_time = _get_current_time()
      # Create a new Op if there aren't any active (not done) ops.
      try:
        old_op_number = self.datastore.max_suggestion_operation_number(
            study_name, request.client_id
        )
      except custom_errors.NotFoundError:
        old_op_number = 0
      new_op_number = old_op_number + 1
      new_op_name = resources.SuggestionOperationResource(
          owner_id, study_id, request.client_id, new_op_number
      ).name
      output_op = operations_pb2.Operation(name=new_op_name, done=False)
      self.datastore.create_suggestion_operation(output_op)

      # Check how many ACTIVE trials already exist for this client only.
      all_trials = self.datastore.list_trials(study_name)
      active_trials = [
          t
          for t in all_trials
          if t.state == study_pb2.Trial.State.ACTIVE
          and t.client_id == request.client_id
      ]
      if len(active_trials) >= request.suggestion_count:
        output_op.response.value = vizier_service_pb2.SuggestTrialsResponse(
            trials=active_trials[: request.suggestion_count],
            start_time=start_time,
        ).SerializeToString()
        output_op.done = True
        self.datastore.update_suggestion_operation(output_op)
        return output_op

      # Get suggestions from the pool of requested trials.
      output_trials = active_trials
      requested_trials = [
          t for t in all_trials if t.state == study_pb2.Trial.State.REQUESTED
      ]
      while requested_trials and request.suggestion_count > len(output_trials):
        assigned_trial = requested_trials.pop()
        assigned_trial.state = study_pb2.Trial.State.ACTIVE
        assigned_trial.client_id = request.client_id
        assigned_trial.start_time.CopyFrom(start_time)
        self.datastore.update_trial(assigned_trial)
        output_trials.append(assigned_trial)

      if len(output_trials) == request.suggestion_count:
        # We've finished collecting enough trials from the REQUESTED pool.
        output_op.response.value = vizier_service_pb2.SuggestTrialsResponse(
            trials=output_trials, start_time=start_time
        ).SerializeToString()
        output_op.done = True
        self.datastore.update_suggestion_operation(output_op)
        return output_op

      # Still need more suggestions. Pythia begins computing missing amount.
      study_config = svz.StudyConfig.from_proto(study.study_spec)
      study_descriptor = vz.StudyDescriptor(
          config=study_config,
          guid=study_name,
          max_trial_id=self.datastore.max_trial_id(study_name),
      )
      suggest_request = pythia.SuggestRequest(
          study_descriptor=study_descriptor,
          count=request.suggestion_count - len(output_trials),
      )

      # Convert request, send to Pythia, and obtain suggestions.
      suggest_request_proto = svz.SuggestConverter.to_request_proto(
          suggest_request
      )
      suggest_request_proto.algorithm = study.study_spec.algorithm

      try:
        temp_pythia_service = self._select_pythia_service(
            study_config.pythia_endpoint
        )
        suggest_decision_proto = temp_pythia_service.Suggest(
            suggest_request_proto
        )
      # Pythia can raise any exception, captured inside grpc.RpcError.
      except grpc.RpcError as e:
        output_op.error.CopyFrom(
            status_pb2.Status(code=code_pb2.Code.INTERNAL, message=str(e))
        )
        logging.exception(
            'Failed to request trials from Pythia for request: %s', request
        )
        output_op.done = True
        self.datastore.update_suggestion_operation(output_op)
        return output_op

      # Check if we received enough suggestions.
      if len(
          suggest_decision_proto.suggestions
      ) < request.suggestion_count - len(output_trials):
        logging.warning(
            'Requested at least %d suggestions but Pythia only produced %d.',
            request.suggestion_count - len(output_trials),
            len(suggest_decision_proto.suggestions),
        )
      suggest_decision = svz.SuggestConverter.from_decision_proto(
          suggest_decision_proto
      )

      # Write the metadata update to the datastore.
      try:
        self.datastore.update_metadata(
            study_name,
            svz.metadata_util.make_key_value_list(
                suggest_decision.metadata.on_study
            ),
            svz.metadata_util.trial_metadata_to_update_list(
                suggest_decision.metadata.on_trials
            ),
        )
      except KeyError as e:
        output_op.error.CopyFrom(
            status_pb2.Status(code=code_pb2.Code.INTERNAL, message=str(e))
        )
        logging.exception(
            'Failed to write metadata update to datastore: %s',
            suggest_decision.metadata,
        )
        output_op.done = True
        self.datastore.update_suggestion_operation(output_op)
        return output_op

      new_py_trials = [
          decision.to_trial() for decision in suggest_decision.suggestions
      ]
      new_trials = svz.TrialConverter.to_protos(new_py_trials)

      while request.suggestion_count > len(output_trials):
        new_trial = new_trials.pop()
        trial_id = self.datastore.max_trial_id(request.parent) + 1
        new_trial.id = str(trial_id)
        new_trial.name = TrialResource(owner_id, study_id, trial_id).name
        new_trial.state = study_pb2.Trial.State.ACTIVE
        new_trial.start_time.CopyFrom(start_time)
        new_trial.client_id = request.client_id
        self.datastore.create_trial(new_trial)
        output_trials.append(new_trial)

      output_op.response.value = vizier_service_pb2.SuggestTrialsResponse(
          trials=output_trials, start_time=start_time
      ).SerializeToString()

      # Store remaining trials as REQUESTED if Pythia over-delivered.
      for remain_trial in new_trials:
        trial_id = self.datastore.max_trial_id(request.parent) + 1
        remain_trial.id = str(trial_id)
        remain_trial.name = TrialResource(owner_id, study_id, trial_id).name
        remain_trial.state = study_pb2.Trial.State.REQUESTED
        self.datastore.create_trial(remain_trial)

      output_op.done = True
      self.datastore.update_suggestion_operation(output_op)
      return output_op

  def GetOperation(
      self,
      request: operations_pb2.GetOperationRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> operations_pb2.Operation:
    """Gets the latest state of a SuggestTrials() long-running operation."""
    return self.datastore.get_suggestion_operation(request.name)

  def CreateTrial(
      self,
      request: vizier_service_pb2.CreateTrialRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Trial:
    """Adds user provided Trial to a Study and assigns the correct fields."""
    if self._study_is_immutable(request.parent):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot create trial.'.format(request.parent)
      )
      grpc_util.handle_exception(e, context)

    trial = request.trial
    with self._study_name_to_lock[request.parent]:
      trial.id = str(self.datastore.max_trial_id(request.parent) + 1)
      study_resource = StudyResource.from_name(request.parent)
      trial.name = (study_resource.trial_resource(trial.id)).name

      if trial.state != study_pb2.Trial.State.SUCCEEDED:
        trial.state = study_pb2.Trial.State.REQUESTED
      trial.ClearField('client_id')

      trial.start_time.CopyFrom(_get_current_time())
      self.datastore.create_trial(trial)
    return trial

  def GetTrial(
      self,
      request: vizier_service_pb2.GetTrialRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> Optional[study_pb2.Trial]:
    """Gets a Trial."""
    return self.datastore.get_trial(request.name)

  def ListTrials(
      self,
      request: vizier_service_pb2.ListTrialsRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> vizier_service_pb2.ListTrialsResponse:
    """Lists the Trials associated with a Study."""
    list_of_trials = self.datastore.list_trials(request.parent)
    return vizier_service_pb2.ListTrialsResponse(trials=list_of_trials)

  def AddTrialMeasurement(
      self,
      request: vizier_service_pb2.AddTrialMeasurementRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Trial:
    """Adds a measurement of the objective metrics to a Trial.

    This measurement is assumed to have been taken before the Trial is
    complete.

    Args:
      request:
      context:

    Returns:
      Trial whose measurement was appended.

    Raises:
      ImmutableStudyError: If study was already immutable.
      ImmutableTrialError: If the trial cannot be modified.
    """
    study_name = TrialResource.from_name(request.trial_name).study_resource.name
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot add measurement.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    with self._study_name_to_lock[study_name]:
      trial = self.datastore.get_trial(request.trial_name)
      if trial.state == study_pb2.Trial.State.INFEASIBLE:
        return trial
      if trial.state not in self._TRIAL_MUTABLE_STATES:
        e = custom_errors.ImmutableTrialError(
            'Trial {} has state {}. Measurements can only be added to trials in'
            ' state ACTIVE or STOPPING'.format(
                request.trial_name, study_pb2.Trial.State.Name(trial.state)
            )
        )
        grpc_util.handle_exception(e, context)

      trial.measurements.extend([request.measurement])
      self.datastore.update_trial(trial)
    return trial

  # TODO: Auto selection defaults to the last measurement.
  # Add support for "best measurement" behavior.
  def CompleteTrial(
      self,
      request: vizier_service_pb2.CompleteTrialRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Trial:
    """Marks a Trial as complete."""
    study_name = TrialResource.from_name(request.name).study_resource.name
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot complete trial.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    with self._study_name_to_lock[study_name]:
      trial = self.datastore.get_trial(request.name)
      if trial.state not in self._TRIAL_MUTABLE_STATES:
        e = custom_errors.ImmutableTrialError(
            'Trial {} has state {}. Only trials in state ACTIVE or STOPPING '
            'can be completed.'.format(
                request.name, study_pb2.Trial.State.Name(trial.state)
            )
        )
        grpc_util.handle_exception(e, context)

      trial.state = study_pb2.Trial.State.SUCCEEDED
      if request.final_measurement.metrics:
        trial.final_measurement.CopyFrom(request.final_measurement)
      elif not request.trial_infeasible:
        # Trial's final measurement auto-selected from latest reported
        # measurement.
        if not trial.measurements:
          e = ValueError(
              'Both the request and trial intermediate measurements are'
              " missing. Cannot determine trial's final_measurement."
          )
          grpc_util.handle_exception(e, context)
        trial.final_measurement.CopyFrom(trial.measurements[-1])

      # Handle infeasibility.
      if request.trial_infeasible:
        trial.state = study_pb2.Trial.State.INFEASIBLE
        trial.infeasible_reason = request.infeasible_reason

      self.datastore.update_trial(trial)
    return trial

  def DeleteTrial(
      self,
      request: vizier_service_pb2.DeleteTrialRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> empty_pb2.Empty:
    """Deletes a Trial."""
    study_name = TrialResource.from_name(request.name).study_resource.name
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot delete trial.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    self.datastore.delete_trial(request.name)
    return empty_pb2.Empty()

  # TODO: This currently uses the same algorithm as suggestion.
  def CheckTrialEarlyStoppingState(
      self,
      request: vizier_service_pb2.CheckTrialEarlyStoppingStateRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> vizier_service_pb2.CheckTrialEarlyStoppingStateResponse:
    """Checks whether a Trial should stop or not.

    The logic is as follows:

    1. We check if there already exists an operation. If not, create it. But if
    it already exists:
      a. If it's still ACTIVE (i.e. early stopping is still being computed by
      Pythia), just return it.
      b. If it's done and the call is very recent, just return it.
      c. If it's been a long time, we can recycle the operation, and remark it
      as ACTIVE to be used again.

    2. Send the operation to Pythia, which will return a set of early stopping
    decisions. The Pythia policy is guaranteed to return a decision for the
    requested trial. However, Pythia policy may also signal whether other trials
    should stop or not (e.g. in algorithms in which proposed trials are batched
    and correlated).

    3. Transfer these decisions' `should_stop` into the relevant operations
    (some of which will also be created in this call, for future use), including
    the original operation which needs to be returned. Mark them all as done.

    4. Return the original trial's `should_stop` decision.

    Args:
      request:
      context:

    Returns:
      A CheckTrialEarlyStoppingStateResponse, containing a bool to denote
      whether the requested trial should stop.

    Raises:
      ImmutableStudyError: If study was already immutable.
      ImmutableTrialError: If the trial cannot be modified.
    """
    trial_resource = TrialResource.from_name(request.trial_name)
    study_name = trial_resource.study_resource.name
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot early stop trial.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    with self._study_name_to_lock[study_name]:
      trial = self.datastore.get_trial(request.trial_name)
      if trial.state not in self._TRIAL_MUTABLE_STATES:
        e = custom_errors.ImmutableTrialError(
            'Trial {} has state {}. Only trials in state ACTIVE or STOPPING '
            'can be completed.'.format(
                request.trial_name, study_pb2.Trial.State.Name(trial.state)
            )
        )
        grpc_util.handle_exception(e, context)
    outer_op_name = trial_resource.early_stopping_operation_resource.name
    # Don't allow simultaneous SuggestTrial or EarlyStopping calls to be
    # processed.
    with self._operation_lock[study_name]:
      try:
        # Reuse any existing early stopping op, since the Pythia policy may have
        # already signaled this trial to stop.
        output_operation = self.datastore.get_early_stopping_operation(
            outer_op_name
        )
      except KeyError:
        output_operation = None

      if output_operation is None:
        # Create fresh new operation.
        output_operation = vizier_oss_pb2.EarlyStoppingOperation(
            name=outer_op_name,
            status=vizier_oss_pb2.EarlyStoppingOperation.Status.ACTIVE,
            should_stop=False,
        )
        output_operation.creation_time.CopyFrom(_get_current_time())
        self.datastore.create_early_stopping_operation(output_operation)
      else:
        if (
            output_operation.status
            == vizier_oss_pb2.EarlyStoppingOperation.Status.ACTIVE
            or datetime.datetime.now(datetime.timezone.utc).replace(tzinfo=None)
            - output_operation.completion_time.ToDatetime()
            < self._early_stop_recycle_period
        ):
          # Operation is already active or very recent. Just return it.
          return vizier_service_pb2.CheckTrialEarlyStoppingStateResponse(
              should_stop=output_operation.should_stop
          )

        # Recycle the operation to ACTIVE again and start Pythia for
        # recomputation.
        output_operation.status = (
            vizier_oss_pb2.EarlyStoppingOperation.Status.ACTIVE
        )
        output_operation.should_stop = False  # Defaulted back to False.
        self.datastore.update_early_stopping_operation(output_operation)

      study = self.datastore.load_study(study_name)
      study_config = svz.StudyConfig.from_proto(study.study_spec)
      study_descriptor = vz.StudyDescriptor(
          config=study_config,
          guid=study_name,
          max_trial_id=self.datastore.max_trial_id(study_name),
      )
      early_stop_request = pythia.EarlyStopRequest(
          study_descriptor=study_descriptor, trial_ids=[trial_resource.trial_id]
      )
      early_stop_request_proto = svz.EarlyStopConverter.to_request_proto(
          early_stop_request
      )
      spec_name = (
          study.study_spec.WhichOneof('automated_stopping_spec')
          or 'default_stopping_spec'
      )
      if spec_name == 'default_stopping_spec':
        # TODO: Add TSGP algorithm when open sourced.
        early_stop_request_proto.algorithm = 'RANDOM_SEARCH'
      else:
        raise ValueError(
            f'Misconfigured automated_stopping_spec: {study.study_spec}'
        )

      # Send request to Pythia.
      temp_pythia_service = self._select_pythia_service(
          study_config.pythia_endpoint
      )
      early_stopping_decisions_proto = temp_pythia_service.EarlyStop(
          early_stop_request_proto
      )
      early_stopping_decisions = svz.EarlyStopConverter.from_decisions_proto(
          early_stopping_decisions_proto
      )
      # Update metadata from result.
      self.datastore.update_metadata(
          study_name,
          svz.metadata_util.make_key_value_list(
              early_stopping_decisions.metadata.on_study
          ),
          svz.metadata_util.trial_metadata_to_update_list(
              early_stopping_decisions.metadata.on_trials
          ),
      )

      # Pythia does not guarantee that the output_operation's id
      # will be in the decisions.
      for early_stopping_decision in early_stopping_decisions.decisions:
        inner_op_name = resources.EarlyStoppingOperationResource(
            trial_resource.owner_id,
            trial_resource.study_id,
            early_stopping_decision.id,
        ).name
        try:
          inner_operation = self.datastore.get_early_stopping_operation(
              inner_op_name
          )
        except KeyError:
          # Create the operation to store early stopping data for future use.
          inner_operation = vizier_oss_pb2.EarlyStoppingOperation(
              name=inner_op_name,
              status=vizier_oss_pb2.EarlyStoppingOperation.Status.ACTIVE,
              should_stop=False,
          )
          inner_operation.creation_time.CopyFrom(_get_current_time())
          self.datastore.create_early_stopping_operation(inner_operation)

        inner_operation.should_stop = early_stopping_decision.should_stop
        inner_operation.status = (
            vizier_oss_pb2.EarlyStoppingOperation.Status.DONE
        )
        inner_operation.completion_time.CopyFrom(_get_current_time())
        self.datastore.update_early_stopping_operation(inner_operation)

      # Operation to be outputted may have changed.
      output_operation = self.datastore.get_early_stopping_operation(
          output_operation.name
      )
      return vizier_service_pb2.CheckTrialEarlyStoppingStateResponse(
          should_stop=output_operation.should_stop
      )

  def StopTrial(
      self,
      request: vizier_service_pb2.StopTrialRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> study_pb2.Trial:
    """Sets the trial state to STOPPING.

    Args:
      request:
      context:

    Returns:
      The stopped Trial

    Raises:
      ImmutableStudyError: If study was already immutable.
      ImmutableTrialError: If the trial cannot be modified.
    """
    study_name = TrialResource.from_name(request.name).study_resource.name
    if self._study_is_immutable(study_name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot stop trial.'.format(study_name)
      )
      grpc_util.handle_exception(e, context)

    with self._study_name_to_lock[study_name]:
      trial = self.datastore.get_trial(request.name)
      if trial.state == study_pb2.Trial.ACTIVE:
        trial.state = study_pb2.Trial.STOPPING
        self.datastore.update_trial(trial)
      elif trial.state in (study_pb2.Trial.STOPPING, study_pb2.Trial.SUCCEEDED):
        logging.warning(
            'Trial %s has state %s. StopTrial will be a no-op.',
            request.name,
            study_pb2.Trial.State.Name(trial.state),
        )
      else:
        e = custom_errors.ImmutableTrialError(
            'Trial {} has state {}. Raising Error.'.format(
                request.name, study_pb2.Trial.State.Name(trial.state)
            )
        )
        grpc_util.handle_exception(e, context)
    return trial

  def ListOptimalTrials(
      self,
      request: vizier_service_pb2.ListOptimalTrialsRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> vizier_service_pb2.ListOptimalTrialsResponse:
    """The definition of pareto-optimal can be checked in wiki page.

    https://en.wikipedia.org/wiki/Pareto_efficiency.

    Args:
      request:
      context:

    Returns:
      A list containing pareto-optimal Trials for multi-objective Study or the
      optimal Trials for single-objective Study.
    """
    raw_trial_list = self.datastore.list_trials(request.parent)
    if not raw_trial_list:
      return vizier_service_pb2.ListOptimalTrialsResponse(optimal_trials=[])

    study_spec = self.datastore.load_study(request.parent).study_spec
    metric_id_to_goal = {m.metric_id: m.goal for m in study_spec.metrics}
    required_metric_ids = set(metric_id_to_goal.keys())

    considered_trials = []
    considered_trial_objective_vectors = []

    for trial in raw_trial_list:
      trial_metric_id_to_value = {
          m.metric_id: m.value for m in trial.final_measurement.metrics
      }
      trial_metric_ids = set(trial_metric_id_to_value.keys())
      # Add trials ONLY if they succeeded and contain all supposed metrics.
      if (
          trial.state == study_pb2.Trial.State.SUCCEEDED
          and required_metric_ids.issubset(trial_metric_ids)
      ):
        objective_vector = []
        for metric_id, goal in metric_id_to_goal.items():
          # Flip sign for convenience when computing optimality.
          if goal == study_pb2.StudySpec.MetricSpec.GoalType.MINIMIZE:
            vector_value = -1.0 * trial_metric_id_to_value[metric_id]
          else:
            vector_value = trial_metric_id_to_value[metric_id]
          objective_vector.append(vector_value)

        considered_trials.append(trial)
        considered_trial_objective_vectors.append(objective_vector)

    if not considered_trials:
      return vizier_service_pb2.ListOptimalTrialsResponse(optimal_trials=[])

    # Find Pareto optimal trials.
    ys = np.array(considered_trial_objective_vectors)
    n = ys.shape[0]
    dominated = np.asarray([
        [np.all(ys[i] <= ys[j]) & np.any(ys[j] > ys[i]) for i in range(n)]
        for j in range(n)
    ])
    optimal_booleans = np.logical_not(np.any(dominated, axis=0))
    optimal_trials = []
    for i, boolean in enumerate(list(optimal_booleans)):
      if boolean:
        optimal_trials.append(considered_trials[i])

    return vizier_service_pb2.ListOptimalTrialsResponse(
        optimal_trials=optimal_trials
    )

  def UpdateMetadata(
      self,
      request: vizier_service_pb2.UpdateMetadataRequest,
      context: Optional[grpc.ServicerContext] = None,
  ) -> vizier_service_pb2.UpdateMetadataResponse:
    """Stores the supplied metadata in the database."""
    if self._study_is_immutable(request.name):
      e = custom_errors.ImmutableStudyError(
          'Study {} is immutable. Cannot update metadata.'.format(request.name)
      )
      grpc_util.handle_exception(e, context)

    try:
      self.datastore.update_metadata(
          request.name,
          [x.metadatum for x in request.delta if not x.HasField('trial_id')],
          [x for x in request.delta if x.HasField('trial_id')],
      )
    except KeyError as e:
      return vizier_service_pb2.UpdateMetadataResponse(
          error_details=';'.join(e.args)
      )
    return vizier_service_pb2.UpdateMetadataResponse()


--- vizier/_src/service/vizier_service_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for vizier.service.vizier_server."""
# TODO: Change the test to create a vizier stub and call its
# methods, instead of directly calling VizierService methods.
import datetime
import time
import grpc

from vizier._src.service import constants
from vizier._src.service import key_value_pb2
from vizier._src.service import resources
from vizier._src.service import study_pb2
from vizier._src.service import vizier_service
from vizier._src.service import vizier_service_pb2
from vizier._src.service.testing import util as test_util

from google.longrunning import operations_pb2

from absl.testing import absltest
from absl.testing import parameterized

UnitMetadataUpdate = vizier_service_pb2.UnitMetadataUpdate


class VizierServicerTest(parameterized.TestCase):

  def setUp(self):
    # TODO: Find a way to cleanly test both datastores.
    self.early_stop_recycle_period = datetime.timedelta(seconds=0.1)
    self.vs = vizier_service.VizierServicer(
        database_url=constants.SQL_MEMORY_URL,
        early_stop_recycle_period=self.early_stop_recycle_period,
    )
    self.owner_id = 'my_username'
    self.study_id = '0123123'
    self.client_id = 'client_0'
    super().setUp()

  @parameterized.named_parameters(
      ('Maximize', study_pb2.StudySpec.MetricSpec.MAXIMIZE),
      ('Minimize', study_pb2.StudySpec.MetricSpec.MINIMIZE),
  )
  def test_single_objective_list_optimal(self, max_or_min_proto):
    metric_id = 'accuracy'
    max_or_min_study = test_util.generate_study(
        self.owner_id,
        self.study_id,
        study_spec=study_pb2.StudySpec(
            metrics=[
                study_pb2.StudySpec.MetricSpec(
                    metric_id=metric_id, goal=max_or_min_proto
                )
            ]
        ),
    )

    self.vs.datastore.create_study(max_or_min_study)

    lowest_trial_final_measurement = study_pb2.Measurement(
        metrics=[study_pb2.Measurement.Metric(metric_id=metric_id, value=-1.0)]
    )

    middle_trial_final_measurement = study_pb2.Measurement(
        metrics=[study_pb2.Measurement.Metric(metric_id=metric_id, value=0.0)]
    )

    highest_trial_final_measurement = study_pb2.Measurement(
        metrics=[study_pb2.Measurement.Metric(metric_id=metric_id, value=1.0)]
    )

    lowest_trial = test_util.generate_trials(
        trial_id_list=[1],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=lowest_trial_final_measurement,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]

    middle_trial = test_util.generate_trials(
        trial_id_list=[2],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=middle_trial_final_measurement,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]

    highest_trial = test_util.generate_trials(
        trial_id_list=[3],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=highest_trial_final_measurement,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]

    another_highest_trial = test_util.generate_trials(
        trial_id_list=[4],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=highest_trial_final_measurement,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]

    trials_to_be_ignored = test_util.generate_trials(
        [5, 6, 7], self.owner_id, self.study_id
    )

    all_trials = [
        lowest_trial,
        middle_trial,
        highest_trial,
        another_highest_trial,
    ] + trials_to_be_ignored

    for trial in all_trials:
      self.vs.datastore.create_trial(trial)

    optimal_trial_list = self.vs.ListOptimalTrials(
        request=vizier_service_pb2.ListOptimalTrialsRequest(
            parent=max_or_min_study.name
        )
    ).optimal_trials

    if max_or_min_study == study_pb2.StudySpec.MetricSpec.MAXIMIZE:
      self.assertLen(optimal_trial_list, 2)
    elif max_or_min_study == study_pb2.StudySpec.MetricSpec.MINIMIZE:
      self.assertEqual(optimal_trial_list[0], lowest_trial)

  def test_multiobjective_list_optimal(self):
    metric_id_1 = 'x1'
    metric_id_2 = 'x2'
    study = test_util.generate_study(
        self.owner_id,
        self.study_id,
        study_spec=study_pb2.StudySpec(
            metrics=[
                study_pb2.StudySpec.MetricSpec(
                    metric_id=metric_id_1,
                    goal=study_pb2.StudySpec.MetricSpec.MAXIMIZE,
                ),
                study_pb2.StudySpec.MetricSpec(
                    metric_id=metric_id_2,
                    goal=study_pb2.StudySpec.MetricSpec.MINIMIZE,
                ),
            ]
        ),
    )
    self.vs.datastore.create_study(study)

    low_x1 = study_pb2.Measurement.Metric(metric_id=metric_id_1, value=-1.0)
    low_x2 = study_pb2.Measurement.Metric(metric_id=metric_id_2, value=-1.0)
    high_x1 = study_pb2.Measurement.Metric(metric_id=metric_id_1, value=1.0)
    high_x2 = study_pb2.Measurement.Metric(metric_id=metric_id_2, value=1.0)

    low_low = study_pb2.Measurement(metrics=[low_x1, low_x2])
    low_high = study_pb2.Measurement(metrics=[low_x1, high_x2])
    high_low = study_pb2.Measurement(metrics=[high_x1, low_x2])
    high_high = study_pb2.Measurement(metrics=[high_x1, high_x2])

    incomplete_measurement = study_pb2.Measurement(metrics=[low_x1])

    ll_trial = test_util.generate_trials(
        trial_id_list=[1],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=low_low,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]
    lh_trial = test_util.generate_trials(
        trial_id_list=[2],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=low_high,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]
    hl_trial = test_util.generate_trials(
        trial_id_list=[3],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=high_low,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]
    hh_trial = test_util.generate_trials(
        trial_id_list=[4],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=high_high,
        state=study_pb2.Trial.State.SUCCEEDED,
    )[0]

    missing_measurement_trials = test_util.generate_trials(
        trial_id_list=[5, 6, 7],
        owner_id=self.owner_id,
        study_id=self.study_id,
        final_measurement=incomplete_measurement,
        state=study_pb2.Trial.State.SUCCEEDED,
    )

    all_trials = [
        ll_trial,
        lh_trial,
        hl_trial,
        hh_trial,
    ] + missing_measurement_trials
    for trial in all_trials:
      self.vs.datastore.create_trial(trial)

    optimal_trial_list = self.vs.ListOptimalTrials(
        request=vizier_service_pb2.ListOptimalTrialsRequest(parent=study.name)
    ).optimal_trials

    self.assertLen(optimal_trial_list, 1)
    self.assertEqual(optimal_trial_list[0], hl_trial)

  def test_suggest_trials(self):
    suggestion_count = 10

    example_study_spec = test_util.generate_all_four_parameter_specs(
        algorithm='RANDOM_SEARCH'
    )
    example_study = test_util.generate_study(
        self.owner_id, self.study_id, study_spec=example_study_spec
    )
    self.vs.datastore.create_study(example_study)

    request = vizier_service_pb2.SuggestTrialsRequest(
        parent=resources.StudyResource(self.owner_id, self.study_id).name,
        suggestion_count=suggestion_count,
        client_id=self.client_id,
    )
    operation = self.vs.SuggestTrials(request)

    # Check if operation was stored in database.
    get_operation_request = operations_pb2.GetOperationRequest(
        name=resources.SuggestionOperationResource(
            self.owner_id, self.study_id, self.client_id, 1
        ).name
    )
    get_operation_output = self.vs.GetOperation(get_operation_request)
    self.assertEqual(operation, get_operation_output)

    # Check operation contents.
    suggest_trials_response = (
        vizier_service_pb2.SuggestTrialsResponse.FromString(
            operation.response.value
        )
    )
    self.assertLen(suggest_trials_response.trials, suggestion_count)
    for trial in suggest_trials_response.trials:
      self.assertLen(trial.parameters, 4)

    # Make sure a different op is created on a different study, even with same
    # client id.
    another_study_id = self.study_id + 'another'
    another_study = test_util.generate_study(
        self.owner_id, another_study_id, study_spec=example_study_spec
    )
    self.vs.datastore.create_study(another_study)
    another_request = vizier_service_pb2.SuggestTrialsRequest(
        parent=resources.StudyResource(self.owner_id, another_study_id).name,
        suggestion_count=suggestion_count,
        client_id=self.client_id,
    )
    another_operation = self.vs.SuggestTrials(another_request)
    self.assertNotEqual(operation, another_operation)

  @parameterized.named_parameters(
      ('IncludeFinalMeasurement', True), ('NoFinalMeasurement', False)
  )
  def test_complete_trial(self, include_final_measurement: bool):
    metric_id = 'x'
    study = test_util.generate_study(
        self.owner_id,
        self.study_id,
        study_spec=study_pb2.StudySpec(
            metrics=[
                study_pb2.StudySpec.MetricSpec(
                    metric_id=metric_id,
                    goal=study_pb2.StudySpec.MetricSpec.MAXIMIZE,
                )
            ]
        ),
    )
    self.vs.datastore.create_study(study)

    trial = test_util.generate_trials(
        trial_id_list=[1],
        owner_id=self.owner_id,
        study_id=self.study_id,
        state=study_pb2.Trial.State.ACTIVE,
    )[0]
    self.vs.datastore.create_trial(trial)
    complete_trial_request = vizier_service_pb2.CompleteTrialRequest(
        name=trial.name
    )

    if include_final_measurement:
      final_measurement = study_pb2.Measurement(
          metrics=[
              study_pb2.Measurement.Metric(metric_id=metric_id, value=-1.0)
          ]
      )
      complete_trial_request.final_measurement.CopyFrom(final_measurement)
      self.vs.CompleteTrial(complete_trial_request)
    else:
      with self.assertRaises(grpc.RpcError):
        # trial and request both do not contain measurements.
        self.vs.CompleteTrial(complete_trial_request)

  def test_early_stopping(self):
    example_study_spec = test_util.generate_all_four_parameter_specs(
        algorithm='RANDOM_SEARCH'
    )
    example_study = test_util.generate_study(
        self.owner_id, self.study_id, study_spec=example_study_spec
    )
    self.vs.datastore.create_study(example_study)

    active_trials = test_util.generate_trials(
        trial_id_list=[1, 2, 3, 4],
        owner_id=self.owner_id,
        study_id=self.study_id,
        state=study_pb2.Trial.State.ACTIVE,
    )
    for t in active_trials:
      self.vs.datastore.create_trial(t)
      request = vizier_service_pb2.CheckTrialEarlyStoppingStateRequest(
          trial_name=t.name
      )
      response = self.vs.CheckTrialEarlyStoppingState(request)
      # Since RandomPolicy picks a random ACTIVE trial to stop and current trial
      # t is the only ACTIVE trial, it should always stop.
      self.assertTrue(
          response.should_stop,
          msg=f'trial={t}, request={request}, response={response}',
      )
      stop_trial_request = vizier_service_pb2.StopTrialRequest(name=t.name)
      new_t = self.vs.StopTrial(stop_trial_request)
      self.assertEqual(new_t.state, study_pb2.Trial.State.STOPPING)

    for t in active_trials:
      trial_resource = resources.TrialResource.from_name(t.name)
      operation_name = resources.EarlyStoppingOperationResource(
          trial_resource.owner_id,
          trial_resource.study_id,
          trial_resource.trial_id,
      ).name
      op = self.vs.datastore.get_early_stopping_operation(operation_name)
      self.assertTrue(op.should_stop)

    # After a while, the opeartion will be recycled, and `should_stop` is
    # defaulted to False. Since the trial is no longer active, RandomPolicy will
    # not consider it.
    time.sleep(self.early_stop_recycle_period.total_seconds())
    request = vizier_service_pb2.CheckTrialEarlyStoppingStateRequest(
        trial_name=active_trials[0].name
    )
    response = self.vs.CheckTrialEarlyStoppingState(request)
    self.assertFalse(response.should_stop)

  def test_update_metadata(self):
    # Construct a study.
    example_study_spec = test_util.generate_all_four_parameter_specs(
        algorithm='RANDOM_SEARCH'
    )
    example_study = test_util.generate_study(
        self.owner_id, self.study_id, study_spec=example_study_spec
    )
    self.vs.datastore.create_study(example_study)
    active_trials = test_util.generate_trials(
        trial_id_list=[1, 2],
        owner_id=self.owner_id,
        study_id=self.study_id,
        state=study_pb2.Trial.State.ACTIVE,
    )
    for t in active_trials:
      self.vs.datastore.create_trial(t)

    # Construct the request.
    study_metadata = UnitMetadataUpdate(
        metadatum=key_value_pb2.KeyValue(key='a', ns='b', value='C')
    )
    trial_metadata = UnitMetadataUpdate(
        trial_id='1',
        metadatum=key_value_pb2.KeyValue(key='d', ns='e', value='F'),
    )
    request = vizier_service_pb2.UpdateMetadataRequest(
        name=resources.StudyResource(self.owner_id, self.study_id).name,
        delta=[study_metadata, trial_metadata],
    )
    # Send it to the server.
    response = self.vs.UpdateMetadata(request)
    # Check that there was no error.
    self.assertEmpty(response.error_details)

  def test_trial_immutable(self):
    study = test_util.generate_study(
        self.owner_id, self.study_id, state=study_pb2.Study.State.ACTIVE
    )
    self.vs.datastore.create_study(study)
    trials = test_util.generate_trials(
        trial_id_list=[1],
        owner_id=self.owner_id,
        study_id=self.study_id,
        state=study_pb2.Trial.State.SUCCEEDED,
    )
    self.vs.datastore.create_trial(trials[0])
    trial_name = resources.TrialResource(self.owner_id, self.study_id, 1).name

    with self.assertRaises(grpc.RpcError):
      self.vs.CompleteTrial(
          vizier_service_pb2.CompleteTrialRequest(
              name=trial_name, final_measurement=study_pb2.Measurement()
          )
      )

    with self.assertRaises(grpc.RpcError):
      self.vs.CheckTrialEarlyStoppingState(
          vizier_service_pb2.CheckTrialEarlyStoppingStateRequest(
              trial_name=trial_name
          )
      )

    with self.assertRaises(grpc.RpcError):
      self.vs.AddTrialMeasurement(
          vizier_service_pb2.AddTrialMeasurementRequest(
              trial_name=trial_name, measurement=study_pb2.Measurement()
          )
      )

  @parameterized.parameters(
      (study_pb2.Study.State.COMPLETED,),
      (study_pb2.Study.State.INACTIVE,),
  )
  def test_study_immutable(self, study_state):
    trial_name = resources.TrialResource(self.owner_id, self.study_id, 1).name
    study_name = resources.StudyResource(self.owner_id, self.study_id).name
    study = test_util.generate_study(
        self.owner_id, self.study_id, state=study_state
    )
    self.vs.datastore.create_study(study)

    with self.assertRaises(grpc.RpcError):
      self.vs.SuggestTrials(
          vizier_service_pb2.SuggestTrialsRequest(parent=study_name)
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.CreateTrial(
          vizier_service_pb2.CreateTrialRequest(
              parent=study_name, trial=study_pb2.Trial()
          )
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.AddTrialMeasurement(
          vizier_service_pb2.AddTrialMeasurementRequest(
              trial_name=trial_name, measurement=study_pb2.Measurement()
          )
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.CompleteTrial(
          vizier_service_pb2.CompleteTrialRequest(name=trial_name)
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.DeleteTrial(
          vizier_service_pb2.DeleteTrialRequest(name=trial_name)
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.CheckTrialEarlyStoppingState(
          vizier_service_pb2.CheckTrialEarlyStoppingStateRequest(
              trial_name=trial_name
          )
      )
    with self.assertRaises(grpc.RpcError):
      self.vs.StopTrial(vizier_service_pb2.StopTrialRequest(name=trial_name))
    with self.assertRaises(grpc.RpcError):
      self.vs.UpdateMetadata(
          vizier_service_pb2.UpdateMetadataRequest(name=study_name)
      )


if __name__ == '__main__':
  absltest.main()


--- vizier/algorithms/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Core algorithm modules."""

from vizier._src.algorithms.core.abstractions import ActiveTrials
from vizier._src.algorithms.core.abstractions import CompletedTrials
from vizier._src.algorithms.core.abstractions import Designer
from vizier._src.algorithms.core.abstractions import DesignerFactory
from vizier._src.algorithms.core.abstractions import PartiallySerializableDesigner
from vizier._src.algorithms.core.abstractions import Prediction
from vizier._src.algorithms.core.abstractions import Predictor
from vizier._src.algorithms.core.abstractions import SerializableDesigner
from vizier._src.algorithms.optimizers.base import BatchTrialScoreFunction
from vizier._src.algorithms.optimizers.base import BranchSelection
from vizier._src.algorithms.optimizers.base import BranchSelector
from vizier._src.algorithms.optimizers.base import BranchThenOptimizer
from vizier._src.algorithms.optimizers.base import GradientFreeOptimizer
from vizier._src.algorithms.policies.designer_policy import DesignerPolicy
from vizier._src.algorithms.policies.designer_policy import InRamDesignerPolicy
from vizier._src.algorithms.policies.designer_policy import PartiallySerializableDesignerPolicy
from vizier._src.algorithms.policies.designer_policy import SerializableDesignerPolicy


--- vizier/algorithms/designers/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""List of Designers and related algorithms."""

from vizier._src.algorithms.designers.bocs import BOCSDesigner
from vizier._src.algorithms.designers.cmaes import CMAESDesigner
from vizier._src.algorithms.designers.eagle_strategy.eagle_strategy import EagleStrategyDesigner
from vizier._src.algorithms.designers.gp_bandit import VizierGPBandit
from vizier._src.algorithms.designers.gp_ucb_pe import VizierGPUCBPEBandit
from vizier._src.algorithms.designers.grid import GridSearchDesigner
from vizier._src.algorithms.designers.harmonica import HarmonicaDesigner
from vizier._src.algorithms.designers.quasi_random import QuasiRandomDesigner
from vizier._src.algorithms.designers.random import RandomDesigner
from vizier._src.algorithms.designers.unsafe_as_infeasible_designer import UnsafeAsInfeasibleDesigner
from vizier._src.algorithms.ensemble.ensemble_design import AdaptiveEnsembleDesign
from vizier._src.algorithms.ensemble.ensemble_design import EnsembleDesign
from vizier._src.algorithms.ensemble.ensemble_design import EXP3IXEnsembleDesign
from vizier._src.algorithms.ensemble.ensemble_design import EXP3UniformEnsembleDesign
from vizier._src.algorithms.ensemble.ensemble_design import RandomEnsembleDesign
from vizier._src.algorithms.evolution.nsga2 import NSGA2Designer


--- vizier/algorithms/evolution.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Evolutionary strategy components."""

# pylint: disable=unused-import, g-importing-member

from vizier._src.algorithms.evolution.nsga2 import NSGA2Survival
from vizier._src.algorithms.evolution.numpy_populations import Offspring as NumpyOffspring
from vizier._src.algorithms.evolution.numpy_populations import Population as NumpyPopulation
from vizier._src.algorithms.evolution.numpy_populations import PopulationConverter as NumpyPopulationConverter
from vizier._src.algorithms.evolution.templates import _OffspringsType
from vizier._src.algorithms.evolution.templates import _PopulationType
from vizier._src.algorithms.evolution.templates import CanonicalEvolutionDesigner
from vizier._src.algorithms.evolution.templates import Mutation
from vizier._src.algorithms.evolution.templates import Sampler


--- vizier/algorithms/policies/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""List of policies."""
from vizier._src.algorithms.policies.random_policy import RandomPolicy


--- vizier/benchmarks/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Core Benchmarking utilities."""

from vizier._src.benchmarks.runners.benchmark_runner import BenchmarkRunner
from vizier._src.benchmarks.runners.benchmark_runner import BenchmarkSubroutine
from vizier._src.benchmarks.runners.benchmark_runner import EvaluateActiveTrials
from vizier._src.benchmarks.runners.benchmark_runner import FillActiveTrials
from vizier._src.benchmarks.runners.benchmark_runner import GenerateAndEvaluate
from vizier._src.benchmarks.runners.benchmark_runner import GenerateSuggestions
from vizier._src.benchmarks.runners.benchmark_state import BenchmarkState
from vizier._src.benchmarks.runners.benchmark_state import BenchmarkStateFactory
from vizier._src.benchmarks.runners.benchmark_state import DesignerBenchmarkStateFactory
from vizier._src.benchmarks.runners.benchmark_state import ExperimenterDesignerBenchmarkStateFactory
from vizier._src.benchmarks.runners.benchmark_state import PolicyBenchmarkStateFactory
from vizier._src.benchmarks.runners.benchmark_state import PolicySuggester
from vizier._src.benchmarks.runners.benchmark_state import SeededPolicyFactory


--- vizier/benchmarks/analyzers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Curve Analysis utilities for benchmarking and metalearning."""
# pylint: disable=unused-import

from vizier._src.benchmarks.analyzers.convergence_curve import ConvergenceComparator
from vizier._src.benchmarks.analyzers.convergence_curve import ConvergenceComparatorFactory
from vizier._src.benchmarks.analyzers.convergence_curve import ConvergenceCurve
from vizier._src.benchmarks.analyzers.convergence_curve import ConvergenceCurveConverter
from vizier._src.benchmarks.analyzers.convergence_curve import HypervolumeCurveConverter
from vizier._src.benchmarks.analyzers.convergence_curve import LogEfficiencyConvergenceCurveComparator
from vizier._src.benchmarks.analyzers.convergence_curve import LogEfficiencyConvergenceCurveComparatorFactory
from vizier._src.benchmarks.analyzers.convergence_curve import MultiMetricCurveConverter
from vizier._src.benchmarks.analyzers.convergence_curve import PercentageBetterConvergenceCurveComparator
from vizier._src.benchmarks.analyzers.convergence_curve import PercentageBetterConvergenceCurveComparatorFactory
from vizier._src.benchmarks.analyzers.convergence_curve import RestartingCurveConverter
from vizier._src.benchmarks.analyzers.convergence_curve import StatefulCurveConverter
from vizier._src.benchmarks.analyzers.convergence_curve import WinRateConvergenceCurveComparator
from vizier._src.benchmarks.analyzers.convergence_curve import WinRateConvergenceCurveComparatorFactory
from vizier._src.benchmarks.analyzers.plot_utils import plot_from_records
from vizier._src.benchmarks.analyzers.plot_utils import plot_mean_convergence
from vizier._src.benchmarks.analyzers.plot_utils import plot_median_convergence
from vizier._src.benchmarks.analyzers.state_analyzer import BenchmarkRecord
from vizier._src.benchmarks.analyzers.state_analyzer import BenchmarkRecordAnalyzer
from vizier._src.benchmarks.analyzers.state_analyzer import BenchmarkStateAnalyzer
from vizier._src.benchmarks.analyzers.state_analyzer import PlotElement


--- vizier/benchmarks/experimenters/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Lightweight experimenters which do not require installing `requirements-benchmarks.txt`."""

from vizier._src.benchmarks.experimenters import experimenter_factory
from vizier._src.benchmarks.experimenters.combo_experimenter import CentroidExperimenter
from vizier._src.benchmarks.experimenters.combo_experimenter import ContaminationExperimenter
from vizier._src.benchmarks.experimenters.combo_experimenter import IsingExperimenter
from vizier._src.benchmarks.experimenters.combo_experimenter import MAXSATExperimenter
from vizier._src.benchmarks.experimenters.combo_experimenter import PestControlExperimenter
from vizier._src.benchmarks.experimenters.discretizing_experimenter import DiscretizingExperimenter
from vizier._src.benchmarks.experimenters.experimenter import Experimenter
from vizier._src.benchmarks.experimenters.experimenter_factory import BBOBExperimenterFactory
from vizier._src.benchmarks.experimenters.experimenter_factory import CombinedExperimenterFactory
from vizier._src.benchmarks.experimenters.experimenter_factory import ExperimenterFactory
from vizier._src.benchmarks.experimenters.experimenter_factory import SerializableExperimenterFactory
from vizier._src.benchmarks.experimenters.experimenter_factory import SingleObjectiveExperimenterFactory
from vizier._src.benchmarks.experimenters.infeasible_experimenter import HashingInfeasibleExperimenter
from vizier._src.benchmarks.experimenters.infeasible_experimenter import ParamRegionInfeasibleExperimenter
from vizier._src.benchmarks.experimenters.l1_categorical_experimenter import L1CategorialExperimenter
from vizier._src.benchmarks.experimenters.multiobjective_experimenter import MultiObjectiveExperimenter
from vizier._src.benchmarks.experimenters.noisy_experimenter import NoisyExperimenter
from vizier._src.benchmarks.experimenters.normalizing_experimenter import HyperCubeExperimenter
from vizier._src.benchmarks.experimenters.normalizing_experimenter import NormalizingExperimenter
from vizier._src.benchmarks.experimenters.numpy_experimenter import MultiObjectiveNumpyExperimenter
from vizier._src.benchmarks.experimenters.numpy_experimenter import NumpyExperimenter
from vizier._src.benchmarks.experimenters.permuting_experimenter import PermutingExperimenter
from vizier._src.benchmarks.experimenters.shifting_experimenter import ShiftingExperimenter
from vizier._src.benchmarks.experimenters.sign_flip_experimenter import SignFlipExperimenter
from vizier._src.benchmarks.experimenters.sparse_experimenter import SparseExperimenter
from vizier._src.benchmarks.experimenters.surrogate_experimenter import PredictorExperimenter
from vizier._src.benchmarks.experimenters.switch_experimenter import SwitchExperimenter
from vizier._src.benchmarks.experimenters.synthetic import bbob
from vizier._src.benchmarks.experimenters.synthetic.branin import Branin2DExperimenter
from vizier._src.benchmarks.experimenters.synthetic.deb import DHExperimenter
from vizier._src.benchmarks.experimenters.synthetic.hartmann import HartmannExperimenter
from vizier._src.benchmarks.experimenters.synthetic.multiarm import BernoulliMultiArmExperimenter
from vizier._src.benchmarks.experimenters.synthetic.multiarm import FixedMultiArmExperimenter
from vizier._src.benchmarks.experimenters.synthetic.simplekd import SimpleKDExperimenter


--- vizier/benchmarks/experimenters/hpo/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""HPO-related benchmarks."""
from vizier._src.benchmarks.experimenters.hpob_experimenter import HPOBExperimenter


--- vizier/benchmarks/experimenters/multiobjective_optproblems/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Experimenter wrappers for the optproblems multiobjective benchmark suite."""

from vizier._src.benchmarks.experimenters.synthetic.multiobjective_optproblems import DTLZExperimenterFactory
from vizier._src.benchmarks.experimenters.synthetic.multiobjective_optproblems import WFGExperimenterFactory
from vizier._src.benchmarks.experimenters.synthetic.multiobjective_optproblems import ZDTExperimenterFactory


--- vizier/benchmarks/experimenters/nas/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Neural Architecture Search (heavyweight) benchmark classes for Vizier."""
from vizier._src.benchmarks.experimenters.nasbench101_experimenter import NASBench101Experimenter
from vizier._src.benchmarks.experimenters.nasbench201_experimenter import NASBench201Experimenter


--- vizier/benchmarks/experimenters/rl/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""RL (heavyweight) benchmark classes for Vizier."""
from vizier._src.benchmarks.experimenters.atari100k_experimenter import Atari100kExperimenter


--- vizier/client/client_abc.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Cross-platform Vizier client interfaces.

Code written using these interfaces are compatible with OSS and Cloud Vertex
Vizier. Note importantly that subclasses may have more methods than what is
required by interfaces, and such methods are not cross compatible. Our
recommendation is to use type annotatations of `StudyInterface` or
`TrialInterface` wherever applicable.

Keywords:

#Materialize: The method returns a deep copy of the underlying pyvizier object.
Modifying the returned object does not update the Vizier service.
"""

import abc
from typing import Any, Iterator, List, Mapping, Optional, Type, TypeVar

from vizier import pyvizier as vz

_T = TypeVar('_T')


# TODO: Add more errors, and unit tests to ensure that
# all vizier clients have the same error handling logic.
class ResourceNotFoundError(LookupError):
  """Error raised by Vizier clients when resource is not found."""

  pass


class TrialInterface(abc.ABC):
  """Responsible for trial-level operations."""

  @property
  @abc.abstractmethod
  def id(self) -> int:
    """Identifier of the trial within the study.

    Ids are sorted in increasing order of creation time.
    """

  @property
  @abc.abstractmethod
  def parameters(self) -> Mapping[str, Any]:
    """#Materializes the parameters of the trial.

    The parameters are parsed to the external types. The values in the returned
    dict can be a list of values, if the search space is configured with
    indices.

    As a result, TrialInterface.parameters can be totally different
    from trial.materialize().parameters.
    """

  @abc.abstractmethod
  def delete(self) -> None:
    """Delete the Trial in Vizier service.

    There is currently no promise on how this object behaves after `delete()`.
    If you are sharing a Trial object in parallel processes, proceed with
    caution.
    """

  @abc.abstractmethod
  def update_metadata(self, delta: vz.Metadata) -> None:
    """Updates Trial metadata.

    New keys will be appended, while old keys will be
    updated with new values.

    Args:
      delta: Change in Metadata from the original.
    """

  @abc.abstractmethod
  def complete(
      self,
      measurement: Optional[vz.Measurement] = None,
      *,
      infeasible_reason: Optional[str] = None,
  ) -> Optional[vz.Measurement]:
    """Completes the trial and #materializes the measurement.

    * If `measurement` is provided, then Vizier writes it as the trial's final
    measurement and returns it.
    * If `infeasible_reason` is provided, `measurement` is not required but
    can still be specified.
    * If neither is provided, then Vizier selects an existing (intermediate)
    measurement to be the final measurement and returns it.

    Args:
      measurement: Final measurement.
      infeasible_reason: Infeasible reason for missing final measurement.

    Returns:
      The final measurement of the trial, or None if the trial is marked
      infeasible.

    Raises:
      ValueError: If neither `measurement` nor `infeasible_reason` is provided
        but the trial does not contain any intermediate measurements.
    """

  @abc.abstractmethod
  def check_early_stopping(self) -> bool:
    """Decide if the trial should stop.

    If the Trial is in ACTIVE state and an early stopping algorithm is
    configured fior the Study, the stopping algorithm makes a stopping decision.
    If the algorithm decides that the trial is not worth continuing, Vizier
    service moves it into `STOPPING` state.

    Then, this method returns True if the Trial is in `STOPPING` state.

    Returns:
      True if trial is already in STOPPING state or entered STOPPING as a
      result of this method invocation.
    """
    pass

  @abc.abstractmethod
  def stop(self) -> None:
    """Asks to change the trial status to STOPPING.

    Should be called only if the trial status is ACTIVE. If the trial is
    STOPPING or COMPLETED, this is a no-op.
    """
    pass

  @abc.abstractmethod
  def add_measurement(self, measurement: vz.Measurement) -> None:
    """Adds an intermediate measurement."""

  @abc.abstractmethod
  def materialize(self, *, include_all_measurements: bool = True) -> vz.Trial:
    """#Materializes the Trial.

    Args:
      include_all_measurements: If True, returned Trial includes all
        intermediate measurements. The `final_measurement` is always included if
        one exists.

    Returns:
      Trial object.
    """

  @property
  @abc.abstractmethod
  def study(self) -> 'StudyInterface':
    """Returns the Study that this Trial belongs to."""


class TrialIterable(abc.ABC):
  """Allows iterating through both `TrialInterface` and `vz.Trial`.

  TrialIterable satisfies Iterable[TrialInterface] Protocol which is not
  explicitly inherited (See https://peps.python.org/pep-0544/). In addition,
  it guarantees that `list(StudyInterface.trials().get())` is at least as fast
  as `[t.materialize() for t in StudyInterface.trials()]`.

  StudyInterface returns this object when trials are already materialized
  while processing a request. A typical implementation of TrialIterable.get()
  uses a generator of the materialized trials.
  """

  @abc.abstractmethod
  def __iter__(self) -> Iterator[TrialInterface]:
    """Returns an iterator of TrialInterfaces, which are clients."""

  @abc.abstractmethod
  def get(self) -> Iterator[vz.Trial]:
    """Returns Trial objects, which are local objects."""


class StudyInterface(abc.ABC):
  """Responsible for study-level operations."""

  @property
  @abc.abstractmethod
  def resource_name(self) -> str:
    """Globally unique identifier of the study in Vizier service."""

  @abc.abstractmethod
  def suggest(
      self, *, count: Optional[int] = None, client_id: str = 'default_client_id'
  ) -> List[TrialInterface]:
    """Returns Trials to be evaluated by client_id.

    Args:
      count: Number of suggestions.
      client_id: When new Trials are generated, their `client_id` field is
        populated with this client_id. The Vizier service first looks for
        existing ACTIVE Trials that are assigned to `client_id`, before
        generating new ones.

    Returns:
      Trials.
    """
    # TODO: Add `parameters` argument. This will allow
    # `suggest` to be used with context.

  # TODO: Request does not play well with boolean or discrete
  # integer parameters.
  @abc.abstractmethod
  def request(self, suggestion: vz.TrialSuggestion) -> TrialInterface:
    """Request a trial to be suggested in the future.

    Requested trials are "queued" in the vizier database. Next time a client
    requests for a new suggestion, it receives requested trials before any
    algorithm-generated suggestions.

    Args:
      suggestion: Suggestion to be requested.
    """

  @abc.abstractmethod
  def delete(self) -> None:
    """Delete the Study in Vizier service.

    There is currently no promise on how this object behaves after `delete()`.
    If you are sharing a Study object in parallel processes, proceed with
    caution.
    """

  @abc.abstractmethod
  def update_metadata(self, delta: vz.Metadata) -> None:
    """Updates StudyConfig metadata.

    New keys will be appended, while old keys will be
    updated with new values.

    Args:
      delta: Change in Metadata from the original.
    """

  @abc.abstractmethod
  def add_trial(self, trial: vz.Trial) -> TrialInterface:
    """Adds a trial to the Study. Allows warm-starting.

    Args:
      trial: Trial to be added.

    Returns:
      Trial client.

    Raises:
      ValueError: If the trial is not within the search space.
    """

  @abc.abstractmethod
  def trials(
      self, trial_filter: Optional[vz.TrialFilter] = None
  ) -> TrialIterable:
    """Fetches a collection of trials. Default uses vz.TrialFilter()."""

  @abc.abstractmethod
  def get_trial(self, uid: int) -> TrialInterface:
    """Fetches a single trial.

    Args:
      uid: Unique identifier of the trial within study.

    Returns:
      Trial.

    Raises:
      ResourceNotFoundError: If trial does not exist.
    """

  @abc.abstractmethod
  def optimal_trials(self, *, count: Optional[int] = None) -> TrialIterable:
    """Returns (pareto) optimal trial(s). Can be multiple Trial(s).

    Args:
      count: If provided, returns that many best Trials, breaking ties by
        earlier Trial id.
    """

  @abc.abstractmethod
  def materialize_problem_statement(self) -> vz.ProblemStatement:
    """#Materializes the problem statement."""

  @classmethod
  @abc.abstractmethod
  # TODO: Make the function have position only parameter
  # once cloud vizier support python 3.8+ version.
  def from_resource_name(cls: Type[_T], name: str) -> _T:
    """Fetches an existing study from the Vizier service.

    Args:
      name: Globally unique identifier of the study.

    Returns:
      Study.

    Raises:
      ResourceNotFoundError: If study does not exist.
    """

  @abc.abstractmethod
  def set_state(self, state: vz.StudyState) -> None:
    """Sets the state of the study.

    Args:
      state: New state of the study.
    """

  # TODO: Make this method purely abstract.
  def materialize_state(self) -> vz.StudyState:
    """#Materializes the study state."""
    raise NotImplementedError(
        f'materialize_state is not implemented in {type(self)}!'
    )


--- vizier/client/client_abc_testing.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for client."""

import abc
import functools
from typing import Callable, Generic, Optional, TypeVar

from absl import logging
from vizier import pyvizier as vz
from vizier.client import client_abc

from absl.testing import parameterized

# Aliases are defined, so when you are developing a new client, you can
# swap it with your subclass. It makes your IDE understand which class
# you are using.
_S = TypeVar('_S', bound=client_abc.StudyInterface)
_TrialClient = client_abc.TrialInterface


class VizierClientTestMixin(abc.ABC, Generic[_S]):

  @abc.abstractmethod
  def create_study(self, study_config: vz.ProblemStatement, name: str) -> _S:
    """Create study given study config and study name."""
    pass


class MyMeta(type(VizierClientTestMixin), type(parameterized.TestCase)):
  pass


class TestCase(
    parameterized.TestCase, VizierClientTestMixin[_S], metaclass=MyMeta
):
  """Generic tests for cross-platform clients.

  This test provides basic coverage and it is not meant to be a comprehensive
  e2e test on all possible inputs to the client.

  Override `create_study` method.
  """

  def create_test_study(self, name: str) -> _S:
    """Creates a study."""
    logging.info('Creating study name=%s, testcasename=%s', name, self.id())
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('float', 0.0, 1.0)
    problem.metric_information.append(
        vz.MetricInformation(
            name='maximize_metric', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    )
    study = self.create_study(problem, name)

    # TODO: Put this line back once we have __eq__ well-defined.
    # self.assertEqual(study.materialize_problem_statement(), problem)
    return study

  def test_create_or_load_study(self):
    study = self.create_test_study(self.id())
    study2 = study.from_resource_name(study.resource_name)
    self.assertEqual(study.resource_name, study2.resource_name)

    self.assertEqual(
        study.materialize_problem_statement(),
        study2.materialize_problem_statement(),
    )

  def test_delete_study(self):
    study = self.create_test_study(self.id())
    resource_name = study.resource_name
    study.delete()

    with self.assertRaises(Exception):
      study.from_resource_name(resource_name)

  def _example_trials(self) -> list[vz.Trial]:
    """Generates example trials."""
    trials = [
        # Completed trial.
        vz.Trial(
            parameters={'float': 0.5},
        ).complete(vz.Measurement({'maximize_metric': 1.0})),
        # Completed trial.
        vz.Trial(
            parameters={'float': 0.5},
        ).complete(vz.Measurement({'maximize_metric': 0.5})),
        # Requested trial, which will be made active below.
        vz.Trial(
            parameters={'float': 0.5},
            measurements=[vz.Measurement({'maximize_metric': 0.7})],
        ),
        # Requested trial.
        vz.Trial(parameters={'float': 0.5}, is_requested=True),
    ]
    for idx, t in enumerate(trials):
      t.metadata['future_id'] = str(idx + 1)  # id to be assigned
    return trials

  def create_test_study_with_trials(self, name: str) -> _S:
    study = self.create_test_study(name)
    trials = self._example_trials()
    for i, t in enumerate(trials):
      study.add_trial(t)
      if i == 2:
        # Make sure the requested trial becomes ACTIVE.
        _ = study.suggest(count=1)
    return study

  @parameterized.parameters(list(state for state in vz.StudyState))
  def test_set_state(self, target_state: vz.StudyState):
    study = self.create_test_study_with_trials(self.id())
    # TODO: Check that the study moved to the target state.
    # This test is currently a placeholder.
    try:
      study.set_state(target_state)
    except NotImplementedError:
      logging.exception(
          'Set study state for %s is not implemented in %s',
          target_state,
          type(self),
      )

  def test_list_trials(self):
    study = self.create_test_study_with_trials(self.id())
    self.assertLen(list(study.trials()), 4)

  def test_trials_iter_and_get_are_equal(self):
    study = self.create_test_study_with_trials(self.id())
    all_trials = study.trials()
    self.assertEqual(
        [t.id for t in all_trials], [t.id for t in all_trials.get()]
    )

  def test_optimal_trials_on_empty(self):
    study = self.create_test_study(self.id())
    self.assertEmpty(list(study.optimal_trials()))

  def test_optimal_trials_notempty(self):
    study = self.create_test_study_with_trials(self.id())
    optimal_trials = list(study.optimal_trials())
    self.assertLen(optimal_trials, 1)
    self.assertEqual(
        optimal_trials[0].materialize().status, vz.TrialStatus.COMPLETED
    )

  def test_suggest_same_worker(self):
    # Given the same client id, should return the same trials.
    study = self.create_test_study(self.id())
    trials1 = study.suggest(count=5, client_id='worker1')
    self.assertLen(trials1, 5)
    trials2 = study.suggest(count=2, client_id='worker1')
    self.assertTrue({t.id for t in trials2}.issubset({t.id for t in trials1}))

  def test_suggest_requested(self):
    # Given the same client id, should return the same trials.
    study = self.create_test_study(self.id())
    requested_parameters = {'float': 0.11112}
    requested_trial = study.request(vz.TrialSuggestion(requested_parameters))
    self.assertCountEqual(
        requested_trial.parameters.items(), requested_parameters.items()
    )
    trials = study.suggest(count=5, client_id='worker1')
    self.assertLen(trials, 5)
    self.assertCountEqual(
        trials[0].parameters.items(), requested_parameters.items()
    )

  def test_suggest_different_workers(self):
    # Given different client ids, should generate new suggestions.
    study = self.create_test_study(self.id())
    worker1_trials = study.suggest(count=5, client_id='worker1')
    self.assertLen(worker1_trials, 5)
    worker1_trial_ids = {t.id for t in worker1_trials}
    worker2_trials = study.suggest(count=2, client_id='worker2')
    self.assertLen(worker2_trials, 2)
    worker2_trial_ids = {t.id for t in worker2_trials}
    self.assertEmpty(
        worker1_trial_ids.intersection(worker2_trial_ids),
        msg=(
            f'worker1_trial_ids={worker1_trial_ids}\n'
            f'worker2_trial_ids={worker2_trial_ids}'
        ),
    )

  def test_suggest_with_immutable_study(self):
    # Given immutable study, suggestions should be empty.
    study = self.create_test_study(self.id())
    study.set_state(vz.StudyState.ABORTED)
    trials = study.suggest(count=5, client_id='worker1')
    self.assertEmpty(trials)

  def test_get_trial(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(2).materialize()
    self.assertEqual(trial.metadata['future_id'], '2')
    self.assertEqual(trial.id, 2)

  def test_get_trials(self):
    study = self.create_test_study_with_trials(self.id())
    completed = study.trials(vz.TrialFilter(status={vz.TrialStatus.COMPLETED}))
    self.assertLen(list(completed), 2)

  def test_parameters(self):
    study = self.create_test_study_with_trials(self.id())
    self.assertEqual(
        study.get_trial(1).parameters, study.get_trial(1).parameters
    )

  def test_study_update_metadata(self):
    """Checks for correct merge behavior."""
    study = self.create_test_study(self.id())
    delta_metadata = vz.Metadata({'bar': 'bar_v'}, foo='foo_v')
    study.update_metadata(delta_metadata)
    self.assertEqual(
        study.materialize_problem_statement().metadata.get('bar'), 'bar_v'
    )

    delta_metadata_2 = vz.Metadata({'bar': 'bar_w'})
    study.update_metadata(delta_metadata_2)

    problem_statement = study.materialize_problem_statement()
    self.assertEqual(problem_statement.metadata.get('bar'), 'bar_w')
    self.assertEqual(problem_statement.metadata.get('foo'), 'foo_v')

  ###########################################
  ## Tests on Trial class start here.
  ###########################################

  def test_delete_trial(self):
    study = self.create_test_study_with_trials(self.id())
    study.get_trial(2).delete()
    with self.assertRaises(Exception):
      study.get_trial(2)

  def test_complete_trial_no_measurements(self):
    study = self.create_test_study_with_trials(self.id())
    with self.assertRaises(Exception):
      study.get_trial(4).complete()

  def test_complete_trial_auto_selection(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    measurement = trial.complete()
    self.assertIn(measurement, trial.materialize().measurements)

  def test_complete_trial_no_measurements_infeasible(self):
    study = self.create_test_study_with_trials(self.id())
    # Delete trial 3 so it's not suggested.
    trial = study.get_trial(3)
    trial.delete()
    # Ask Vizier to suggest the trial so it becomes ACTIVE.
    trial = study.suggest(count=1)[0]
    self.assertEqual(trial.id, 4)
    self.assertIsNone(trial.complete(infeasible_reason='just because'))
    self.assertTrue(trial.materialize().infeasible)

  def test_complete_trial_manual_measurement(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    final_measurement = vz.Measurement(metrics={'maximize_metric': 0.1})
    self.assertEqual(trial.complete(final_measurement), final_measurement)
    self.assertEqual(trial.materialize().status, vz.TrialStatus.COMPLETED)

  def test_should_trial_stop(self):
    # TODO: Improve coverage.
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    self.assertIsInstance(trial.check_early_stopping(), bool)

  def test_trial_stop(self):
    """Checks for correct stopping behavior."""
    study = self.create_test_study_with_trials(self.id())
    active_trial = study.suggest(count=1)[0]
    active_trial.stop()
    self.assertEqual(active_trial.materialize().status, vz.TrialStatus.STOPPING)

    # COMPLETED, STOPPING
    noop_trials = [study.get_trial(2), active_trial]
    original_statuses = [trial.materialize().status for trial in noop_trials]
    for trial, status in zip(noop_trials, original_statuses):
      trial.stop()  # Should do nothing.
      self.assertEqual(trial.materialize().status, status)

    requested_trial = study.get_trial(4)
    with self.assertRaises(Exception):
      requested_trial.stop()

  def test_intermediate_measurement(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    before = trial.materialize()
    trial.add_measurement(
        vz.Measurement(steps=2, metrics={'maximize_metric': 0.2})
    )
    after = trial.materialize()
    self.assertLen(after.measurements, len(before.measurements) + 1)

  def test_intermediate_measurement_infeasible(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    trial.complete(infeasible_reason='just because')
    before = trial.materialize()
    trial.add_measurement(
        vz.Measurement(steps=2, metrics={'maximize_metric': 0.2})
    )
    after = trial.materialize()
    # Can't add measurements to infeasible trials, but doing so isn't an error.
    self.assertEqual(before, after)

  def test_intermediate_measurement_completed(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    trial.complete(vz.Measurement(steps=1, metrics={'maximize_metric': 0.5}))
    # Can't add measurements to completed trials; doing so is an error.
    with self.assertRaises(Exception):
      trial.add_measurement(
          vz.Measurement(steps=2, metrics={'maximize_metric': 0.2})
      )

  def test_study_property(self):
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(2)
    self.assertEqual(
        trial.materialize(), trial.study.get_trial(2).materialize()
    )

  def assertPassesE2ETuning(
      self,
      *,
      study_factory: Optional[Callable[[vz.ProblemStatement], _S]] = None,
      batch_size: int = 2,
      num_iterations: int = 5,
      multi_objective: bool = False,
  ):
    """Runs an e2e test.

    This test simulates a hyperparameter tuning scenario, where the model
    hyperparameters are being tuned, and automated trial early stopping is
    used to stop trials early.

    Args:
      study_factory: If not specified, uses the default factory of this class.
      batch_size:
      num_iterations:
      multi_objective:
    """
    study_factory = study_factory or functools.partial(
        self.create_study, study_id=self.id()
    )

    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param(
        'learning_rate', min_value=0.0, max_value=1.0, default_value=0.5
    )
    problem.search_space.root.add_int_param(
        'num_layers', min_value=1, max_value=5
    )
    problem.metric_information = [
        vz.MetricInformation(
            name='accuracy', goal=vz.ObjectiveMetricGoal.MAXIMIZE
        )
    ]
    if multi_objective:
      problem.metric_information.append(
          vz.MetricInformation(
              name='latency', goal=vz.ObjectiveMetricGoal.MINIMIZE
          )
      )

    study: _S = study_factory(problem)  # pytype: disable=wrong-keyword-args

    def learning_curve_simulator(learning_rate: float) -> list[float]:
      return [learning_rate * step for step in range(10)]

    for _ in range(num_iterations):
      suggestions = study.suggest(count=batch_size)
      for trial in suggestions:
        trial: _TrialClient = trial
        assert isinstance(trial, _TrialClient), type(trial)
        learning_rate = trial.parameters['learning_rate']
        num_layers = trial.parameters['num_layers']
        possible_curve = learning_curve_simulator(learning_rate)
        evaluated_curve = []
        for i, obj in enumerate(possible_curve, start=1):
          if i > 1 and trial.check_early_stopping():
            break
          evaluated_curve.append(obj)
          trial.add_measurement(
              vz.Measurement(
                  steps=i,
                  elapsed_secs=0.1 * i,
                  metrics={'accuracy': obj, 'latency': 0.5 * num_layers},
              )
          )
        trial.complete()
        stored_curve = [
            m.metrics['accuracy'].value
            for m in trial.materialize().measurements
        ]

        # See if curve was stored correctly.
        self.assertEqual(evaluated_curve, stored_curve)

        # See if final_measurement is defaulted to end of curve.
        final_measurement = trial.materialize().final_measurement
        assert final_measurement is not None
        final_accuracy = final_measurement.metrics['accuracy'].value
        self.assertEqual(evaluated_curve[-1], final_accuracy)

  def test_trial_update_metadata(self):
    """Checks for correct merge behavior."""
    study = self.create_test_study_with_trials(self.id())
    trial = study.get_trial(3)
    delta_metadata = vz.Metadata({'bar': 'bar_v'}, foo='foo_v')
    trial.update_metadata(delta_metadata)
    self.assertEqual(trial.materialize().metadata.get('bar'), 'bar_v')
    delta_metadata_2 = vz.Metadata({'bar': 'bar_w'})
    trial.update_metadata(delta_metadata_2)
    self.assertEqual(trial.materialize().metadata.get('bar'), 'bar_w')
    self.assertEqual(trial.materialize().metadata.get('foo'), 'foo_v')


--- vizier/interfaces/serializable.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Serializable interface."""

import abc
from typing import Type, TypeVar

from vizier import pyvizier as vz

_S = TypeVar('_S', bound='Serializable')


class DecodeError(Exception):
  """Parent class for errors during decoding."""
  pass


class HarmlessDecodeError(DecodeError):
  """Failed to load state from metadata, and the object was untouched."""


class FatalDecodeError(DecodeError):
  """Failed to load state from metadata, and the object became invalid."""


class PartiallySerializable(abc.ABC):
  """Partially serializable objects except initialization.

  As long as the object is created in the same manner (via __init__ or a
  factory @classmethod), `load()` recovers the exact same `State`.

  NOTE: `State` here refers to the behavioral state that can be verified by
  calling "public" (not prefixed with an underline) methods. Subclasses do not
  guarantee that all private variables are recovered precisely.


  class Foo(PartiallySerializable):
   ...

  foo = Foo(*init_args)
  # do something with foo
  ...
  md = foo.dump()

  # `foo` and `foo2` are indistinguishable using "public" methods of Foo.
  foo2 = Foo(*init_args).load(md)

  # `foo` and `foo3` can be arbitrarily different.
  foo3 = Foo(*different_init_args).load(md)
  """

  @abc.abstractmethod
  def load(self, metadata: vz.Metadata) -> None:
    """Recovers the object's state stored in metadata.

    Args:
      metadata:

    Raises:
      HarmlessDecodeError: The object is still valid and can be used.
      FatalDecodeError: The object became invalid due to failure during
        load. It should be discarded. Raising a FatalDecodeError is NOT
        recommended, unless doing so avoids a signifcant overhead in performance
        or code complexity.
    """
    pass

  @abc.abstractmethod
  def dump(self) -> vz.Metadata:
    pass


class Serializable(abc.ABC):
  """Objects that can be fully serialized.

  Compared to PartiallySerializable, Serializable.recover() is a _classmethod_
  not a regular method. The object's `State` in its entirety can be serialized
  and then recovered.

  NOTE: `State` here refers to the behavioral state that can be verified by
  calling "public" (not prefixed with an underline) methods. Subclasses do not
  guarantee that all private variables are recovered precisely.


  class Foo(Serializable):
   ...

  foo = Foo()
  # do something with foo
  ...
  md = foo.dump()
  foo2 = Foo.recover(md)

  Then `foo` and `foo2` are indistinguishable using "public" methods of Foo.
  """

  # NOTE: cannot declare it as an abstract method due to Pytype limitations.
  @classmethod
  def recover(cls: Type['_S'], metadata: vz.Metadata) -> '_S':
    """Creates a new Serializable object from metadata.

    Args:
      metadata:

    Raises:
      DecodeError: Object could not be recovered from the metadata.
    """
    raise NotImplementedError('')

  @abc.abstractmethod
  def dump(self) -> vz.Metadata:
    pass


--- vizier/jax/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Jax modules that has no dependency on Vizier constructs such as Trials."""


--- vizier/jax/models.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Includes both tools for defining a flax model and a library of models."""
# pylint: disable=unused-import

from vizier._src.jax.models.tuned_gp_models import VizierGaussianProcess
from vizier._src.jax.stochastic_process_model import Constraint
from vizier._src.jax.stochastic_process_model import get_constraints
from vizier._src.jax.stochastic_process_model import InitFn
from vizier._src.jax.stochastic_process_model import ModelCoroutine
from vizier._src.jax.stochastic_process_model import ModelParameter
from vizier._src.jax.stochastic_process_model import ModelParameterGenerator
from vizier._src.jax.stochastic_process_model import StochasticProcessModel
from vizier._src.jax.stochastic_process_model import VectorToArrayTree


--- vizier/jax/optimizers.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Thin wrappers around Jax optimizers."""
# pylint: disable=unused-import

import functools

from vizier._src.jax.optimizers.core import LossFunction
from vizier._src.jax.optimizers.core import Optimizer
from vizier._src.jax.optimizers.core import Params
from vizier._src.jax.optimizers.jaxopt_wrappers import JaxoptLbfgsB
from vizier._src.jax.optimizers.jaxopt_wrappers import JaxoptScipyLbfgsB
from vizier._src.jax.optimizers.jaxopt_wrappers import LbfgsBOptions
from vizier._src.jax.optimizers.optax_wrappers import OptaxTrain

DEFAULT_RANDOM_RESTARTS = 4


@functools.lru_cache
def default_optimizer(maxiter: int = 50) -> Optimizer:
  """Default optimizer and random restarts that work okay for most cases."""
  # NOTE: Production algorithms are recommended to stay away from using this.
  return JaxoptScipyLbfgsB(LbfgsBOptions(maxiter=maxiter, best_n=None))


--- vizier/pyglove/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Vizier backend for PyGlove."""
from vizier._src.pyglove.algorithms import BuiltinAlgorithm
from vizier._src.pyglove.converters import VizierConverter
from vizier._src.pyglove.oss_vizier import init
from vizier._src.pyglove.pythia import create_policy


--- vizier/pythia.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""These are the externally-useful symbols for OSS Pythia."""
# pylint: disable=unused-import

# The API for a Pythia Policy -- i.e. the algorithm that Pythia serves.
from vizier._src.pythia.local_policy_supporters import InRamPolicySupporter
from vizier._src.pythia.policy import EarlyStopDecision
from vizier._src.pythia.policy import EarlyStopDecisions
from vizier._src.pythia.policy import EarlyStopRequest
from vizier._src.pythia.policy import Policy
from vizier._src.pythia.policy import SuggestDecision
from vizier._src.pythia.policy import SuggestRequest
from vizier._src.pythia.policy_factory import PolicyFactory
from vizier._src.pythia.policy_supporter import PolicySupporter
from vizier._src.pythia.pythia_errors import CancelComputeError
from vizier._src.pythia.pythia_errors import InactivateStudyError
from vizier._src.pythia.pythia_errors import LoadTooLargeError
from vizier._src.pythia.pythia_errors import PythiaError
from vizier._src.pythia.pythia_errors import PythiaFallbackError
from vizier._src.pythia.pythia_errors import PythiaProtocolError
from vizier._src.pythia.pythia_errors import TemporaryPythiaError
from vizier._src.pythia.pythia_errors import VizierDatabaseError
from vizier._src.pythia.suggest_default import seed_with_default

# Documentation
assert issubclass(CancelComputeError, Exception)
assert issubclass(PythiaProtocolError, Exception)
assert issubclass(VizierDatabaseError, Exception)
assert issubclass(InactivateStudyError, PythiaError)
assert issubclass(LoadTooLargeError, PythiaError)
assert issubclass(TemporaryPythiaError, PythiaError)


--- vizier/pyvizier/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""PyVizier classes for general use.

Contains API shared across all platforms (Internal, Cloud, OSS).
"""

from vizier._src.pyvizier.oss.study_config import ParameterValueSequence
from vizier._src.pyvizier.pythia.study import StudyDescriptor
from vizier._src.pyvizier.pythia.study import StudyState
from vizier._src.pyvizier.pythia.study import StudyStateInfo
from vizier._src.pyvizier.shared.base_study_config import MetricInformation
from vizier._src.pyvizier.shared.base_study_config import MetricsConfig
from vizier._src.pyvizier.shared.base_study_config import MetricType
from vizier._src.pyvizier.shared.base_study_config import ObjectiveMetricGoal
from vizier._src.pyvizier.shared.base_study_config import ProblemStatement
from vizier._src.pyvizier.shared.common import Metadata
from vizier._src.pyvizier.shared.common import MetadataValue
from vizier._src.pyvizier.shared.common import Namespace
from vizier._src.pyvizier.shared.parameter_config import ExternalType
from vizier._src.pyvizier.shared.parameter_config import FidelityConfig
from vizier._src.pyvizier.shared.parameter_config import MonotypeParameterSequence
from vizier._src.pyvizier.shared.parameter_config import ParameterConfig
from vizier._src.pyvizier.shared.parameter_config import ParameterType
from vizier._src.pyvizier.shared.parameter_config import ScaleType
from vizier._src.pyvizier.shared.parameter_config import SearchSpace
from vizier._src.pyvizier.shared.parameter_config import SearchSpaceSelector
from vizier._src.pyvizier.shared.parameter_iterators import SequentialParameterBuilder
from vizier._src.pyvizier.shared.study import ProblemAndTrials
from vizier._src.pyvizier.shared.trial import CompletedTrial
from vizier._src.pyvizier.shared.trial import CompletedTrialWithMeasurements
from vizier._src.pyvizier.shared.trial import Measurement
from vizier._src.pyvizier.shared.trial import MetadataDelta
from vizier._src.pyvizier.shared.trial import Metric
from vizier._src.pyvizier.shared.trial import NaNMetric
from vizier._src.pyvizier.shared.trial import ParameterDict
from vizier._src.pyvizier.shared.trial import ParameterValue
from vizier._src.pyvizier.shared.trial import ParameterValueTypes
from vizier._src.pyvizier.shared.trial import PendingTrial
from vizier._src.pyvizier.shared.trial import PendingTrialWithMeasurements
from vizier._src.pyvizier.shared.trial import Trial
from vizier._src.pyvizier.shared.trial import TrialFilter
from vizier._src.pyvizier.shared.trial import TrialStatus
from vizier._src.pyvizier.shared.trial import TrialSuggestion

FlatSpace = SearchSpace

# Documentation
assert isinstance(NaNMetric, Metric)
assert issubclass(CompletedTrial, Trial)
assert issubclass(CompletedTrialWithMeasurements, Trial)
assert issubclass(PendingTrial, Trial)
assert issubclass(PendingTrialWithMeasurements, Trial)
assert issubclass(Trial, TrialSuggestion)


--- vizier/pyvizier/converters/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Import target for converters."""

from vizier.pyvizier.converters.core import DefaultModelInputConverter
from vizier.pyvizier.converters.core import DefaultModelOutputConverter
from vizier.pyvizier.converters.core import DefaultTrialConverter
from vizier.pyvizier.converters.core import dict_to_array
from vizier.pyvizier.converters.core import DictOf2DArrays
from vizier.pyvizier.converters.core import ModelInputConverter
from vizier.pyvizier.converters.core import ModelOutputConverter
from vizier.pyvizier.converters.core import NumpyArraySpec
from vizier.pyvizier.converters.core import NumpyArraySpecType
from vizier.pyvizier.converters.core import STUDY_ID_FIELD
from vizier.pyvizier.converters.core import TrialToArrayConverter
from vizier.pyvizier.converters.core import TrialToNumpyDict
from vizier.pyvizier.converters.embedder import ProblemAndTrialsScaler
from vizier.pyvizier.converters.jnp_converters import PaddedTrialToArrayConverter
from vizier.pyvizier.converters.jnp_converters import TrialToContinuousAndCategoricalConverter
from vizier.pyvizier.converters.jnp_converters import TrialToModelInputConverter
from vizier.pyvizier.converters.spatio_temporal import DenseSpatioTemporalConverter
from vizier.pyvizier.converters.spatio_temporal import SparseSpatioTemporalConverter
from vizier.pyvizier.converters.spatio_temporal import TimedLabels
from vizier.pyvizier.converters.spatio_temporal import TimedLabelsExtractor


--- vizier/pyvizier/converters/core.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Abstractions and default converters."""

import abc
import copy
import dataclasses
import enum
from typing import Any, Callable, Collection, Dict, Iterator, List, Mapping, Optional, Sequence, Tuple, Type, Union

from absl import logging
import attr
import numpy as np
from vizier import pyvizier

# The study identifier for cross-study learning must be stored in
# Trial.Metadata and StudyConfig.Metadata with this key.
# TODO: Use metadata namespace instead.
STUDY_ID_FIELD = 'metalearn_study_id'


class NumpyArraySpecType(enum.Enum):
  """Type information for NumpyArraySpec.

  CONTINUOUS: Continuous parameter
  DISCRETE: Discrete/integer/categorical parameter
  ONEHOT_EMBEDDING: One-hot embedding of DISCRETE.
  OBJECT: Object type that can hold any value.
  """

  CONTINUOUS = 'CONTINUOUS'
  DISCRETE = 'DISCRETE'
  ONEHOT_EMBEDDING = 'ONEHOT_EMBEDDING'
  OBJECT = 'OBJECT'

  @classmethod
  def default_factory(
      cls, pc: pyvizier.ParameterConfig
  ) -> 'NumpyArraySpecType':
    """SpecType when encoding discretes as integer indices."""
    if pc.type == pyvizier.ParameterType.DOUBLE:
      return NumpyArraySpecType.CONTINUOUS
    elif pc.type in (
        pyvizier.ParameterType.DISCRETE,
        pyvizier.ParameterType.CATEGORICAL,
        pyvizier.ParameterType.INTEGER,
    ):
      return NumpyArraySpecType.DISCRETE
    elif pc.type == pyvizier.ParameterType.CUSTOM:
      return NumpyArraySpecType.OBJECT
    raise ValueError(f'Unknown type {pc.type}')

  @classmethod
  def embedding_factory(
      cls, pc: pyvizier.ParameterConfig
  ) -> 'NumpyArraySpecType':
    """SpecType when encoding discretes as onehot embedding."""
    if pc.type == pyvizier.ParameterType.DOUBLE:
      return NumpyArraySpecType.CONTINUOUS
    elif pc.type in (
        pyvizier.ParameterType.DISCRETE,
        pyvizier.ParameterType.CATEGORICAL,
        pyvizier.ParameterType.INTEGER,
    ):
      return NumpyArraySpecType.ONEHOT_EMBEDDING
    raise ValueError(f'Unknown type {pc.type}')


@attr.define(frozen=True, auto_attribs=True, eq=True, hash=True)
class NumpyArraySpec:
  """Encodes what a feature array represents.

  This class can be seen as a counterpart of Vizier ParameterConfig.
  Vizier ParameterConfigs describe Trial parameters. NumpyArraySpec describes
  numpy arrays returned from trial-to-numpy converters.

  This class is similar to `BoundedTensorSpec` in tf agents, except it carries
  extra information specific to vizier.

  If `type` is `DOUBLE`, then `dtype` is a floating type, and bounds are
  floating numbers. num_dimensions is always 1, and num_oovs is zero.

  If 'type' is `DISCRETE`, then `dtype` is an integer type, and bounds are
  integers. num_dimensions is always 1. Suppose `bounds=(x,y)`. Then integers
  x to-and-including (y-num_oovs) correspond to valid parameter values. The rest
  represent out-of-vocabulary values. For example, an integer parameter in
  the range 1 <= x <= 3 can be represented by a DISCRETE NumpyArraySpec with
  bounds=(1,4) and oov=1.

  If 'type' is `ONEHOT_EMBEDDING`, then `dtype` is a floating type, and bounds
  are floating numbers. In this case, the output array can have multiple
  columns.

  Attributes:
    type: Underlying type of the Vizier parameter corresponding to the array.
    dtype: The numpy array's type.
    bounds: Always inclusive in both directions.
    num_dimensions: Corresponds to shape[-1] of the numpy array. When `type` is
      `ONEHOT_EMBEDDING`, the first X dimensions correspond to valid parameter
      values. The other dimensions correspond to out-of-vocabulary values.
    name: Parameter name.
    num_oovs: Number of out-of-vocabulary items, for non-continuous type.
    scale: Scaling of the values.
  """

  type: NumpyArraySpecType = attr.field(
      validator=attr.validators.instance_of(NumpyArraySpecType)
  )
  dtype: np.dtype = attr.field(
      converter=np.dtype,
      validator=attr.validators.in_(
          [np.float32, np.int32, np.float64, np.int64, np.object_]
      ),
  )
  bounds: Union[Tuple[float, float], Tuple[int, int]]
  num_dimensions: int = attr.field(validator=attr.validators.instance_of(int))
  name: str = attr.field(validator=attr.validators.instance_of(str))
  num_oovs: int = attr.field(validator=attr.validators.instance_of(int))
  scale: Optional[pyvizier.ScaleType] = attr.field(
      default=None,
      validator=attr.validators.optional(
          attr.validators.instance_of(pyvizier.ScaleType)
      ),
  )

  def __attrs_post_init__(self):
    object.__setattr__(
        self, 'bounds', tuple(np.array(self.bounds, dtype=self.dtype))
    )

  @classmethod
  def from_parameter_config(
      cls,
      pc: pyvizier.ParameterConfig,
      type_factory: Callable[
          [pyvizier.ParameterConfig], NumpyArraySpecType
      ] = NumpyArraySpecType.default_factory,
      floating_dtype: Union[np.dtype, Type[np.floating]] = np.float32,
      int_dtype: Union[np.dtype, Type[np.integer]] = np.int32,
      *,
      pad_oovs: bool = True,
  ) -> 'NumpyArraySpec':
    """Factory function.

    Args:
      pc:
      type_factory: NumpyArraySpecType has `default_factory` and
        `embedding_factory`. The difference is in how they handle non-continuous
        parameters.
      floating_dtype: Dtype of the floating outputs.
      int_dtype: Dtype of the integer outputs.
      pad_oovs: If True, pad the out-of-vocabulary dimensions to onehot
        embedding.

    Returns:
      NumpyArraySpec.
    """
    the_type = type_factory(pc)
    if the_type == NumpyArraySpecType.CONTINUOUS:
      return NumpyArraySpec(
          the_type,
          np.dtype(floating_dtype),
          bounds=pc.bounds,
          num_dimensions=1,
          scale=pc.scale_type,
          name=pc.name,
          num_oovs=0,
      )
    elif the_type == NumpyArraySpecType.DISCRETE:
      return NumpyArraySpec(
          the_type,
          np.dtype(int_dtype),
          bounds=(0, len(pc.feasible_values)),
          num_dimensions=1,
          name=pc.name,
          num_oovs=1 if pad_oovs else 0,
      )
    elif the_type == NumpyArraySpecType.ONEHOT_EMBEDDING:
      return NumpyArraySpec(
          the_type,
          np.dtype(floating_dtype),
          bounds=(0.0, 1.0),
          num_dimensions=len(pc.feasible_values) + 1,
          name=pc.name,
          num_oovs=1 if pad_oovs else 0,
      )
    elif the_type == NumpyArraySpecType.OBJECT:
      return NumpyArraySpec(
          the_type,
          dtype=np.object_,
          bounds=(0, 0),
          num_dimensions=0,
          name=pc.name,
          num_oovs=0,
      )
    raise ValueError(f'Unknown type {type}')


def dict_to_array(array_dict: Mapping[Any, np.ndarray]) -> np.ndarray:
  r"""Converts a dict of (..., D_i) arrays to a (..., \sum_i D_i) array."""
  return np.concatenate(list(array_dict.values()), axis=-1)


class DictOf2DArrays(Mapping[str, np.ndarray]):
  """Dictionary of string to 2D arrays.

  All arrays share the first dimension, which is at a high level, the number of
  objects that this dictionary corresponds to.

  Attributes:
    size: Array's shape[0].
  """

  def __init__(self, d: Mapping[str, np.ndarray]):
    self._d = d
    shape = None
    for k, v in self.items():
      if shape is None:
        shape = v.shape
        if len(shape) != 2:
          raise ValueError(
              f'{k} has shape {v.shape} which is not length 2.'
              'DictOf2DArrays only supports 2D numpy arrays.'
          )
      if shape[0] != v.shape[0]:
        raise ValueError(
            f'{k} has shape {v.shape} which is not equal to {shape}.'
        )
    self._size = shape[0]

  def __getitem__(self, key: str) -> np.ndarray:
    return self._d[key]

  def __iter__(self) -> Iterator[str]:
    return iter(self._d)

  def __len__(self) -> int:
    return len(self._d)

  def __add__(self, other: 'DictOf2DArrays') -> 'DictOf2DArrays':
    if not isinstance(other, DictOf2DArrays):
      raise ValueError('You can add DictOf2DArrays only.')
    if len(self) != len(other):
      # We don't check the keys because it's too slow.
      raise ValueError('Two arrays have different length!')
    return DictOf2DArrays(
        {k: np.concatenate([self[k], other[k]], axis=0) for k in self}
    )

  @property
  def size(self) -> int:
    return self._size

  def asarray(self) -> np.ndarray:
    return dict_to_array(self._d)

  def dict_like(self, array: np.ndarray) -> 'DictOf2DArrays':
    """[Experimental] Converts an array into a dict with the same keys as this.

    This function acts like an inverse of `asarray()`, i.e. it satisfies
      `self.dict_like(self.asarray()) == self`.

    Example:
      d = DictOf2DArrays({'p1': [[1], [2], [3]], 'p2': [[4], [5], [6]]})
      d.dict_like([[1, 2], [3, 4]]) == {'p1': [[1], [3]], 'p2': [[2],[4]]}

    Args:
      array:

    Returns:
      DictOf2DArrays with the same shape spec as this.
    """
    begin = 0
    new_dict = dict()
    for k, v in self.items():
      end = begin + v.shape[1]
      new_dict[k] = array[:, begin:end].astype(v.dtype)
      begin = end

    return DictOf2DArrays(new_dict)


class TrialToNumpyDict(abc.ABC):
  """Parses a sequence of Trials to a dict keyed by parameter and metric names.

  Design note:
    A typical Keras pipeline consists of:
      1. Load data into arrays.
      2. Call Model.build() to initialize a model for the loaded data shape.
      3. Call Model.fit() to train the model.
      4. Call Model.__call__() to predict with the model.
    This abstraction allows us to switch implementations for the step number 1.

  This class consists of `to_xy()` function and properties that describe
  the returned values of `to_xy()`. Subclasses are free to have extra functions
  for convenience.
  """

  @abc.abstractmethod
  def to_xy(
      self, trials: Sequence[pyvizier.Trial]
  ) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """Extracts features-labels pairs that can be used for fitting a model."""
    pass

  @property
  @abc.abstractmethod
  def features_shape(self) -> Dict[str, Sequence[Union[int, None]]]:
    """Describes the shape of the first element returned from to_xy().

    Returns:
      Dict of feature names to shapes. `None` means that the shape depends
      on the input "trials".
    # TODO: Add compute_feature_shape(trials) that returns exact
    shapes.
    """
    pass

  @property
  @abc.abstractmethod
  def output_specs(self) -> Dict[str, NumpyArraySpec]:
    """Describes the semantics of the first element returned from to_xy().

    See NumpyArraySpec for more details.

    Returns:
      Dict of feature names to NumpyArraySpecs.
    """
    pass

  @property
  @abc.abstractmethod
  def labels_shape(self) -> Dict[str, Any]:
    """Describes the shape of the second element returned from to_xy().

    Returns:
      Dict of label names to shapes. `None` means that the shape depends
      on the input "trials".
    """
    pass

  @property
  @abc.abstractmethod
  def metric_information(self) -> Dict[str, pyvizier.MetricInformation]:
    """Describes the semantics of the second element returned from to_xy().

    See ModelOutputConverter for more details.
    Returns:
      Dict of label names to MetricInformation.
    """


class ModelInputConverter(metaclass=abc.ABCMeta):
  """Interface for extracting inputs to the model."""

  @abc.abstractmethod
  def convert(self, trials: Sequence[pyvizier.TrialSuggestion]) -> np.ndarray:
    """Returns an array of shape (number of trials, feature dimension).

    Args:
      trials:

    Returns:
      Returns an array of shape (number of trials, feature dimension).
      Subclasses must use a fixed feature dimension. In particular, it should
      be a constant function of the input trials.
    """

  @property
  @abc.abstractmethod
  def output_spec(self) -> NumpyArraySpec:
    """Provides specification of the output from this converter."""

  @property
  @abc.abstractmethod
  def parameter_config(self):
    """Original ParameterConfig that this converter acts on."""

  @abc.abstractmethod
  def to_parameter_values(
      self, array: np.ndarray
  ) -> List[Optional[pyvizier.ParameterValue]]:
    """Convert and clip to the nearest feasible parameter values.

    NOTE: value is automatically truncated to lie inside the search space.
      This method should only be called to convert suggestions.

    Args:
      array: has shape (number of trials, feature dimension)

    Returns:
      List of (ParameterValue or None).  A list entry is None when this
      converter's parameter doesn't exist in the Trial.  (This is common
      when parameters are conditional.)
    """


@dataclasses.dataclass
class ModelInputArrayBijector:
  """Transformations on the numpy arrays generated by ModelInputConverter."""

  forward_fn: Callable[[np.ndarray], np.ndarray]
  backward_fn: Callable[[np.ndarray], np.ndarray]
  output_spec: NumpyArraySpec  # Spec after forward_fn is applied.

  @classmethod
  def identity(cls, spec: NumpyArraySpec) -> 'ModelInputArrayBijector':
    return cls(lambda x: x, lambda x: x, spec)

  @classmethod
  def scaler_from_spec(cls, spec: NumpyArraySpec) -> 'ModelInputArrayBijector':
    """For continuous specs, linearize and scale it to (0, 1) range."""
    low, high = spec.bounds
    if spec.type != NumpyArraySpecType.CONTINUOUS:
      return cls.identity(attr.evolve(spec, scale=None))
    if low == high:

      def backward_fn(y):
        return np.where(np.isfinite(y), y + low - 0.5, y)

      def forward_fn(y):
        return np.where(np.isfinite(y), y - low + 0.5, y)

      return cls(
          forward_fn,
          backward_fn,
          attr.evolve(spec, bounds=(0.5, 0.5), scale=None),
      )

    if spec.scale == pyvizier.ScaleType.LOG:
      if low < 0 or high < 0:
        raise ValueError(
            'Log scale requires both parameter boundaries to be positive,'
            f' though low bound is {low} and high bound is {high}.'
        )
      low, high = np.log(low), np.log(high)
      denom = (high - low) or 1.0
      if denom < 1e-6:
        logging.warning('Unusually small range detected for %s', spec)
      scale_fn = lambda x, low=low, denom=denom: (np.log(x) - low) / denom
      unscale_fn = lambda x, low=low, denom=denom: np.exp(x * denom + low)
    elif spec.scale == pyvizier.ScaleType.REVERSE_LOG:
      raw_sum = low + high
      low, high = np.log(low), np.log(high)
      denom = (high - low) or 1.0
      if denom < 1e-6:
        logging.warning('Unusually small range detected for %s', spec)

      def scale_fn(x, low=low, raw_sum=raw_sum, denom=denom):
        return 1.0 - (np.log(raw_sum - x) - low) / denom

      def unscale_fn(x, high=high, raw_sum=raw_sum):
        return raw_sum - np.exp(high - denom * x)

    else:
      if not (spec.scale == pyvizier.ScaleType.LINEAR or spec.scale is None):
        logging.warning('Unknown scale type %s. Applying LINEAR', spec.scale)
      denom = high - low
      if denom < 1e-6:
        logging.warning('Unusually small range detected for %s', spec)
      if denom == 1.0 and low == 0:
        return cls.identity(attr.evolve(spec, scale=None))
      scale_fn = lambda x, high=high, low=low: (x - low) / (high - low)
      unscale_fn = lambda x, high=high, low=low: x * (high - low) + low

    return cls(
        scale_fn, unscale_fn, attr.evolve(spec, bounds=(0.0, 1.0), scale=None)
    )

  @classmethod
  def onehot_embedder_from_spec(
      cls, spec: NumpyArraySpec, *, dtype=np.float32, pad_oovs: bool = True
  ) -> 'ModelInputArrayBijector':
    """Given a discrete spec, one-hot embeds it."""
    if spec.type != NumpyArraySpecType.DISCRETE:
      return cls.identity(spec)

    num_oovs = 1 if pad_oovs else 0
    output_spec = NumpyArraySpec(
        NumpyArraySpecType.ONEHOT_EMBEDDING,
        dtype,
        bounds=(0.0, 1.0),
        num_dimensions=int(spec.bounds[1] - spec.bounds[0] + num_oovs),
        name=spec.name,
        num_oovs=num_oovs,
        scale=None,
    )

    def embed_fn(x: np.ndarray, output_spec=output_spec):
      """x is integer array of [N, 1]."""
      return np.eye(output_spec.num_dimensions, dtype=output_spec.dtype)[
          x.flatten()
      ]

    def unembed_fn(x: np.ndarray, spec=spec, output_spec=output_spec):
      return np.argmax(
          x[:, : output_spec.num_dimensions - output_spec.num_oovs], axis=1
      ).astype(spec.dtype)

    return cls(embed_fn, unembed_fn, output_spec)


def _create_default_getter(
    pconfig: pyvizier.ParameterConfig,
) -> Callable[[pyvizier.TrialSuggestion], Any]:
  """Create a default getter for the given parameter config."""

  def getter(trial, pconfig=pconfig):
    if pconfig.name not in trial.parameters:
      return None

    pvalue = trial.parameters[pconfig.name]
    if pconfig.type == pyvizier.ParameterType.DOUBLE:
      return pvalue.as_float
    elif pconfig.type == pyvizier.ParameterType.DISCRETE:
      return pvalue.as_float
    elif pconfig.type == pyvizier.ParameterType.INTEGER:
      return pvalue.as_int
    else:
      return pvalue.as_str

  return getter


class DefaultModelInputConverter(ModelInputConverter):
  """Converts trials into a (None, 1) array corresponding to a parameter.

  If the parameter_config is continuous, values obtained from `getter()` are
  directly returned as floating numbers. Otherwise, this converter returns
  the index of the value obtained from `getter()` within
  `parameter_config.feasible_points` as int32.
  """

  def __init__(
      self,
      parameter_config: pyvizier.ParameterConfig,
      getter: Optional[Callable[[pyvizier.TrialSuggestion], Any]] = None,
      *,
      float_dtype: Union[np.dtype, Type[np.floating]] = np.float32,
      max_discrete_indices: int = 10,
      scale: bool = False,
      onehot_embed: bool = False,
      converts_to_parameter: bool = True,
      pad_oovs: bool = True,
      should_clip: bool = True,
  ):
    """Init.

    Given B trials, convert() always converts to (B, 1) array. The returned
    array may contain NaNs.

    Args:
      parameter_config:
      getter: See class pydoc. If the getter is not specified, the default
        getter looks up `parameter_config.name` inside `Trial.parameters`.
      float_dtype: floating precision to be used.
      max_discrete_indices: If the parameter config has more than this many
        number of DISCRETE/INTEGER feasible points, then the parameter config is
        continuified first.
      scale:
      onehot_embed:
      converts_to_parameter: If False, this converter does not correspond to an
        actual parameter in Vizier search space, and `to_parameter_value` always
        returns None
      pad_oovs: If True, pad the out-of-vocabulary dimensions to onehot
        embedding.
      should_clip: If True, clipping should be applied to parameter values.
    """
    self._converts_to_parameter = converts_to_parameter
    self._parameter_config = copy.deepcopy(parameter_config)
    if (
        parameter_config.type
        in (pyvizier.ParameterType.INTEGER, pyvizier.ParameterType.DISCRETE)
        and parameter_config.num_feasible_values > max_discrete_indices
    ):
      parameter_config = parameter_config.continuify()

    # TODO: Make the default getter raise an Error if they encounter an
    # out-of-vocabulary value but pad_oovs is False.
    self._getter = getter or _create_default_getter(parameter_config)
    # Getter spec can only have DISCRETE or CONTINUOUS types.
    self._getter_spec = NumpyArraySpec.from_parameter_config(
        parameter_config,
        NumpyArraySpecType.default_factory,
        floating_dtype=float_dtype,
    )

    # Optionally scale and onehot embed.
    spec = self._getter_spec
    self.scaler = (
        ModelInputArrayBijector.scaler_from_spec(spec)
        if scale
        else ModelInputArrayBijector.identity(spec)
    )
    spec = self.scaler.output_spec
    if onehot_embed:
      self.onehot_encoder = ModelInputArrayBijector.onehot_embedder_from_spec(
          spec, dtype=float_dtype, pad_oovs=pad_oovs
      )
    else:
      self.onehot_encoder = ModelInputArrayBijector.identity(spec)

    spec = self.onehot_encoder.output_spec
    self._output_spec = spec
    self._should_clip = should_clip

  def convert(self, trials: Sequence[pyvizier.TrialSuggestion]) -> np.ndarray:
    """Returns an array of shape [len(trials), output_spec.num_dimensions].

    Args:
      trials:

    Returns:
      For each `trial`, if `self.getter(trial)` returns `None`, we _impute_ the
      value; otherwise, we _extract_ the value.
      If `self.parameter_config.type` is `DOUBLE`, then
        * EXTRACT: Directly use the getter's return value as float.
        * IMPUTE: Return `nan`.
      Otherwise,
        * EXTRACT: Returns the integer index of the getter's return value within
          feasible values.
        * IMPUTE: Returns `len(feasible_values)`.
    """
    if not trials:
      return np.zeros(
          [0, self.output_spec.num_dimensions], dtype=self.output_spec.dtype
      )

    value_converter = (
        self._convert_index
        if self._getter_spec.type == NumpyArraySpecType.DISCRETE
        else self._convert_continuous
    )
    values = [value_converter(t) for t in trials]
    array = np.asarray(values, dtype=self._getter_spec.dtype).reshape([-1, 1])
    return self.onehot_encoder.forward_fn(self.scaler.forward_fn(array))

  def _to_parameter_value(
      self, value: Union['np.float', float, int]
  ) -> Optional[pyvizier.ParameterValue]:
    """Converts to a single parameter value; see to_parameter_values().

    Be aware that the value is automatically truncated.

    Args:
      value:

    Returns:
      ParameterValue.
    """
    if not self._converts_to_parameter:
      return None
    elif not np.isfinite(value):
      return None
    elif self.parameter_config.type == pyvizier.ParameterType.DOUBLE:
      # Input parameter was DOUBLE. Output is also DOUBLE.
      if self._should_clip:
        value = np.clip(
            value,
            np.float64(self._parameter_config.bounds[0]),
            np.float64(self._parameter_config.bounds[1]),
        )
      return pyvizier.ParameterValue(float(value))
    elif self.output_spec.type == NumpyArraySpecType.CONTINUOUS:
      # The parameter config is originally discrete, but continuified.
      # Round to the closest number.
      diffs = np.abs(
          np.asarray(self.parameter_config.feasible_values, dtype=self.dtype)
          - value
      )

      idx = np.argmin(diffs)
      closest_number = pyvizier.ParameterValue(
          self.parameter_config.feasible_values[idx]
      )
      return pyvizier.ParameterValue(
          closest_number.cast_as_internal(self.parameter_config.type)
      )

    elif value >= len(self.parameter_config.feasible_values):
      return None
    else:
      return pyvizier.ParameterValue(
          self.parameter_config.feasible_values[value]
      )

  def to_parameter_values(
      self, array: np.ndarray
  ) -> List[Optional[pyvizier.ParameterValue]]:
    """Convert and clip to the nearest feasible parameter values."""
    array = self.scaler.backward_fn(self.onehot_encoder.backward_fn(array))
    return [self._to_parameter_value(v) for v in list(array.flatten())]

  def _convert_index(self, trial: pyvizier.TrialSuggestion):
    """Called by `convert()` if configured for a non-continuous parameter."""
    raw_value = self._getter(trial)
    if raw_value in self.parameter_config.feasible_values:
      return self.parameter_config.feasible_values.index(raw_value)
    else:
      # Return the catch-all missing index.
      return len(self.parameter_config.feasible_values)

  def _convert_continuous(self, trial: pyvizier.TrialSuggestion):
    """Called by `convert()` if configured for a continuous parameter."""
    raw_value = self._getter(trial)
    if raw_value is None:
      return np.nan
    else:
      return raw_value

  @property
  def dtype(self):
    """dtype of the array returned by convert()."""
    return self.output_spec.dtype

  @property
  def output_spec(self) -> NumpyArraySpec:
    return self._output_spec

  @property
  def parameter_config(self) -> pyvizier.ParameterConfig:
    """Original parameter config that this converter is defined on."""
    return self._parameter_config


class ModelOutputConverter(metaclass=abc.ABCMeta):
  """Metric converter interface.

  Unlike ModelInputConverter, this class is currently designed to always
  output a single-dimensional metric. This is because in standard Keras
  workflows, output shapes must be known when defining a model. (In contrast,
  input shapes are required at the time of _building_ a model.)
  """

  @abc.abstractmethod
  def convert(self, measurements: Sequence[pyvizier.Measurement]) -> np.ndarray:
    """Returns N x 1 array."""
    pass

  @abc.abstractmethod
  def to_metrics(
      self, labels: np.ndarray
  ) -> Sequence[Optional[pyvizier.Metric]]:
    """Returns a list of pyvizier metrics.

    The metrics are converted from an array of labels with shape (len(metrics),)
    or (len(metrics), 1) and nan values in the labels are translated to None.

    Args:
      labels: (len(metrics),) or (len(metrics), 1) shaped array of labels.

    Returns:
      A list of pyvizier metrics created with `labels`.
    """

  @property
  @abc.abstractmethod
  def metric_information(self) -> pyvizier.MetricInformation:
    """Describes the semantics of the return value from convert() method.

    Returns:
      The MetricInformation that reflects how the labels should be
      interpreted. It may not be identical to the MetricInformation that the
      converter was created from. For example, if the converter flips the signs
      or changes the semantics of safety configs, then the returned
      MetricInformation should reflect such changes.
    """

  @property
  def output_shape(self) -> Tuple[None, int]:
    return (None, 1)


class DefaultModelOutputConverter(ModelOutputConverter):
  """Converts measurements into numpy arrays."""

  def __init__(
      self,
      metric_information: pyvizier.MetricInformation,
      *,
      flip_sign_for_minimization_metrics: bool = False,
      shift_safe_metrics: bool = True,
      dtype: Union[
          Type[float], Type[int], Type[np.generic], np.dtype
      ] = np.float32,
      raise_errors_for_missing_metrics: bool = False,
  ):
    """Init.

    Args:
      metric_information:
      flip_sign_for_minimization_metrics: Flips the sign if the metric is to
        minimize.
      shift_safe_metrics: If True, subtract the safety threshold from the metric
        value. i.e. center the metric so that the safety threshold is 0. For a
        minimization safety metric if flip_sign_for_minimization_metrics is
        true, subtraction happens begore flipping the sign.
      dtype:
      raise_errors_for_missing_metrics: If True, raise errors. Otherwise, fill
        with NaN.
    """

    self._original_metric_information = metric_information
    self.flip_sign_for_minimization_metrics = flip_sign_for_minimization_metrics
    self.raise_errors_for_missing_metrics = raise_errors_for_missing_metrics
    self.dtype = dtype
    self.shift_safe_metrics = shift_safe_metrics

  @property
  def _should_flip_sign(self) -> bool:
    return (
        self._original_metric_information.goal
        == pyvizier.ObjectiveMetricGoal.MINIMIZE
        and self.flip_sign_for_minimization_metrics
    )

  def convert(
      self, measurements: Sequence[Optional[pyvizier.Measurement]]
  ) -> np.ndarray:
    """Returns a (len(measurements), 1) array."""
    if not measurements:
      return np.zeros([0, 1], dtype=self.dtype)

    all_metrics = [m.metrics if m is not None else dict() for m in measurements]
    if not self.raise_errors_for_missing_metrics:
      metricvalues = [
          metrics.get(self._original_metric_information.name, None)
          for metrics in all_metrics
      ]
      labels = [mv.value if mv else np.nan for mv in metricvalues]
    else:
      labels = [
          metrics[self._original_metric_information.name].value
          for metrics in all_metrics
      ]
    labels = np.asarray(labels, dtype=self.dtype)[:, np.newaxis]
    if (
        self.shift_safe_metrics
        and self._original_metric_information.type.is_safety
    ):
      labels -= self._original_metric_information.safety_threshold
    return labels * (-1 if self._should_flip_sign else 1)

  def to_metrics(
      self, labels: np.ndarray
  ) -> Sequence[Optional[pyvizier.Metric]]:
    """Converts an array of labels to pyvizier Metrics.

    Args:
      labels: (len(metrics),) or (len(metrics), 1) shaped array of labels.

    Returns:
      metrics: a list of pyvizier metrics.
    """
    if labels.ndim == 1:
      labels = labels[:, None]
    if labels.shape[1] > 1 or len(labels.shape) > 2:
      raise ValueError('The input array must be of shape (num,) or (num, 1).')

    labels = labels.flatten()
    if (
        self.shift_safe_metrics
        and self._original_metric_information.type.is_safety
    ):
      labels -= self._original_metric_information.safety_threshold

    labels = labels * (-1 if self._should_flip_sign else 1)
    metrics = [
        pyvizier.Metric(value=l) if np.isfinite(l) else None for l in labels
    ]
    return metrics

  @property
  def metric_information(self) -> pyvizier.MetricInformation:
    """Returns a copy that reflects how the converter treats the metric."""
    metric_information = copy.deepcopy(self._original_metric_information)
    if self.shift_safe_metrics and metric_information.type.is_safety:
      metric_information.safety_threshold = 0.0
    if self._should_flip_sign:
      metric_information = metric_information.flip_goal()
    return metric_information


class DefaultTrialConverter(TrialToNumpyDict):
  """Combines parameter and metric converters.

  Always takes the final measurement and convert to metrics.

  WARNING: This class is not responsible for filtering out non-completed or
  infeasible trials. Labels may contain NaN in such cases.

  For standard use cases, use one of the factory functions. For more
  flexibility, inject parameter_converters and metric_converters on your own.
  """

  def __init__(
      self,
      parameter_converters: Collection[ModelInputConverter],
      metric_converters: Collection[ModelOutputConverter] = tuple(),
  ):
    self.parameter_converters = list(parameter_converters)
    self.parameter_converters_dict = {
        pc.parameter_config.name: pc for pc in self.parameter_converters
    }
    self.metric_converters = list(metric_converters)
    self._metric_converters_dict = {
        mc.metric_information.name: mc for mc in self.metric_converters
    }

  def to_features(
      self, trials: Sequence[pyvizier.TrialSuggestion]
  ) -> Dict[str, np.ndarray]:
    """Shorthand for to_xy(trials))[0]."""
    result_dict = dict()
    for converter in self.parameter_converters:
      result_dict[converter.parameter_config.name] = converter.convert(trials)
    return result_dict

  def to_trials(
      self,
      features: Mapping[str, np.ndarray],
      labels: Optional[Mapping[str, np.ndarray]] = None,
  ) -> List[pyvizier.Trial]:
    """Inverse of `to_features` and optionally the inverse of `to_labels`.

    We assume that label values are already normalized and their signs are
    flipped if required.

    Args:
      features: A dictionary where keys correspond to parameter names in the
        returned Trial and values correspond to parameter values and have shape
        (num_obs, 1).
      labels: A dictionary of labels where each key corresponds to a metric name
        and its value is an array of shape (num_obs, 1) of observed metric
        values.

    Returns:
      A list of pyvizier trials created with parameters corresponding to
      `features` and final measurements corresponding to `labels`.
      NOTE: All final measurements have steps=1. If 'labels' is not passed, the
      output trials include `final_measurement=None` and `measurements=[]`.
    """
    invalid_features = [
        (k, v.shape)
        for k, v in features.items()
        if len(v.shape) != 2 or (len(v.shape) == 2 and v.shape[1] != 1)
    ]
    if invalid_features:
      raise ValueError(
          'Features need to contain 2d arrays with shape (num_obs, 1). Invalid'
          f' feature shapes: {invalid_features}'
      )

    if labels:
      invalid_labels = [
          (k, v.shape)
          for k, v in labels.items()
          if len(v.shape) != 2 or (len(v.shape) == 2 and v.shape[1] != 1)
      ]
      if invalid_labels:
        raise ValueError(
            'Labels need to contain 2d arrays with shape (num_obs, 1). Invalid'
            f' label shapes: {invalid_labels}'
        )

    if labels is None:
      return [
          pyvizier.Trial(parameters=p) for p in self.to_parameters(features)
      ]

    try:
      labels = DictOf2DArrays(labels)
    except ValueError as e:
      raise ValueError('Please check the shape of "labels"') from e

    try:
      features = DictOf2DArrays(features)
    except ValueError as e:
      raise ValueError('Please check the shape of "features"') from e

    if labels.size != features.size:
      raise ValueError(
          'The number of features and labels observations do not match.'
      )
    parameters = self.to_parameters(features)
    measurements = self._to_measurements(labels)
    for m in measurements:
      m.steps = 1

    trials = []
    for p, m in zip(parameters, measurements):
      trial = pyvizier.Trial(parameters=p, final_measurement=m)
      # _to_measurements returns an empty dict for NaN and non-finite metric
      # values.
      if not m.metrics:
        trial.complete(m, infeasibility_reason='Metrics are empty')
      trials.append(trial)
    return trials

  def _to_measurements(
      self, labels: Mapping[str, np.ndarray]
  ) -> List[pyvizier.Measurement]:
    """Converts a dictionary of labels into a list of pyvizier measurements.

    Each key in the dictionary corresponds to a metric and the length of the
    list of pyvizier measurements equals to the number of keys. Note that this
    method generates measurements without setting steps, hence steps are
    defaulted to zero. The caller should assign them later if desired.

    Args:
      labels: A dictionary of labels where each key corresponds to a metric name
        with dictionary values as metric values casted as an array of shape
        (num_obs,) or (num_obs, 1).

    Returns:
      A list of pyvizier measurements created with final measurements
        corresponding to `labels`.
    """
    try:
      DictOf2DArrays(labels)
    except ValueError as e:
      raise ValueError('Please check the shape of "labels"') from e
    measurements = [
        pyvizier.Measurement() for _ in range(DictOf2DArrays(labels).size)
    ]
    # Iterate through labels names and convert them.
    for metric_name, metric_values in labels.items():
      metric_converter = self._metric_converters_dict[metric_name]
      metrics_values = metric_converter.to_metrics(metric_values)
      for measurement_dict, value in zip(measurements, metrics_values):
        if value is not None:
          measurement_dict.metrics[metric_name] = value
    return measurements

  def to_parameters(
      self, features: Mapping[str, np.ndarray]
  ) -> List[pyvizier.ParameterDict]:
    """Convert to nearest feasible parameter value. NaNs trigger errors."""
    # TODO: NaNs should be ignored instead of triggering errors.

    # Validate features's shape, and create empty ParameterDicts.
    parameters = [
        pyvizier.ParameterDict() for _ in range(DictOf2DArrays(features).size)
    ]

    # Iterate through parameter names and convert them.
    for key, values in features.items():
      parameter_converter = self.parameter_converters_dict[key]
      parameter_values = parameter_converter.to_parameter_values(values)
      for param_dict, value in zip(parameters, parameter_values):
        if value is not None:
          param_dict[key] = value
    return parameters

  def to_labels(
      self, trials: Sequence[pyvizier.Trial]
  ) -> Dict[str, np.ndarray]:
    """Shorthand for to_xy(trials))[1]."""
    result_dict = dict()
    for converter in self.metric_converters:
      result_dict[converter.metric_information.name] = converter.convert(
          [t.final_measurement for t in trials]
      )
    return result_dict

  def to_labels_array(self, trials: Sequence[pyvizier.Trial]) -> np.ndarray:
    """Shorthand for dict_to_array(self.to_labels(trials))."""
    return dict_to_array(self.to_labels(trials))

  def to_xy(
      self, trials: Sequence[pyvizier.Trial]
  ) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
    """See base class."""
    return self.to_features(trials), self.to_labels(trials)

  @property
  def features_shape(self) -> Dict[str, Tuple[Union[int, None], int]]:
    """See base class."""
    return {
        pc.output_spec.name: (None, pc.output_spec.num_dimensions)
        for pc in self.parameter_converters
    }

  @property
  def output_specs(self) -> Dict[str, NumpyArraySpec]:
    """See base class."""
    return {
        pc.output_spec.name: pc.output_spec for pc in self.parameter_converters
    }

  @property
  def labels_shape(self) -> Dict[str, Sequence[Union[int, None]]]:
    """See base class."""
    return {
        mc.metric_information.name: mc.output_shape
        for mc in self.metric_converters
    }

  @property
  def metric_information(self) -> Dict[str, pyvizier.MetricInformation]:
    """See base class."""
    return {
        mc.metric_information.name: mc.metric_information
        for mc in self.metric_converters
    }

  # TODO: Deprecate or update so that it returns SearchSpace.
  @property
  def parameter_configs(self) -> Dict[str, pyvizier.ParameterConfig]:
    """Returns a dict of the original Parameter configs."""
    return {
        converter.parameter_config.name: converter.parameter_config
        for converter in self.parameter_converters
    }

  @classmethod
  def from_study_configs(
      cls,
      study_configs: Sequence[pyvizier.ProblemStatement],
      metric_information: Collection[pyvizier.MetricInformation],
      *,
      use_study_id_feature: bool = True,
  ) -> 'DefaultTrialConverter':
    """Creates a converter from a list of study configs.

    Args:
      study_configs: StudyConfigs to be merged.
      metric_information: MetricInformation of metrics to be used as y-values.
      use_study_id_feature: If True, an extra parameter is added that
        corresponds to the STUDY_ID_FIELD inside metadata.

    Returns:
      `DefaultTrialConverter`.
    """
    # Merge parameter configs by name.
    merged_configs = list(
        pyvizier.SearchSpaceSelector([sc.search_space for sc in study_configs])
        .select_all()
        .merge()
    )

    merged_configs = {pc.name: pc for pc in merged_configs}
    parameter_converters = [
        DefaultModelInputConverter(pc) for pc in merged_configs.values()
    ]

    # Append study id feature if configured to do so.
    if use_study_id_feature:
      # Collect study ids.
      study_ids = set()
      for sc in study_configs:
        metalearn_study_id = sc.metadata.get(STUDY_ID_FIELD, None)
        if metalearn_study_id is None:
          continue
        study_ids.add(metalearn_study_id)

      # Validate.
      if not study_ids:
        logging.error(
            'use_study_id_feature was True, but none of the studies '
            'had study id configured.'
        )
        use_study_id_feature = False
      elif STUDY_ID_FIELD in merged_configs:
        raise ValueError(
            'Dataset name conflicts with a ParameterConfig '
            'that already exists: {}'.format(merged_configs[STUDY_ID_FIELD])
        )

      # Create new parameter config.
      parameter_config = pyvizier.ParameterConfig.factory(
          STUDY_ID_FIELD, feasible_values=list(study_ids)
      )
      merged_configs[STUDY_ID_FIELD] = parameter_config
      logging.info('Created a new ParameterConfig %s', parameter_config)

      # Create converter.
      parameter_converters.append(
          DefaultModelInputConverter(
              parameter_config,
              lambda t: t.metadata.get(STUDY_ID_FIELD, None),
              converts_to_parameter=False,
          )
      )

    return cls(
        parameter_converters,
        [DefaultModelOutputConverter(m) for m in metric_information],
    )

  @classmethod
  def from_study_config(
      cls, study_config: pyvizier.ProblemStatement
  ) -> 'DefaultTrialConverter':
    return cls.from_study_configs(
        [study_config],
        study_config.metric_information,
        use_study_id_feature=False,
    )


@attr.define
class TrialToArrayConverter:
  """EXPERIMENTAL: A quick-and-easy converter that returns a single array.

  Unlike TrialtoNumpyDict converters, `to_features` and `to_labels`
  return a single array of floating numbers. CATEGORICAL and DISCRETE parameters
  are one-hot embedded.

  IMPORTANT: Use a factory method (currently, there is one: `from_study_config`)
  instead of `__init__`.

  WARNING: This class is not exchangable with `TrialToNumpyDict`, and does not
  have functions that return shape or metric informations. Use it at your own
  risk.
  """

  _impl: DefaultTrialConverter

  def to_features(self, trials) -> np.ndarray:
    """Returns the features array with dimension: (n_trials, n_features)."""
    return dict_to_array(self._impl.to_features(trials))

  def to_labels(self, trials) -> np.ndarray:
    """Returns the labels array with dimension: (n_trials, n_metrics)."""
    # Pad up the labels.
    return dict_to_array(self._impl.to_labels(trials))

  def to_xy(self, trials) -> Tuple[np.ndarray, np.ndarray]:
    return self.to_features(trials), self.to_labels(trials)

  def to_parameters(self, arr: np.ndarray) -> Sequence[pyvizier.ParameterDict]:
    """Convert to nearest feasible parameter value. NaNs are preserved."""
    arrformat = DictOf2DArrays(self._impl.to_features([]))
    return self._impl.to_parameters(arrformat.dict_like(arr))

  @classmethod
  def from_study_config(
      cls,
      study_config: pyvizier.ProblemStatement,
      *,
      scale: bool = True,
      pad_oovs: bool = True,
      max_discrete_indices: int = 0,
      flip_sign_for_minimization_metrics: bool = True,
      should_clip=True,
      dtype=np.float64,
  ) -> 'TrialToArrayConverter':
    """From study config.

    Args:
      study_config:
      scale: If True, scales the parameters to [0, 1] range.
      pad_oovs: If True, add an extra dimension for out-of-vocabulary values for
        non-CONTINUOUS parameters.
      max_discrete_indices: For DISCRETE and INTEGER parameters that have more
        than this many feasible values will be continuified. When generating
        suggestions, values are rounded to the nearest feasible value. Note this
        default is different from the default in DefaultModelInputConverter.
      flip_sign_for_minimization_metrics: If True, flips the metric signs so
        that every metric maximizes.
      should_clip: Whether or not clipping should be done.
      dtype: dtype

    Returns:
      TrialToArrayConverter
    """

    def create_input_converter(parameter):
      return DefaultModelInputConverter(
          parameter,
          scale=scale,
          max_discrete_indices=max_discrete_indices,
          onehot_embed=True,
          float_dtype=dtype,
          pad_oovs=pad_oovs,
          should_clip=should_clip,
      )

    def create_output_converter(metric):
      return DefaultModelOutputConverter(
          metric,
          flip_sign_for_minimization_metrics=flip_sign_for_minimization_metrics,
          dtype=dtype,
      )

    sc = study_config  # alias, to keep pylint quiet in the next line.
    converter = DefaultTrialConverter(
        [
            create_input_converter(p)
            for p in sc.search_space.root.select_all().merge()
        ],
        [create_output_converter(m) for m in sc.metric_information],
    )
    return cls(converter)

  @property
  def output_specs(self) -> Sequence[NumpyArraySpec]:
    return [
        converter.output_spec for converter in self._impl.parameter_converters
    ]

  @property
  def metric_specs(self) -> Sequence[pyvizier.MetricInformation]:
    return [mc.metric_information for mc in self._impl.metric_converters]

  @property
  def dtype(self) -> np.dtype:
    return self.to_features([]).dtype


--- vizier/pyvizier/converters/core_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for core."""

from absl import logging
import numpy as np
from vizier import pyvizier
from vizier._src.algorithms.designers import random
from vizier._src.algorithms.testing import test_runners
from vizier.pyvizier.converters import core
from vizier.testing import test_studies

from absl.testing import absltest
from absl.testing import parameterized


Trial = pyvizier.Trial


class TrialToArrayConverterConditionalSpaceTest(parameterized.TestCase):

  def test_automl_study(self):
    space = test_studies.conditional_automl_space()
    converter = core.TrialToArrayConverter.from_study_config(
        pyvizier.ProblemStatement(search_space=space)
    )
    features = converter.to_features([pyvizier.Trial()])
    np.testing.assert_equal(
        features,
        np.array(
            [[0.0, 0.0, 1.0, np.nan, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, np.nan]]
        ),
    )


class TrialToArrayConverterTest(parameterized.TestCase):
  """Test TrialToArrayConverter class."""

  def setUp(self):
    super().setUp()
    self._study_config = pyvizier.ProblemStatement(
        search_space=test_studies.flat_space_with_all_types(),
        metric_information=[
            pyvizier.MetricInformation(
                'x1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            )
        ],
    )

    self._designer = random.RandomDesigner(
        self._study_config.search_space, seed=0
    )
    self._trials = test_runners.run_with_random_metrics(
        self._designer, self._study_config, iters=1, batch_size=10
    )
    self.maxDiff = None

  def test_back_to_back_conversion(self):
    converter = core.TrialToArrayConverter.from_study_config(self._study_config)
    self.assertSequenceEqual(
        [t.parameters for t in self._trials],
        converter.to_parameters(converter.to_features(self._trials)),
    )

  def test_dtype(self):
    space = pyvizier.SearchSpace()
    root = space.root
    root.add_float_param('double', -2.0, 2.0)
    root.add_int_param('integer', -2, 2)
    converter = core.TrialToArrayConverter.from_study_config(
        pyvizier.ProblemStatement(search_space=space)
    )
    self.assertEqual(converter.dtype, np.float64)

  def test_parameter_continuify(self):
    space = pyvizier.SearchSpace()
    root = space.root
    root.add_float_param('double', -2.0, 2.0)
    root.add_int_param('integer', -2, 2)
    root.add_categorical_param('categorical', ['b', 'c'])
    root.add_discrete_param('discrete', [-1.0, 2.0, 3.0])

    converter = core.TrialToArrayConverter.from_study_config(
        pyvizier.ProblemStatement(search_space=space)
    )
    trial = pyvizier.Trial(
        parameters={
            'double': pyvizier.ParameterValue(3.0),
            'integer': pyvizier.ParameterValue(-1),
            'discrete': pyvizier.ParameterValue(2.0),
            'categorical': pyvizier.ParameterValue('d'),
        }
    )
    # Scaled outputs + 1 hot embedding for categoricals.
    expected = np.array([[1.25, 0.25, 0, 0, 1, 0.75]])
    np.testing.assert_equal(converter.to_features([trial]), expected)

  def test_onehot_embedding(self):
    space = pyvizier.SearchSpace()
    root = space.root
    root.add_categorical_param('c1', ['0', '1', '2'])
    root.add_categorical_param('c2', ['0', '1', '2', '3'])

    converter = core.TrialToArrayConverter.from_study_config(
        pyvizier.ProblemStatement(search_space=space)
    )
    n_trials = 10
    n_repeat_check = 50
    for _ in range(n_repeat_check):
      trials = []
      for _ in range(n_trials):
        trials.append(
            pyvizier.Trial(
                parameters={
                    'c1': pyvizier.ParameterValue(str(np.random.randint(0, 3))),
                    'c2': pyvizier.ParameterValue(str(np.random.randint(0, 4))),
                }
            )
        )
      features = converter.to_features(trials)
      np.testing.assert_equal(
          np.sum(features, axis=1), np.array([2] * n_trials)
      )

  def test_multi_metrics(self):
    search_space = pyvizier.SearchSpace()
    search_space.root.add_float_param('x', 0.0, 1.0)
    problem = pyvizier.ProblemStatement(
        search_space=search_space,
        metric_information=pyvizier.MetricsConfig(
            metrics=[
                pyvizier.MetricInformation(
                    'obj1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
                ),
                pyvizier.MetricInformation(
                    'obj2', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
                ),
                pyvizier.MetricInformation(
                    'obj3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
                ),
            ]
        ),
    )
    trial1 = pyvizier.Trial()
    trial2 = pyvizier.Trial()
    trial1.final_measurement = pyvizier.Measurement(
        metrics={'obj1': 1.0, 'obj2': 2.0, 'obj3': 3.0}
    )
    trial2.final_measurement = pyvizier.Measurement(
        metrics={'obj1': -1.0, 'obj2': 5.0, 'obj3': 0.0}
    )
    converter = core.TrialToArrayConverter.from_study_config(problem)
    # Notice that the sign is flipped for MINIMIZE objective.
    expected_labels = np.array([[1.0, 2.0, -3.0], [-1.0, 5.0, 0.0]])
    np.testing.assert_equal(
        converter.to_labels([trial1, trial2]), expected_labels
    )
    self.assertEqual(converter.metric_specs[0].name, 'obj1')
    self.assertEqual(converter.metric_specs[1].name, 'obj2')
    self.assertEqual(converter.metric_specs[2].name, 'obj3')


class DictToArrayTest(absltest.TestCase):

  def test_2d(self):
    self.assertSequenceEqual(
        core.dict_to_array(
            {1: np.random.random((10, 2)), 2: np.random.random((10, 1))}
        ).shape,
        (10, 3),
    )

  def test_4d(self):
    self.assertSequenceEqual(
        core.dict_to_array({
            1: np.random.random((2, 1, 10, 2)),
            2: np.random.random((2, 1, 10, 1)),
        }).shape,
        (2, 1, 10, 3),
    )

  def test_dict_like(self):
    d = core.DictOf2DArrays({
        'p1': np.array([[1], [2], [3]]),
        'p2': np.array([[0.0, 0.1], [0.5, 0.5], [0.1, 0.0]]),
    })
    self.assertCountEqual(
        d.dict_like(np.array([[1, 1, 0.1], [3, 0, 0.1]])),
        {'p1': np.array([[1], [3]]), 'p2': np.array([[1, 0.1], [0, 0.1]])},
    )

  def test_dict_like_and_as_array_are_inverse(self):
    d = core.DictOf2DArrays({
        'p1': np.array([[1], [2], [3]]),
        'p2': np.array([[0.0, 0.1], [0.5, 0.5], [0.1, 0.0]]),
    })
    self.assertCountEqual(d, d.dict_like(d.asarray()))


rnd = np.random.random


class DictOf2DArraysTest(absltest.TestCase):

  def test_bad_shapes(self):
    with self.assertRaises(ValueError):
      core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([5, 2])})

  def test_bad_dim(self):
    with self.assertRaises(ValueError):
      core.DictOf2DArrays({'a': rnd([10, 2, 1])})

  def test_size(self):
    d = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})
    self.assertEqual(d.size, 10)

  def test_addition(self):
    d1 = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})
    d2 = core.DictOf2DArrays({'a': rnd([5, 2]), 'b': rnd([5, 1])})
    d = d1 + d2
    self.assertEqual(d.size, 15)
    self.assertLen(d, 2)
    self.assertSequenceEqual(d['a'].shape, (15, 2))
    self.assertSequenceEqual(d['b'].shape, (15, 1))

  def test_bad_addition(self):
    # Shape of b doesn't match.
    d1 = core.DictOf2DArrays({'a': rnd([10, 2]), 'b': rnd([10, 1])})
    d2 = core.DictOf2DArrays({'a': rnd([5, 2]), 'b': rnd([5, 3])})
    with self.assertRaises(ValueError):
      _ = d1 + d2


class DefaultTrialConverterFromStudyConfigsTest(absltest.TestCase):

  @property
  def _study_configs(self):
    study_configs = []
    space1 = pyvizier.SearchSpace()
    root = space1.root
    root.add_float_param('double', -1.0, 1.0)
    root.add_int_param('integer', -1, 1)
    root.add_categorical_param('categorical', ['a', 'b'])
    root.add_discrete_param('discrete', [-1.0, 1.0])
    study_configs.append(
        pyvizier.ProblemStatement(
            metadata=pyvizier.Metadata({core.STUDY_ID_FIELD: 'study1'}),
            search_space=space1,
        )
    )

    space2 = pyvizier.SearchSpace()
    root = space2.root
    root.add_float_param('double', -1.0, 2.0)
    root.add_int_param('integer', -2, 1)
    root.add_categorical_param('categorical', ['b', 'c'])
    root.add_discrete_param('discrete', [-1.0, 1.0, 2.0])
    study_configs.append(
        pyvizier.ProblemStatement(
            metadata=pyvizier.Metadata({core.STUDY_ID_FIELD: 'study2'}),
            search_space=space2,
        )
    )
    return study_configs

  def test_parameters(self):
    converter = core.DefaultTrialConverter.from_study_configs(
        self._study_configs, [], use_study_id_feature=True
    )
    self.assertCountEqual(
        ['study1', 'study2'],
        converter.parameter_configs[core.STUDY_ID_FIELD].feasible_values,
    )
    self.assertEqual((-1.0, 2.0), converter.parameter_configs['double'].bounds)
    self.assertEqual((-2, 1), converter.parameter_configs['integer'].bounds)
    self.assertCountEqual(
        ('a', 'b', 'c'),
        converter.parameter_configs['categorical'].feasible_values,
    )
    self.assertCountEqual(
        (-1.0, 1.0, 2.0),
        converter.parameter_configs['discrete'].feasible_values,
    )

    expected = {
        'metalearn_study_id': (None, 1),
        'double': (None, 1),
        'integer': (None, 1),
        'categorical': (None, 1),
        'discrete': (None, 1),
    }
    self.assertDictEqual(converter.features_shape, expected)

    trial = pyvizier.Trial(
        parameters={
            'double': pyvizier.ParameterValue(3.0),
            'integer': pyvizier.ParameterValue(-1),
            'categorical': pyvizier.ParameterValue('d'),
        }
    )
    expected = {
        'categorical': np.array([[3]]),
        'discrete': np.array([[3]]),
        'double': np.array([[3.0]]),
        'integer': np.array([[1]]),
        'metalearn_study_id': np.array([[2]]),
    }
    self.assertDictEqual(converter.to_features([trial]), expected)

    # The value for `categorical` was invalid, so we can't recover it.
    expected_parameters = pyvizier.ParameterDict({
        'double': pyvizier.ParameterValue(2.0),
        'integer': pyvizier.ParameterValue(-1),
    })
    self.assertEqual(
        converter.to_trials(expected)[0].parameters, expected_parameters
    )

  def test_parameters_and_labels(self):
    study_config = pyvizier.ProblemStatement()
    root = study_config.search_space.root
    root.add_float_param('x1', 0.0, 10.0)
    root.add_float_param('x2', 0.0, 10.0)

    study_config.metric_information.extend([
        pyvizier.MetricInformation(
            name='y1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
        ),
        pyvizier.MetricInformation(
            name='y2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        pyvizier.MetricInformation(
            name='y3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
    ])
    converter = core.DefaultTrialConverter.from_study_config(study_config)
    actual_features = {
        'x1': np.array([[1.0, 3.0]]).T,
        'x2': np.array([[2.0, 4.0]]).T,
    }
    actual_labels = {
        'y1': np.array([[10.0, 40.0]]).T,
        'y2': np.array([[20.0, 50.0]]).T,
        'y3': np.array([[np.nan, 60.0]]).T,
    }
    actual_trials = [
        pyvizier.Trial(
            parameters={
                'x1': pyvizier.ParameterValue(1.0),
                'x2': pyvizier.ParameterValue(2.0),
            },
            final_measurement=pyvizier.Measurement(
                steps=1, metrics={'y1': 10.0, 'y2': 20.0}
            ),
            creation_time=None,
        ),
        pyvizier.Trial(
            parameters={
                'x1': pyvizier.ParameterValue(3.0),
                'x2': pyvizier.ParameterValue(4.0),
            },
            final_measurement=pyvizier.Measurement(
                steps=1, metrics={'y1': 40.0, 'y2': 50.0, 'y3': 60.0}
            ),
            creation_time=None,
        ),
    ]
    trials = converter.to_trials(actual_features, actual_labels)
    for t in trials:
      t.creation_time = None
    self.assertEqual(
        actual_trials,
        trials,
        msg='conversion from features and labels to trials failed.',
    )
    features, labels = converter.to_xy(actual_trials)
    np.testing.assert_equal(
        actual_features,
        features,
        err_msg='conversion from trials to features failed.',
    )
    np.testing.assert_equal(
        actual_labels,
        labels,
        err_msg='conversion from trials to lables failed.',
    )
    features_, labels_ = converter.to_xy(trials)
    np.testing.assert_equal(
        actual_features,
        features_,
        err_msg='roundtrip conversion for features failed.',
    )
    np.testing.assert_equal(
        actual_labels,
        labels_,
        err_msg='roundtrip conversion for labels failed.',
    )

  def test_infeasible_trials(self):
    labels = np.array([np.nan, np.nan, -100, -200, 1, 2, 3, 4])[:, np.newaxis]
    features = np.arange(labels.shape[0])[:, np.newaxis]
    study_config = pyvizier.ProblemStatement(
        metric_information=[
            pyvizier.MetricInformation(
                'y', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
            )
        ]
    )
    study_config.search_space.root.add_float_param('x', 0, 20)
    converter = core.DefaultTrialConverter.from_study_config(study_config)
    trials = converter.to_trials({'x': features}, {'y': labels})
    for t in trials:
      assert t.final_measurement is not None
      if not t.final_measurement.metrics:
        self.assertTrue(t.infeasible)
    for label, t in zip(labels.flatten(), trials):
      if np.isnan(label):
        self.assertEmpty(t.final_measurement.metrics)

  def test_parameters_and_labels_shape(self):
    study_config = pyvizier.ProblemStatement()
    root = study_config.search_space.root
    root.add_float_param('x1', 0.0, 10.0)

    study_config.metric_information.extend([
        pyvizier.MetricInformation(
            name='y1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
        ),
        pyvizier.MetricInformation(
            name='y2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
    ])
    converter = core.DefaultTrialConverter.from_study_config(study_config)
    features_1d = {
        'x1': np.array([1.0, 3.0]),
        'x2': np.array([2.0, 4.0]),
    }

    with self.assertRaisesRegex(
        ValueError,
        'Features need to contain 2d arrays with shape (num_obs, 1)*',
    ):
      converter.to_trials(features_1d)

    features_right_shape = {
        'x1': np.array([[1.0, 3.0]]).T,
        'x2': np.array([[2.0, 4.0]]).T,
    }
    labels_1d = {
        'y1': np.array([10.0, 40.0]),
        'y2': np.array([20.0, 50.0]),
    }
    with self.assertRaisesRegex(
        ValueError, 'Labels need to contain 2d arrays with shape (num_obs, 1)*'
    ):
      converter.to_trials(features_right_shape, labels_1d)

    features_2d_reverse_shape = {
        'x1': np.array([[1.0, 3.0]]),
        'x2': np.array([[2.0, 4.0]]),
    }
    with self.assertRaisesRegex(
        ValueError,
        'Features need to contain 2d arrays with shape (num_obs, 1)*',
    ):
      converter.to_trials(features_2d_reverse_shape)

    labels_2d_reverse_shape = {
        'y1': np.array([[10.0, 40.0]]),
        'y2': np.array([[20.0, 50.0]]),
    }
    with self.assertRaisesRegex(
        ValueError, 'Labels need to contain 2d arrays with shape (num_obs, 1)*'
    ):
      converter.to_trials(features_right_shape, labels_2d_reverse_shape)

  def test_metrics(self):
    converter = core.DefaultTrialConverter.from_study_configs(
        [],
        [
            pyvizier.MetricInformation(
                name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
            )
        ],
        use_study_id_feature=False,
    )

    expected = {'metric1': (None, 1)}
    self.assertDictEqual(converter.labels_shape, expected)


class DefaultModelOutputConverterTest(parameterized.TestCase):

  @property
  def _measurements(self):
    return [
        pyvizier.Measurement(
            metrics={
                'metric1': pyvizier.Metric(1.0),
                'metric2': pyvizier.Metric(1.1),
            }
        ),
        pyvizier.Measurement(
            metrics={
                'metric1': pyvizier.Metric(2.0),
                'metric2': pyvizier.Metric(2.1),
            }
        ),
        pyvizier.Measurement(
            metrics={
                'metric1': pyvizier.Metric(4.0),
                'metric2': pyvizier.Metric(4.1),
            }
        ),
    ]

  def test_empty(self):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=True,
    )
    np.testing.assert_array_equal(
        converter.convert([]), np.zeros([0, 1], dtype=converter.dtype)
    )

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_sign_flips(self, dtype):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=True,
        dtype=dtype,
    )
    actual = converter.convert(self._measurements)

    expected = -np.asarray([[1.0], [2.0], [4.0]], dtype)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)
    self.assertEqual(
        converter.metric_information,
        pyvizier.MetricInformation(
            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE
        ),
    )

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_no_sign_flips(self, dtype):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=False,
        dtype=dtype,
    )
    actual = converter.convert(self._measurements)

    expected = np.asarray([[1.1], [2.1], [4.1]], dtype)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)
    self.assertEqual(
        converter.metric_information,
        pyvizier.MetricInformation(
            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
    )

  def test_shift_threshold(self):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric2',
            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=5.0,
        ),
        flip_sign_for_minimization_metrics=False,
        dtype=float,
    )
    converter.shift_safe_metrics = False
    self.assertEqual(5.0, converter.metric_information.safety_threshold)
    converter.shift_safe_metrics = True
    self.assertEqual(0.0, converter.metric_information.safety_threshold)

  def test_raise_errors_for_missing_metrics(self):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=False,
        raise_errors_for_missing_metrics=True,
    )
    with self.assertRaises(KeyError):
      converter.convert(self._measurements)

  def test_do_not_raise_errors_for_missing_metrics(self):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric3', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=False,
        raise_errors_for_missing_metrics=False,
    )
    np.testing.assert_equal(
        converter.convert(self._measurements), np.asarray([[np.nan]] * 3)
    )

  @parameterized.parameters([dict(flip_sign=True), dict(flip_sign=False)])
  def test_safe_maximize_metric(self, flip_sign):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE,
            safety_threshold=3.0,
        ),
        flip_sign_for_minimization_metrics=flip_sign,
    )
    actual = converter.convert(self._measurements)
    np.testing.assert_equal(actual, [[-2.0], [-1.0], [1.0]])

  @parameterized.parameters([dict(flip_sign=True), dict(flip_sign=False)])
  def test_safe_minimize_metric(self, flip_sign):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=3.0,
        ),
        flip_sign_for_minimization_metrics=flip_sign,
    )
    actual = converter.convert(self._measurements)
    expected = np.array([[-2.0], [-1.0], [1.0]], dtype=np.float32) * (
        -1 if flip_sign else 1
    )
    np.testing.assert_equal(actual, expected)

  @parameterized.parameters([
      dict(flip_sign=True, raise_error=True),
      dict(flip_sign=False, raise_error=True),
      dict(flip_sign=True, raise_error=False),
      dict(flip_sign=False, raise_error=False),
  ])
  def test_to_metrics(self, flip_sign: bool, raise_error: bool):
    expected = [
        pyvizier.Metric(1.0),
        pyvizier.Metric(2.0),
        None,
        pyvizier.Metric(4.0),
        None,
    ]
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=flip_sign,
        raise_errors_for_missing_metrics=raise_error,
        dtype=float,
    )
    arr = np.array([[1.0], [2.0], [np.nan], [4.0], [np.nan]]) * (
        -1 if flip_sign else 1
    )
    actual = converter.to_metrics(arr)
    self.assertEqual(actual, expected)

  @parameterized.parameters([
      dict(flip_sign=True, raise_error=True, safety_threshold=5.0),
      dict(flip_sign=False, raise_error=True, safety_threshold=5.0),
      dict(flip_sign=True, raise_error=False, safety_threshold=5.0),
      dict(flip_sign=False, raise_error=False, safety_threshold=5.0),
      dict(flip_sign=True, raise_error=True, safety_threshold=-5.0),
      dict(flip_sign=False, raise_error=True, safety_threshold=-5.0),
      dict(flip_sign=True, raise_error=False, safety_threshold=-5.0),
      dict(flip_sign=False, raise_error=False, safety_threshold=-5.0),
  ])
  def test_to_safe_minimize_metrics_parametrized(
      self, flip_sign: bool, raise_error: bool, safety_threshold: float
  ):
    expected = [
        pyvizier.Metric((1.0 - safety_threshold) * (-1 if flip_sign else 1)),
        pyvizier.Metric((2.0 - safety_threshold) * (-1 if flip_sign else 1)),
        None,
        pyvizier.Metric((4.0 - safety_threshold) * (-1 if flip_sign else 1)),
        None,
    ]
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=safety_threshold,
        ),
        flip_sign_for_minimization_metrics=flip_sign,
        raise_errors_for_missing_metrics=raise_error,
        dtype=float,
    )
    arr = np.array([[1.0], [2.0], [np.nan], [4.0], [np.nan]])
    actual = converter.to_metrics(arr)
    self.assertEqual(actual, expected)

  def test_to_safe_maximize_metrics(self):
    expected = [
        pyvizier.Metric(-4.0),
        pyvizier.Metric(-3.0),
        None,
        pyvizier.Metric(-1.0),
        None,
    ]
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE,
            safety_threshold=5.0,
        ),
        dtype=float,
    )
    arr = np.array([[1.0], [2.0], [np.nan], [4.0], [np.nan]])
    actual = converter.to_metrics(arr)
    self.assertEqual(actual, expected)

  def test_to_safe_minimize_metrics(self):
    expected = [
        pyvizier.Metric(4.0),
        pyvizier.Metric(3.0),
        None,
        pyvizier.Metric(1.0),
        None,
    ]
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=5.0,
        ),
        flip_sign_for_minimization_metrics=True,
        dtype=float,
    )
    arr = np.array([[1.0], [2.0], [np.nan], [4.0], [np.nan]])
    actual = converter.to_metrics(arr)
    self.assertEqual(actual, expected)

  @parameterized.parameters([
      dict(flip_sign=True, safety_threshold=5.0),
      dict(flip_sign=True, safety_threshold=-5.0),
  ])
  def test_first_shift_then_flip(
      self, flip_sign: bool, safety_threshold: float
  ):
    not_expected = [
        pyvizier.Metric((1.0 * (-1 if flip_sign else 1)) - safety_threshold),
        pyvizier.Metric((2.0 * (-1 if flip_sign else 1)) - safety_threshold),
        None,
        pyvizier.Metric((4.0 * (-1 if flip_sign else 1)) - safety_threshold),
        None,
    ]
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric1',
            goal=pyvizier.ObjectiveMetricGoal.MINIMIZE,
            safety_threshold=safety_threshold,
        ),
        flip_sign_for_minimization_metrics=flip_sign,
        dtype=float,
    )
    arr = np.array([[1.0], [2.0], [np.nan], [4.0], [np.nan]])
    actual = converter.to_metrics(arr)
    self.assertNotEqual(actual, not_expected)

  @parameterized.parameters([
      dict(flip_sign=True, raise_error=True),
      dict(flip_sign=False, raise_error=True),
      dict(flip_sign=True, raise_error=False),
      dict(flip_sign=False, raise_error=False),
  ])
  def test_to_metrics_from_measurements(
      self, flip_sign: bool, raise_error: bool
  ):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=flip_sign,
        raise_errors_for_missing_metrics=raise_error,
        dtype=float,
    )
    measurements = self._measurements
    ys = converter.convert(measurements)
    ms = converter.to_metrics(ys)
    from_ms = np.array(
        [measurements[i].metrics['metric2'].value for i in range(len(ms))]
    )[:, None] * (-1 if flip_sign else 1)
    self.assertTrue((from_ms == ys).all())

  @parameterized.parameters(
      [dict(labels=np.array([[1, 2]])), dict(labels=np.array([[[1, 2]]]))]
  )
  def test_bad_labels(self, labels: np.ndarray):
    converter = core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='metric2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE
        ),
        flip_sign_for_minimization_metrics=True,
        raise_errors_for_missing_metrics=True,
        dtype=float,
    )
    with self.assertRaises(ValueError):
      converter.to_metrics(labels)


class DefaultModelInputConverterTest(parameterized.TestCase):

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', bounds=(-3.0, 3.0)),
        scale=False,
        onehot_embed=True,
        float_dtype=dtype,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray([[1.0], [2.0], [-3.0], [np.nan], [np.nan]], dtype)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double_log(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(1e-4, 1e2), scale_type=pyvizier.ScaleType.LOG
        ),
        scale=True,
        float_dtype=dtype,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1e-4)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(1e2)}),
    ])
    expected = np.asarray([[0.0], [1.0]], dtype)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double_log_inverse(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(1e-4, 1e2), scale_type=pyvizier.ScaleType.LOG
        ),
        scale=True,
        float_dtype=dtype,
    )

    scaled = np.asarray([[0.0], [0.5], [1.1]], dtype)
    # Pytype still thinks `actual` entries might be None, hence we specify type.
    actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(  # pytype:disable=annotation-type-mismatch
        scaled
    )
    self.assertAlmostEqual(actual[0].value, 1e-4, delta=1e-6)
    self.assertGreaterEqual(actual[0].value, 1e-4)
    self.assertAlmostEqual(actual[1].value, 0.1, delta=1e-5)
    self.assertAlmostEqual(actual[2].value, 100.0, delta=1e-2)
    self.assertLessEqual(actual[2].value, 100.0)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double_log_inverse_noclipping(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(1e-4, 1e2), scale_type=pyvizier.ScaleType.LOG
        ),
        scale=True,
        float_dtype=dtype,
        should_clip=False,
    )

    scaled = np.asarray([[0.0], [0.5], [1.1]], dtype)
    # Pytype still thinks `actual` entries might be None, hence we specify type.
    actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(  # pytype:disable=annotation-type-mismatch
        scaled
    )
    self.assertAlmostEqual(actual[0].value, 1e-4, delta=1e-6)
    self.assertAlmostEqual(actual[1].value, 0.1, delta=1e-5)
    self.assertAlmostEqual(actual[2].value, 398, delta=1)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double_reverse_log(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(1e-4, 1e2), scale_type=pyvizier.ScaleType.REVERSE_LOG
        ),
        scale=True,
        float_dtype=dtype,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1e-4)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(1e2)}),
    ])
    expected = np.asarray([[0.0], [7.273945e-4], [1.0]], dtype)
    np.testing.assert_allclose(expected, actual, rtol=1e-3)
    self.assertEqual(expected.dtype, actual.dtype)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_double_reverse_log_inverse(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(1e-4, 1e2), scale_type=pyvizier.ScaleType.REVERSE_LOG
        ),
        scale=True,
        float_dtype=dtype,
    )

    scaled = np.asarray([[0.0], [0.5], [1.0]], dtype)
    # Pytype still thinks `actual` entries might be None, hence we specify type.
    actual: list[pyvizier.ParameterValue] = converter.to_parameter_values(  # pytype:disable=annotation-type-mismatch
        scaled
    )
    self.assertGreaterEqual(actual[0].value, 1e-4)
    self.assertGreaterEqual(actual[1].value, 0.5)
    self.assertLessEqual(actual[2].value, 1e2)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_zero_range_linear_double(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(0.9, 0.9), scale_type=pyvizier.ScaleType.LINEAR
        ),
        scale=True,
        onehot_embed=True,
        float_dtype=dtype,
    )
    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(0.9)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray([[0.5], [0.6], [np.nan], [np.nan]], dtype)
    np.testing.assert_equal(expected, actual)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_zero_range_log_double(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1',
            bounds=(np.exp(0.9), np.exp(0.9)),
            scale_type=pyvizier.ScaleType.LOG,
        ),
        scale=True,
        onehot_embed=True,
        float_dtype=dtype,
    )
    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(0.9))}),
        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(1.2))}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[0.5], [0.5 + np.exp(1.2) - np.exp(0.9)], [np.nan], [np.nan]],
        dtype=dtype,
    )
    np.testing.assert_almost_equal(expected, actual)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_zero_range_reverse_log_double(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1',
            bounds=(np.exp(0.9), np.exp(0.9)),
            scale_type=pyvizier.ScaleType.REVERSE_LOG,
        ),
        scale=True,
        onehot_embed=True,
        float_dtype=dtype,
    )
    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(0.9))}),
        Trial(parameters={'x1': pyvizier.ParameterValue(np.exp(1.2))}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[0.5], [0.5 + np.exp(1.2) - np.exp(0.9)], [np.nan], [np.nan]],
        dtype=dtype,
    )
    np.testing.assert_almost_equal(expected, actual)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_double_into_scaled_double(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(
            'x1', bounds=(-3.0, 3.0), scale_type=pyvizier.ScaleType.LINEAR
        ),
        scale=True,
        onehot_embed=True,
        float_dtype=dtype,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[4 / 6], [5 / 6], [0 / 6], [np.nan], [np.nan]], dtype=dtype
    )
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)

  def test_integer_discretes_into_discretes(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),
        max_discrete_indices=10,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray([[0], [1], [3], [3], [3]], np.int32)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_integer_discretes_into_doubles(self, dtype):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),
        max_discrete_indices=1,
        float_dtype=dtype,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[1.0], [2.0], [-3.0], [np.nan], [np.nan]], dtype=dtype
    )
    np.testing.assert_allclose(expected, actual)

    pvs = converter.to_parameter_values(actual)
    self.assertSequenceEqual(
        pvs,
        [
            pyvizier.ParameterValue(1.0),
            pyvizier.ParameterValue(2.0),
            # TODO: This rounding behavior can be harmful.
            pyvizier.ParameterValue(1.0),
            None,
            None,
        ],
    )

    self.assertEqual(expected.dtype, actual.dtype)

  def test_integer_discretes_into_onehot(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),
        scale=True,
        onehot_embed=True,
        max_discrete_indices=10,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [
            [1.0, 0.0, 0.0, 0.0],
            [0.0, 1.0, 0.0, 0.0],
            [0.0, 0.0, 0.0, 1.0],
            [0.0, 0.0, 0.0, 1.0],
            [0.0, 0.0, 0.0, 1.0],
        ],
        dtype=np.float32,
    )
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)
    self.assertEqual(
        converter.output_spec,
        core.NumpyArraySpec(
            core.NumpyArraySpecType.ONEHOT_EMBEDDING,
            np.dtype(np.float32),
            (0, 1),
            4,
            'x1',
            1,
        ),
        msg=repr(converter.output_spec),
    )

  def test_integer_discretes_into_onehot_empty(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1, 2, 3)),
        scale=True,
        onehot_embed=True,
        max_discrete_indices=10,
    )

    actual = converter.convert([])
    expected = np.zeros([0, 4], dtype=np.float32)
    np.testing.assert_allclose(expected, actual)
    self.assertEqual(expected.dtype, actual.dtype)

  @parameterized.parameters([
      dict(max_discrete_indices=10),
      dict(max_discrete_indices=np.inf),
  ])
  def test_discretes_into_discretes(self, max_discrete_indices):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),
        max_discrete_indices=max_discrete_indices,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray([[0], [1], [3], [3], [3]], dtype=np.float32)
    np.testing.assert_equal(expected, actual)

  def test_discretes_into_doubles(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),
        max_discrete_indices=1,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[1.0], [2.0], [-3.0], [np.nan], [np.nan]], dtype=np.float32
    )
    np.testing.assert_equal(expected, actual)

  def test_discretes_into_scaled_doubles(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=(1.0, 2.0, 3.0)),
        max_discrete_indices=1,
        scale=True,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue(1.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(2.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue(-3.0)}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray(
        [[0.0], [0.5], [-2.0], [np.nan], [np.nan]], dtype=np.float32
    )
    np.testing.assert_equal(expected, actual)

  def test_categorical(self):
    converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', feasible_values=('1', '2', '3')),
        max_discrete_indices=1,
    )

    actual = converter.convert([
        Trial(parameters={'x1': pyvizier.ParameterValue('1')}),
        Trial(parameters={'x1': pyvizier.ParameterValue('2')}),
        Trial(parameters={'x1': pyvizier.ParameterValue('-3')}),
        Trial(parameters={'x1': pyvizier.ParameterValue('a')}),
        Trial(),
    ])
    expected = np.asarray([[0], [1], [3], [3], [3]], dtype=np.float32)
    np.testing.assert_equal(expected, actual)

  @parameterized.parameters([
      dict(dtype=np.float32),
      dict(dtype=np.float64),
      dict(dtype='float32'),
      dict(dtype='float64'),
  ])
  def test_bounds(self, dtype):
    input_converter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory('x1', bounds=(-3.0, 3.0)),
        scale=True,
        onehot_embed=False,
        float_dtype=dtype,
    )
    self.assertEqual(input_converter.output_spec.bounds[0], 0.0)
    self.assertEqual(input_converter.output_spec.bounds[1], 1.0)


class ModelInputArrayBijectorTest(absltest.TestCase):

  def testIdentityBijector(self):
    parameter_config = pyvizier.ParameterConfig.factory('p', bounds=(0.0, 1.0))
    spec = core.NumpyArraySpec(
        name='p',
        type=core.NumpyArraySpecType.default_factory(parameter_config),
        dtype=np.dtype(np.float32),
        bounds=(0.0, 1.0),
        num_dimensions=1,
        num_oovs=0,
    )
    bijector = core.ModelInputArrayBijector.identity(spec)
    for x in [0.0, 0.25, 0.5, 0.625, 1.0]:
      x = np.array(x)
      self.assertLessEqual(
          bijector.forward_fn(x), bijector.output_spec.bounds[1]
      )
      self.assertGreaterEqual(
          bijector.forward_fn(x), bijector.output_spec.bounds[0]
      )
      self.assertEqual(bijector.backward_fn(bijector.forward_fn(x)), x)

  def testScalarFromSpec(self):
    parameter_config = pyvizier.ParameterConfig.factory('p', bounds=(1.0, 3.0))
    spec = core.NumpyArraySpec(
        name='p',
        type=core.NumpyArraySpecType.default_factory(parameter_config),
        dtype=np.dtype(np.float32),
        bounds=(1.0, 3.0),
        num_dimensions=1,
        num_oovs=0,
    )
    bijector = core.ModelInputArrayBijector.scaler_from_spec(spec)
    for x in [1.0, 1.25, 1.5, 2.625, 3.0]:
      x = np.array(x)
      self.assertLessEqual(
          bijector.forward_fn(x), bijector.output_spec.bounds[1]
      )
      self.assertGreaterEqual(
          bijector.forward_fn(x), bijector.output_spec.bounds[0]
      )
      self.assertEqual(bijector.backward_fn(bijector.forward_fn(x)), x)

  def testScalarFromSinglePointSpec(self):
    parameter_config = pyvizier.ParameterConfig.factory('p', bounds=(1.0, 1.0))
    spec = core.NumpyArraySpec(
        name='p',
        type=core.NumpyArraySpecType.default_factory(parameter_config),
        dtype=np.dtype(np.float32),
        bounds=(1.0, 1.0),
        num_dimensions=1,
        num_oovs=0,
    )
    bijector = core.ModelInputArrayBijector.scaler_from_spec(spec)
    x = np.array(1.0)
    logging.info(
        'fwd(x)=%s output_spec.bounds=%s',
        bijector.forward_fn(x),
        bijector.output_spec.bounds,
    )
    self.assertLessEqual(bijector.forward_fn(x), bijector.output_spec.bounds[1])
    self.assertGreaterEqual(
        bijector.forward_fn(x), bijector.output_spec.bounds[0]
    )
    self.assertLessEqual(bijector.forward_fn(x), bijector.output_spec.bounds[1])
    self.assertEqual(bijector.backward_fn(bijector.forward_fn(x)), x)


if __name__ == '__main__':
  absltest.main()


--- vizier/pyvizier/converters/embedder.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Vizier problem statement and trials scaler.

The ProblemAndTrialsScaler class has two main functionalities:
1. Scale (aka "embed") the ProblemStatment to a new ProblemStatment with scaled
search space.
2. Map and unmap Vizier TrialSuggestion from and to the scaled search space.

The parameters mapping is as follow:
- FLOAT parameters are mapped to FLOAT with [0,1] bounds.
- INTEGER parameters are mapped to FLOAT with [0,1] bounds.
- DISCRETE parameters are mapped to DISCRETE with converted feasible values.
- CATEGORICAL parameters don't change.

The scaling is performed using the DefaultTrialConverter.
"""

import copy
from typing import Sequence, TypeVar
import attr
import numpy as np
from vizier import pyvizier as vz
from vizier.pyvizier.converters import core


_T = TypeVar('_T', vz.Trial, vz.TrialSuggestion)


class ProblemAndTrialsScaler:
  """Vizier problem statement and trials scaler."""

  def __init__(self, problem: vz.ProblemStatement):
    """Initializes the class by creating a converter and scaled problem statement.

    Arguments:
      problem: The ProblemStatemet to be scaled.
    """

    def create_param_converter(pc):
      return core.DefaultModelInputConverter(
          pc, max_discrete_indices=0, scale=True
      )

    self._converter = core.DefaultTrialConverter(
        parameter_converters=[
            create_param_converter(pc) for pc in problem.search_space.parameters
        ]
    )
    # Create the new embedded search space.
    emb_search_space = vz.SearchSpace()
    for param in problem.search_space.parameters:
      if param.type in [vz.ParameterType.DOUBLE, vz.ParameterType.INTEGER]:
        # DOUBLE/INTEGER params are scaled to [0.0, 1.0] and converted to FLOAT.
        emb_search_space.root.add_float_param(
            param.name,
            self._converter.output_specs[param.name].bounds[0],
            self._converter.output_specs[param.name].bounds[1],
        )
      elif param.type == vz.ParameterType.DISCRETE:
        # DISCRETE params feasible values are scaled.
        emb_search_space.root.add_discrete_param(
            param.name,
            feasible_values=self._scale_discrete_feasible_values(
                problem, param
            ),
        )
      elif param.type == vz.ParameterType.CATEGORICAL:
        # CATEGORICAL parameters are left unchanged.
        emb_search_space.root.add_categorical_param(
            param.name, feasible_values=param.feasible_values
        )
      else:
        raise ValueError('Unsupported parameter type (%s)' % param.type)
    # Clone the entire problem statement and only update the search space.
    self._embedded_problem_statement = copy.deepcopy(problem)
    self._embedded_problem_statement.search_space = emb_search_space

  @property
  def problem_statement(self) -> vz.ProblemStatement:
    """Returns the scaled problem statement."""
    return self._embedded_problem_statement

  def _scale_discrete_feasible_values(
      self, problem: vz.ProblemStatement, param: vz.ParameterConfig
  ):
    """Scales the DISCRETE parameter feasible values."""
    if param.type != vz.ParameterType.DISCRETE:
      raise ValueError('Expects DISCRETE parameter but got %s.' % param.type)
    # Obtain the converter for the DISCRETE parameter.
    param_converter = self._converter.parameter_converters_dict[param.name]
    # Convert each original feasible values to form the new feasible values.
    new_feasible_values = []
    for feasible_value in problem.search_space.get(param.name).feasible_values:
      tmp_trial = vz.Trial({param.name: feasible_value})
      new_feasible_value = param_converter.convert([tmp_trial]).item(0, 0)
      new_feasible_values.append(new_feasible_value)
    return new_feasible_values

  def map(self, trials: Sequence[_T]) -> list[_T]:
    """Map trials from the original search space to the embedded space."""
    embedded_trials = []
    for trial in trials:
      parameters = vz.ParameterDict()
      for name, feature in self._converter.to_features([trial]).items():
        if (
            self.problem_statement.search_space.get(name).type
            == vz.ParameterType.CATEGORICAL
        ):
          # CATEGORICAL parameters values are unchanged.
          parameters[name] = trial.parameters[name].value
        else:
          # Assign the converted value.
          parameters[name] = feature.item(0, 0)
      # Create a copy of the trial with updated parameters.
      embedded_trials.append(attr.evolve(trial, parameters=parameters))

    return embedded_trials

  def unmap(self, trials: Sequence[_T]) -> list[_T]:
    """Unmap trials from the embedded search space to the original space."""
    unmapped_trials = []
    for trial in trials:
      parameters = vz.ParameterDict()
      for name, parameter_value in trial.parameters.items():
        value = parameter_value.value
        if (
            self.problem_statement.search_space.get(name).type
            == vz.ParameterType.CATEGORICAL
        ):
          # CATEGORICAL parameter values are left unchanged.
          parameters[name] = value
        else:
          # Get the parameter converter.
          param_converter = self._converter.parameter_converters_dict[name]
          # Convert back the feature to parameters in the original space.
          parameters[name] = param_converter.to_parameter_values(
              np.array(value)
          )[0]
      # Create a copy of the trial with updated parameters.
      unmapped_trials.append(attr.evolve(trial, parameters=parameters))
    return unmapped_trials


--- vizier/pyvizier/converters/embedder_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for problem statement converter."""

import copy

import numpy as np
from vizier import pyvizier as vz
from vizier.pyvizier.converters import embedder

from absl.testing import absltest


class ProblemStatementTest(absltest.TestCase):

  def test_convert_search_space(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_int_param('i1', 0, 10)
    problem.search_space.root.add_discrete_param('d1', [0, 10, 20])
    problem.search_space.root.add_categorical_param('c1', ['a', 'b', 'c'])
    converter = embedder.ProblemAndTrialsScaler(problem)

    new_space = converter.problem_statement.search_space
    self.assertEqual(new_space.get('f1').type, vz.ParameterType.DOUBLE)
    self.assertEqual(new_space.get('f1').bounds, (0.0, 1.0))

    self.assertEqual(new_space.get('i1').type, vz.ParameterType.DOUBLE)
    self.assertEqual(new_space.get('i1').bounds, (0.0, 1.0))

    self.assertEqual(new_space.get('d1').type, vz.ParameterType.DISCRETE)
    self.assertEqual(new_space.get('d1').feasible_values, [0.0, 0.5, 1.0])

    self.assertEqual(new_space.get('c1').type, vz.ParameterType.CATEGORICAL)
    self.assertEqual(new_space.get('c1').feasible_values, ['a', 'b', 'c'])

  def test_map_trials(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_int_param('i1', 0, 10)
    problem.search_space.root.add_discrete_param('d1', [0, 10, 20])
    problem.search_space.root.add_categorical_param('c1', ['a', 'b', 'c'])
    converter = embedder.ProblemAndTrialsScaler(problem)

    trial1 = vz.Trial(parameters={'f1': 1.0, 'i1': 0, 'd1': 10, 'c1': 'b'})
    trial2 = vz.Trial(parameters={'f1': 4.0, 'i1': 8, 'd1': 0, 'c1': 'a'})
    trial1.final_measurement = vz.Measurement(metrics={'o': 1.0})
    trial2.final_measurement = vz.Measurement(metrics={'o': 2.0})
    suggestion_trials = [trial1, trial2]
    trial1_copy = copy.deepcopy(trial1)
    trial2_copy = copy.deepcopy(trial2)

    mapped_trials = converter.map(suggestion_trials)
    self.assertAlmostEqual(mapped_trials[0].parameters['f1'].value, 0.1)
    self.assertAlmostEqual(mapped_trials[0].parameters['i1'].value, 0.0)
    self.assertAlmostEqual(mapped_trials[0].parameters['d1'].value, 0.5)
    self.assertEqual(mapped_trials[0].parameters['c1'].value, 'b')
    # Check that the trial measurement remains the same.
    self.assertEqual(mapped_trials[0].final_measurement.metrics['o'].value, 1.0)
    self.assertAlmostEqual(mapped_trials[1].parameters['f1'].value, 0.4)
    self.assertAlmostEqual(mapped_trials[1].parameters['i1'].value, 0.8)
    self.assertAlmostEqual(mapped_trials[1].parameters['d1'].value, 0.0)
    self.assertEqual(mapped_trials[1].parameters['c1'].value, 'a')
    # Check that the trial measurement remains the same.
    self.assertEqual(mapped_trials[1].final_measurement.metrics['o'].value, 2.0)
    # Check that the original trials weren't change during map.
    self.assertEqual(trial1, trial1_copy)
    self.assertEqual(trial2, trial2_copy)

  def test_unmap_trials(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 10.0)
    problem.search_space.root.add_int_param('i1', 0, 10)
    problem.search_space.root.add_discrete_param('d1', [0, 10, 20])
    problem.search_space.root.add_categorical_param('c1', ['a', 'b', 'c'])
    converter = embedder.ProblemAndTrialsScaler(problem)

    trial1 = vz.Trial(parameters={'f1': 1.0, 'i1': 0, 'd1': 10, 'c1': 'b'})
    trial2 = vz.Trial(parameters={'f1': 4.0, 'i1': 8, 'd1': 0, 'c1': 'c'})
    trial3 = vz.Trial(parameters={'f1': 0.0, 'i1': 8, 'd1': 20, 'c1': 'b'})
    trial4 = vz.Trial(parameters={'f1': 10.0, 'i1': 2, 'd1': 10, 'c1': 'a'})
    suggestion_trials = [trial1, trial2, trial3, trial4]
    suggestion_trials_copy = copy.deepcopy(suggestion_trials)
    recovered_trials = converter.unmap(converter.map(suggestion_trials))

    for i, trial in enumerate(suggestion_trials):
      for param_config in problem.search_space.parameters:
        name = param_config.name
        if param_config.type == vz.ParameterType.CATEGORICAL:
          self.assertEqual(
              trial.parameters[name].value,
              recovered_trials[i].parameters[name].value,
          )
        else:
          self.assertAlmostEqual(
              trial.parameters[name].value,
              recovered_trials[i].parameters[name].value,
              places=5,
          )
    # Check that the original trials weren't change during unmap.
    for i in range(len(suggestion_trials)):
      self.assertEqual(suggestion_trials_copy[i], suggestion_trials[i])

  def test_non_linear_scale(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param(
        'f1', 1.0, 100.0, scale_type=vz.ScaleType.LOG
    )
    trial = vz.Trial(parameters={'f1': 10.0})
    converter = embedder.ProblemAndTrialsScaler(problem)
    new_space = converter.problem_statement.search_space
    self.assertEqual(new_space.get('f1').type, vz.ParameterType.DOUBLE)
    self.assertEqual(new_space.get('f1').bounds, (0.0, 1.0))
    mapped_trial = converter.map([trial])[0]
    self.assertAlmostEqual(
        mapped_trial.parameters['f1'].value, np.log(10.0) / np.log(100.0)
    )
    unmapped_trial = converter.unmap([mapped_trial])[0]
    self.assertAlmostEqual(
        unmapped_trial.parameters['f1'].value, 10.0, places=5
    )


if __name__ == '__main__':
  absltest.main()


--- vizier/pyvizier/converters/feature_mapper.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Mappers between different feature formats."""

import jax
from jax import numpy as jnp
import numpy as np
from vizier._src.jax import types
from vizier.pyvizier.converters import core


class ContinuousCategoricalFeatureMapper:
  """Maps features from TrialToArrayConverter to ContinuousAndCategoricalValues.

  The class allows to separate the continuous features from the categorical
  features and organize them in an ContinuousAndCategoricalValues namedtuple.

  Naming convension:
    parameter - Vizier parameter.
    feature - Numpy array representing a converted Vizier parameter.

  Notation for dimensions:
    C - continuous parameters count
    P - categorical parameters count
    F - total categorial dimensions (i.e. the total feasible values count, which
      associated with the onehot-encoding features total count)
    B - number of trials
  """

  def __init__(self, converter: core.TrialToArrayConverter):
    """Initiate helper objects to split features by type."""
    # Number of categorical parameters.
    self.n_categorical_params = 0
    # The continuous feature indices of output_spec, which also correspond to
    # the columns indices of continuous parameters in features.
    self._continuous_indices = []
    # The categorical feature indices.
    self._categorical_indices = []
    # The dimensions of categorical parameters.
    categorical_dims = []
    ind = 0
    # Extract the continuous and categorical feature indices.
    for spec in converter.output_specs:
      if spec.type == core.NumpyArraySpecType.CONTINUOUS:
        self._continuous_indices.append(ind)
        ind += 1
      elif spec.type == core.NumpyArraySpecType.ONEHOT_EMBEDDING:
        self._categorical_indices.extend(range(ind, ind + spec.num_dimensions))
        ind += spec.num_dimensions
        self.n_categorical_params += 1
        categorical_dims.append(spec.num_dimensions)
      elif spec.type == core.NumpyArraySpecType.DISCRETE:
        # There shouldn't be DISCRETE spec type as onehot_embed=True is the
        # default in TrialToArrayConverter, and DISCRETE parameters are either
        # converted to CONTINUOUS or ONEHOT_EMBEDDING.
        raise ValueError("DISCRETE spec type shouldn't exists.")
      else:
        raise ValueError("Unexpected spec type %s!" % spec.type)
    # Create shift array containing the cumulative number of categorical
    # dimensions until each categorical parameter (itself not included). One-hot
    # active bit (index) is mapped to integer value, by subtracting 'shift'.
    # For example, if categorical_dims=[3, 2], then cumsum(pad(..)) will
    # generate [0, 3, 5], so 'shift' will be [0, 3].
    self.shift = jnp.cumsum(jnp.pad(jnp.array(categorical_dims), (1, 0)))[
        0:-1
    ]  # (P,)
    self.categorical_dims = categorical_dims
    self.converter = converter

  def map(self, features: types.Array) -> types.ContinuousAndCategoricalArray:
    """Split features by 'continuous' and 'categorical'.

    In addition to splitting the method converts the one-hot encoding
    categorical features to integer indices.

    For example, if the search space is:
      - categorical parameter C1 with values ['a', 'b', 'c].
      - continuous parameter F1.
      - categorical parameter C2 with values ['x', 'y'].

    Then for input parameters C1='b', F1=0.23, C2='y' and associated features:
    [[0, 1, 0, 0.23, 0, 1]] the result would be:
    ContinuousAndCategoricalValues(
        continuous=[[0.23]]
        categorical=[[1, 1]])

    Arguments:
      features: Numpy array (n_trials, n_features) which is the output of
        'to_features' method.

    Returns:
      namedtuple with continuous features and categorical integer indices.
    """
    # Split features to continuous and categorical.
    batch_shape = features.shape[:-1]
    continuous_features = features[..., self._continuous_indices]  # (B,C)
    # Assign empty array as the default value.
    categorical_index_features = jnp.zeros(
        batch_shape + (0,), dtype=types.INT_DTYPE
    )
    if self.n_categorical_params > 0:
      categorical_features = features[..., self._categorical_indices]  # (B,F)
      # Find the non-zero column indices associated with categorical parameters.
      # For example (cont. from above), with the input of [0,1,0, 0.23, 0,1]
      # 'categorical_features' is [[0,1,0,0,1]] and 'nonzero_indices' is [1,4]
      nonzero_size = np.prod(batch_shape) * self.n_categorical_params
      # nonzero_size: (P*B,)
      nonzero_indices = jnp.nonzero(categorical_features, size=nonzero_size)[1]
      # Reshape the non-zero indices to align with the no. of categorical params
      # and shift by the cardinality of each parameter to compute the integer
      # category value. For example (cont. from above), [[1,4]] - [0,3] = [1, 1]
      categorical_index_features = (
          jnp.reshape(
              nonzero_indices, batch_shape + (self.n_categorical_params,)
          )  # (B,P)
          - self.shift  # (P,)
      ).astype(
          types.INT_DTYPE
      )  # (B,P)
    return types.ContinuousAndCategoricalArray(
        continuous=continuous_features, categorical=categorical_index_features
    )

  def unmap(self, features: types.ContinuousAndCategoricalArray) -> jax.Array:
    """Convert back from ContinuousAndCategoricalArray to features."""
    if features.continuous.shape[0] != features.categorical.shape[0]:
      raise ValueError(
          "'continuous' and 'categorical' first dimension doesn't match!"
      )
    batch_shape = features.continuous.shape[:-1]
    unmapped_features = jnp.zeros(
        batch_shape
        + (sum(self.categorical_dims) + len(self._continuous_indices),)
    )
    con_ind = 0
    cat_ind = 0
    ind = 0
    # Extract the continuous and categorical feature indices.
    for spec in self.converter.output_specs:
      if spec.type == core.NumpyArraySpecType.CONTINUOUS:
        unmapped_features = unmapped_features.at[..., ind].set(
            features.continuous[..., con_ind]
        )
        con_ind += 1
        ind += 1

      elif spec.type == core.NumpyArraySpecType.ONEHOT_EMBEDDING:
        unmapped_features = unmapped_features.at[
            ..., ind : ind + spec.num_dimensions
        ].set(jnp.eye(spec.num_dimensions)[features.categorical[..., cat_ind]])
        cat_ind += 1
        ind += spec.num_dimensions
      else:
        raise ValueError("Unexpected spec type %s!" % spec.type)

    return unmapped_features


--- vizier/pyvizier/converters/feature_mapper_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for feature_mapper."""

from jax import config
import numpy as np
from vizier import pyvizier as vz
from vizier.pyvizier.converters import core
from vizier.pyvizier.converters import feature_mapper as fm

from absl.testing import absltest
from absl.testing import parameterized


class ContinuousCategoricalConverterTest(parameterized.TestCase):

  @parameterized.product(pad_oovs=[False, True])
  def test_categorical_only(self, pad_oovs):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_categorical_param('C1', ['a', 'b', 'c'])
    problem.search_space.root.add_categorical_param('C2', ['w', 'x', 'y', 'z'])
    problem.search_space.root.add_categorical_param('C3', ['0', '1'])
    problem.search_space.root.add_categorical_param('C4', ['*'])
    trial1 = vz.Trial(parameters={'C1': 'b', 'C2': 'z', 'C3': '0', 'C4': '*'})
    trial2 = vz.Trial(parameters={'C1': 'a', 'C2': 'z', 'C3': '0', 'C4': '*'})
    trial3 = vz.Trial(parameters={'C1': 'c', 'C2': 'x', 'C3': '1', 'C4': '*'})
    converter = core.TrialToArrayConverter.from_study_config(
        problem,
        pad_oovs=pad_oovs,
        max_discrete_indices=0,
    )
    features = converter.to_features([trial1, trial2, trial3])
    feature_mapper = fm.ContinuousCategoricalFeatureMapper(converter)
    res = feature_mapper.map(features)
    np.testing.assert_array_equal(
        res.categorical, np.array([[1, 3, 0, 0], [0, 3, 0, 0], [2, 1, 1, 0]])
    )
    np.testing.assert_array_equal(np.zeros((3, 0)), res.continuous)
    # Test un-mapping
    unmapped_features = feature_mapper.unmap(res)
    np.testing.assert_array_equal(unmapped_features, features)

  @parameterized.product(pad_oovs=[False, True], max_discrete_indices=[0, 10])
  def test_discrete_only(self, pad_oovs, max_discrete_indices):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_discrete_param('d1', [1, 10, 20])
    trial1 = vz.Trial(parameters={'d1': 10})
    trial2 = vz.Trial(parameters={'d1': 1})
    trial3 = vz.Trial(parameters={'d1': 20})
    converter = core.TrialToArrayConverter.from_study_config(
        problem, pad_oovs=pad_oovs, max_discrete_indices=max_discrete_indices
    )
    features = converter.to_features([trial1, trial2, trial3])
    feature_mapper = fm.ContinuousCategoricalFeatureMapper(converter)
    res = fm.ContinuousCategoricalFeatureMapper(converter).map(features)
    if max_discrete_indices == 10:
      # DISCRETE params are one-hot encoded. Should be mapped to categorical.
      np.testing.assert_array_equal(res.categorical, np.array([[1], [0], [2]]))
      np.testing.assert_array_equal(np.zeros((3, 0)), res.continuous)
    elif max_discrete_indices == 0:
      # DISCRETE params are continufied. Should be mapped to continuous.
      np.testing.assert_almost_equal(
          res.continuous, np.array([[(10 - 1) / (20 - 1)], [0.0], [1.0]])
      )
      np.testing.assert_array_equal(np.zeros((3, 0)), res.categorical)
    # Test un-mapping
    unmapped_features = feature_mapper.unmap(res)
    np.testing.assert_allclose(unmapped_features, features)

  def test_integer_only(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_int_param('i1', 0, 10)
    problem.search_space.root.add_int_param('i2', 0, 10)
    converter = core.TrialToArrayConverter.from_study_config(
        problem, pad_oovs=True, max_discrete_indices=0
    )
    trial1 = vz.Trial(parameters={'i1': 10, 'i2': 5})
    trial2 = vz.Trial(parameters={'i1': 1, 'i2': 3})
    trial3 = vz.Trial(parameters={'i1': 2, 'i2': 4})
    features = converter.to_features([trial1, trial2, trial3])
    feature_mapper = fm.ContinuousCategoricalFeatureMapper(converter)
    res = fm.ContinuousCategoricalFeatureMapper(converter).map(features)
    np.testing.assert_array_equal(
        res.continuous, np.array([[1.0, 0.5], [0.1, 0.3], [0.2, 0.4]])
    )
    np.testing.assert_array_equal(np.zeros((3, 0)), res.categorical)
    # Test un-mapping
    unmapped_features = feature_mapper.unmap(res)
    np.testing.assert_allclose(unmapped_features, features)

  def test_float_only(self):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    converter = core.TrialToArrayConverter.from_study_config(
        problem, pad_oovs=True, max_discrete_indices=0
    )
    trial1 = vz.Trial(parameters={'f1': 1.0, 'f2': 0.5})
    trial2 = vz.Trial(parameters={'f1': 0.1, 'f2': 0.3})
    trial3 = vz.Trial(parameters={'f1': 0.2, 'f2': 0.4})
    features = converter.to_features([trial1, trial2, trial3])
    feature_mapper = fm.ContinuousCategoricalFeatureMapper(converter)
    res = fm.ContinuousCategoricalFeatureMapper(converter).map(features)
    np.testing.assert_allclose(
        res.continuous, np.array([[1.0, 0.5], [0.1, 0.3], [0.2, 0.4]])
    )
    np.testing.assert_array_equal(np.zeros((3, 0)), res.categorical)
    # Test un-mapping
    unmapped_features = feature_mapper.unmap(res)
    np.testing.assert_allclose(unmapped_features, features, atol=1e-5)

  @parameterized.product(pad_oovs=[False, True], max_discrete_indices=[0, 10])
  def test_mixed_space(self, pad_oovs, max_discrete_indices):
    problem = vz.ProblemStatement()
    problem.search_space.root.add_float_param('f1', 0.0, 1.0)
    problem.search_space.root.add_float_param('f2', 0.0, 1.0)
    problem.search_space.root.add_discrete_param('d1', [1, 10, 20])
    problem.search_space.root.add_categorical_param('c1', ['a', 'b', 'c'])
    trial1 = vz.Trial(parameters={'f1': 1.0, 'f2': 0.5, 'd1': 10, 'c1': 'b'})
    trial2 = vz.Trial(parameters={'f1': 0.1, 'f2': 0.3, 'd1': 10, 'c1': 'a'})
    trial3 = vz.Trial(parameters={'f1': 0.2, 'f2': 0.4, 'd1': 20, 'c1': 'c'})
    converter = core.TrialToArrayConverter.from_study_config(
        problem, pad_oovs=pad_oovs, max_discrete_indices=max_discrete_indices
    )
    features = converter.to_features([trial1, trial2, trial3])
    feature_mapper = fm.ContinuousCategoricalFeatureMapper(converter)
    res = fm.ContinuousCategoricalFeatureMapper(converter).map(features)
    if max_discrete_indices == 10:
      np.testing.assert_array_equal(
          res.continuous, np.array([[1.0, 0.5], [0.1, 0.3], [0.2, 0.4]])
      )
      np.testing.assert_array_equal(
          res.categorical, np.array([[1, 1], [1, 0], [2, 2]])
      )
    if max_discrete_indices == 0:
      np.testing.assert_allclose(
          res.continuous,
          np.array([
              [1.0, 0.5, (10 - 1) / (20 - 1)],
              [0.1, 0.3, (10 - 1) / (20 - 1)],
              [0.2, 0.4, 1.0],
          ]),
      )
      np.testing.assert_array_equal(res.categorical, np.array([[1], [0], [2]]))
    # Test un-mapping
    unmapped_features = feature_mapper.unmap(res)
    np.testing.assert_allclose(unmapped_features, features)


if __name__ == '__main__':
  # Jax disables float64 computations by default and will silently convert
  # float64s to float32s. We must explicitly enable float64.
  config.update('jax_enable_x64', True)
  absltest.main()


--- vizier/pyvizier/converters/input_warping.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Input warping.

Input warping has shown to help optimize non-stationary functions:
http://proceedings.mlr.press/v32/snoek14.pdf.

Note that the dth output dimension is a function of the dth input dimension.

Example
-------
search_space = ...
problem = vz.ProblemStatement(search_space)
converter = converters.TrialToArrayConverter.from_study_config(problem)
input_warper = input_warping.KumaraswamyInputWarpingConverter(
    converter, a=0.1, b=0.8
)
trials = ...
features = input_warper.to_features(trials)
"""

from typing import Sequence

import attr
import numpy as np
from vizier import pyvizier as vz
from vizier.pyvizier.converters import core


def kumaraswamy_cdf(x: np.ndarray, a: float, b: float) -> np.ndarray:
  """Compute the Kumaraswamy CDF.

  Arguments:
    x: values in [0,1]. shape: (num_samples, num_features)
    a: positive value.
    b: positive value.

  Returns:
    The CDF(x). shape: (num_samples, num_cdfs).
  """
  return 1 - (1 - x**a) ** b


def kumaraswamy_inv_cdf(f: np.ndarray, a: float, b: float) -> np.ndarray:
  """Compute the inverse of the Kumaraswamy CDF.

  Arguments:
    f: values in [0,1]. shape: (num_samples, num_cdfs)
    a: positive value.
    b: positive value.

  Returns:
    The Inv_CDF(x). shape: (num_samples, num_features).
  """
  return (1 - (1 - f) ** (1 / b)) ** (1 / a)


@attr.define
class KumaraswamyInputWarpingConverter:
  """Input Warping based on Kumaraswamy distribution.

  Reference: https://en.wikipedia.org/wiki/Kumaraswamy_distribution
  """

  converter: core.TrialToArrayConverter
  a: float = attr.field(validator=attr.validators.instance_of(float))
  b: float = attr.field(validator=attr.validators.instance_of(float))

  def __attrs_post_init__(self):
    if self.a <= 0:
      raise ValueError(f"Attribute 'a' has to be positive, received {self.a}")
    if self.b <= 0:
      raise ValueError(f"Attribute 'b' has to be positive, received {self.b}")

  def to_features(self, trials) -> np.ndarray:
    features = self.converter.to_features(trials)
    return kumaraswamy_cdf(features, self.a, self.b)

  def to_xy(self, trials) -> tuple[np.ndarray, np.ndarray]:
    return self.to_features(trials), self.converter.to_labels(trials)

  def to_parameters(self, features: np.ndarray) -> Sequence[vz.ParameterDict]:
    """Convert to warput features into parameters."""
    return self.converter.to_parameters(
        kumaraswamy_inv_cdf(features, self.a, self.b)
    )


--- vizier/pyvizier/converters/input_warping_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for input_warping."""

import numpy as np
from vizier import pyvizier as vz
from vizier.pyvizier.converters import core as converters
from vizier.pyvizier.converters import input_warping

from absl.testing import absltest


class InputWarpingTest(absltest.TestCase):

  def test_kumaraswamy_transformation(self):
    rng = np.random.default_rng(seed=0)
    for a in np.linspace(0.01, 2.0, 50):
      for b in np.linspace(0.01, 2.0, 50):
        x = rng.uniform(size=(50, 20))
        f = input_warping.kumaraswamy_cdf(x, a, b)
        # Test that all values are in [0,1]
        self.assertEqual(np.sum((0 <= f) & (f <= 1)), x.shape[0] * x.shape[1])
        # Test the inverse CDF.
        np.testing.assert_allclose(
            input_warping.kumaraswamy_inv_cdf(f, a, b), x, atol=1e-07
        )

  def test_convert_trials(self):
    rng = np.random.default_rng(seed=0)
    search_space = vz.SearchSpace()
    for i in range(16):
      search_space.root.add_float_param(f"x{i}", 0.0, 1.0)
    problem = vz.ProblemStatement(search_space)
    converter = converters.TrialToArrayConverter.from_study_config(problem)
    input_warper = input_warping.KumaraswamyInputWarpingConverter(
        converter, a=0.1, b=0.8
    )

    trial1 = vz.Trial(parameters={f"x{i}": rng.uniform() for i in range(16)})
    trial2 = vz.Trial(parameters={f"x{i}": rng.uniform() for i in range(16)})
    trial3 = vz.Trial(parameters={f"x{i}": rng.uniform() for i in range(16)})
    trials = [trial1, trial2, trial3]

    features = input_warper.to_features(trials)
    self.assertEqual(np.sum((0 <= features) & (features <= 1)), 3 * 16)
    parameters = input_warper.to_parameters(features)
    for trial_idx, trial in enumerate(trials):
      for i in range(16):
        self.assertAlmostEqual(
            parameters[trial_idx][f"x{i}"].value,
            trial.parameters[f"x{i}"].value,
        )


if __name__ == "__main__":
  absltest.main()


--- vizier/pyvizier/converters/jnp_converters.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Converters that use JAX."""

from typing import Sequence, Tuple

import attr
import attrs
import jax
from jax import numpy as jnp
import numpy as np
from vizier import pyvizier as vz
from vizier._src.jax import types as vt
from vizier.pyvizier.converters import core
from vizier.pyvizier.converters import padding


# TODO: Slated for deprecation.
class PaddedTrialToArrayConverter:
  """Converts trials to arrays and pads / masks them."""

  _experimental_override = 'I am aware that this code may break at any point.'

  def __init__(
      self,
      impl: core.TrialToArrayConverter,
      experimental_override: str = '',
      *,
      padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule(
          num_trials=padding.PaddingType.NONE,
          num_features=padding.PaddingType.NONE,
      ),
  ):
    """SHOULD NOT BE USED! Use factory classmethods e.g. from_study_config."""

    if experimental_override != self._experimental_override:
      raise ValueError(
          'Set "experimental_override" if you want to call __init__ directly. '
          'Otherwise, use TrialToArrayConverter.from_study_config.'
      )
    self._impl = impl
    self._padding_schedule = padding_schedule

  def to_features(self, trials: Sequence[vz.Trial]) -> vt.PaddedArray:
    """Returns the features array with dimension: (n_trials, n_features)."""
    # Pad up the features.
    features = self._impl.to_features(trials)
    return self._padding_schedule.pad_features(features)

  def to_labels(self, trials: Sequence[vz.Trial]) -> vt.PaddedArray:
    """Returns the labels array with dimension: (n_trials, n_metrics)."""
    # Pad up the labels.
    labels = self._impl.to_labels(trials)
    return self._padding_schedule.pad_labels(labels)

  def to_xy(
      self, trials: Sequence[vz.Trial]
  ) -> Tuple[vt.PaddedArray, vt.PaddedArray]:
    return self.to_features(trials), self.to_labels(trials)

  def to_parameters(self, arr: np.ndarray) -> Sequence[vz.ParameterDict]:
    """Convert to nearest feasible parameter value. NaNs are preserved."""
    # Undo the padding of the parameters in this array.
    if self._padding_schedule is not None:
      # Get the original shape for the features without padding and
      # remove them.
      n_features = sum([spec.num_dimensions for spec in self.output_specs])
      arr = arr[..., :n_features]
    return self._impl.to_parameters(arr)

  @classmethod
  def from_study_config(
      cls,
      study_config: vz.ProblemStatement,
      *,
      scale: bool = True,
      padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule(
          num_trials=padding.PaddingType.NONE,
          num_features=padding.PaddingType.NONE,
      ),
      pad_oovs: bool = True,
      max_discrete_indices: int = 0,
      flip_sign_for_minimization_metrics: bool = True,
      dtype=np.float64,
  ) -> 'PaddedTrialToArrayConverter':
    """From study config.

    Args:
      study_config:
      scale: If True, scales the parameters to [0, 1] range.
      padding_schedule: Pads features and labels according to a padding
        schedule. This is to reduce the number of shapes a designer may see, and
        thus reduce JIT retracing.
      pad_oovs: If True, add an extra dimension for out-of-vocabulary values for
        non-CONTINUOUS parameters.
      max_discrete_indices: For DISCRETE and INTEGER parameters that have more
        than this many feasible values will be continuified. When generating
        suggestions, values are rounded to the nearest feasible value. Note this
        default is different from the default in DefaultModelInputConverter.
      flip_sign_for_minimization_metrics: If True, flips the metric signs so
        that every metric maximizes.
      dtype: dtype

    Returns:
      PaddedTrialToArrayConverter
    """
    converter = core.TrialToArrayConverter.from_study_config(
        study_config,
        scale=scale,
        pad_oovs=pad_oovs,
        max_discrete_indices=max_discrete_indices,
        flip_sign_for_minimization_metrics=flip_sign_for_minimization_metrics,
        dtype=dtype,
    )
    return cls(
        converter, cls._experimental_override, padding_schedule=padding_schedule
    )

  @property
  def output_specs(self) -> Sequence[core.NumpyArraySpec]:
    return self._impl.output_specs

  @property
  def metric_specs(self) -> Sequence[vz.MetricInformation]:
    return self._impl.metric_specs

  @property
  def padding_schedule(self) -> padding.PaddingSchedule:
    return self._padding_schedule


@attrs.define
class TrialToModelInputConverter:
  """Converts trials to arrays and pads / masks them."""

  _impl: 'TrialToContinuousAndCategoricalConverter'
  _problem: vz.ProblemStatement
  _padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule()

  @classmethod
  def from_problem(
      cls,
      problem: vz.ProblemStatement,
      *,
      scale: bool = True,
      max_discrete_indices: int = 0,
      flip_sign_for_minimization_metrics: bool = True,
      dtype=np.float64,
      padding_schedule: padding.PaddingSchedule = padding.PaddingSchedule(),
  ) -> 'TrialToModelInputConverter':
    """Create a new instance from ProblemStatement.

    Args:
      problem:
      scale: If True, scales the parameters to [0, 1] range.
      max_discrete_indices: For DISCRETE and INTEGER parameters that have more
        than this many feasible values will be continuified. When generating
        suggestions, values are rounded to the nearest feasible value. Note this
        default is different from the default in DefaultModelInputConverter.
      flip_sign_for_minimization_metrics: If True, flips the metric signs so
        that every metric maximizes.
      dtype: dtype
      padding_schedule:

    Returns:
      TrialToContinuousAndCategoricalConverter
    """

    def create_input_converter(parameter):
      return core.DefaultModelInputConverter(
          parameter,
          scale=scale,
          max_discrete_indices=max_discrete_indices,
          onehot_embed=False,
          float_dtype=dtype,
      )

    def create_output_converter(metric):
      return core.DefaultModelOutputConverter(
          metric,
          flip_sign_for_minimization_metrics=flip_sign_for_minimization_metrics,
          dtype=dtype,
      )

    sc = problem  # alias, to keep pylint quiet in the next line.
    converter = core.DefaultTrialConverter(
        [create_input_converter(p) for p in sc.search_space.parameters],
        [create_output_converter(m) for m in sc.metric_information],
    )
    return cls(
        TrialToContinuousAndCategoricalConverter(converter),
        problem=problem,
        padding_schedule=padding_schedule,
    )

  def to_features(self, trials: Sequence[vz.TrialSuggestion]) -> vt.ModelInput:
    """Returns the features array with dimension: (n_trials, n_features)."""
    # Pad up the features.
    features = self._impl.to_features(trials)
    return jax.tree_util.tree_map(self._padding_schedule.pad_features, features)

  def to_labels(self, trials: Sequence[vz.Trial]) -> vt.PaddedArray:
    """Returns the labels array with dimension: (n_trials, n_metrics)."""
    # Pad up the labels.
    labels = self._impl.to_labels(trials)
    return self._padding_schedule.pad_labels(labels)

  def to_xy(self, trials: Sequence[vz.Trial]) -> vt.ModelData:
    return vt.ModelData(self.to_features(trials), self.to_labels(trials))

  def to_parameters(self, arr: vt.ModelInput) -> Sequence[vz.ParameterDict]:
    """Convert to nearest feasible parameter value. NaNs are preserved."""
    # Undo the padding of the parameters in this array.
    unpadded = vt.ContinuousAndCategoricalArray(
        arr.continuous.unpad(), arr.categorical.unpad()
    )
    return self._impl.to_parameters(unpadded)

  def to_trials(self, data: vt.ModelData) -> Sequence[vz.Trial]:
    unpadded_features = vt.ContinuousAndCategoricalArray(
        data.features.continuous.unpad(), data.features.categorical.unpad()
    )
    unpadded_labels = data.labels.unpad()

    return self._impl.to_trials(unpadded_features, unpadded_labels)

  @property
  def output_specs(self):  # TODO: Add back pytype
    return self._impl.output_specs

  @property
  def metric_specs(self) -> Sequence[vz.MetricInformation]:
    return self._impl.metric_specs

  def continuous_feasible_values(
      self, max_num_feasible_values: int | None = None
  ) -> list[jax.Array]:
    """Returns a list of feasible values for each continuified parameter.

    The list is ordered the same as the parameters in the search space. An empty
    array means that the parameter is continuous and all values within its
    range are feasible. The returned feasible values are in the scaled space.
    Categorical parameters are ignored. If `max_num_feasible_values` is
    specified, discrete or integer parameters with more than
    `max_num_feasible_values` feasible values are considered continuous and
    empty arrays are returned for them.

    Args:
      max_num_feasible_values: If specified, discrete or integer parameters with
        more than this many feasible values are considered continuous and empty
        arrays are returned for them.

    Returns:
      A list of feasible values for each continuified parameter. The list is
      ordered the same as the parameters in the search space.
    """

    continuous_feasible_values = []
    for param in self._problem.search_space.parameters:
      if param.type in [vz.ParameterType.DISCRETE, vz.ParameterType.INTEGER]:
        if param.type == vz.ParameterType.INTEGER:
          num_feasible_values = param.bounds[1] - param.bounds[0] + 1
        else:
          num_feasible_values = len(param.feasible_values)
        if (
            max_num_feasible_values is not None
            and num_feasible_values > max_num_feasible_values
        ):
          continuous_feasible_values.append(jnp.asarray([]))
        else:
          converted_feasible_values = self.to_features([
              vz.TrialSuggestion(parameters={param.name: value})
              for value in param.feasible_values
          ]).continuous.unpad()
          continuous_feasible_values.append(
              # `self.to_features` returns NaNs for missing parameters (we call
              # it with one specific parameter instead of all parameters in the
              # search space), so we remove them here.
              converted_feasible_values[~jnp.isnan(converted_feasible_values)]
          )
      elif param.type == vz.ParameterType.DOUBLE:
        continuous_feasible_values.append(jnp.asarray([]))
      elif param.type == vz.ParameterType.CUSTOM:
        continuous_feasible_values.append(jnp.asarray([]))
    return continuous_feasible_values


@attr.define
class TrialToContinuousAndCategoricalConverter:
  """EXPERIMENTAL: A converter that returns `ContinuousAndCategoricalArray`s.

  Unlike TrialtoNumpyDict converters, `to_features` returns a structure of
  floating and integer data. CATEGORICAL and DISCRETE parameters are encoded as
  integers and not one-hot embedded.

  IMPORTANT: Use a factory method (currently, there is one: `from_study_config`)
  instead of `__init__`.

  WARNING: This class is not exchangable with `TrialToNumpyDict`, and does not
  have functions that return shape or metric informations. Use it at your own
  risk.
  """

  _impl: core.DefaultTrialConverter

  def to_features(
      self, trials: Sequence[vz.TrialSuggestion]
  ) -> vt.ContinuousAndCategoricalArray:
    """Returns a structure of arrays with first dimension `n_trials`."""
    features = self._impl.to_features(trials)

    continuous = []
    categorical = []
    for converter, v in zip(self._impl.parameter_converters, features.values()):
      spec = converter.output_spec
      if spec.type == core.NumpyArraySpecType.CONTINUOUS:
        continuous.append(v)
      elif spec.type == core.NumpyArraySpecType.DISCRETE:
        categorical.append(v)
      else:
        raise NotImplementedError(
            f'Expected spec to be CONTINUOUS or DISCRETE, saw {spec.type}'
        )
    if continuous:
      continuous_array = np.concatenate(continuous, axis=-1)
    else:
      continuous_array = np.zeros([len(trials), 0], dtype=np.float64)
    if categorical:
      categorical_array = np.concatenate(categorical, axis=-1).astype(
          vt.INT_DTYPE
      )
    else:
      categorical_array = np.zeros([len(trials), 0], dtype=vt.INT_DTYPE)
    return vt.ContinuousAndCategoricalArray(continuous_array, categorical_array)

  @property
  def dtype(self) -> vt.ContinuousAndCategorical[np.dtype]:
    empty_features = self.to_features([])
    return vt.ContinuousAndCategorical(
        empty_features.continuous.dtype, empty_features.categorical.dtype
    )

  def to_labels(self, trials: Sequence[vz.Trial]) -> np.ndarray:
    """Returns the labels array with dimension: (n_trials, n_metrics)."""
    return core.dict_to_array(self._impl.to_labels(trials))

  def to_xy(
      self, trials
  ) -> Tuple[vt.ContinuousAndCategoricalArray, np.ndarray]:
    return self.to_features(trials), self.to_labels(trials)

  def _to_dict_2d_arrays(
      self, feat: vt.ContinuousAndCategoricalArray
  ) -> core.DictOf2DArrays:
    """Converts to a single dict of 2d arrays."""
    feat_dict = {}
    cont_ind = 0
    cat_ind = 0
    for converter in self._impl.parameter_converters:
      spec = converter.output_spec
      if spec.type == core.NumpyArraySpecType.CONTINUOUS:
        feat_dict[spec.name] = feat.continuous[:, cont_ind : cont_ind + 1]
        cont_ind += 1
      elif spec.type == core.NumpyArraySpecType.DISCRETE:
        feat_dict[spec.name] = feat.categorical[:, cat_ind : cat_ind + 1]
        cat_ind += 1
      else:
        raise ValueError(
            f'Expected spec to be CONTINUOUS or DISCRETE, saw {spec.type}'
        )
    return core.DictOf2DArrays(feat_dict)

  def to_trials(
      self, feat: vt.ContinuousAndCategoricalArray, labels: np.ndarray
  ) -> Sequence[vz.Trial]:
    # Pass in an empty array to get the structure
    labels_dict = core.DictOf2DArrays(self._impl.to_labels([])).dict_like(
        labels
    )
    return self._impl.to_trials(self._to_dict_2d_arrays(feat), labels_dict)

  def to_parameters(
      self, feat: vt.ContinuousAndCategoricalArray
  ) -> Sequence[vz.ParameterDict]:
    """Convert to nearest feasible parameter value. NaNs are preserved."""
    return self._impl.to_parameters(self._to_dict_2d_arrays(feat))

  # TODO: Move this method to TrialToModelInputConverter, and
  # make this class private to the module.
  @classmethod
  def from_study_config(
      cls,
      study_config: vz.ProblemStatement,
      *,
      scale: bool = True,
      max_discrete_indices: int = 0,
      flip_sign_for_minimization_metrics: bool = True,
      dtype=np.float64,
  ) -> 'TrialToContinuousAndCategoricalConverter':
    """From study config.

    Args:
      study_config:
      scale: If True, scales the parameters to [0, 1] range.
      max_discrete_indices: For DISCRETE and INTEGER parameters that have more
        than this many feasible values will be continuified. When generating
        suggestions, values are rounded to the nearest feasible value. Note this
        default is different from the default in DefaultModelInputConverter.
      flip_sign_for_minimization_metrics: If True, flips the metric signs so
        that every metric maximizes.
      dtype: dtype

    Returns:
      TrialToContinuousAndCategoricalConverter
    """

    def create_input_converter(parameter):
      return core.DefaultModelInputConverter(
          parameter,
          scale=scale,
          max_discrete_indices=max_discrete_indices,
          onehot_embed=False,
          float_dtype=dtype,
      )

    def create_output_converter(metric):
      return core.DefaultModelOutputConverter(
          metric,
          flip_sign_for_minimization_metrics=flip_sign_for_minimization_metrics,
          dtype=dtype,
      )

    sc = study_config  # alias, to keep pylint quiet in the next line.
    converter = core.DefaultTrialConverter(
        [create_input_converter(p) for p in sc.search_space.parameters],
        [create_output_converter(m) for m in sc.metric_information],
    )
    return cls(converter)

  @property
  def output_specs(
      self,
  ) -> vt.ContinuousAndCategorical[Sequence[core.NumpyArraySpec]]:
    """Output `NumpyArraySpec`s."""
    continuous = []
    categorical = []
    for converter in self._impl.parameter_converters:
      spec = converter.output_spec
      if spec.type == core.NumpyArraySpecType.CONTINUOUS:
        continuous.append(spec)
      elif spec.type == core.NumpyArraySpecType.DISCRETE:
        categorical.append(spec)
      else:
        raise ValueError(
            f'Expected spec to be CONTINUOUS or DISCRETE, saw {spec.type}'
        )
    return vt.ContinuousAndCategorical(continuous, categorical)

  @property
  def metric_specs(self) -> Sequence[vz.MetricInformation]:
    return [mc.metric_information for mc in self._impl.metric_converters]


--- vizier/pyvizier/converters/jnp_converters_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for jnp_converters."""

import numpy as np
from vizier import pyvizier as vz
from vizier._src.algorithms.designers import random as random_designer_lib
from vizier._src.algorithms.testing import test_runners
from vizier._src.jax import types
from vizier.pyvizier.converters import jnp_converters as jnpc
from vizier.pyvizier.converters import padding
from vizier.testing import test_studies

from absl.testing import absltest
from absl.testing import parameterized


Trial = vz.Trial


class PaddedTrialToArrayConverterTest(parameterized.TestCase):

  def test_padding(self):
    """Tests various padding schedules."""

    padding_schedule = padding.PaddingSchedule(
        num_trials=padding.PaddingType.POWERS_OF_2,
        num_features=padding.PaddingType.MULTIPLES_OF_10,
        num_metrics=padding.PaddingType.NONE,
    )
    problem = vz.ProblemStatement(test_studies.flat_space_with_all_types())
    problem.metric_information.append(
        vz.MetricInformation(
            name='metric',
            goal=vz.ObjectiveMetricGoal.MAXIMIZE,
            min_value=-1.0,
            max_value=1.0,
        )
    )
    trials = test_runners.RandomMetricsRunner(
        problem,
        iters=1,
        batch_size=13,
        verbose=False,
        validate_parameters=False,
    ).run_designer(random_designer_lib.RandomDesigner(problem.search_space))

    converter = jnpc.TrialToModelInputConverter.from_problem(
        problem,
        padding_schedule=padding_schedule,
    )

    data = converter.to_xy(trials)
    self.assertSequenceEqual(
        data.features.continuous.padded_array.shape,
        (16, 10),
    )
    self.assertSequenceEqual(
        data.features.categorical.padded_array.shape,
        (16, 10),
    )

    labels = converter.to_labels(trials)
    self.assertSequenceEqual(labels.padded_array.shape, (16, 1))

    recovererd_trials = converter.to_trials(data)

    for i in range(13):
      self.assertSequenceAlmostEqual(
          recovererd_trials[i].parameters.as_dict().values(),
          trials[i].parameters.as_dict().values(),
          places=3,
          msg=f'Failed at {i}th trial',
      )
      problem.search_space.assert_contains(recovererd_trials[i].parameters)


class TrialToContinuousAndCategoricalConverterTest(parameterized.TestCase):
  """Test TrialToContinuousAndCategoricalConverter class."""

  def setUp(self):
    super().setUp()
    self._study_config = vz.ProblemStatement(
        search_space=test_studies.flat_space_with_all_types(),
        metric_information=[
            vz.MetricInformation('x1', goal=vz.ObjectiveMetricGoal.MAXIMIZE)
        ],
    )

    self._designer = random_designer_lib.RandomDesigner(
        self._study_config.search_space, seed=0
    )
    self._trials = test_runners.run_with_random_metrics(
        self._designer, self._study_config, iters=1, batch_size=10
    )
    self.maxDiff = None

  def test_continuous_feasible_values(self):
    """Tests the `continuous_feasible_values` method.

    The returned feasible values are expected to be in the scaled space, and
    categorical parameters are ignored.
    """
    space = vz.SearchSpace()
    root = space.root
    root.add_discrete_param('discrete_double', [-0.4, -0.3, 0.4, 1.1, 1.15])
    root.add_discrete_param('discrete_int', [5, 9])
    root.add_categorical_param('categorical', ['b', 'c'])
    root.add_discrete_param(
        'discrete_logdouble', [1e-5, 1e-2, 1e-1], scale_type=vz.ScaleType.LOG
    )
    root.add_int_param('integer', -2, 2)
    root.add_float_param('float', -3, 4)
    root.add_int_param('integer_with_many_feasible_values', -1, 6 * 10**7)
    converter = jnpc.TrialToModelInputConverter.from_problem(
        vz.ProblemStatement(
            search_space=space,
            metric_information=[
                vz.MetricInformation(
                    'obj', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                )
            ],
        )
    )
    continuous_feasible_values = converter.continuous_feasible_values(
        max_num_feasible_values=1000
    )
    self.assertLen(continuous_feasible_values, 6)
    self.assertSequenceAlmostEqual(
        continuous_feasible_values[0],
        # The original feasible values are linearly scaled to [0, 1].
        (np.asarray([-0.4, -0.3, 0.4, 1.1, 1.15]) + 0.4) / 1.55,
    )
    self.assertSequenceAlmostEqual(
        # The original feasible values [5, 9] are linearly scaled to [0, 1].
        continuous_feasible_values[1],
        np.asarray([0.0, 1.0]),
    )
    self.assertSequenceAlmostEqual(
        continuous_feasible_values[2],
        # The original feasible values [1e-5, 1e-2, 1e-1] are log-then-linearly
        # scaled to [0, 1].
        (
            (np.log([1e-5, 1e-2, 1e-1]) - np.log(1e-5))
            / (np.log(1e-1) - np.log(1e-5))
        ),
    )
    self.assertSequenceAlmostEqual(
        continuous_feasible_values[3],
        # The integer parameter with range [-2, 2] are internally scaled to
        # evenly spaced values in [0, 1].
        np.asarray([0.0, 0.25, 0.5, 0.75, 1.0]),
    )
    # Feasible values for float parameters are empty.
    self.assertEmpty(continuous_feasible_values[4])
    # Feasible values for int parameters with many feasible values are empty.
    self.assertEmpty(continuous_feasible_values[5])

  def test_back_to_back_conversion(self):
    converter = jnpc.TrialToContinuousAndCategoricalConverter.from_study_config(
        self._study_config
    )
    self.assertSequenceEqual(
        [t.parameters for t in self._trials],
        converter.to_parameters(converter.to_features(self._trials)),
    )

  def test_dtype(self):
    space = vz.SearchSpace()
    root = space.root
    root.add_float_param('double', -2.0, 2.0)
    root.add_int_param('integer', -2, 2)
    root.add_categorical_param('categorical', ['b', 'c'])
    converter = jnpc.TrialToContinuousAndCategoricalConverter.from_study_config(
        vz.ProblemStatement(search_space=space)
    )
    self.assertEqual(converter.dtype.continuous, np.float64)
    self.assertEqual(converter.dtype.categorical, np.int32)

  def test_parameter_continuify(self):
    space = vz.SearchSpace()
    root = space.root
    root.add_float_param('double', -2.0, 2.0)
    root.add_int_param('integer', -2, 2)
    root.add_categorical_param('categorical', ['b', 'c'])
    root.add_discrete_param('discrete', [-1.0, 2.0, 3.0])

    converter = jnpc.TrialToContinuousAndCategoricalConverter.from_study_config(
        vz.ProblemStatement(search_space=space)
    )
    trial = vz.Trial(
        parameters={
            'double': vz.ParameterValue(3.0),
            'integer': vz.ParameterValue(-1),
            'discrete': vz.ParameterValue(2.0),
            'categorical': vz.ParameterValue('d'),
        }
    )
    expected = types.ContinuousAndCategoricalArray(
        continuous=np.array([[1.25, 0.25, 0.75]]), categorical=np.array([[2]])
    )
    actual = converter.to_features([trial])
    np.testing.assert_equal(actual.continuous, expected.continuous)
    np.testing.assert_equal(actual.categorical, expected.categorical)

  def test_multi_metrics(self):
    search_space = vz.SearchSpace()
    search_space.root.add_float_param('x', 0.0, 1.0)
    problem = vz.ProblemStatement(
        search_space=search_space,
        metric_information=vz.MetricsConfig(
            metrics=[
                vz.MetricInformation(
                    'obj1', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
                vz.MetricInformation(
                    'obj2', goal=vz.ObjectiveMetricGoal.MAXIMIZE
                ),
                vz.MetricInformation(
                    'obj3', goal=vz.ObjectiveMetricGoal.MINIMIZE
                ),
            ]
        ),
    )
    trial1 = vz.Trial()
    trial2 = vz.Trial()
    trial1.final_measurement = vz.Measurement(
        metrics={'obj1': 1.0, 'obj2': 2.0, 'obj3': 3.0}
    )
    trial2.final_measurement = vz.Measurement(
        metrics={'obj1': -1.0, 'obj2': 5.0, 'obj3': 0.0}
    )
    converter = jnpc.TrialToContinuousAndCategoricalConverter.from_study_config(
        problem
    )
    # Notice that the sign is flipped for MINIMIZE objective.
    expected_labels = np.array([[1.0, 2.0, -3.0], [-1.0, 5.0, 0.0]])
    np.testing.assert_equal(
        converter.to_labels([trial1, trial2]), expected_labels
    )
    self.assertEqual(converter.metric_specs[0].name, 'obj1')
    self.assertEqual(converter.metric_specs[1].name, 'obj2')
    self.assertEqual(converter.metric_specs[2].name, 'obj3')


if __name__ == '__main__':
  absltest.main()


--- vizier/pyvizier/converters/padding.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Library for padding inputs and arrays in order to reduce chances of recompilation."""

import enum
import math
from typing import Sequence

import attrs
import numpy as np
from vizier._src.jax import types


class PaddingType(enum.Enum):
  NONE = 1
  MULTIPLES_OF_10 = 2
  POWERS_OF_2 = 3


def _padded_dimensions(
    dims: Sequence[int], padding_types: Sequence[PaddingType]
) -> tuple[int, ...]:
  """Returns the padded shape according to `padding_types`."""
  new_dims = []
  for dim, padding_type in zip(dims, padding_types):
    if padding_type == PaddingType.NONE:
      new_dims.append(dim)
    elif padding_type == PaddingType.MULTIPLES_OF_10:
      new_dims.append(int(math.ceil(dim / 10.0)) * 10)
    elif padding_type == PaddingType.POWERS_OF_2:
      if dim == 0:
        new_dims.append(0)
      else:
        new_dims.append(int(2 ** (math.ceil(math.log(dim, 2)))))
    else:
      raise ValueError(f'{padding_type} unexpected.')
  return tuple(new_dims)


@attrs.define(frozen=True, kw_only=True, hash=True, eq=True)
class PaddingSchedule:
  """Convenience class for creating `PaddedArray`."""

  _num_trials: PaddingType = PaddingType.NONE
  _num_features: PaddingType = PaddingType.NONE
  _num_metrics: PaddingType = PaddingType.NONE

  _int_default: int = -1
  _float_default: float = np.nan

  def _pad_trailing_dims(
      self, array: np.ndarray, padding_types: Sequence[PaddingType]
  ) -> types.PaddedArray:
    """Pads features in to a `PaddedArray`."""
    assert len(padding_types) == len(array.shape)

    original_shape = array.shape[-len(padding_types) :]
    padded_shape = array.shape[: -len(padding_types)] + _padded_dimensions(
        original_shape, padding_types
    )

    if np.issubdtype(array.dtype, np.integer):
      fill_value = self._int_default
    else:
      fill_value = np.nan
    return types.PaddedArray.from_array(
        array, padded_shape, fill_value=fill_value
    )

  def pad_features(self, features: types.Array) -> types.PaddedArray:
    """Pads features in to a `PaddedArray`."""
    return self._pad_trailing_dims(
        features, [self._num_trials, self._num_features]
    )

  def pad_labels(
      self,
      labels: types.Array,
  ) -> types.PaddedArray:
    """Pads labels in to a `PaddedArray`."""
    return self._pad_trailing_dims(
        labels, [self._num_trials, self._num_metrics]
    )


--- vizier/pyvizier/converters/padding_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for gp_bandit."""

import numpy as np
from vizier.pyvizier.converters import padding

from absl.testing import absltest
from absl.testing import parameterized


class PaddingTest(parameterized.TestCase):

  def test_padding(self):
    features = np.random.randn(13, 7)
    labels = np.random.randn(13, 3)

    schedule = padding.PaddingSchedule(
        num_trials=padding.PaddingType.MULTIPLES_OF_10,
        num_features=padding.PaddingType.POWERS_OF_2,
        num_metrics=padding.PaddingType.POWERS_OF_2,
    )

    padded = schedule.pad_features(features)

    self.assertSequenceEqual(padded.padded_array.shape, (20, 8))
    self.assertSequenceEqual(padded.unpad().shape, features.shape)

    padded = schedule.pad_labels(labels)
    self.assertSequenceEqual(padded.padded_array.shape, (20, 4))
    self.assertSequenceEqual(padded.unpad().shape, labels.shape)

  def test_nopadding(self):
    features = np.random.randn(13, 7)
    labels = np.random.randn(13, 3)

    schedule = padding.PaddingSchedule(
        num_trials=padding.PaddingType.NONE,
        num_features=padding.PaddingType.NONE,
        num_metrics=padding.PaddingType.NONE,
    )

    padded = schedule.pad_features(features)
    self.assertSequenceEqual(padded.padded_array.shape, features.shape)

    padded = schedule.pad_features(labels)
    self.assertSequenceEqual(padded.padded_array.shape, labels.shape)


if __name__ == '__main__':
  absltest.main()


--- vizier/pyvizier/converters/spatio_temporal.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Spatio-temporal converters."""

import collections
import copy
import dataclasses
from typing import Any, Literal, Optional, Sequence, Union

from absl import logging
import numpy as np
from vizier import pyvizier
from vizier.pyvizier.converters import core


@dataclasses.dataclass
class TimedLabels:
  """Corresponds to a *single* Trial's (intermediate) measurements.

  Let M be the number of measurement a trial had, Then,
    time: (M, 1) array of timestamps (See StudyMerger.to_time_and_labels_array
     pydoc for details.)
    labels: (M, 1) arrays keyed by strings, corresponding to metrics.
  """
  times: np.ndarray
  labels: dict[str, np.ndarray]


class TimedLabelsExtractor:
  """Extracts TimeLabels."""

  CUMMAX = 'cummax'
  CUMMAX_LASTONLY = 'cummax_lastonly'
  CUMMAX_FIRSTONLY = 'cummax_firstonly'
  RAW = 'raw'

  def __init__(
      self,
      metric_converters: Sequence[core.ModelOutputConverter],
      timestamp: Literal['steps', 'elapsed_secs', 'index'] = 'steps',
      *,
      temporal_index_points: Sequence[float] = tuple(),
      value_extraction: str = 'cummax_lastonly',
  ):
    """Init.

    Args:
      metric_converters:
      timestamp: One of ['steps', 'elapsed_secs', 'index'].
      temporal_index_points: If provided, extract at the provided temporal index
        points.
      value_extraction: One of ['raw', 'cummax','cummax_lastonly',
        'cummax_firstonly'].
        'cummax': take the best value up to the time. For example, a
          maximization metric raw values
          (2, 1, 0, 3, 3, 2, 4, 2, 1) will be converted to: (2, 2, 2, 3, 3, 3,
            4, 4, 4).
        'cummax_lastonly': Every time there is an improvement, record the
          measurement before that improvement. For example, a maximization
          metric raw values
          (2, 1, 0, 3, 3, 2, 4, 2, 1) will be converted to: (_, _, 2, _, _, 3,
            _, _, 4).
        'cummax_firstonly':  Every time there is an improvement, record that
          value. Discard the rest except the very last measurement, so that
          stopping algorithms don't keep running a trial that's stuck in a
          plateau. For example, a maximization metric raw values
          (2, 1, 0, 3, 3, 2, 4, 2, 1) will be converted to: (2, _, _, 3, _, _,
            4, _, 4).
    """
    self.metric_converters = metric_converters
    self.temporal_index_points = np.asarray(temporal_index_points).squeeze()
    self.value_extraction = value_extraction
    self.timestamp = timestamp

    if value_extraction not in (self.CUMMAX, self.RAW, self.CUMMAX_LASTONLY,
                                self.CUMMAX_FIRSTONLY):
      raise ValueError(
          'Bad value for value_extraction rule: {}'.format(value_extraction))
    if timestamp not in ('steps', 'elapsed_secs', 'index'):
      raise ValueError(f'Invalid timestamp: {timestamp}')
    if value_extraction in [self.CUMMAX_LASTONLY, self.CUMMAX_FIRSTONLY]:
      if len(metric_converters) > 1:
        raise ValueError(
            '{} mode supports only a single metric.'.format(value_extraction))
      if self.temporal_index_points.size > 1:
        raise ValueError(
            '{} mode does not support fixed temporal_index_points.'.format(
                value_extraction))

  def _cummax_fn(self, metric_converter: core.ModelOutputConverter) -> Any:
    if (metric_converter.metric_information.goal ==
        pyvizier.ObjectiveMetricGoal.MAXIMIZE):
      return np.maximum
    else:
      return np.minimum

  def convert(self, trials: Sequence[pyvizier.Trial]) -> list[TimedLabels]:
    """Converts each trial into TimedLabels object."""
    timedlabels = []
    if self.temporal_index_points.size == 0:
      # Default mode. Temporal index points are not specified. Take all
      # available measurements.
      for t in trials:
        times = self.to_timestamps(t.measurements)
        labels = dict()
        for mc in self.metric_converters:
          # Process one metric at a time, to keep things simple.
          if self.value_extraction == self.CUMMAX:
            labels[mc.metric_information.name] = self._cummax_fn(mc).accumulate(
                mc.convert(t.measurements), axis=0)
          elif self.value_extraction in [
              self.CUMMAX_LASTONLY, self.CUMMAX_FIRSTONLY
          ]:
            # Squeeze into 1D to make indexing easier.
            marr = self._cummax_fn(mc).accumulate(
                mc.convert(t.measurements), axis=0).reshape(-1)
            if marr.size > 0:
              # Take the first and last index and the indices
              # where there was a delta.
              if self.value_extraction == self.CUMMAX_LASTONLY:
                delta_idx = np.concatenate(
                    [marr[:-1] < marr[1:],
                     np.array([True])])
              else:
                delta_idx = np.concatenate(
                    [np.array([True]), marr[:-1] < marr[1:]])
                delta_idx[-1] = True
            else:
              # No measurements were found.
              delta_idx = np.array([], dtype=bool)
            # Filter labels and times.
            labels[mc.metric_information.name] = marr[delta_idx][:, np.newaxis]
            times = times[delta_idx]
          else:
            assert self.value_extraction == self.RAW, 'Unreachable.'
            labels[mc.metric_information.name] = mc.convert(t.measurements)
        timedlabels.append(TimedLabels(times, labels))
    elif self.temporal_index_points.size and self.value_extraction == self.RAW:
      num_empty_trials = 0
      for t in trials:
        # Apply masks to only take observations at the temporal points.
        times = self.to_timestamps(t.measurements)  # shape T x 1
        mask = np.isin(times, self.temporal_index_points).squeeze()  # shape T
        if np.sum(mask) == 0:
          num_empty_trials += 1

        # Apply the mask and extract the labels.
        measurements = np.array(t.measurements)[mask]
        times = times[mask]
        labels = dict()
        for mc in self.metric_converters:
          labels[mc.metric_information.name] = mc.convert(measurements)
        timedlabels.append(TimedLabels(times, labels))
      if num_empty_trials:
        logging.warning(
            '%s Out of %s trials had zero measurement after masking.',
            num_empty_trials, len(trials))
    else:
      assert self.value_extraction == self.CUMMAX, 'Unreachable.'
      for trial in trials:
        times = self.to_timestamps(trial.measurements)  # shape T x 1

        # mask[i] is the index of the first measurement that is taken after the
        # temporal_index_points[i].
        mask = []
        for t in self.temporal_index_points:
          greater_indices = np.where(t >= times)
          if not greater_indices:
            mask.append(0)
          else:
            mask.append(greater_indices[-1])
        mask = np.asarray(mask)

        # Extract the labels and apply the mask.
        labels = dict()
        for mc in self.metric_converters:
          labels[mc.metric_information.name] = self._cummax_fn(mc).accumulate(
              mc.convert(trial.measurements), axis=0)[mask]
        timedlabels.append(TimedLabels(times, labels))

    return timedlabels

  def to_timestamps(self,
                    measurements: Sequence[pyvizier.Measurement]) -> np.ndarray:
    """"Returns an arry of shape (len(measurements), 1)."""
    if self.timestamp == 'steps':
      timestamps = [m.steps for m in measurements]
    elif self.timestamp == 'elapsed_secs':
      timestamps = [m.elapsed_secs for m in measurements]
    elif self.timestamp == 'index':
      timestamps = list(range(len(measurements)))
    else:
      # Should be unreachable given check in __init__.
      raise ValueError(f'Invalid timestamp: {self.timestamp}')
    return np.asarray(timestamps)[:, np.newaxis]

  def extract_all_timestamps(
      self, trials: Sequence[pyvizier.Trial]) -> Sequence[float]:
    """Returns a sorted list of unique temporal indices occurring in trials.

    This is a cheaper alternative to:
    ```
    ts = np.concatenate([np.asarray(tl.times).flatten() for tl in timed_labels])
    return sorted(set(ts))
    ```
    because it skips the label conversions.

    Args:
      trials:

    Returns:
      Sorted list of temporal indices.
    """
    all_timestamps: set[float] = set()
    for t in trials:
      all_timestamps.update(self.to_timestamps(t.measurements).flatten())
    return sorted(all_timestamps)


class SparseSpatioTemporalConverter(core.TrialToNumpyDict):
  """Optimized for when the temporal dimensions are not well-aligned.

  This converter inherits from the `TrialToNumpyDict` abstractions. Timestamp
  becomes an extra feature dimension.

  For inference, however, the user has to provide `temporal_index_points` to
  specify where to predict a Trial at.
  """

  def __init__(self,
               parameter_converters: Sequence[core.DefaultModelInputConverter],
               timed_labels_extractor: TimedLabelsExtractor):
    self.trial_converter = core.DefaultTrialConverter(
        parameter_converters, timed_labels_extractor.metric_converters)
    self.timed_labels_extractor = timed_labels_extractor

  def to_xy(
      self, trials: Sequence[pyvizier.Trial]
  ) -> tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:
    """Returned values can be used as `x` and `y` for keras.Model.fit().

    Args:
      trials:

    Returns:
      Tuple. Both have length equal to the total number of measurements.
      x values have an extra element of (None, 1). y values are the same as
      metric_converters.
    """
    timed_labels = self.timed_labels_extractor.convert(trials)
    all_features = dict()
    all_labels = dict()
    for trial, tl in zip(trials, timed_labels):
      # Let T_i be the number of temporal observations for this trial.
      # tl.times and tl.labels have shape [T_i, 1]
      features = self.to_features(trial, tl.times)
      # Add to the return values.
      for k in features:
        if k in all_features:
          all_features[k] = np.concatenate([all_features[k], features[k]],
                                           axis=0)
        else:
          all_features[k] = features[k]

      labels = tl.labels
      for k in labels:
        if k in all_labels:
          all_labels[k] = np.concatenate([all_labels[k], labels[k]], axis=0)
        else:
          all_labels[k] = labels[k]

    return all_features, all_labels

  def to_features(self, trial, temporal_index_points) -> dict[str, np.ndarray]:
    """Converts a trial at the specified index points.

    Args:
      trial:
      temporal_index_points:

    Returns:
      Each array has shape (B,_) where _ is determined by injected
      ModelInputConverters and B = len(trials) * len(temporal_index_points).
      In addition to what ModelInputConverters return, there is an extra array
      of shape (B, 1) corresponding to the time stamps.
    """
    features = self.trial_converter.to_features(
        [trial])  # Dict values have shape [1,1]
    # Repeat the features T_i times. Dict values have shape [T_i,1]
    features = {
        k: np.tile(v, (len(temporal_index_points), 1))
        for k, v in features.items()
    }

    # Add the timestamp feature.
    features[self.timed_labels_extractor.timestamp] = temporal_index_points
    return features

  @property
  def features_shape(self) -> dict[str, Sequence[Union[int, None]]]:
    """Returned value can be used as `input_shape` for keras.Model.build()."""
    shapes = copy.deepcopy(self.trial_converter.features_shape)
    shapes[self.timed_labels_extractor.timestamp] = (None, 1)
    return shapes

  @property
  def output_specs(self) -> dict[str, core.NumpyArraySpec]:
    specs = copy.deepcopy(self.trial_converter.output_specs)
    name = self.timed_labels_extractor.timestamp
    # Can't use float32 max, because
    # isinstance(np.finfo(np.float32).max, float) == False
    specs[name] = core.NumpyArraySpec.from_parameter_config(
        pyvizier.ParameterConfig.factory(
            name=name, bounds=(0.0, np.finfo(float).max)),
        core.NumpyArraySpecType.default_factory)
    return specs

  @property
  def labels_shape(self) -> dict[str, Sequence[Union[int, None]]]:
    return self.trial_converter.labels_shape

  @property
  def metric_information(self) -> dict[str, pyvizier.MetricInformation]:
    return self.trial_converter.metric_information


class DenseSpatioTemporalConverter(core.TrialToNumpyDict):
  """Optimized for when the temporal dimensions are well-aligned.

  This converter inherits from the `TrialToNumpyDict` abstraction. Each label
  in the dict has shape [number of trials, number of temproal index points].
  To operate in this mode, temporal_index_points must be specified.

  We can also use `to_xty()` instead of `to_xy()` in an alternative workflow
  that does not depend on the `TrialToNumpyDict` interface.
  """

  def __init__(self,
               parameter_converters: Sequence[core.DefaultModelInputConverter],
               timed_labels_extractor: TimedLabelsExtractor,
               temporal_index_points: Optional[np.ndarray] = None):
    self.trial_converter = core.DefaultTrialConverter(
        parameter_converters, timed_labels_extractor.metric_converters)
    self.timed_labels_extractor = timed_labels_extractor
    if temporal_index_points is None:
      logging.info('Empty temporal_index_points.')
      self.temporal_index_points = np.zeros([0])
    else:
      self.temporal_index_points = np.sort(temporal_index_points)

  def _single_timedlabels_to_temporal_observations(
      self, timed_labels: TimedLabels, ts: Union[Sequence[float], Sequence[int]]
  ) -> dict[str, np.ndarray]:
    """Subroutine of _to_temporal_observations().

    Args:
      timed_labels: A single TimedLabels object.
      ts: Length T sequence of timestamps.

    Returns:
      Dict of metric names to array of shape (T).
    """

    # Reshape things into 1-D array, so we can iterate through them like
    # python list.
    ts = np.asarray(ts).reshape([-1])
    if ts.size == 0:
      # Caller asked for nothing.
      return {k: np.array([]) for k in timed_labels.labels}

    observed_times, labels_dict = timed_labels.times.reshape(
        [-1]), timed_labels.labels
    if observed_times.size == 0:
      # Nothing was observed.
      return {k: np.zeros_like(ts) * np.nan for k in timed_labels.labels}

    query_time_iter = iter(ts)
    query_time = next(query_time_iter)
    observed_time_iter = iter(enumerate(observed_times))
    idx, observed_time = next(observed_time_iter)
    this_labels = collections.defaultdict(list)

    while query_time is not None:
      if query_time == observed_time:
        # Times match. add it.
        for key in labels_dict:
          this_labels[key].append(labels_dict[key][idx, 0])
        query_time = next(query_time_iter, None)
        idx, observed_time = next(observed_time_iter, (None, None))
      elif observed_time is None or observed_time > query_time:
        # query_time is behind, or we've scanned through all observations.
        # In either case, we don't have observed data at query_time.
        # Fill with nan and advance query_time.
        query_time = next(query_time_iter, None)
        for key in labels_dict:
          this_labels[key].append(np.nan)
      else:
        # query_time is ahead.
        idx, observed_time = next(observed_time_iter, (None, None))
    return {k: np.asarray(v) for k, v in this_labels.items()}

  def _to_temporal_observations(
      self,
      timed_labels_sequence: Sequence[TimedLabels],
      ts: Union[Sequence[float], Sequence[int]],
  ) -> dict[str, np.ndarray]:
    """Returns a dict of np arrays of temporal observations.

    Filters timed_labels.labels to leave only the time indices that appear in
    ts.

    Args:
      timed_labels_sequence: Length B sequence.
      ts: Length T sequence.

    Returns:
      Dict of metric names to array of shape (B, T).
    """

    all_labels = collections.defaultdict(lambda: np.zeros(shape=(0, len(ts))))

    for tl in timed_labels_sequence:
      labels = self._single_timedlabels_to_temporal_observations(tl, ts)
      # labels have shape [T]. Add a new axis to make it (1,T) so we
      # can concat.
      labels = {k: v[np.newaxis, :] for k, v in labels.items()}
      for k in labels:
        all_labels[k] = np.concatenate([all_labels[k], labels[k]], axis=0)
    return all_labels

  def to_xy(
      self, trials: Sequence[pyvizier.Trial]
  ) -> tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:
    """Returned value can be used as `x`, `y` for keras.Model.fit()."""
    all_timed_labels = self.timed_labels_extractor.convert(trials)
    labels = self._to_temporal_observations(all_timed_labels,
                                            self.temporal_index_points)

    return self.trial_converter.to_features(trials), labels

  @property
  def features_shape(self) -> dict[str, Sequence[Union[int, None]]]:
    """Returned value can be used as `input_shape` for keras.Model.build()."""
    return self.trial_converter.features_shape

  @property
  def output_specs(self) -> dict[str, core.NumpyArraySpec]:
    return self.trial_converter.output_specs

  @property
  def labels_shape(self) -> dict[str, Sequence[Union[int, None]]]:
    shapes = dict()
    for mc in self.trial_converter.metric_converters:
      shapes[mc.metric_information.name] = (None,
                                            len(self.temporal_index_points))
    return shapes

  @property
  def metric_information(self) -> dict[str, pyvizier.MetricInformation]:
    return self.trial_converter.metric_information

  def to_features(
      self, trials: Sequence[pyvizier.Trial]
  ) -> dict[str, np.ndarray]:
    return self.trial_converter.to_features(trials)

  def to_xty(
      self, trials: Sequence[pyvizier.Trial], temporal_selection: str = 'auto'
  ) -> tuple[dict[str, np.ndarray], np.ndarray, dict[str, np.ndarray]]:
    """Returns x, t, and y.

    Args:
      trials:
      temporal_selection: 'infer', 'default', 'auto'. If 'infer', values are
        auto-inferred based on timestamps that exist in `trials`. If 'default',
        use the class-wide `self.temporal_index_points`. If 'auto', auto-infer
        only if necessary.

    Returns:
      3-tuple of (input, temporal_index_points, observations).
      input: A Dict whose values correspond to parameters.
      temporal_index_points: 1-D array of temporal index points. If provided
        as input, then it's the same as the input value.  The return value is
        np.array([]) if there are no labels.
      observations: Dict of length equal to metrics, whose values
        have shape [len(trials), len(temporal_index_points)]. May contain NaNs.
    """
    timed_labels: list[TimedLabels] = self.timed_labels_extractor.convert(
        trials
    )

    if temporal_selection == 'default' or (temporal_selection == 'auto' and
                                           self.temporal_index_points.size):
      temporal_index_points = self.temporal_index_points
    elif temporal_selection == 'infer' or (temporal_selection == 'auto' and
                                           not self.temporal_index_points.size):
      tmp: set[float | int] = set()
      for tl in timed_labels:
        tmp.update(tl.times.flatten())
      temporal_index_points = sorted(tmp)  # [] when tmp is the empty set.
    else:
      raise ValueError('Invalid value for temporal_index_points: '
                       f'{temporal_selection}.')

    observations = self._to_temporal_observations(timed_labels,
                                                  temporal_index_points)
    inputs = self.to_features(trials)
    return inputs, np.asarray(temporal_index_points), observations


--- vizier/pyvizier/converters/spatio_temporal_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for spatio_temporal."""

import numpy as np
from vizier import pyvizier
from vizier.pyvizier.converters import core
from vizier.pyvizier.converters import spatio_temporal as st

from absl.testing import absltest

_metric_converters = [
    core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='y1', goal=pyvizier.ObjectiveMetricGoal.MAXIMIZE)),
    core.DefaultModelOutputConverter(
        pyvizier.MetricInformation(
            name='y2', goal=pyvizier.ObjectiveMetricGoal.MINIMIZE))
]

_trials = [
    pyvizier.Trial(
        id=1,
        parameters={'x1': pyvizier.ParameterValue(1)},
        measurements=[
            pyvizier.Measurement(
                steps=1, elapsed_secs=10, metrics={
                    'y1': 1,
                    'y2': -1
                }),
            pyvizier.Measurement(
                steps=2, elapsed_secs=20, metrics={
                    'y1': 3,
                    'y2': -3
                }),
            pyvizier.Measurement(
                steps=3, elapsed_secs=30, metrics={
                    'y1': 2,
                    'y2': -2
                })
        ]),
    pyvizier.Trial(
        id=2,
        parameters={'x1': pyvizier.ParameterValue(2)},
        measurements=[
            pyvizier.Measurement(
                steps=1, elapsed_secs=10, metrics={
                    'y1': -4,
                    'y2': 4
                }),
            pyvizier.Measurement(
                steps=5, elapsed_secs=50, metrics={
                    'y1': -6,
                    'y2': 6
                }),
            pyvizier.Measurement(
                steps=6, elapsed_secs=60, metrics={
                    'y1': -5,
                    'y2': 5
                })
        ])
]


class TimedLabelsExtractorTest(absltest.TestCase):

  def test_steps(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='raw')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].times,
        np.asarray([1, 2, 3], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].times,
        np.asarray([1, 5, 6], dtype=np.float32)[:, np.newaxis])

  def test_elapsed_secs(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'elapsed_secs', value_extraction='raw')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].times,
        np.asarray([10, 20, 30], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].times,
        np.asarray([10, 50, 60], dtype=np.float32)[:, np.newaxis])

  def test_index(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'index', value_extraction='raw')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].times,
        np.asarray([0, 1, 2], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].times,
        np.asarray([0, 1, 2], dtype=np.float32)[:, np.newaxis])

  def test_labels_raw(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'elapsed_secs', value_extraction='raw')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y1'],
        np.asarray([1, 3, 2], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y2'],
        np.asarray([-1, -3, -2], dtype=np.float32)[:, np.newaxis])

  def test_labels_cummax(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'elapsed_secs', value_extraction='cummax')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y1'],
        np.asarray([1, 3, 3], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y2'],
        np.asarray([-1, -3, -3], dtype=np.float32)[:, np.newaxis])

  def test_labels_strict_cummax_firstonly(self):
    converter = st.TimedLabelsExtractor([_metric_converters[0]],
                                        'elapsed_secs',
                                        value_extraction='cummax_firstonly')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].times,
        np.asarray([10, 20, 30], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y1'],
        np.asarray([1, 3, 3], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].times,
        np.asarray([10, 60], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].labels['y1'],
        np.asarray([-4, -4], dtype=np.float32)[:, np.newaxis])

  def test_labels_cummax_lastonly(self):
    converter = st.TimedLabelsExtractor([_metric_converters[0]],
                                        'elapsed_secs',
                                        value_extraction='cummax_lastonly')
    timed_labels = converter.convert(_trials)
    np.testing.assert_almost_equal(
        timed_labels[0].times,
        np.asarray([10, 30], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[0].labels['y1'],
        np.asarray([1, 3], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].times,
        np.asarray([60], dtype=np.float32)[:, np.newaxis])
    np.testing.assert_almost_equal(
        timed_labels[1].labels['y1'],
        np.asarray([-4], dtype=np.float32)[:, np.newaxis])

  def test_extract_all_timestamps(self):
    converter = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='cummax')

    all_ts = converter.extract_all_timestamps(_trials)
    np.testing.assert_almost_equal(
        all_ts, np.asarray([1, 2, 3, 5, 6], dtype=np.float32))


class SparseSpatioTemporalConverterTest(absltest.TestCase):

  def test_all(self):
    extractor = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='raw')
    converter = st.SparseSpatioTemporalConverter([], extractor)
    features, labels = converter.to_xy(_trials)

    np.testing.assert_equal(features, {
        'steps':
            np.array([[1.], [2.], [3.], [1.], [5.], [6.]], dtype=np.float32)
    })
    np.testing.assert_equal(
        labels, {
            'y1': [[1.], [3.], [2.], [-4.], [-6.], [-5.]],
            'y2': [[-1.], [-3.], [-2.], [4.], [6.], [5.]],
        })

    self.assertEqual(converter.features_shape, {'steps': (None, 1)})
    self.assertEqual(converter.labels_shape, {'y1': (None, 1), 'y2': (None, 1)})
    self.assertEqual(
        converter.output_specs, {
            'steps':
                core.NumpyArraySpec.from_parameter_config(
                    pyvizier.ParameterConfig.factory(
                        name='steps', bounds=(0.0, np.finfo(float).max)),
                    core.NumpyArraySpecType.default_factory)
        })


class DenseSpatioTemporalConverterTest(absltest.TestCase):

  def test_all(self):
    extractor = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='raw')
    converter = st.DenseSpatioTemporalConverter([],
                                                extractor,
                                                temporal_index_points=np.array(
                                                    [1., 2., 3., 5., 6.]))
    features, labels = converter.to_xy(_trials)

    np.testing.assert_equal(features, {})
    self.assertEqual(converter.features_shape, {})

    np.testing.assert_equal(
        labels, {
            'y1':
                np.array([[1., 3., 2., np.nan, np.nan],
                          [-4., np.nan, np.nan, -6., -5.]]),
            'y2':
                np.array([[-1., -3., -2., np.nan, np.nan],
                          [4., np.nan, np.nan, 6., 5.]])
        },
        err_msg=f'{labels}')

    self.assertEqual(converter.labels_shape, {'y1': (None, 5), 'y2': (None, 5)})

  def test_xty(self):
    extractor = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='raw')
    parameter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(name='x1', bounds=(0, 5)))
    converter = st.DenseSpatioTemporalConverter([parameter],
                                                extractor,
                                                temporal_index_points=np.array(
                                                    [1., 5., 6.]))
    features, temporal_index_points, labels = converter.to_xty(_trials, 'infer')

    np.testing.assert_equal(features, {'x1': [[1], [2]]})

    np.testing.assert_equal(temporal_index_points, [
        1.,
        2.,
        3.,
        5.,
        6.,
    ])

    np.testing.assert_equal(
        labels, {
            'y1':
                np.array([[1., 3., 2., np.nan, np.nan],
                          [-4., np.nan, np.nan, -6., -5.]]),
            'y2':
                np.array([[-1., -3., -2., np.nan, np.nan],
                          [4., np.nan, np.nan, 6., 5.]])
        },
        err_msg=f'{labels}')

  def test_xty_no_trials(self):
    """Tests that to_xty returns empty temporal_index_points for no trials."""
    trials = []
    extractor = st.TimedLabelsExtractor(
        _metric_converters, 'steps', value_extraction='raw'
    )
    parameter = core.DefaultModelInputConverter(
        pyvizier.ParameterConfig.factory(name='x1', bounds=(0, 5))
    )
    converter = st.DenseSpatioTemporalConverter(
        [parameter], extractor, temporal_index_points=None
    )
    _, temporal_index_points, _ = converter.to_xty(
        trials, temporal_selection='infer'
    )
    self.assertEqual(temporal_index_points.size, 0)


if __name__ == '__main__':
  absltest.main()


--- vizier/pyvizier/multimetric/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Import target for multimetric."""

from vizier._src.pyvizier.multimetric.hypervolume import ParetoFrontier
from vizier._src.pyvizier.multimetric.pareto_optimal import FastParetoOptimalAlgorithm
from vizier._src.pyvizier.multimetric.pareto_optimal import NaiveParetoOptimalAlgorithm
from vizier._src.pyvizier.multimetric.safety import SafetyChecker


--- vizier/pyvizier/multimetric/xla_pareto.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""XLA-enabled algorithm for determining pareto optimality."""
# pylint: disable=unused-import
from vizier._src.jax.xla_pareto import is_frontier
from vizier._src.jax.xla_pareto import jax_cum_hypervolume_origin
from vizier._src.jax.xla_pareto import JaxParetoOptimalAlgorithm
from vizier._src.jax.xla_pareto import pareto_rank


--- vizier/raytune/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Module run running tuners."""
from vizier._src.raytune import run_tune

from vizier._src.raytune.converters import ExperimenterConverter
from vizier._src.raytune.converters import SearchSpaceConverter


--- vizier/service/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""General modules used throughout the service."""
from vizier._src.service.constants import NO_ENDPOINT
from vizier._src.service.constants import SQL_LOCAL_URL
from vizier._src.service.constants import SQL_MEMORY_URL
from vizier._src.service.constants import VIZIER_DB_PATH


--- vizier/service/clients/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""OSS Client API."""

from vizier._src.service.clients import environment_variables
from vizier._src.service.clients import ResourceNotFoundError
from vizier._src.service.clients import Study
from vizier._src.service.clients import Trial
from vizier._src.service.clients import TrialIterable
from vizier._src.service.clients import UNUSED_CLIENT_ID


--- vizier/service/protos/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""All protos used in the service."""
from vizier._src.service import key_value_pb2
from vizier._src.service import pythia_service_pb2
from vizier._src.service import study_pb2
from vizier._src.service import vizier_oss_pb2
from vizier._src.service import vizier_service_pb2


--- vizier/service/pyvizier/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""PyVizier classes relevant for the service, including OSS-specific objects."""

# TODO: Re-evalaute what to expose to users, once closed.
# TODO: Alternatively, remove this import.
from vizier._src.pyvizier.oss import metadata_util
from vizier._src.pyvizier.oss.automated_stopping import AutomatedStoppingConfig
from vizier._src.pyvizier.oss.automated_stopping import AutomatedStoppingConfigProto
from vizier._src.pyvizier.oss.proto_converters import EarlyStopConverter
from vizier._src.pyvizier.oss.proto_converters import MeasurementConverter
from vizier._src.pyvizier.oss.proto_converters import MetadataDeltaConverter
from vizier._src.pyvizier.oss.proto_converters import MonotypeParameterSequence
from vizier._src.pyvizier.oss.proto_converters import ParameterConfigConverter
from vizier._src.pyvizier.oss.proto_converters import ParameterType
from vizier._src.pyvizier.oss.proto_converters import ParameterValueConverter
from vizier._src.pyvizier.oss.proto_converters import ProblemStatementConverter
from vizier._src.pyvizier.oss.proto_converters import ScaleType
from vizier._src.pyvizier.oss.proto_converters import StudyStateConverter
from vizier._src.pyvizier.oss.proto_converters import SuggestConverter
from vizier._src.pyvizier.oss.proto_converters import TrialConverter
from vizier._src.pyvizier.oss.proto_converters import TrialSuggestionConverter
from vizier._src.pyvizier.oss.study_config import Algorithm
from vizier._src.pyvizier.oss.study_config import ObservationNoise
from vizier._src.pyvizier.oss.study_config import ParameterValueSequence
from vizier._src.pyvizier.oss.study_config import StudyConfig
from vizier._src.pyvizier.pythia.study import StudyDescriptor
from vizier._src.pyvizier.pythia.study import StudyState
from vizier._src.pyvizier.pythia.study import StudyStateInfo
from vizier._src.pyvizier.shared.base_study_config import MetricInformation
from vizier._src.pyvizier.shared.base_study_config import MetricsConfig
from vizier._src.pyvizier.shared.base_study_config import ObjectiveMetricGoal
from vizier._src.pyvizier.shared.base_study_config import ProblemStatement
from vizier._src.pyvizier.shared.common import Metadata
from vizier._src.pyvizier.shared.common import MetadataValue
from vizier._src.pyvizier.shared.common import Namespace
from vizier._src.pyvizier.shared.parameter_config import ExternalType
from vizier._src.pyvizier.shared.parameter_config import ParameterConfig
from vizier._src.pyvizier.shared.parameter_config import SearchSpace
from vizier._src.pyvizier.shared.parameter_config import SearchSpaceSelector
from vizier._src.pyvizier.shared.study import ProblemAndTrials
from vizier._src.pyvizier.shared.trial import CompletedTrial
from vizier._src.pyvizier.shared.trial import CompletedTrialWithMeasurements
from vizier._src.pyvizier.shared.trial import Measurement
from vizier._src.pyvizier.shared.trial import MetadataDelta
from vizier._src.pyvizier.shared.trial import Metric
from vizier._src.pyvizier.shared.trial import NaNMetric
from vizier._src.pyvizier.shared.trial import ParameterDict
from vizier._src.pyvizier.shared.trial import ParameterValue
from vizier._src.pyvizier.shared.trial import PendingTrial
from vizier._src.pyvizier.shared.trial import PendingTrialWithMeasurements
from vizier._src.pyvizier.shared.trial import Trial
from vizier._src.pyvizier.shared.trial import TrialFilter
from vizier._src.pyvizier.shared.trial import TrialStatus
from vizier._src.pyvizier.shared.trial import TrialSuggestion


--- vizier/service/servers/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Different servers for the user."""
from vizier._src.service.vizier_server import DefaultVizierServer
from vizier._src.service.vizier_server import DistributedPythiaVizierServer


--- vizier/testing/__init__.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations



--- vizier/testing/numpy_assertions.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Useful assertions for testing."""
from typing import Any, Mapping

import numpy as np


def assert_arraytree_allclose(
    d1: Mapping[str, Any], d2: Mapping[str, Any], **kwargs
) -> None:
  """Assertion function for comparing two (nested) dictionaries."""
  np.testing.assert_equal(d1.keys(), d2.keys())

  for k, v in d1.items():
    if isinstance(v, dict):
      assert_arraytree_allclose(v, d2[k], **kwargs)
    else:
      try:
        np.testing.assert_allclose(v, d2[k], **kwargs)
      except TypeError as e:
        np.testing.assert_equal(
            v, d2[k], err_msg=f'Using assert_equal due to typeerror: {e}.'
        )


--- vizier/testing/test_studies.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Test study generator."""

from typing import List
import numpy as np
from vizier import pyvizier as vz


def flat_continuous_space_with_scaling() -> vz.SearchSpace:
  """Search space with float parameter types."""

  space = vz.SearchSpace()
  root = space.root
  root.add_float_param('lineardouble', -1., 2.)
  root.add_float_param('logdouble', 1e-4, 1e2, scale_type=vz.ScaleType.LOG)
  return space


def flat_categorical_space() -> vz.SearchSpace:
  """Search space with categorical parameter types."""

  space = vz.SearchSpace()
  root = space.root
  root.add_categorical_param('categorical_0', ['a', 'aa', 'aaa'])
  root.add_categorical_param('categorical_1', ['b', 'bb', 'bbb'])
  return space


def flat_boolean_space() -> vz.SearchSpace:
  """Search space with boolean parameter types, encoded as categorical."""

  space = vz.SearchSpace()
  root = space.root
  root.add_bool_param('bool_0')
  root.add_bool_param('bool_1')
  root.add_bool_param('bool_2')
  root.add_bool_param('bool_3')
  return space


def flat_continuous_space_with_scaling_trials(
    count: int = 1,
) -> list[vz.TrialSuggestion]:
  """Trials of search space with float parameter types."""
  trials = []
  for _ in range(count):
    trials.append(
        vz.Trial({
            'lineardouble': np.random.uniform(low=-1.0, high=2.0),
            'logdouble': np.random.uniform(low=1e-4, high=1e2),
        })
    )
  return trials


def flat_space_with_all_types() -> vz.SearchSpace:
  """Search space with all parameter types."""

  space = vz.SearchSpace()
  root = space.root
  root.add_float_param('lineardouble', -1., 2.)
  root.add_float_param('logdouble', 1e-4, 1e2, scale_type=vz.ScaleType.LOG)
  root.add_int_param('integer', -2, 2)
  root.add_categorical_param('categorical', ['a', 'aa', 'aaa'])
  root.add_bool_param('boolean')
  root.add_discrete_param('discrete_double', [-.5, 1.0, 1.2])
  root.add_discrete_param(
      'discrete_logdouble', [1e-5, 1e-2, 1e-1], scale_type=vz.ScaleType.LOG)
  root.add_discrete_param('discrete_int', [-1, 1, 2])

  return space


def flat_space_with_all_types_with_singletons() -> vz.SearchSpace:
  """Search space with all parameter types."""

  space = vz.SearchSpace()
  root = space.root
  root.add_float_param('double_singleton', min_value=1.0, max_value=1.0)
  root.add_int_param('integer_singleton', min_value=5, max_value=5)
  root.add_categorical_param('categorical_singleton', feasible_values=['a'])
  root.add_discrete_param('discrete_singleton', feasible_values=[3])
  root.add_float_param('double', min_value=0.0, max_value=5.0)
  root.add_int_param('integer', min_value=0, max_value=10)
  root.add_categorical_param(
      'categorical', feasible_values=['a', '1', 'b', '2']
  )
  root.add_discrete_param('discrete', feasible_values=[0.0, 0.6])
  return space


def conditional_automl_space() -> vz.SearchSpace:
  """Conditional space for a simple AutoML task."""
  space = vz.SearchSpace()
  root = space.select_root()
  root.add_categorical_param(
      'model_type', ['linear', 'dnn'], default_value='dnn'
  )

  dnn = root.select('model_type', ['dnn'])
  dnn.add_float_param(
      'learning_rate',
      0.0001,
      1.0,
      default_value=0.001,
      scale_type=vz.ScaleType.LOG,
  )

  linear = root.select('model_type', ['linear'])
  linear.add_float_param(
      'learning_rate', 0.1, 1.0, default_value=0.1, scale_type=vz.ScaleType.LOG
  )

  _ = dnn.add_categorical_param('optimizer_type', ['adam', 'evolution'])

  # Chained select() calls, path length of 1.
  root.select('model_type', ['dnn']).select(
      'optimizer_type', ['adam']
  ).add_float_param(
      'learning_rate', 0.1, 1.0, default_value=0.1, scale_type=vz.ScaleType.LOG
  )

  # Chained select() calls, path length of 2.
  ko = (
      root.select('model_type', ['dnn'])
      .select('optimizer_type', ['adam'])
      .add_bool_param('use_special_logic', default_value=False)
  )

  ko2 = ko.select_values(['True'])
  _ = ko2.add_float_param(
      'special_logic_parameter', 1.0, 3.0, default_value=2.1
  )
  return space


def metrics_objective_goals() -> List[vz.MetricInformation]:
  return [
      vz.MetricInformation('gain', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
      vz.MetricInformation('loss', goal=vz.ObjectiveMetricGoal.MINIMIZE),
      vz.MetricInformation(
          'auc',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          min_value=0.0,
          max_value=1.0),
      vz.MetricInformation(
          'crossentropy', goal=vz.ObjectiveMetricGoal.MINIMIZE, min_value=0.0),
  ]


def metrics_all_unconstrained() -> List[vz.MetricInformation]:
  return [
      vz.MetricInformation('gain', goal=vz.ObjectiveMetricGoal.MAXIMIZE),
      vz.MetricInformation('loss', goal=vz.ObjectiveMetricGoal.MINIMIZE),
      vz.MetricInformation(
          'gt2', goal=vz.ObjectiveMetricGoal.MAXIMIZE, safety_threshold=2.0),
      vz.MetricInformation(
          'lt2', goal=vz.ObjectiveMetricGoal.MINIMIZE, safety_threshold=2.0),
  ]


def metrics_all_constrained() -> List[vz.MetricInformation]:
  return [
      vz.MetricInformation(
          'auc',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          min_value=0.0,
          max_value=1.0),
      vz.MetricInformation(
          'crossentropy', goal=vz.ObjectiveMetricGoal.MINIMIZE, min_value=0.0),
      vz.MetricInformation(
          'gt2',
          goal=vz.ObjectiveMetricGoal.MAXIMIZE,
          safety_threshold=2.0,
          min_value=-1.0,
          max_value=5.0),
      vz.MetricInformation(
          'lt2',
          goal=vz.ObjectiveMetricGoal.MINIMIZE,
          safety_threshold=2.0,
          min_value=-1.0,
          max_value=5.0),
  ]


--- vizier/utils/attrs_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utils for attrs."""

import re
from typing import Any, Callable, Collection, Optional

import attr

_Validator = Callable[[Any, attr.Attribute, Any], None]


def assert_not_empty(instance: Any, attribute: attr.Attribute,
                     value: Any) -> None:
  if not value:
    raise ValueError(f'{attribute.name} must not be empty in {type(instance)}.')


def assert_not_negative(instance: Any, attribute: attr.Attribute,
                        value: int) -> None:
  if value < 0:
    raise ValueError(
        f'{attribute.name} must not be negative in {type(instance)}.')


def assert_not_none(instance: Any, attribute: attr.Attribute,
                    value: Any) -> None:
  if value is None:
    raise ValueError(f'{attribute.name} must not be None in {type(instance)}.')


def assert_between(
    low: float, high: float
) -> Callable[[Any, attr.Attribute, str], None]:
  def validator(instance, attribute, value):
    del instance
    if value < low or value > high:
      raise ValueError(
          f'{attribute.name} ({value}) must be between {low} and {high}'
      )

  return validator


def assert_re_fullmatch(
    regex: str) -> Callable[[Any, attr.Attribute, str], None]:

  def validator(instance: Any, attribute: attr.Attribute, value: str) -> None:
    if not re.fullmatch(regex, value):
      raise ValueError(
          f'{attribute.name} must match the regex {regex} in {type(instance)}.')

  return validator


def shape_equals(instance_to_shape: Callable[[Any], Collection[Optional[int]]]):
  """Creates a shape validator for attrs.

  For example, _shape_equals(lambda s : [3, None]) validates that the shape has
  length 2 and its first element is 3.

  Code Example:
  @attrs.define
  class TestAttr:
    x = attrs.field(validator=attrs_utils.shape_equals(lambda v: (3, v.d)))
    d = attrs.field()

  _TestAttr(np.zeros([3, 2]), 2)  # OK
  _TestAttr(np.zeros([3, 5]), None) # OK
  _TestAttr(np.zeros([3, 2]), 4)  # Raises ValueError

  Args:
    instance_to_shape: Takes instance as input and returns the desired shape for
      the instance. `None` is treated as "any number".

  Returns:
    A validator that can be passed into attrs.ib or attrs.field.
  """

  def validator(instance, attribute, value) -> None:
    shape = instance_to_shape(instance)

    def _validator_boolean():
      if len(value.shape) != len(shape):
        return False
      for s1, s2 in zip(value.shape, shape):
        if (s2 is not None) and (s1 != s2):
          return False
      return True

    if not _validator_boolean():
      raise ValueError(f'{attribute.name} has shape {value.shape} '
                       f'which does not match the expected shape {shape}')

  return validator


--- vizier/utils/attrs_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for attrs_utils."""

from typing import Any

import attrs
import numpy as np

from vizier.utils import attrs_utils
from absl.testing import absltest
from absl.testing import parameterized


class ValidatorsTest(parameterized.TestCase):

  @parameterized.parameters(
      (attrs_utils.assert_not_empty, [], False),
      (attrs_utils.assert_not_empty, [1], True),
      (attrs_utils.assert_not_negative, 0, -1),
      (attrs_utils.assert_not_negative, 0, True),
      (attrs_utils.assert_not_negative, 1, True),
      (attrs_utils.assert_not_none, None, False),
      (attrs_utils.assert_not_none, 0, True),
  )
  def test_validator(self, validator, value: Any, result: bool):
    self.assertValidatorWorksAsIntended(validator, value, result)

  def assertValidatorWorksAsIntended(self, validator, value: Any, result: bool):

    @attrs.define
    class Test:
      x = attrs.field(validator=validator)

    if result:
      Test(value)
    else:
      with self.assertRaises(ValueError):
        Test(value)

  @parameterized.parameters(
      (0.0, 1.0, 0.5, True),
      (0.0, 1.0, 1.0, True),
      (0.0, 1.0, 0.0, True),
      (0.0, 1.0, 5.0, False),
  )
  def test_between_validator(
      self, low: float, high: float, value: float, result: bool
  ):
    self.assertValidatorWorksAsIntended(
        attrs_utils.assert_between(low, high), value, result
    )

  @parameterized.parameters(
      (r'[^\/]+', 'good', True),
      (r'[^\/]+', 'bad/', False),
      (r'[^\/]+', '', False),
  )
  def test_regex_validator(self, regex: str, value: str, result: bool):
    self.assertValidatorWorksAsIntended(
        attrs_utils.assert_re_fullmatch(regex), value, result)

  def test_good_shape_none(self):
    _ShapeEqualsTestAttr(np.zeros([3, 5]), None)
    _ShapeEqualsTestAttr(np.zeros([3, 0]), None)

  def test_bad_shape(self):
    with self.assertRaises(ValueError):
      _ShapeEqualsTestAttr(np.zeros([3, 2]), 4)


@attrs.define
class _ShapeEqualsTestAttr:
  x = attrs.field(validator=attrs_utils.shape_equals(lambda v: (3, v.d)))
  d = attrs.field()


class ShapeEqualsTest(absltest.TestCase):

  def test_good_shape(self):
    _ShapeEqualsTestAttr(np.zeros([3, 2]), 2)

  def test_good_shape_none(self):
    _ShapeEqualsTestAttr(np.zeros([3, 5]), None)
    _ShapeEqualsTestAttr(np.zeros([3, 0]), None)

  def test_bad_shape(self):
    with self.assertRaises(ValueError):
      _ShapeEqualsTestAttr(np.zeros([3, 2]), 4)


if __name__ == '__main__':
  absltest.main()


--- vizier/utils/json_utils.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Json utils."""

from typing import Mapping
import json
from typing import Any

import numpy as np
from vizier import pyvizier as vz


class NumpyEncoder(json.JSONEncoder):
  """Example: json.dumps(np.array(...), cls=NumpyEncoder)."""

  def default(self, o: Any) -> Any:
    if not isinstance(o, dict) and isinstance(o, Mapping):
      # Handles FrozenDict and any dict-like structures.
      return dict(o)
    elif set(dir(o)).issuperset(set(['tolist', 'dtype', 'shape'])):
      # Any numpy.array-like objects.
      # Shape must be dumped and restored, in case that the array has shape
      # that includes 0-sized dimensions.
      return {
          'dtype': np.dtype(o.dtype).name,
          'value': o.tolist(),
          'shape': o.shape
      }
    else:
      return o


def numpy_hook(obj: Any) -> Any:
  """Example: json.loads(..., object_hook=numpy_hook)."""
  if 'dtype' not in obj:
    return obj
  if 'shape' not in obj:
    return obj
  return np.array(obj['value'], dtype=obj['dtype']).reshape(obj['shape'])


class NumpyDecoder(json.JSONDecoder):
  """Convert list to numpy if we see a list during json decoding.

  Example: json.loads(json_string, cls=NumpyDecoder).
  """

  def __init__(self, *args, **kargs):
    super(NumpyDecoder, self).__init__(object_hook=numpy_hook, *args, **kargs)


class MetadataEncoder(json.JSONEncoder):

  def default(self, o: Any) -> Any:
    if isinstance(o, vz.Metadata):
      d = dict(o)
      d |= {ns.encode(): o.ns(ns) for ns in o.subnamespaces()}
      return d
    elif not isinstance(o, dict) and isinstance(o, Mapping):
      # Handles FrozenDict and any dict-like structures.
      return dict(o)
    return o


--- vizier/utils/json_utils_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for json_utils."""

import json
import numpy as np

from vizier.utils import json_utils
from absl.testing import absltest
from absl.testing import parameterized


class JsonUtilsTest(parameterized.TestCase):

  @parameterized.parameters(
      dict(shape=[3, 0]),
      dict(shape=[3, 5]),
  )
  def test_dump_and_recover(self, shape):
    original = {'a': np.zeros(shape), 'b': 3}
    dumped = json.dumps(original, cls=json_utils.NumpyEncoder)
    loaded = json.loads(dumped, cls=json_utils.NumpyDecoder)
    np.testing.assert_array_equal(original['a'], loaded['a'])
    self.assertEqual(original['b'], loaded['b'])


if __name__ == '__main__':
  absltest.main()


--- vizier/utils/profiler.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Utility functions for performance tracking.

A typical use case is:

```
@jax.jit
@profiler.record_tracing
def _compute(self, x: jax.Array):
  return x + 1

class MyDesigner:

  @profiler.record_runtime
  def suggest(self) -> None:
    # An example of how to annotate an inline function call, so the function
    # name `_compute` is used. Note that the name must be specified.
    with profiler.timeit('compute'):
      _compute(jnp.array(1.))

with profiler.collect_events() as events:
  MyDesigner().suggest()
```

`events` is a list of `ProfileEvent`s. In this example, there are 3 events:
* Tracing of `_compute`, in scope `suggest::compute`
* Timing of `compute`, in scope `suggest`
* Timing of `suggest`, in empty scope.

`profiler.get_latencies_dict(events)` returns a dictionary whose values are
list of timedelta objects.

NOTE: @profiler.record_runtime and @profiler.record_tracing should appear
BEFORE the @jax.jit decorator, or only the first runtime will be recorded.
"""
import collections
import contextlib
import datetime
import enum
import functools
from typing import Any, Callable, Dict, Generator, List, Optional, Sequence

from absl import logging
import attrs
import jax


class EventType(enum.Enum):
  JIT_TRACING = 'tracing'
  TIMER = 'timer'


@attrs.define
class ProfileEvent:
  """An event logged by the profiler and stored in _Storage.

  Attributes:
    etype: type of the event.
    scope: context of the event.
    data: For JIT_TRACING event, it is either a singleton "function_name: str"
      OR a tuple of "function_name: str, args: tuple, kwargs: dict". For TIMER
      event, it's a tuple of "name: str" and "duration: timedelta".
  """

  etype: EventType = attrs.field()
  scope: str = attrs.field(converter='::'.join)
  data: Any = attrs.field()


@attrs.define
class _Storage:
  """Storage for function call runtimes.

  There is a single `_Storage` instance used globally. The global instance
  stays in "inactive" state until code enters a `collect_events()`
  contextmanager.

  Attributes:
    active: If not True, `add` method is a no-op.
    events:
    scope: Currently active "scope". Each time code enters
      `_enter_profile_scope` contextmanager, the scope name is appended to the
      list. Makes it easier to track where things happened.
  """

  active: bool = attrs.field(default=False)
  events: List[ProfileEvent] = attrs.field(factory=list)
  scope: list[str] = attrs.field(factory=list)

  def add(self, etype: EventType, data: Any) -> ProfileEvent:
    """Create and add a new event and return the added event.

    Args:
      etype: See ProfileEvent.
      data: See ProfileEvent.

    Returns:
      Newly added event.
    """
    event = ProfileEvent(etype, self.scope, data)
    if self.active:
      self.events.append(event)
    return event


_GLOBAL_SOTRAGE: _Storage = _Storage()


@contextlib.contextmanager
def _enter_profile_scope(scope: str):
  try:
    _GLOBAL_SOTRAGE.scope.append(scope)
    yield current_scope()
  finally:
    _GLOBAL_SOTRAGE.scope.pop()


def current_scope() -> str:
  return '::'.join(_GLOBAL_SOTRAGE.scope)


@contextlib.contextmanager
def collect_events() -> Generator[List[ProfileEvent], None, None]:
  """Context manager for turning on the profiler."""
  try:
    if _GLOBAL_SOTRAGE.active:
      raise RuntimeError(
          'There can be only one `collect_events()` context manager active at'
          ' the same time.'
      )
    _GLOBAL_SOTRAGE.active = True
    yield _GLOBAL_SOTRAGE.events
  finally:
    _GLOBAL_SOTRAGE.active = False
    # It's important to create a new instance here as opposed to resetting
    # the same object, so the yieled `events` persist outside the context.
    _GLOBAL_SOTRAGE.events = list()


@contextlib.contextmanager
def timeit(
    name: str, also_log: bool = False
) -> Generator[Callable[[], datetime.timedelta], None, None]:
  """Context manager for measuring the timing.

  Example:
  ```
  with timeit('scope_name') as duration:
    ...

  duration() # returns the duration.
  ```

  Also see: record_runtime, which is the decorator equivalent of this.

  Args:
    name:
    also_log: If True, also create a log.

  Yields:
    A callable with zero input arguments. Returns the elapsed time until call,
    or the end of the context, whichever comes earlier.
  """
  start = datetime.datetime.now()
  duration = None
  try:
    with _enter_profile_scope(name):

      def func() -> datetime.timedelta:
        if duration is None:
          return datetime.datetime.now() - start
        return duration

      yield func

  finally:
    duration = datetime.datetime.now() - start
    event = _GLOBAL_SOTRAGE.add(EventType.TIMER, (name, duration))
    if also_log:
      logging.info(
          'Timed %s: took %s seconds. Full event string: %s',
          name,
          duration.total_seconds(),
          event,
      )


def get_latencies_dict(
    events: Sequence[ProfileEvent],
) -> Dict[str, list[datetime.timedelta]]:
  d = collections.defaultdict(list)
  for e in events:
    if e.etype == EventType.TIMER:
      d[e.data[0]].append(e.data[1])
  return dict(d)


def record_runtime(
    func: Optional[Callable[..., Any]] = None,
    *,
    name_prefix: str = '',
    name: str = '',
    also_log: bool = False,
    block_until_ready: bool = False,
) -> Any:
  """Decorates the function to record the runtime.

  Also see: timeit(), which is the context manager equivalent of this.

  Args:
    func: Function being decorated.
    name_prefix: A prefix to add to the function name.
    name: The name to record. Defaults to func.__qualname__.
    also_log: Whether to also logging.info the runtime duration.
    block_until_ready: If True and running on an acceleartor, wait for the async
      dispatch results from this function, if any. See
      https://jax.readthedocs.io/en/latest/async_dispatch.html

  Returns:
    Decorated function, or decorator.
  """
  # This is required for the decorator to work both with and without
  # optional arguments.
  if func is None:
    return functools.partial(
        record_runtime,
        name_prefix=name_prefix,
        name=name,
        also_log=also_log,
        block_until_ready=block_until_ready,
    )
  name = name or func.__qualname__
  full_name = name
  if name_prefix:
    full_name = f'{name_prefix}.{name}'

  @functools.wraps(func)
  def wrapper(*args, **kwargs):
    """Calculates runtime of the given function."""
    with timeit(name=full_name, also_log=also_log):
      result = func(*args, **kwargs)
      if block_until_ready:
        result = jax.block_until_ready(result)
    return result
  return wrapper


@attrs.define
class _Config:
  """Configuration for this module."""

  _include_args_in_trace_records: bool = attrs.field(default=False)

  def include_args_in_trace_records(self, v: Optional[bool] = None) -> bool:
    """(Setter+getter): Whether to include input arguments in tracing events.

    NOTE: Use this feature for debugging only. If the function decorated with
    `@record_tracing` is not jitted, then arguments and kwargs will be actual
    Array(Tree)s instead of Tracers. The logs may include full values of the
    array which can greatly slow down your binary.

    Args:
      v: If None, leaves the behavior unchanged.

    Returns:
      New include_args_in_trace_records value.
    """
    if v is not None:
      self._include_args_in_trace_records = v
    return self._include_args_in_trace_records


config = _Config()


def record_tracing(
    func: Optional[Callable[..., Any]] = None,
    *,
    name: str = '',
    also_log: bool = True,
) -> Any:
  """Decorates the function to record the runtime of functions.

  Args:
    func: Function being decorated.
    name: The name to record. Defaults to func.__qualname__.
    also_log: Whether to also logging.info the runtime duration.

  Returns:
    Decorated function, or decorator.
  """
  # This is required for the decorator to work both with and without
  # optional arguments.
  if func is None:
    return functools.partial(
        record_tracing,
        name=name,
        also_log=also_log,
    )
  name = name or func.__qualname__

  @functools.wraps(func)
  def wrapper(*args, **kwargs):
    if config.include_args_in_trace_records():
      data = f'{name} with args={args} kwargs={kwargs}'
    else:
      data = name

    event = _GLOBAL_SOTRAGE.add(EventType.JIT_TRACING, data)
    if also_log:
      logging.info('Tracing %s. Full event string: %s', name, event)
    with _enter_profile_scope(name):
      result = func(*args, **kwargs)
    return result

  return wrapper


--- vizier/utils/profiler_test.py ---

# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

"""Tests for profiler."""

import equinox as eqx
import jax
import jax.numpy as jnp
from vizier.utils import profiler

from absl.testing import absltest


class MyModule(eqx.Module):

  @profiler.record_tracing
  def add_one(self, x: int) -> int:
    return x + 1


@profiler.record_tracing
def add_one(x):
  return x + 1


class PerformanceUtilsTest(absltest.TestCase):

  def setUp(self):
    super().setUp()
    jax.clear_caches()

  def test_eqx_jit(self):
    """`record_tracing` is tested on a bound method that is `filter_jit`-ed."""

    @profiler.record_runtime(name='return_two', also_log=True)
    def return_two():
      one = eqx.filter_jit(MyModule().add_one)(jnp.array(0.0))
      two = eqx.filter_jit(MyModule().add_one)(one)
      return two

    with profiler.collect_events() as events:
      return_two()
      return_two()

    # tracing is logged once, timer is logged twice.
    self.assertLen(events, 3)

    self.assertEqual(events[0].etype, profiler.EventType.JIT_TRACING)
    self.assertEqual(events[1].etype, profiler.EventType.TIMER)
    self.assertEqual(events[2].etype, profiler.EventType.TIMER)

    # Tracing happens within the timed functions' scope.
    self.assertIn(events[1].scope, events[0].scope)

    latencies = profiler.get_latencies_dict(events)
    self.assertIn('return_two', latencies)
    self.assertLen(latencies['return_two'], 2)

  def test_regular_jit(self):
    """`record_tracing` is tested on an unbound method."""

    @profiler.record_runtime(name='return_two', also_log=True)
    def return_two():
      one = jax.jit(add_one)(jnp.array(0.0))
      two = jax.jit(add_one)(one)
      return two

    with profiler.collect_events() as events:
      return_two()

    # tracing is logged once, timer is logged once.
    self.assertLen(events, 2)
    self.assertEqual(events[0].etype, profiler.EventType.JIT_TRACING)
    self.assertEqual(events[1].etype, profiler.EventType.TIMER)

    # Tracing happens within the timed functions' scope.
    self.assertIn(events[1].scope, events[0].scope)


if __name__ == '__main__':
  jax.config.update('jax_enable_x64', True)
  absltest.main()
